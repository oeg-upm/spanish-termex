Gestión de flujo de tráfico aéreo basado en agentes distribuidos Kagan Tumer Oregon State University 204 Rogers Hall Corvallis, OR 97331, EE. UU. Kagan.tumer@oregonstate.edu Adrian Agogino UCSC, Centro de Investigación de Ames NASA 269-3 Moffett Field, CA 94035, USA Adrian@Email.arc.nasa.gov Abstract Air Traffic Management es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) hoy. La FAA estima que solo en 2005, había más de 322,000 horas de retrasos a un costo para la industria por más de tres mil millones de dólares. Encontrar soluciones confiables y adaptativas al problema de gestión del flujo es de suma importancia si los sistemas de transporte aéreo de próxima generación logran lograr el objetivo establecido de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo, ya que requiere la integración y/o coordinación de muchos factores, incluidos: nuevos datos (por ejemplo, cambio de información climática), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y muyVolumen de tráfico pesado (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los EE. UU.). En este documento usamos Facet, un simulador de flujo de tráfico aéreo desarrollado en la NASA y utilizada ampliamente por la FAA y la industria, para probar un algoritmo de agente múltiple para la gestión del flujo de tráfico. Un agente está asociado con una solución (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que van a través de esa solución. Los agentes usan el aprendizaje de refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en facetas muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% sobre los agentes que reciben una recompensa global y hasta un 67% sobre un enfoque actual de la industria (estimación de Monte Carlo). Categorías y descriptores de sujetos I.2.11 [Metodologías de computación]: Sistemas de inteligencia artificiales de Sistemas Generales Algoritmos, rendimiento 1. Introducción La gestión eficiente, segura y confiable de nuestro tráfico aéreo cada vez mayor es uno de los desafíos fundamentales que enfrenta la industria aeroespacial actual. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los EE. UU. [14]. Para enrutar este tráfico aéreo de manera eficiente y de manera segura, el control actual del flujo del tráfico se basa en una estrategia de enrutamiento jerárquica centralizada que realiza proyecciones de flujo que van de una a seis horas. Como consecuencia, el sistema es lento para responder al desarrollo del clima o las condiciones del aeropuerto que conducen demoras locales potencialmente menores para cascade en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, representando 322,272 horas de retrasos. Se estimó que el costo total de estos retrasos superaría los tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionar a ninguno de ellos medios para dar forma a los patrones de tráfico más allá de las redes menores. La iniciativa de los sistemas de transporte aéreo de próxima generación (NGATS) tiene como objetivo abordar estos problemas y, no solo representar un triple aumento del tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el tráfico cada vez mayor es absorbido en cierta medida por un hardware mejorado (por ejemplo, más servidores con recuerdos más grandes y CPU más rápidas para el enrutamiento de Internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, como la infraestructura (por ejemplo, número de númerode los aeropuertos) no cambiarán significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar soluciones nuevas, distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y de múltiples agentes es un ajuste ideal para este problema de distribución naturalmente donde la interacción compleja entre los aviones, aeropuertos y controladores de tráfico provoca una solución centralizada predeterminada severamente subóptima en la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptativa (por ejemplo, el vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimización del flujo, también proporciona la desviación más radical del sistema actual. Como consecuencia, un cambio a dicho sistema presenta tremendas dificultades tanto en términos de implementación (por ejemplo, programación y capacidad del aeropuerto) como consolentes políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este documento, nos centramos en el sistema basado en agentes que se pueden implementar fácilmente. En este enfoque, asignamos un 342 978-81-904262-7-5 (RPS) C 2007 IFAAMAS Agent a una solución, una ubicación específica en 2D. Debido a que los planes de vuelo de la aeronave consisten en una secuencia de correcciones, esta representación permite que las soluciones (o agentes) localizadas tengan un impacto directo en el flujo del tráfico aéreo1. En este enfoque, las acciones de los agentes deben establecer la separación que se requiere que los aviones que se acerquen a mantener. Este par simple de acción de agente permite a los agentes reducir la velocidad o acelerar el tráfico local y permite a los agentes tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje de refuerzo (RL) [15]. En un enfoque de aprendizaje de refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agentes diferentes, y las comparamos con simular varios cambios en el sistema y seleccionar la mejor solución (por ejemplo, equivalente a una búsqueda de Monte-Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Las tres recompensas personalizadas tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio cayó en una de las dos categorías distintas: los primeros enfoques de modelado basados en principios utilizados por los expertos en dominios [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 9, 9, 9, 9, 9,12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha mediante el uso de facetas para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, más de 40 organizaciones y 5000 usuarios) al trabajar en la primera categoría [4, 11]. La principal contribución de este documento es presentar un algoritmo de gestión de flujo de tráfico aéreo adaptado distribuido que se pueda implementar fácilmente y probar ese algoritmo usando faceta. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, faceta. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discuten las propiedades de escala de las diferentes recompensas de agentes y discuten el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos y mapeamos el trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico por triples.2. Gestión del flujo de tráfico aéreo Con más de 40,000 vuelos que operan dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo existen preocupaciones para la eficiencia del sistema, sino también para la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, desarrollar patrones climáticos), confiabilidad y seguridad (por ejemplo, gestión del aeropuerto). Para abordar tales problemas, la gestión de este flujo de tráfico ocurre en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2-30 minutos);1 Discutimos cómo los planes de vuelo con pocas correcciones se pueden manejar con más detalle en la Sección 2. 2. Flujo regional (20 minutos a 2 horas);3. Flujo nacional (1-8 horas);y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las directrices estrictas y las preocupaciones de seguridad que rodean la separación de la aeronave, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos centraremos en los problemas regionales y nacionales de gestión del flujo, restringiendo nuestro impacto en las decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo por parte de la FAA y las decisiones a muy corto plazo de los controladores de tráfico aéreo. El espacio aéreo continental de EE. UU. Consta de 20 centros regionales (que manejan 200-300 vuelos en un día determinado) y 830 sectores (manejo de 10-40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, explicar la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar cambios en las políticas causadas por los patrones climáticos. Dos de los problemas fundamentales para abordar el problema de flujo son: (i) modelar y simular un sistema complejo tan grande como la fidelidad requerida para proporcionar resultados confiables es difícil de lograr;y (ii) establecer el método por el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede conducir a desigualdades hacia regiones o entidades comerciales particulares. A continuación, discutimos cómo abordamos ambos temas, a saber, presentamos a la faceta una herramienta de simulación ampliamente utilizada y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla que muestra rutas de tráfico y estadísticas de flujo de aire.2.1 Facet Facet (Future ATM Concepts Evaluation Tool), se desarrolló un modelo basado en física del espacio aéreo estadounidense para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar las trayectorias de los vuelos propuestos hacia adelante en el tiempo. La faceta se puede usar para simular y mostrar el tráfico aéreo (una porción de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora RAM de 3 GHz y 1 GB) o proporcionar estadísticas rápidas sobre datos registrados (trayectorias 4D para 10,000 vuelos, incluidos sectores,aeropuertos y arreglar estadísticas en 10 segundos en la misma computadora) [11]. La faceta es ampliamente utilizada por el sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 343 La FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. La faceta simula el tráfico aéreo en función de los planes de vuelo y a través de una interfaz gráfica de usuario permite al usuario analizar patrones de congestión de diferentes sectores y centros (Figura 1). La faceta también permite al usuario cambiar los patrones de flujo de la aeronave a través de una serie de mecanismos, incluida la medición de aviones a través de las correcciones. El usuario puede observar los efectos de estos cambios a la congestión. En este documento, los agentes usan la faceta directamente a través del modo por lotes, donde los agentes envían scripts a faceta pidiéndole que simule el tráfico aéreo en función de las órdenes de medición impuestas por los agentes. Luego, los agentes producen sus recompensas en función de recibir comentarios de la faceta sobre el impacto de estos medidores.2.2 Evaluación del sistema La función de evaluación del rendimiento del sistema Seleccionamos enfoques en el retraso y la congestión, pero no tiene en cuenta el impacto de la equidad en diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector en particular y en la cantidad de retraso de tráfico aéreo medido. La combinación lineal de estos dos términos proporciona la función completa de evaluación del sistema, G (z) en función del estado completo del sistema z. Más precisamente, tenemos: G (Z) = - ((1 - α) B (Z) + αC (Z)), (1) donde B (Z) es la penalización de retraso total para todos los aviones en el sistema, yC (z) es la penalización total de congestión. La importancia relativa de estas dos sanciones está determinada por el valor de α, y exploramos varias compensaciones basadas en α en la Sección 4. El retraso total, b, es una suma de retrasos sobre un conjunto de sectores S y viene dado por: b (z) = x s∈S BS (z) (2) donde bs (z) = x t θ (t -−τs) kt, s (t - τs), (3) donde ks, t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y θ (·) es la función paso que iguala 1 cuandoSu argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. Intuitivamente, BS (z) proporciona el número total de aviones que permanecen en un sector más allá de un tiempo predeterminado τs, y escala su contribución para contar por la cantidad por la cual llegan tarde. De esta manera, BS (z) proporciona un factor de retraso que no solo explica todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de los aviones deberían haber alcanzado el sector por tiempo τs y que los aviones que llegan después de este tiempo llegan tarde. En este documento, el valor de τs se determina evaluando los recuentos de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. Del mismo modo, la penalización de congestión total es una suma sobre las penalizaciones de congestión sobre los sectores de observación, s: c (z) = x s∈S cs (z) (4) donde cs (z) = a x t θ (ks,T - Cs) EB (KS, T - CS), (5) donde A y B están normalizando las constantes, y CS es la capacidad del sector S según lo definido por la FAA. Intuitivamente, CS (Z) penaliza un estado del sistema donde el número de aviones en un sector excede la capacidad del sector oficial de FAA. Cada capacidad del sector se calcula utilizando varias métricas que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial está destinada a proporcionar una fuerte retroalimentación para devolver el número de aviones en un sector por debajo de las capacidades obligatorias de la FAA.3. Flujo de tráfico aéreo basado en agentes El enfoque de múltiples agentes para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección de agentes, selección del conjunto de acciones del agente, selección de algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente.3.1 La selección de agentes seleccionar la aeronave como agentes es quizás la opción más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumentar o disminuir la velocidad y la altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aviones en un día determinado, lo que lleva a un sistema de agente múltiple masivamente grande. En segundo lugar, como los agentes no podrían probar su espacio estatal lo suficiente, el aprendizaje sería prohibitivamente lento. Como alternativa, asignamos agentes a ubicaciones de tierra individuales en todo el espacio aéreo llamado correcciones. Cada agente es responsable de cualquier avión que revisa su solución. Las correcciones ofrecen muchas ventajas como agentes: 1. Su número puede variar según la necesidad. El sistema puede tener tantos agentes como sea necesario para una situación dada (por ejemplo, los agentes que vienen viven alrededor de un área con condiciones climáticas en desarrollo).2. Debido a que las correcciones son estacionarias, la recopilación de datos y el comportamiento coincidente para recompensar es más fácil.3. Debido a que los planes de vuelo de la aeronave consisten en soluciones, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico.4. Se pueden implementar dentro de los procedimientos actuales de enrutamiento de tráfico aéreo y pueden usarse como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agente. Los agentes que rodean una congestión o condición climática afectan el flujo de tráfico para reducir la carga de regiones particulares.3.2 Acciones del agente El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una vez más, una opción obvia puede ser que las soluciones oferten en aviones, afectando sus planes de vuelo. Aunque apele desde una perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo no sea confiable y complican significativamente el problema de programación (por ejemplo, llegada a los aeropuertos y el proceso de asignación de puerta posterior). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre la aeronave) que los aviones tienen 344 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) para mantener, al pasar por la solución de los agentes. Esto se conoce como establecer las millas en el sendero o el MIT. Cuando un agente establece el valor del MIT en D, se instruye a los aviones que se dirigen hacia su solución para que se alineen y mantengan m millas de separación (aunque las aeronaves siempre mantendrán una distancia segura entre sí, independientemente del valor de D). Cuando hay muchos aviones que pasan por una solución, el efecto de emitir valores más altos del MIT es ralentizar la tasa de aeronaves que pasan por la solución. Al aumentar el valor de D, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su solución, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones que rodean una posible congestión se vuelven en vivo y comienzan a establecer nuevos tiempos de separación.3.3 Agente Aprendiendo el objetivo de cada agente es aprender los mejores valores de D que conducirán al mejor rendimiento del sistema, G. En este documento, suponemos que cada agente tendrá una función de recompensa y tendrá como objetivo maximizar su recompensa utilizando su propiaEl aprendizaje de refuerzo [15] (aunque alternativas como los neurocontroladores en evolución también son efectivas [1]). Para problemas complejos de recompensa retrasada, pueden tener que usarse sistemas de aprendizaje de refuerzo relativamente sofisticados, como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y un conjunto de acciones de agente, el dominio de congestión de tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se usa un simple aprendizaje de refuerzo de recompensa inmediata basado en la tabla. Nuestro alumno de refuerzo es equivalente a un Greeded Q -Learner con una tasa de descuento de 0 [15]. En cada episodio, un agente toma una acción y luego recibe una recompensa que evalúa esa acción. Después de tomar medidas A y recibir recompensa, un agente actualiza su tabla Q (que contiene su estimación del valor para tomar esa acción [15]) de la siguiente manera: Q (a) = (1 - L) Q (a) + L (R), (6) donde l es la tasa de aprendizaje. En cada tiempo, el agente elige la acción con el valor de tabla más alto con la probabilidad 1, y elige una acción aleatoria con probabilidad. En los experimentos descritos en este documento, α es igual a 0.5 y es igual a 0.25. Los parámetros se eligieron experimentalmente, aunque el rendimiento del sistema no era demasiado sensible a estos parámetros.3.4 Estructura de recompensa del agente El problema final que debe abordarse es seleccionar la estructura de recompensas para los agentes de aprendizaje. El primer y más directo enfoque es permitir que cada agente reciba el rendimiento del sistema como su recompensa. Sin embargo, en muchos dominios, tal estructura de recompensa conduce a un aprendizaje lento. Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente. Dado que los agentes apuntan a maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas de agentes, o recompensas que, cuando los siguen los agentes, conducen a un buen rendimiento general del sistema. En este trabajo nos centramos en las recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18].3.4.1 Las recompensas de diferencia consideran las recompensas de diferencia de la forma [2, 17, 18]: di ≡ g (z) - g (z - zi + ci), (7) donde zi es la acción del agente i. Todos los componentes de Z afectados por el Agente I son reemplazados por el CI constante fijo 2. En muchas situaciones es posible usar un CI que sea equivalente a sacar al agente I del sistema. Intuitivamente esto causa el segundo término de la recompensa de la diferencia para evaluar el rendimiento del sistema sin I y, por lo tanto, d evalúa la contribución de los agentes al rendimiento del sistema. Hay dos ventajas en el uso de D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio se ha denominado la capacidad de aprendizaje (los agentes tienen más fácilaprendizaje de tiempo) en trabajos anteriores [2, 17]. Segundo, debido a que el segundo término no depende de las acciones del Agente I, cualquier acción del Agente I que mejore D, también mejora G. Este término que mide la cantidad de alineación entre dos recompensas se ha denominado factorización en trabajos anteriores [2,17].3.4.2 Estimaciones de recompensas de diferencia Sin embargo, proporcionar un buen compromiso entre el objetivo del rendimiento del sistema y eliminar el impacto de otros agentes de una recompensa de los agentes, un problema que puede plagar D es el costo computacional. Debido a que se basa en el cálculo del término contrafactual G (z - zi + ci) (es decir, el rendimiento del sistema sin agente i) puede ser difícil o imposible de calcular, particularmente cuando no se conoce la forma matemática exacta de G. Centrémonos en las funciones G en la siguiente forma: g (z) = gf (f (z)), (8) donde gf () no es lineal con una forma funcional conocida y, f (z) = x i fi F(Zi), (9) donde cada FI es una función no lineal desconocida. Suponemos que podemos probar valores de f (z), lo que nos permite calcular G, pero que no podemos probar de cada FI (Zi).2 Esta notación utiliza el relleno cero y la adición de vectores en lugar de la concatenación para formar vectores de estado completos de vectores de estado parciales. El vector Zi en nuestra notación sería ZIEI en la notación vectorial estándar, donde EI es un vector con un valor de 1 en el componente ésano y cero en cualquier otro lugar. El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 345 Además, suponemos que GF es mucho más fácil de calcular que F (Z), o que es posible que incluso no podamos calcular F (Z) directamente y debemos probarlode un cálculo de caja negra. Esta forma de G coincide con la evaluación de nuestro sistema en el dominio del tráfico aéreo. Cuando organizamos agentes para que cada aeronave solo se ve afectado por un solo agente, cada agente impacto en los recuentos del número de aviones en un sector, KT, S, será principalmente independiente de los otros agentes. Estos valores de KT, S son los f (z) s en nuestra formulación y las funciones de penalización forman GF. Tenga en cuenta que dados los recuentos de aeronaves, las funciones de penalización (GF) se pueden calcular fácilmente en microsegundos, mientras que los recuentos de aeronaves (F) solo se pueden calcular ejecutando facetas tomando el orden de segundos. Para calcular nuestro contrafactual G (Z - Zi + Ci) necesitamos calcular: GF (F (Z - Zi + Ci)) = GF 0 @ X J = I FJ (ZJ) + Fi (CI) 1 A (10)= GF (F (Z) - Fi (Zi) + Fi (CI)).(11) Desafortunadamente, no podemos calcular esto directamente ya que se desconocen los valores de Fi (Zi). Sin embargo, si los agentes toman acciones de forma independiente (no observa cómo actúan otros agentes antes de tomar su propia acción) podemos aprovechar la forma lineal de F (Z) en el FIS con la siguiente igualdad: E (F - I (Z (z−i) | zi) = e (f - i (z - i) | ci) (12) donde e (f - i (z - i) | zi) es el valor esperado de todas las fs que no se dan FI dadaEl valor de Zi y E (F - I (Z - I) | Ci) es el valor esperado de todas las FS que no sean FI dado el valor de Zi se cambia a CI. Luego podemos estimar f (z - zi + ci): f (z) - fi (zi) + fi (ci) = f (z) - fi (zi) + fi (ci) + e (f - i (z−i) | ci) - e (f - i (z - i) | zi) = f (z) - e (fi (zi) | zi) + e (fi (ci) | ci) + e (f−i (z - i) | ci) - e (f - i (z - i) | zi) = f (z) - e (f (z) | zi) + e (f (z) | ci). Por lo tanto, podemos evaluar di = g (z) - g (z - zi + ci) como: dest1 i = gf (f (z)) - gf (f (z) - e (f (z) | zi) + e(f (z) | ci)), (13) dejándonos con la tarea de estimar los valores de e (f (z) | zi) y e (f (z) | ci)). Estas estimaciones se pueden calcular manteniendo una tabla de promedios donde promediamos los valores de la F (z) observada para cada valor de Zi que hemos visto. Esta estimación debería mejorar a medida que aumenta el número de muestras. Para mejorar nuestras estimaciones, podemos establecer CI = E (z) y si hacemos la aproximación media cuadrada de F (E (Z)) ≈ E (F (Z)), entonces podemos estimar G (Z) - G (Z− Zi + Ci) AS: Dest2 I = GF (F (Z)) - Gf (F (Z) - E (F (Z) | Zi) + E (F (Z)))).(14) Esta formulación tiene la ventaja en que tenemos más muestras a nuestra disposición para estimar E (F (z)) que nosotros para estimar E (F (Z) | Ci)).4. Los resultados de la simulación en este documento probamos el rendimiento de nuestro método de optimización de tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo facetado. En todos los experimentos probamos el rendimiento de cinco métodos diferentes. El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias, con la mejor política que se elige. Los otros cuatro métodos son métodos basados en agentes donde los agentes maximizan una de las siguientes recompensas: 1. La recompensa del sistema, g (z), como se define en la ecuación 1. 2. La diferencia de la diferencia, DI (z), suponiendo que los agentes puedan calcular contrafactuales.3. Estimación de la recompensa de diferencia, Dest1 I (Z), donde los agentes estiman el contrafactual usando E (F (Z) | Zi) y E (F (Z) | Ci).4. Estimación de la recompensa de diferencia, Dest2 I (Z), donde los agentes estiman el contrafactual usando E (F (Z) | Zi) y E (F (Z)). Estos métodos se prueban primero en un dominio de tráfico aéreo con 300 aviones, donde 200 de la aeronave están pasando por un solo punto de congestión durante una simulación de cuatro horas. Los agentes son responsables de reducir la congestión en este punto único, mientras intentan minimizar el retraso. Luego, los métodos se prueban en un problema más difícil, donde se agrega un segundo punto de congestión con los 100 aviones restantes que pasan por este segundo punto de congestión. En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G (z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos. Estos valores de τ se obtienen examinando el tiempo en el que la mayoría de las aeronaves salen de los sectores, cuando no se realiza el control de congestión. Excepto cuando se indique, la compensación entre la congestión y la tardanza, α se establece en 0.5. En todos los experimentos para hacer que los resultados del agente sean comparables a la estimación de Monte Carlo, las mejores políticas elegidas por los agentes se utilizan en los resultados. Todos los resultados son un promedio de treinta ensayos independientes con las diferencias en la media (σ/ √ n) que se muestra como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver. Figura 3: rendimiento en un solo problema de congestión, con 300 aviones, 20 agentes y α = .5.4.1 Congestión única En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un solo punto de congestión, con veinte agentes. Este punto de congestión se crea estableciendo una serie de planes de vuelo que causan el número de aviones en 346 el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), el sector de interés es significativamente más que el número permitido por la FAA. Los resultados que se muestran en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistema diferentes. En ambos casos, los métodos basados en agente superan significativamente el método Monte Carlo. Este resultado no es sorprendente ya que los métodos basados en agentes exploran inteligentemente su espacio, donde el método Monte Carlo explora el espacio al azar. Figura 4: rendimiento en un solo problema de congestión, con 300 aviones, 20 agentes y α = .75. Entre los métodos basados en agentes, los agentes que usan las recompensas de diferencia funcionan mejor que los agentes que usan la recompensa del sistema. Una vez más, esto no es sorprendente, ya que con veinte agentes, un agente que intenta maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa. Incluso si un agente toma una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar medidas que aumentan la congestión y la tardanza, lo que hace que el agente crea erróneamente que su acción fue pobre. En contraste, los agentes que usan la recompensa de la diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa. Este experimento también muestra que estimar la recompensa de la diferencia no solo es posible, sino también bastante efectivo, cuando no se puede calcular el verdadero valor de la recompensa de la diferencia. Si bien los agentes que usan las estimaciones no logran resultados tan altos como los agentes que usan la verdadera recompensa de diferencia, todavía tienen un rendimiento significativamente mejor que los agentes que usan la recompensa del sistema. Sin embargo, tenga en cuenta que el beneficio de las recompensas de diferencia estimadas solo está presente más adelante en el aprendizaje. Anteriormente en el aprendizaje, las estimaciones son pobres, y los agentes que usan las recompensas de diferencia estimada no tienen mejor que los agentes que usan la recompensa del sistema.4.2 Dos congestiones en el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión. En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se agrega en una parte diferente del país. La segunda congestión es menos severa que la primera, por lo que los agentes tienen que formar diferentes políticas dependiendo de qué punto de congestión están influyendo. Figura 5: rendimiento en dos problemas de congestión, con 300 aviones, 20 agentes y α = .5. Figura 6: rendimiento en dos problemas de congestión, con 300 aviones, 50 agentes y α = .5. Los resultados que se muestran en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión única. Una vez más, los métodos basados en agentes funcionan mejor que el método Monte Carlo y los agentes que usan recompensas de diferencia funcionan mejor que los agentes que usan la recompensa del sistema. Para verificar que la mejora del rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes. Los resultados que se muestran en la Figura 6 muestran que, de hecho, los rendimientos relativos de los métodos son comparables cuando el número de agentes aumenta a 50. La Figura 7 muestra los resultados de escala y demuestra que las conclusiones se mantienen en un amplio rango de número de agentes. Los agentes que usan Dest2 funcionan ligeramente mejor que los agentes que usan Dest1 en todos los casos pero para 50 agentes. Esta ligera ventaja proviene de Dest2 que proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos.4.3 Comercio de penalización La función de evaluación del sistema utilizada en los experimentos es G (Z) = - ((1 - α) D (Z)+αC (Z)), que comprende sanciones tanto para la congestión como para la tardanza. Esta función de evaluación es la sexta intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 347 Figura 7: Impacto del número de agentes en el rendimiento del sistema. Dos problemas de congestión, con 300 aviones y α = .5.obliga a los agentes a intercambiar estas sanciones relativas dependiendo del valor de α. Con una alta α, la optimización se centra en reducir la congestión, mientras que con bajo α el sistema se centra en reducir la tardanza. Para verificar que los resultados obtenidos anteriormente no son específicos de un valor particular de α, repitimos el experimento con 20 agentes para α = .75. La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos sigue siendo el mismo. A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0. La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • Primero, hay una solución de penalización de congestión cero. Esta solución hace que los agentes impongan grandes valores del MIT para bloquear todo el tráfico aéreo, lo que parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos. Todos los algoritmos encuentran esta solución, aunque es de poco interés en la práctica debido a los grandes retrasos que causaría.• En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales. Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes. Tenga en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una opción razonable para probar los algoritmos en un entorno difícil.• Tercero, Monte Carlo y G son particularmente pobres para manejar múltiples objetivos. Para ambos algoritmos, el rendimiento se degrada significativamente para los rangos medios de α.4.4 Costo computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios. Esos resultados muestran que D es significativamente superior a los otros algoritmos. Sin embargo, una pregunta que surge es qué gastos generales computacionales D pone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D se pone a disposición de los otros algoritmos. El costo de cálculo de la evaluación del sistema, G (Ecuación 1) depende casi por completo del cálculo de la Figura 8: rendimiento en dos problemas de congestión, con 300 aviones, 20 agentes y α = .75. Figura 9: Comercio entre objetivos en dos problemas de congestión, con 300 aviones y 20 agentes. Tenga en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos.El avión cuenta para los sectores KT, S, que deben calcularse utilizando faceta. Excepto cuando se usa D, los valores de K se calculan una vez por episodio. Sin embargo, para calcular el término contrafactual en D, si la faceta se trata como un cuadro negro, cada agente tendría que calcular sus propios valores de K para su contrafactual, lo que resulta en cálculos n + 1 de k por episodio. Si bien es posible racionalizar el cálculo de D con cierto conocimiento de las partes internas de la faceta, dada la complejidad de la simulación de faceta, no es irrazonable en este caso tratarlo como una caja negra. La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 g de cálculos para cada uno de los algoritmos para las simulaciones presentadas en la Figura 5 donde había 20 agentes, 2 congestiones y α = .5. Todos los algoritmos, excepto el D completamente calculado, alcanzan los cálculos de 2100 K en el paso de tiempo 2100. D Sin embargo, calcula K una vez para el sistema, y luego una vez para cada agente, lo que lleva a 21 cálculos por paso de tiempo. Por lo tanto, alcanza los 2100 cálculos en el paso de tiempo 100. También mostramos los resultados del cálculo D completo en T = 2100, que necesita 44100 cálculos de K como D44K.348 El sexto intl. Conf.en agentes autónomos y sistemas de agentes múltiples (AAMAS 07) Tabla 1: rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 g de evaluaciones (excepto D44K que tiene 44100 g de evaluaciones en T = 2100). Recompensa G σ/ √ n Tiempo Dest2 -232.5 7.55 2100 Dest1 -234.4 6.83 2100 D -277.0 7.8 100 D44K -219.9 4.48 2100 G -412.6 13.6 2100 MC -639.0 16.4 2100 Aunque D44K proporciona el mejor resultado por un ligero margen, es es un margen, es, es es un margen, es es un margen, es, es es un margen, es es un margen, es, es es un margen, es es un margen, es, es es un margen, es es un margen.logrado a un costo computacional considerable. De hecho, el rendimiento de las dos estimaciones D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k.Además, las dos estimaciones D, superan significativamente el cálculo D completo para un número dado de cálculos de K y validan los supuestos hechos en la Sección 3.4.2. Esto muestra que para este dominio, en la práctica, es más fructífero realizar más pasos de aprendizaje y aproximarse a D, que pocos pasos de aprendizaje con computación D completa cuando tratamos la faceta como una caja negra.5. Discusión La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo, que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año. La principal contribución de este documento es presentar un algoritmo de gestión de flujo de tráfico aéreo adaptado distribuido que se puede implementar fácilmente y probar ese algoritmo utilizando faceta, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria. Nuestro método se basa en agentes que representan correcciones y que cada agente determina la separación entre la aeronave que se acerca a su solución. Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, se puede implementar fácilmente. Los agentes usan el aprendizaje de refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de agentes y diferentes formas de estimar esas funciones. Actualmente estamos extendiendo este trabajo en tres direcciones. Primero, estamos explorando nuevos métodos para estimar las recompensas de agentes, para acelerar aún más las simulaciones. En segundo lugar, estamos investigando estrategias de implementación y buscando modificaciones que tengan un mayor impacto. Una de esas modificaciones es extender la definición de agentes desde correcciones hasta sectores, otorgando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes para eliminar la congestión. Finalmente, en cooperación con expertos en dominios, estamos investigando diferentes funciones de evaluación del sistema, más allá de la g dependiente de retraso y congestión presentadas en este documento. Agradecimientos: Los autores agradecen a Banavar Sridhar por su invaluable ayuda para describir tanto la gestión actual del flujo de tráfico aéreo como los ngats, y Shon Grabbe por sus tutoriales detallados sobre la faceta.6. Referencias [1] A. Agogino y K. Tumer. Funciones de evaluación eficientes para sistemas de múltiples grifos. En la Conferencia de Computación Genética y Evolutiva, Páginas 1-12, Seatle, WA, junio de 2004. [2] A. Agogino y K. Tumer. Análisis de recompensas de agentes múltiples para el aprendizaje en dominios ruidosos. En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas de Multi-Agentes, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer. Manejo de restricciones de comunicación y formación del equipo en juegos de congestión. Revista de Agentes Autónomos y Sistemas de Agentes Multi, 13 (1): 97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterjee, K. S. Shethand y S. R. Grabbe. Faceta: herramienta de evaluación de conceptos de cajeros automáticos futuros. Control de tráfico aéreo Quarterly, 9 (1), 2001. [5] Karl D. Bilimoria. Un enfoque de optimización geométrica para la resolución de conflictos de aeronaves. En AIAA Guidance, Navegación y Control Conf, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III. Garantía de separación de vuelo gratuita utilizando algoritmos distribuidos. En Proc de Aeroespace Conf, 1999, Aspen, CO, 1999. [7] Datos de Opsnet de la FAA en Jan-dic 2005. Sitio web del Departamento de Transporte de los Estados Unidos.[8] S. Grabbe y B. Sridhar. Enrutamiento de vuelo del Central East Pacific. En la Conferencia y Anexo y Exposición de la guía, navegación y control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling. Un enfoque cooperativo de múltiples agentes para el vuelo libre. En AAMAS 05: Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagentes, páginas 1083-1090, Nueva York, NY, EE. UU., 2005. ACM Press.[10] P. K. Menon, G. D. Sweriduk y B. Sridhar. Estrategias óptimas para vuelo gratuito Resolución de conflictos de tráfico aéreo. Journal of Guidance, Control y Dynamics, 22 (2): 202-211, 1999. [11] 2006 Nominación del Premio del Software del Año de la NASA. Faceta: herramienta de evaluación de conceptos de cajeros automáticos futuros. No caso. Arc-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller. Agentes autónomos para la desconflicción de tráfico aéreo. En Proc de la quinta int jt conf en agentes autónomos y sistemas de múltiples agentes, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe. Beneficios del Sistema Nacional del Espacio Aéreo en el Sistema Nacional. En AIAA Guidance, Navegación y Control Conf, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji. Modelo de flujo agregado para la gestión del tráfico aéreo. Journal of Guidance, Control y Dynamics, 29 (4): 992-997, 2006. [15] R. S. Sutton y A. G. Barto. Aprendizaje de refuerzo: una introducción. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas y S. Sastry. Resolución de conflictos para la gestión del tráfico aéreo. IEEE Tran sobre control automático, 43 (4): 509-521, 1998. [17] K. Tumer y D. Wolpert, editores. Colectivos y el diseño de sistemas complejos. Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer. Funciones de pago óptimas para miembros de colectivos. Avances en sistemas complejos, 4 (2/3): 265-279, 2001. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 349