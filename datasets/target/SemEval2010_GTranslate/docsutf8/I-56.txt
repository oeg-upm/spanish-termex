Algoritmos de restricción distribuidos unificadoras en un marco de negociación de BDI Bao Chau le Dinh y Kiam Tian SEOW Escuela de Ingeniería Informática Nanyang Universidad Tecnológica República de Singapur {LEDI0002, AsktSeow}@ntu.edu.sg Resumen Este documento presenta una novela, un marco de satisfacción distribuida de distribución unificada.Basado en la negociación automatizada. El problema de satisfacción de restricción distribuido (DCSP) es uno que implica varios agentes para buscar un acuerdo, que es una combinación consistente de acciones que satisface sus limitaciones mutuas en un entorno compartido. Al anclar la búsqueda de DCSP en la negociación automatizada, mostramos que varios algoritmos DCSP conocidos son en realidad mecanismos que pueden llegar a acuerdos a través de un protocolo común de intención de creencias (BDI), pero utilizando diferentes estrategias. Una motivación importante para este marco BDI es que no solo proporciona una comprensión conceptualmente más clara de los algoritmos DCSP existentes desde una perspectiva del modelo de agente, sino que también abre las oportunidades para extender y desarrollar nuevas estrategias para DCSP. Con este fin, se propone una nueva estrategia llamada asesoramiento mutuo no solicitado (UMA). La evaluación del rendimiento muestra que la estrategia UMA puede superar a algunos mecanismos existentes en términos de ciclos computacionales. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial distribuida]: Agentes inteligentes, Sistemas Multiagentes Algoritmos de términos generales, Diseño, Experimentación 1. La introducción en el núcleo de muchas aplicaciones distribuidas emergentes es el problema de satisfacción de restricciones distribuidas (DCSP), uno que implica encontrar una combinación consistente de acciones (abstraída como valores de dominio) para satisfacer las restricciones entre múltiples agentes en un entorno compartido. Los ejemplos de aplicaciones importantes incluyen asignación de recursos distribuidos [1] y programación distribuida [2]. Se han desarrollado muchos algoritmos importantes, como la ruptura distribuida (DBO) [3], el retroceso asíncrono (ABT) [4], la superposición parcial asincrónica (APO) [5] y el compromiso débil asincrónico (AWC) [4] [4], para abordarel DCSP y proporcionar la base de la solución del agente para sus aplicaciones. En términos generales, estos algoritmos se basan en dos enfoques diferentes, ya sea que se extienden desde los algoritmos de retroceso clásico [6] o introducen mediación entre los agentes. Si bien no ha habido falta de esfuerzos en este campo de investigación prometedor, especialmente en el tratamiento de problemas pendientes como restricciones de recursos (por ejemplo, límites en el tiempo y la comunicación) [7] y los requisitos de privacidad [8], desafortunadamente no hay un tratamiento conceptualmente claroPara abrir el funcionamiento teórico modelo de los diversos algoritmos de agentes que se han desarrollado. Como resultado, por ejemplo, una comprensión intelectual más profunda sobre por qué un algoritmo es mejor que el otro, más allá de los problemas computacionales, no es posible. En este documento, presentamos un marco de satisfacción de restricciones distribuidas unificadas novedosas basado en la negociación automatizada [9]. La negociación se considera un proceso de varios agentes que buscan una solución llamada acuerdo. La búsqueda se puede realizar a través de un mecanismo de negociación (o algoritmo) por el cual los agentes siguen un protocolo de alto nivel que prescribe las reglas de interacciones, utilizando un conjunto de estrategias ideadas para seleccionar sus propias preferencias en cada paso de negociación. Anclar la búsqueda DCSP en la negociación automatizada, mostramos en este documento que varios algoritmos DCSP bien conocidos [3] son en realidad mecanismos que comparten el mismo protocolo de interacción de la intención de creencias (BDI) para alcanzar acuerdos, pero usan diferentes estrategias de selección de acción o valor.. El marco propuesto proporciona no solo una comprensión más clara de los algoritmos DCSP existentes desde una perspectiva de agente BDI unificado, sino que también abre las oportunidades para extender y desarrollar nuevas estrategias para DCSP. Con este fin, se propone una nueva estrategia llamada asesoramiento mutuo no solicitado (UMA). Nuestra evaluación de rendimiento muestra que UMA puede superar a ABT y AWC en términos del número promedio de ciclos computacionales para los problemas de coloración escasos y críticos [6]. El resto de este documento está organizado de la siguiente manera. En la Sección 2, proporcionamos una descripción formal de DCSP. La Sección 3 presenta un modelo de negociación BDI por el cual razona un agente DCSP. La Sección 4 presenta los algoritmos existentes ABT, AWC y DBO como diferentes estrategias formalizadas en un protocolo común. En la Sección 5 se propone una nueva estrategia llamada asesoramiento mutuo no solicitado;Nuestros resultados empíricos y discusión intentan resaltar los méritos de la nueva estrategia sobre las existentes. La Sección 6 concluye el documento y señala algún trabajo futuro.2. DCSP: Formalización del problema El DCSP [4] considera el siguiente entorno.• Hay N agentes con variables K x0, x1, · · ·, xk - 1, n ≤ k, que tienen valores en los dominios D1, D2, · · ·, DK, respectivamente. Definimos una función parcial B sobre el ProductRange {0, 1 ,..., (n - 1)} × {0, 1 ,..., (k −1)} tal que esa variable XJ pertenece al agente I es denotada por B (i, j)!. ¡La marca de exclamación!se define los medios.• Existen m restricciones C0, C1, · · · cm - 1 para satisfacer conjuntamente. De manera similar, como se define para B (I, J), usamos E (L, J)!, (0 ≤ L <M, 0 ≤ J <K), para denotar que XJ es relevante para la restricción Cl. El DCSP puede establecerse formalmente de la siguiente manera. Declaración del problema: ∀i, j (0 ≤ i <n) (0 ≤ j <k) donde b (i, j)!, Encuentre la tarea xj = dj ∈ Dj tal que ∀l (0 ≤ l <m) dondeE (L, J)!, Cl está satisfecho. Una restricción puede consistir en diferentes variables pertenecientes a diferentes agentes. Un agente no puede cambiar o modificar los valores de asignación de las variables de otros agentes. Por lo tanto, al buscar cooperativamente una solución DCSP, los agentes necesitarían comunicarse entre sí y ajustar y volver a ajustar sus propias tareas variables en el proceso.2.1 Modelo de agente DCSP En general, todos los agentes DCSP deben interactuar cooperativamente, y esencialmente realizar la asignación y reasignación de valores de dominio a variables para resolver todas las violaciones de restricciones. Si los agentes tienen éxito en su resolución, se encuentra una solución. Para participar en el comportamiento cooperativo, un agente DCSP necesita cinco parámetros fundamentales, a saber, (i) una variable [4] o un conjunto variable [10], (ii) dominios, (iii) prioridad, (iv) una lista de vecinosy (v) una lista de restricciones. Cada variable asume un rango de valores llamado dominio. Un valor de dominio, que generalmente abstrae una acción, es una posible opción que un agente puede tomar. Cada agente tiene una prioridad asignada. Estos valores prioritarios ayudan a decidir el orden en que revisan o modifican sus asignaciones de variables. La prioridad de los agentes puede ser fijada (estática) o cambiando (dinámica) al buscar una solución. Si un agente tiene más de una variable, a cada variable se le puede asignar una prioridad diferente, para ayudar a determinar qué asignación de variable debe modificar primero. Un agente que comparte la misma restricción con otro agente se llama vecino de los últimos. Cada agente debe consultar su lista de vecinos durante el proceso de búsqueda. Esta lista también puede mantenerse sin cambios o actualizarse en consecuencia en tiempo de ejecución. Del mismo modo, cada agente mantiene una lista de restricciones. El agente debe asegurarse de que no haya violación de las limitaciones en esta lista. Las restricciones se pueden agregar o eliminar de una lista de restricciones de agentes en tiempo de ejecución. Al igual que con un agente, una restricción también puede asociarse con un valor de prioridad. Se dice que las restricciones con una alta prioridad son más importantes que las restricciones con una prioridad menor. Para distinguirlo de la prioridad de un agente, la prioridad de una restricción se llama peso.3. El modelo de negociación BDI El modelo BDI se origina con el trabajo de M. Bratman [11]. Según [12, Ch.1], la arquitectura BDI se basa en un modelo filosófico de razonamiento práctico humano, y extrae el proceso de razonamiento por el cual un agente decide qué acciones realizar en momentos consecutivos cuando persiguen ciertos objetivos. Con una base del marco DCSP, el objetivo común de todos los agentes es encontrar una combinación de valores de dominio para satisfacer un conjunto de restricciones predefinidas. En la negociación automatizada [9], dicha solución se denomina acuerdo entre los agentes. Dentro de este alcance, descubrimos que pudimos descubrir el comportamiento genérico de un agente DCSP y formularlo en un protocolo de negociación, prescrito utilizando los poderosos conceptos de BDI. Por lo tanto, se puede decir que nuestro modelo de negociación propuesto combina los conceptos BDI con una negociación automatizada en un marco multiagente, lo que nos permite separar los mecanismos DCSP conceptualmente en un protocolo de interacción BDI común y las estrategias adoptadas.3.1 El protocolo genérico Figura 1 muestra los pasos de razonamiento básicos en una ronda arbitraria de negociación que constituye el nuevo protocolo. La línea continua indica el componente o transición común que siempre existe independientemente de la estrategia utilizada. La línea punteada indica la percepción del deseo de deseo de la mediación de la intención P B D I I I Info Mensaje Información Mensaje Negociación Negociación Mensaje de negociación Mensaje Negociación Mensaje de negociación Mensaje de negociación Mensaje de negociación Figura 1: El componente o transición del protocolo de interacción BDI que puede o no aparecer dependiendo de la adopción de la adopción de la adopción de la adopción de la Figura 1:estrategia. Se intercambian dos tipos de mensajes a través de este protocolo, a saber, el mensaje de información y el mensaje de negociación. Un mensaje de información percibido es un mensaje enviado por otro agente. El mensaje contendrá los valores y prioridades seleccionados actuales de las variables de ese agente de envío. El objetivo principal de este mensaje es actualizar al agente sobre el entorno actual. El mensaje de información se envía al final de una ronda de negociación (también llamada ciclo de negociación) y se recibe al comienzo de la próxima ronda. Un mensaje de negociación es un mensaje que puede enviarse dentro de una ronda. Este mensaje es para fines de mediación. El agente puede poner diferentes contenidos en este tipo de mensaje siempre que se acuerde entre el grupo. El formato del mensaje de negociación y cuándo se enviará está sujeto a la estrategia. Se puede enviar un mensaje de negociación al final de un paso de razonamiento y recibir al comienzo del siguiente paso. La mediación es un paso del protocolo que depende de si la interacción de los agentes con otros es sincrónica o asincrónica. En el mecanismo sincrónico, se requiere mediación en cada ronda de negociación. En una asíncrona, la mediación solo se necesita en una ronda de negociación cuando el agente recibe un mensaje de negociación. Una visión más profunda de este paso de mediación se proporciona más adelante en esta sección. El protocolo BDI prescribe la estructura esquelética para la negociación DCSP. Mostraremos en la Sección 4 que varios mecanismos DCSP conocidos heredan este modelo genérico. Los detalles de los seis pasos de razonamiento principales para el protocolo (ver Figura 1) se describen de la siguiente manera para un agente DCSP. Para una descripción conceptualmente más clara, suponemos que solo hay una variable por agente.• percepción. En este paso, el agente recibe mensajes de información de sus vecinos en el entorno, y utilizando su función de percepción, devuelve una imagen P. Esta imagen contiene los valores actuales asignados a las variables de todos los agentes en su lista de vecinos. La imagen P impulsará las acciones de los agentes en pasos posteriores. El agente también actualiza su lista de restricciones C utilizando algunos criterios de la estrategia adoptada.• Creencia. Usando la imagen P y la lista de restricciones C, el agente verificará si hay alguna restricción violada. Si no hay violación, el agente creerá que está eligiendo una opción correcta y, por lo tanto, no tomará medidas. El agente no hará nada si está en un estado estable local, una instantánea de las asignaciones de variables del agente y todos sus vecinos por los cuales satisfacen sus limitaciones compartidas. Cuando todos los agentes se encuentran en sus estados estables locales, se dice que todo el entorno está en un estado estable global y un acuerdo sexto INTL. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) se encuentra 525 mentes. En caso de que el agente encuentre su valor en conflicto con algunos de sus vecinos, es decir, la combinación de valores asignados a las variables conduce a una violación de restricción, el agente primero intentará reasignar su propia variable utilizando una estrategia específica. Si encuentra una opción adecuada que cumple con algunos criterios de la estrategia adoptada, el agente creerá que debería cambiar a la nueva opción. Sin embargo, no siempre sucede que un agente pueda encontrar con éxito tal opción. Si no se puede encontrar ninguna opción, el agente creerá que no tiene opción y, por lo tanto, solicitará a sus vecinos que reconsideren sus tareas variables. Para resumir, hay tres tipos de creencias que un agente DCSP puede formarse: (i) puede cambiar su asignación variable para mejorar la situación actual, (ii) no puede cambiar su asignación variable y algunas violaciones de restricciones no se pueden resolver y (iii) No necesita cambiar su asignación variable ya que todas las restricciones están satisfechas. Una vez que se forman las creencias, el agente determinará sus deseos, que son las opciones que intentan resolver las violaciones de restricción actuales.• Deseo. Si el agente toma creencia (i), generará una lista de sus propios valores de dominio adecuados como su conjunto de deseo. Si el agente toma creencia (ii), no puede determinar su set de deseo, pero generará un sublista de agentes de su lista de vecinos, a quien solicitará reconsiderar sus tareas variables. La forma en que se crea este sublista depende de la estrategia ideada para el agente. En esta situación, el agente utilizará un conjunto de deseo virtual que determina en función de su estrategia adoptada. Si el agente toma creencia (iii), no tendrá ganas de revisar su valor de dominio y, por lo tanto, no hay intención.• Intención. El agente seleccionará un valor de su deseo establecido como su intención. Una intención es la mejor opción deseada que el agente asigna a su variable. Los criterios para seleccionar un deseo, ya que la intención de los agentes depende de la estrategia utilizada. Una vez que se forma la intención, el agente puede proceder al paso de ejecución o someterse a una mediación. Nuevamente, la decisión de hacerlo está determinada por algunos criterios de la estrategia adoptada.• Mediación. Esta es una función importante del agente. Dado que, si el agente ejecuta su intención sin realizar una mediación de intención con sus vecinos, la violación de la restricción entre los agentes no puede resolverse. Tomemos, por ejemplo, suponga que dos agentes tienen variables, x1 y x2, asociadas con el mismo dominio {1, 2}, y su restricción compartida es (x1 + x2 = 3). Entonces, si ambas variables se inicializan con el valor 1, cambiarán simultáneamente entre los valores 2 y 1 en ausencia de mediación entre ellas. Hay dos tipos de mediación: mediación local y mediación grupal. En el primero, los agentes intercambian sus intenciones. Cuando un agente recibe la intención de Another que entra en conflicto con la suya, el agente debe mediar entre las intenciones, ya sea cambiando su propia intención o informando al otro agente que cambie su intención. En este último, hay un agente que actúa como mediador grupal. Este mediador recopilará las intenciones del grupo, una unión del agente y sus vecinos, y determinará qué intención se ejecutará. El resultado de esta mediación se transfiere a los agentes del grupo. Después de la mediación, el agente puede proceder al siguiente paso de razonamiento para ejecutar su intención o comenzar una nueva ronda de negociación.• Ejecución. Este es el último paso de una ronda de negociación. El agente se ejecutará actualizando su asignación variable si la intención obtenida en este paso es suya. Después de la ejecución, el agente informará a sus vecinos sobre su nueva asignación de variable y su prioridad actualizada. Para hacerlo, el agente enviará un mensaje de información.3.2 La estrategia Una estrategia juega un papel importante en el proceso de negociación. Dentro del protocolo, a menudo determinará la eficiencia de la percepción de la ejecución de la mediación de la intención del deseo P B D I Información Mensaje Información Mensaje de negociación Mensaje de negociación Mensaje de negociación Figura 2: Protocolo BDI Con proceso de búsqueda de estrategia de retroceso asíncrono en términos de ciclos computacionales y costos de comunicación de mensajes. El espacio de diseño al diseñar una estrategia está influenciado por las siguientes dimensiones: (i) asíncrono o sincrónico, (ii) prioridad dinámica o estática, (iii) peso dinámico o de restricción estática, (iv) Número de mensajes de negociación que se comunicarán ((v) el formato del mensaje de negociación y (vi) la propiedad de integridad. En otras palabras, estas dimensiones proporcionan consideraciones técnicas para un diseño de estrategia.4. Algoritmos DCSP: Protocolo BDI + Estrategias En esta sección, aplicamos el modelo de negociación BDI propuesto presentado en la Sección 3 para exponer el protocolo BDI y las diferentes estrategias utilizadas para tres algoritmos bien conocidos, ABT, AWC y DBO. Todos estos algoritmos suponen que solo hay una variable por agente. Bajo nuestro marco, llamamos a las estrategias aplicadas las estrategias ABT, AWC y DBO, respectivamente. Para describir cada estrategia formalmente, se utilizan las siguientes anotaciones matemáticas: • n es el número de agentes, M es el número de restricciones;• Xi denota la variable sostenida por el agente I, (0 ≤ i <n);• Di denota el dominio de la variable Xi;Fi denota la lista vecina del Agente I;CI denota su lista de restricciones;• Pi denota la prioridad del agente I;y pi = {(xj = vj, pj = k) |agente j ∈ Fi, vj ∈ Dj es el valor actual asignado a XJ y el valor de prioridad k es un entero positivo} es la percepción del agente I;• WL denota el peso de la restricción l, (0 ≤ l <m);• Si (V) es el peso total de las restricciones violadas en CI cuando su variable tiene el valor v ∈ Di.4.1 El retroceso asíncrono Figura 2 presenta el modelo de negociación BDI que incorpora la estrategia de retroceso asíncrono (ABT). Como se menciona en la Sección 3, para un mecanismo asincrónico que es ABT, el paso de mediación solo se necesita en una ronda de negociación cuando un agente recibe un mensaje de negociación. Para el agente I, comenzando inicialmente con (wl = 1, (0 ≤ l <m); pi = i, (0 ≤ i <n)) y Fi contiene todos los agentes que comparten las limitaciones con el agente I, su BDI impulsadoLa estrategia ABT se describe de la siguiente manera. Paso 1 - Percepción: Actualice PI al recibir los mensajes de información de los vecinos (en FI). Actualice CI para ser la lista de 526 el sexto intl. Conf.en agentes autónomos y restricciones de sistemas de múltiples agentes (AAMAS 07) que solo consisten en agentes en FI que tienen la misma o mayor prioridad que este agente. Paso 2 - Creencia: la función de creencia GB (PI, CI) devolverá un valor bi ∈ {0, 1, 2}, decidió lo siguiente: • Bi = 0 Cuando agente puedo encontrar una opción óptima, es decir, if (Si (Si(vi) = 0 o VI está en la lista de valores malos) y (∃a ∈ Di) (si (a) = 0) y A no está en una lista de valores de dominio llamado Lista de valores malos. Inicialmente, esta lista está vacía y se borrará cuando un vecino de mayor prioridad cambie su asignación variable.• Bi = 1 Cuando no puede encontrar una opción óptima, es decir, if (∀a ∈ Di) (Si (a) = 0) o A está en la lista de valores malos.• BI = 2 Cuando su asignación de variable actual es una opción óptima, es decir, si SI (VI) = 0 y VI no está en la lista de valor malo. Paso 3 - Deseo: la función de deseo GD (BI) devolverá un deseo establecido denotado por DS, decidido de la siguiente manera: • Si bi = 0, entonces ds = {a |(a = vi), (si (a) = 0) y a no está en la lista de valor incorrecto}.• Si bi = 1, entonces ds = ∅, el agente también encuentra el agente k que está determinado por {k |Pk = min (PJ) con agente J ∈ Fi y Pk> Pi}.• Si bi = 2, entonces ds = ∅. Paso 4 - Intención: la función de intención GI (DS) devolverá una intención, decidida de la siguiente manera: • Si ds = ∅, luego seleccione un valor arbitrario (digamos, vi) de DS como la intención.• Si ds = ∅, asigne nil como intención (para denotar su falta de ella). Paso 5 - Ejecución: • Si el agente I tiene un valor de dominio como intención, el agente actualizará su asignación variable con este valor.• Si bi = 1, agente enviaré un mensaje de negociación al Agente K, luego elimine K de FI y comenzará su próxima ronda de negociación. El mensaje de negociación contendrá la lista de asignaciones variables de esos agentes en su lista de vecinos FI que tienen una prioridad más alta que el Agente I en la imagen actual PI. Mediación: cuando el agente I recibe un mensaje de negociación, se llevan a cabo varios subpasos, de la siguiente manera: • Si la lista de agentes asociados con el mensaje de negociación contiene agentes que no están en FI, agregará estos agentes a FI y solicitaráEstos agentes se suman a sus listas de vecinos. La solicitud se considera como un tipo de mensaje de negociación.• Agente Primero verificaré si el agente del remitente se actualiza con su valor actual VI. El agente agregará VI a su lista de valores malos si es así, o enviará su valor actual al agente del remitente. Después de este paso, el agente I procede a la próxima ronda de negociación.4.2 Búsqueda de compromiso débil asincrónico La Figura 3 presenta el modelo de negociación BDI que incorpora la estrategia de compromiso débil asincrónico (AWC). El modelo es similar al de incorporar la estrategia ABT (ver Figura 2). Esto no es sorprendente;AWC y ABT son estratégicamente similares, que difieren solo en los detalles de algunos pasos de razonamiento. El punto distintivo de AWC es que cuando el agente no puede encontrar una asignación de variable adecuada, cambiará su prioridad a los más altos entre los miembros de su grupo ({I} ∪ Fi). Para el agente I, comenzando inicialmente con (wl = 1, (0 ≤ l <m); pi = i, (0 ≤ i <n)) y Fi contiene todos los agentes que comparten las limitaciones con el agente I, su bdi impulsadoLa estrategia AWC se describe de la siguiente manera. Paso 1 - Percepción: este paso es idéntico al paso de percepción de ABT. Paso 2 - Creencia: la función de creencia GB (PI, CI) devolverá un valor bi ∈ ∈ {0, 1, 2}, decidido de la siguiente manera: Percept BRISTA DESEMIENTO MEDIACIÓN Ejecución de mediación P B D I Información Información Información Mensaje Mensaje Mensaje Mensaje Figura de mensajes de negociación3: Protocolo BDI con estrategia de comisión débil asincrónica • Bi = 0 Cuando el agente puede encontrar una opción óptima, es decir, IF (Si (VI) = 0 o la asignación xi = VI y las asignaciones de variables actuales de los vecinos en FiFORMA A NOGOOD [4]) almacenado en una lista llamada Nogood List y ∃a ∈ Di, Si (a) = 0 (inicialmente la lista está vacía).• bi = 1 Cuando el agente no puede encontrar ninguna opción óptima, es decir, si ∀a ∈ Di, si (a) = 0. • bi = 2 cuando la asignación actual es una opción óptima, es decir, si si (vi) = 0 y elEl estado actual no es un nogood en la lista de Nogood. Paso 3 - Deseo: la función de deseo GD (BI) devolverá un desire establecido DS, decidido de la siguiente manera: • Si bi = 0, entonces ds = {a |(a = vi), (si (a) = 0) y el número de violaciones de restricción con agentes de menor prioridad se minimiza}.• Si bi = 1, entonces ds = {a |a ∈ Di y el número de violaciones de todas las restricciones relevantes se minimiza}.• Si bi = 2, entonces ds = ∅. A continuación, si Bi = 1, agente encontraré una lista Ki de vecinos de mayor prioridad, definido por ki = {k |agente k ∈ Fi y Pk> pi}. Paso 4 - Intención: este paso es similar al paso de intención de ABT. Sin embargo, para esta estrategia, el mensaje de negociación contendrá las asignaciones de variables (de la imagen actual PI) para todos los agentes en KI. Esta lista de asignación se considera un nogood. Si el mismo mensaje de negociación se había enviado antes, agente, tendré una intención nula. De lo contrario, el agente enviará el mensaje y guardará el Nogood en la lista de Nogood. Paso 5 - Ejecución: • Si el agente I tiene un valor de dominio como intención, el agente actualizará su asignación variable con este valor.• Si bi = 1, enviará el mensaje de negociación a sus vecinos en ki, y establecerá pi = max {pj} + 1, con agente j ∈ Fi. Mediación: Este paso es idéntico al paso de mediación de ABT, excepto que el agente ahora agregaré el Nogood contenido en el mensaje de negociación recibido a su propia lista de Nogood.4.3 Breakout distribuido La Figura 4 presenta el modelo de negociación BDI que incorpora la estrategia de ruptura distribuida (DBO). Esencialmente, según esta estrategia sincrónica, cada agente buscará iterativamente la mejora al reducir el peso total de las restricciones violadas. La iteración continuará hasta que ningún agente pueda mejorar aún más, momento en el que se persiguen algunas limitaciones, los pesos del sexto INTL. Conf.Sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 527 Percept Believe Desprecio Mediación Mediación Ejecución P B D I A Información Mensaje Información Mensaje de negociación Mensaje de negociación Figura 4: Protocolo BDI Con estrategia de ruptura distribuida Estas limitaciones aumentarán en 1 para ayudar a Breakout de un local de un local de un Local Local.mínimo. Para el agente I, comenzando inicialmente con (wl = 1, (0 ≤ l <m), pi = i, (0 ≤ i <n)) y Fi contiene todos los agentes que comparten las limitaciones con el agente I, su BDI impulsadoLa estrategia DBO se describe de la siguiente manera. Paso 1 - Percepción: Actualice PI al recibir los mensajes de información de los vecinos (en FI). Actualice CI para que sea la lista de sus restricciones relevantes. Paso 2 - Creencia: la función de creencia GB (PI, CI) devolverá un valor bi ∈ {0, 1, 2}, decidió lo siguiente: • Bi = 0 Cuando agente puedo encontrar una opción para reducir las violaciones numéricas de larestricciones en CI, es decir, si ∃a ∈ Di, si (a) <si (vi).• Bi = 1 Cuando no puede encontrar ninguna opción para mejorar la situación, es decir, si ∀a ∈ Di, a = vi, si (a) ≥ Si (vi).• Bi = 2 Cuando su asignación actual es una opción óptima, es decir, si SI (VI) = 0. Paso 3 - Deseo: la función de deseo GD (BI) devolverá un desire establecido DS, decidido de la siguiente manera: • Si bi = 0, entonces ds = {a |a = vi, si (a) <si (vi) y (si (vi) −si (a)) se maximiza}.(Max {(Si (VI) −si (a))} será referenciado por Hmax I en pasos posteriores, y define la reducción máxima en las violaciones de restricciones).• De lo contrario, ds = ∅. Paso 4 - Intención: la función de intención GI (DS) devolverá una intención, decidida de la siguiente manera: • Si ds = ∅, luego seleccione un valor arbitrario (digamos, vi) de DS como la intención.• Si ds = ∅, entonces asigne nil como intención. Siguiendo, agente, enviaré su intención a todos sus vecinos. A cambio, recibirá intenciones de estos agentes antes de proceder al paso de mediación. Mediación: el agente I recibe todas las intenciones de sus vecinos. Si encuentra que la intención recibida de un agente vecino J está asociada con Hmax J> Hmax I, el agente cancelará automáticamente su intención actual. Paso 5 - Ejecución: • Si el agente no cancelé su intención, actualizará su asignación variable con el valor previsto. Percepta creencia Deseo de la intención de ejecución de mediación P B D I I A Información Información Información Mensaje Mensaje Negociación Mensaje de negociación Mensaje Mensaje de negociación Figura 5: Protocolo BDI Con estrategia de asesoramiento mutuo no solicitado • Si todas las intenciones recibidas y su propia son intención, el agente aumentará el peso de cada unoRestricción violada actualmente por 1. 5. La Figura 5 de la Estrategia UMA presenta el modelo de negociación BDI que incorpora la estrategia de asesoramiento mutuo no solicitado (UMA). A diferencia de cuando se usa las estrategias de la sección anterior, un agente DCSP que usa UMA no solo enviará un mensaje de negociación al concluir su paso de intención, sino también al concluir su paso de deseo. El mensaje de negociación que envía para concluir el paso del deseo constituye un consejo no solicitado para todos sus vecinos. A su vez, el agente esperará para recibir consejos no solicitados de todos sus vecinos, antes de continuar para determinar su intención. Para el agente I, comenzando inicialmente con (wl = 1, (0 ≤ l <m), pi = i, (0 ≤ i <n)) y Fi contiene todos los agentes que comparten las limitaciones con el agente I, su BDI impulsadoLa estrategia de UMA se describe de la siguiente manera. Paso 1 - Percepción: Actualice PI al recibir los mensajes de información de los vecinos (en FI). Actualizar CI para que sea la lista de restricciones relevantes para el agente i. Paso 2 - Creencia: la función de creencia GB (PI, CI) devolverá un valor bi ∈ {0, 1, 2}, decidió lo siguiente: • Bi = 0 Cuando agente puedo encontrar una opción para reducir las violaciones numéricas de larestricciones en CI, es decir, si ∃a ∈ Di, Si (a) <Si (vi) y la asignación xi = a y las asignaciones variables actuales de sus vecinos no forman un estado local almacenado en una lista llamada lista de estados malos (Inicialmente, esta lista está vacía).• bi = 1 cuando no puede encontrar un valor a tal como a ∈ Di, si (a) <si (vi), y la asignación xi = a y las asignaciones variables actuales de sus vecinos no forman un estado local almacenado en elLista de estados malos.• Bi = 2 Cuando su asignación actual es una opción óptima, es decir, si SI (VI) = 0. Paso 3 - Deseo: la función de deseo GD (BI) devolverá un desire establecido DS, decidido de la siguiente manera: • Si bi = 0, entonces ds = {a |a = vi, si (a) <si (vi) y (si (vi) - si (a)) se maximiza} y la asignación xi = a y las asignaciones variables actuales del agente no forman un estado en el estado en el estado en el estado en el estadoLista de estados malos. En este caso, DS se llama un conjunto de deseos voluntarios.Max {(Si (VI) −si (a))} será referenciado por Hmax I en pasos posteriores, y define 528 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) la reducción máxima en las violaciones de restricciones. También se conoce como una mejora).• Si bi = 1, entonces ds = {a |a = vi, Si (a) se minimiza} y la asignación xi = a y las asignaciones variables actuales del agente son vecinos no forman un estado en la lista de estados malos. En este caso, DS se llama un conjunto de deseos reacios • Si Bi = 2, entonces ds = ∅. A continuación, si Bi = 0, agente enviaré un mensaje de negociación que contenga Hmax I a todos sus vecinos. Este mensaje se llama consejo voluntario. Si bi = 1, agente, enviaré un mensaje de negociación llamado consejo de cambio a los vecinos en FI que comparten las limitaciones violadas con el agente i. Agente recibí consejos de todos sus vecinos y los almacena en una lista llamada A, antes de continuar con el siguiente paso. Paso 4 - Intención: la función de intención GI (DS, A) devolverá una intención, decidida de la siguiente manera: • Si hay un consejo voluntario de un agente J asociado con Hmax J> Hmax I, asigne nil como intención.• Si DS = ∅, DS es un conjunto de deseos voluntarios y Hmax I es la mayor mejora entre los asociados con los consejos voluntarios recibidos, seleccione un valor arbitrario (digamos, vi) de DS como la intención. Esta intención se llama intención voluntaria.• Si ds = ∅, ds es un conjunto de deseos reacios y agente I recibe algunos consejos de cambio, seleccione un valor arbitrario (digamos, vi) de DS como intención. Esta intención se llama intención reacia.• Si ds = ∅, entonces asigne nil como intención. A continuación, si la mejora Hmax I es la mayor mejora y es igual a algunas mejoras asociadas con los consejos voluntarios recibidos, el agente enviaré su intención calculada a todos sus vecinos. Si el agente I tiene una intención reacia, también enviará esta intención a todos sus vecinos. En ambos casos, el agente I adjuntará el número de consejos de cambio recibidos en la ronda de negociación actual con su intención. A cambio, agente, recibiré las intenciones de sus vecinos antes de proceder al paso de mediación. Mediación: si el agente I no envía su intención antes de este paso, es decir, el agente tiene una intención nula o una intención voluntaria con la mayor mejora, procederá al siguiente paso. De lo contrario, agente, seleccionaré la mejor intención entre todas las intenciones recibidas, incluidas las suyas (si corresponde). Los criterios para seleccionar la mejor intención se enumeran, se aplican en orden descendente de importancia de la siguiente manera.• Se prefiere una intención voluntaria sobre una intención reacia.• Se selecciona una intención voluntaria (si la hay) con mayor mejora.• Si no hay intención voluntaria, se selecciona la intención reacia con el menor número de violaciones de restricciones.• Se selecciona la intención de un agente que ha recibido un mayor número de consejos de cambio en la ronda de negociación actual.• Se selecciona la intención de un agente con la más alta prioridad. Si la intención seleccionada no es el agente es intención, cancelará su intención. Paso 5 - Ejecución: si el Agente I no cancela su intención, actualizará su asignación variable con el valor previsto. Condición de terminación: dado que cada agente no tiene información completa sobre el estado global, puede no saber cuándo ha alcanzado una solución, es decir, cuándo todos los agentes están en un estado estable global. Por lo tanto, se necesita un observador que realizará un seguimiento de los mensajes de negociación comunicados en el entorno. Después de un cierto período de tiempo cuando no hay más comunicación de mensajes (y esto sucede cuando todos los agentes no tienen más intención de actualizar sus tareas variables), el observador informará a los agentes en el entorno que se ha encontrado una solución.1 2 3 4 5 6 7 8 9 10 Figura 6: Problema de ejemplo 5.1 Un ejemplo para ilustrar cómo funciona UMA, considere un problema de gráfico de 2 colores [6] como se muestra en la Figura 6. En este ejemplo, cada agente tiene una variable de color que representa un nodo. Hay 10 variables de color que comparten el mismo dominio {negro, blanco}. El siguiente registra el resultado de cada paso en cada ronda de negociación ejecutada. Ronda 1: Paso 1 - Percepción: cada agente obtiene las asignaciones de color actuales de esos nodos (agentes) adyacentes a él, es decir, sus vecinos. Paso 2 - Creencia: los agentes que tienen mejoras positivas son el Agente 1 (este agente cree que debería cambiar su color a blanco), Agente 2 (esto cree que debería cambiar su color a blanco), Agente 7 (este agente cree que debería cambiar su colora negro) y agente 10 (este agente cree que debería cambiar su valor a negro). En esta ronda de negociación, las mejoras logradas por estos agentes son 1. Los agentes que no tienen mejoras son los Agentes 4, 5 y 8. Los agentes 3, 6 y 9 no necesitan cambiar ya que todas sus limitaciones relevantes se cumplen. Paso 3 - Deseo: los agentes 1, 2, 7 y 10 tienen el deseo voluntario (color blanco para los agentes 1, 2 y el color negro para los agentes 7, 10). Estos agentes enviarán los consejos voluntarios a todos sus vecinos. Mientras tanto, los agentes 4, 5 y 8 tienen los deseos reacios (color blanco para el agente 4 y el color negro para los agentes 5, 8). El Agente 4 enviará un consejo de cambio al Agente 2, ya que el Agente 2 comparte la restricción violada con él. Del mismo modo, los agentes 5 y 8 enviarán consejos de cambio a los agentes 7 y 10 respectivamente. Los agentes 3, 6 y 9 no tienen ningún deseo de actualizar sus tareas de color. Paso 4 - Intención: los agentes 2, 7 y 10 reciben los consejos de cambio de los agentes 4, 5 y 8, respectivamente. Forman sus intenciones voluntarias. Los agentes 4, 5 y 8 reciben los consejos voluntarios de los Agentes 2, 7 y 10, por lo tanto, no tendrán ninguna intención. Los agentes 3, 6 y 9 no tienen ninguna intención. A continuación, la intención de los agentes se enviará a todos sus vecinos. Mediación: el Agente 1 encuentra que la intención del Agente 2 es mejor que su intención. Esto se debe a que, aunque ambos agentes tienen intenciones voluntarias con una mejora de 1, el Agente 2 ha recibido un consejo de cambio del Agente 4, mientras que el Agente 1 no ha recibido ninguno. Por lo tanto, el Agente 1 cancela su intención. El agente 2 mantendrá su intención. Los agentes 7 y 10 mantienen sus intenciones ya que ninguno de sus vecinos tiene una intención. El resto de los agentes no hacen nada en este paso, ya que no tienen ninguna intención. Paso 5 - Ejecución: el agente 2 cambia su color a blanco. Los agentes 7 y 10 cambian sus colores a negro. El nuevo estado después de la Ronda 1 se muestra en la Figura 7. Ronda 2: Paso 1 - Percepción: los agentes obtienen las asignaciones de color actuales de sus vecinos. Paso 2 - Creencia: el Agente 3 es el único agente que tiene una mejora positiva que es 1. Cree que debería cambiar su sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 529 1 2 3 4 5 6 7 8 9 10 Figura 7: El gráfico después de la ronda 1 color a negro. El agente 2 no tiene ninguna mejora positiva. El resto de los agentes no necesitan hacer ningún cambio ya que todas sus limitaciones relevantes se cumplen. No tendrán ningún deseo y, por lo tanto, no hay intención. Paso 3 - Deseo: el Agente 3 desea cambiar su color a negro voluntariamente, por lo tanto, envía un consejo voluntario a su vecino, es decir, el Agente 2. El agente 2 no tiene ningún valor para su deseo reacio establecido como la única opción, Black Color, traerá el Agente 2 y sus vecinos al estado anterior, que se sabe que es un mal estado. Dado que el Agente 2 comparte la violación de la restricción con el Agente 3, envía un consejo de cambio al Agente 3. Paso 4 - Intención: el Agente 3 tendrá una intención voluntaria, mientras que el Agente 2 no tendrá ninguna intención, ya que recibe el consejo voluntario del Agente 3. Mediación: el Agente 3 mantendrá su intención como su único vecino, el Agente 2, no tiene ninguna intención. Paso 5 - Ejecución: el agente 3 cambia su color a negro. El nuevo estado después de la ronda 2 se muestra en la Figura 8. Ronda 3: En esta ronda, cada agente encuentra que no tiene deseo y, por lo tanto, no tiene intención de revisar su asignación variable. A continuación, sin más comunicación de mensajes de negociación en el entorno, el observador informará a todos los agentes que se ha encontrado una solución.2 3 4 5 6 7 8 91 10 Figura 8: La solución obtuvo una evaluación de rendimiento 5.2 para facilitar las comparaciones creíbles con las estrategias existentes, medimos el tiempo de ejecución en términos de ciclos computacionales como se define en [4], y construimos un simulador que podría reproducirLos resultados publicados para ABT y AWC. La definición de un ciclo computacional es la siguiente.• En un ciclo, cada agente recibe todos los mensajes entrantes, realiza el cálculo local y envía una respuesta.• Un mensaje que se envía en el momento t se recibirá en el momento t + 1. El retraso de la red se descuida.• Cada agente tiene su propio reloj. El valor inicial de los relojes es 0. Los agentes adjuntan su valor de reloj como una tabla de tiempo en el mensaje saliente y usan la tabla de tiempo en el mensaje entrante para actualizar el valor de sus propios relojes. Se consideraron cuatro problemas de referencia [6], a saber, colorante N-coles y nodo para gráficos escasos, densos y críticos. Para cada problema, se generó un número finito de casos de prueba para varios tamaños de problemas n.El tiempo de ejecución máximo se estableció en 0 200 400 600 800 1000 10 50 100 Número de ciclos de reinas de reinas retroceso asíncrono Compromiso débil asincrónico Asesoramiento mutuo no solicitado Figura 9: Relación entre el tiempo de ejecución y el tamaño del problema 10000 ciclos para el color de nodos para gráficos críticos y 1000 ciclos paraotros problemas. El programa del simulador se terminó después de este período y se consideró que el algoritmo falla un caso de prueba si no encontraba una solución para entonces. En tal caso, el tiempo de ejecución para la prueba se contó como 1000 ciclos.5.2.1 Evaluación con el problema de N-Queens El problema N-Queens es un problema tradicional de satisfacción de restricción.Se generaron 10 casos de prueba para cada tamaño de problema n ∈ {10, 50 y 100}. La Figura 9 muestra el tiempo de ejecución para diferentes tamaños de problemas cuando se ejecutaron ABT, AWC y UMA.5.2.2 Evaluación con problema de coloración de gráficos El problema de coloración del gráfico puede caracterizarse por tres parámetros: (i) el número de colores k, el número de nodos/agentes n y el número de enlaces m.Según la relación m/n, el problema se puede clasificar en tres tipos [3]: (i) escaso (con m/n = 2), (ii) crítico (con m/n = 2.7 o 4.7) y (iii) denso (con m/n = (n - 1)/4). Para este problema, no incluimos ABT en nuestros resultados empíricos, ya que se encontró que su tasa de falla era muy alta. Se esperaba este bajo rendimiento de ABT, ya que el problema de coloración del gráfico es más difícil que el problema de N-Queens, en el que ABT ya no funcionó bien (ver Figura 9). Los tipos de problemas escasos y densos (colorantes) son relativamente fáciles, mientras que el tipo crítico es difícil de resolver. En los experimentos, corrigimos k = 3. 10 casos de prueba se crearon utilizando el método descrito en [13] para cada valor de n ∈ {60, 90, 120}, para cada tipo de problema. Los resultados de la simulación para cada tipo de problema se muestran en las Figuras 10 - 12. 0 40 80 120 160 200 60 90 120 150 Número de nodos Ciclos Asincrónicos Compromiso débil de asesoramiento mutuo no solicitado Figura 10: Comparación entre AWC y UMA (coloración de gráficos dispersos) 5.3Discusión 5.3.1 Comparación con ABT y AWC 530 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 0 1000 2000 3000 4000 5000 6000 60 90 120 Número de nodos ciclos de compromiso débil asincrónico Asesoramiento mutuo no solicitado Figura 11: Comparación entre AWC y UMA (color gráfico crítico) 0 20 30 40 40 40 40 40 40 4050 60 90 120 Número de nodos Ciclos de compromiso débil asincrónico Consejo mutuo no solicitado Figura 12: Comparación entre AWC y UMA (coloración densa gráfica) La Figura 10 muestra que el rendimiento promedio de UMA es ligeramente mejor que AWC para el problema escaso. UMA supera a AWC para resolver el problema crítico como se muestra en la Figura 11. Se observó que la última estrategia falló en algunos casos de prueba. Sin embargo, como se ve en la Figura 12, ambas estrategias son muy eficientes al resolver el problema denso, y AWC muestra un rendimiento ligeramente mejor. El rendimiento de UMA, en el peor caso (complejidad del tiempo), es similar al de todas las estrategias evaluadas. El peor de los casos ocurre cuando se alcanzan todos los posibles estados globales de la búsqueda. Dado que solo unos pocos agentes tienen derecho a cambiar sus tareas variables en una ronda de negociación, se reduce el número de ciclos computacionales redundantes y mensajes de información. Como observamos del retroceso en ABT y AWC, la diferencia en el orden de los mensajes entrantes puede dar lugar a un número diferente de ciclos computacionales para ser ejecutados por los agentes.5.3.2 Comparación con DBO El rendimiento computacional de UMA es posiblemente mejor que DBO por las siguientes razones: • Uma puede garantizar que habrá una reasignación variable después de cada ronda de negociación, mientras que DBO no puede.• Uma presenta un viaje de ida y vuelta de comunicación más (el de enviar un mensaje y esperar una respuesta) que DBO, que ocurre debido a la necesidad de comunicar consejos no solicitados. Aunque esto aumenta el costo de comunicación por ronda de negociación, observamos a partir de nuestras simulaciones que el costo general de comunicación incurrido por UMA es menor debido al número significativamente menor de rondas de negociación.• Usando UMA, en el peor de los casos, un agente solo tomará 2 o 3 viajes de la ronda de comunicación por ronda de negociación, después de lo cual el agente o su vecino hará una actualización de asignación variable. Usando DBO, este número de viajes redondos es incierto ya que cada agente podría tener que aumentar los pesos de las restricciones violadas hasta que un agente tenga una mejora positiva;Esto podría dar lugar a un bucle infinito [3].6. Conclusión Aplicación de negociación automatizada a DCSP, este documento ha propuesto un protocolo que prescribe el razonamiento genérico de un agente DCSP en una arquitectura BDI. Nuestro trabajo muestra que varios algoritmos DCSP conocidos, a saber, ABT, AWC y DBO, pueden describirse como mecanismos que comparten el mismo protocolo propuesto, y solo difieren en las estrategias empleadas para los pasos de razonamiento por ronda de negociación como se rige por el protocolo. Es importante destacar que esto significa que podría proporcionar un marco unificado para DCSP que no solo proporciona una visión teórica teórica de agente BDI más clara de los enfoques DCSP existentes, sino que también abre las oportunidades para mejorar o desarrollar nuevas estrategias. Hacia este último, hemos propuesto y formulado una nueva estrategia: la estrategia de UMA. Los resultados empíricos y nuestra discusión sugieren que UMA es superior a ABT, AWC y DBO en algunos aspectos específicos. De nuestras simulaciones se observó que Uma posee la propiedad de integridad. El trabajo futuro intentará establecer formalmente esta propiedad, así como formalizar otros algoritmos DSCP existentes como mecanismos de negociación BDI, incluido el reciente esfuerzo que emplea a un mediador grupal [5]. También se investigará la idea de que los agentes DCSP que usen diferentes estrategias en el mismo entorno.7. Referencias [1] P. J. Modi, H. Jung, M. Tambe, W.-M.Shen y S. Kulkarni, Asignación dinámica de recursos distribuidos: un enfoque de satisfacción de restricciones distribuidas, en Notas de conferencias en Ciencias de la Computación, 2001, p.264. [2] H. Schlenker y U. Geske, simulando grandes redes ferroviarias utilizando la satisfacción de restricciones distribuidas, en la segunda conferencia internacional IEEE sobre informática industrial (Indin-04), 2004, pp. 441-446. [3] M. Yokoo, Satisfacción de restricción distribuida: fundamentos de cooperación en sistemas de agentes múltiples. Springer Verlag, 2000, Serie Springer sobre tecnología de agentes.[4] M. Yokoo, E. H. Durfee, T. Ishida y K. Kuwabara, El problema de satisfacción de restricciones distribuidas: formalización y algoritmos, transacciones IEEE sobre conocimiento e ingeniería de datos, vol.10, no.5, pp. 673-685, septiembre/octubre de 1998. [5] R. Mailler y V. Lesser, utilizando mediación cooperativa para resolver problemas distribuidos de satisfacción de restricciones, en los procedimientos de la tercera conferencia conjunta internacional sobre agentes autónomos y sistemas multiagentes (AAMAS (AAMAS-04), 2004, pp. 446-453.[6] E. Tsang, Fundación de satisfacción de restricción. Academic Press, 1993. [7] R. Mailler, R. Vincent, V. Lesser, T. Middlekoop y J. Shen, en tiempo real suave, negociación cooperativa para la asignación de recursos distribuidos, Simposio AAAI Fall sobre métodos de negociación autónomos para la cooperativa autónomaSystems, noviembre de 2001. [8] M. Yokoo, K. Suzuki y K. Hirayama, Satisfacción de restricción distribuida segura: llegar a un acuerdo sin revelar información privada, Inteligencia Artificial, vol.161, no.1-2, pp. 229-246, 2005. [9] J. S. Rosenschein y G. Zlotkin, Reglas de encuentro. The MIT Press, 1994. [10] M. Yokoo y K. Hirayama, Algoritmo distribuido de satisfacción de restricciones para problemas locales complejos, en las actas de la tercera conferencia internacional sobre sistemas multiagentes (ICMAS-98), 1998, pp. 372-379.[11] M. E. Bratman, Intenciones, planes y razones prácticas. Harvard University Press, Cambridge, M.A, 1987. [12] G. Weiss, ed., Sistema multiagente: un enfoque moderno para la inteligencia artificial distribuida. The MIT Press, Londres, Reino Unido, 1999. [13] S. Minton, M. D. Johnson, A. B. Philips y P. Laird, Minimización de conflictos: un método de reparación heurística para la satisfacción de la restricción y los problemas de programación, Inteligencia Artificial, vol.E58, no.1-3, pp. 161-205, 1992. El sexto intl. Conf.Sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 531