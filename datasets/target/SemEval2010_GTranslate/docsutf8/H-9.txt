Aprenda de los registros de búsqueda web para organizar los resultados de búsqueda Xuanhui Wang Departamento de Ciencias de la Computación de la Universidad de Illinois en Urbana-Champaign Urbana, IL 61801 xwang20@cs.uiuc.edu Chengxiang Zhai Departamento de Ciencias de la Computación de la Universidad de Illinois en Urbana Champaign Urbana, il 61801czhai@cs.uiuc.edu Resumen La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Los resultados de búsqueda de agrupación son una forma efectiva de organizar los resultados de búsqueda, lo que permite a un usuario navegar por documentos relevantes rápidamente. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no se corresponden necesariamente a los aspectos interesantes de un tema desde la perspectiva de los usuarios;y (2) las etiquetas de clúster generadas no son lo suficientemente informativas como para permitir que un usuario identifique el clúster correcto. En este documento, proponemos abordar estas dos deficiencias al aprender aspectos interesantes de un tema de los registros de búsqueda web y organizar los resultados de búsqueda en consecuencia;y (2) generar etiquetas de clúster más significativas utilizando palabras de consulta pasadas ingresadas por los usuarios. Evaluamos nuestro método propuesto en los datos de registro de motor de búsqueda comercial. En comparación con los métodos tradicionales para agrupar los resultados de búsqueda, nuestro método puede dar una mejor organización de resultados y etiquetas más significativas. Categorías y descriptores de sujetos: H.3.3 [Búsqueda y recuperación de información]: agrupación, proceso de búsqueda Términos generales: algoritmo, experimentación 1. Introducción La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, cómo organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de resultados de búsqueda. La estrategia más común de presentar los resultados de búsqueda es una lista simple de clasificación. Intuitivamente, dicha estrategia de presentación es razonable para resultados de búsqueda homogéneos y no ambiguos;En general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados más clasificados. Sin embargo, cuando los resultados de búsqueda son diversos (por ejemplo, debido a la ambigüedad o múltiples aspectos de un tema), como suele ser el caso en la búsqueda web, la presentación de la lista clasificada no sería efectiva;En tal caso, sería mejor agrupar los resultados de búsqueda en grupos para que un usuario pueda navegar fácilmente hacia un grupo interesante particular. Por ejemplo, los resultados en la primera página regresaron de Google para la consulta ambigua Jaguar (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de Jaguar (es decir, automóvil, animal, software y un equipo deportivo);Incluso para una consulta más refinada, como la imagen del equipo de Jaguar, los resultados siguen siendo bastante ambiguos, incluidos al menos cuatro equipos de Jaguar diferentes: un equipo de lucha libre, un equipo de autos de Jaguar, el equipo de softbol Jaguar de Jacksonville Southwestern y el equipo de fútbol Jaguar Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software Jaguar, una consulta como la descarga de Jaguar tampoco es muy efectivo ya que los resultados dominantes se tratan de descargar el folleto de Jaguar, el fondo de pantalla Jaguar y el DVD de Jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una lista simple de clasificación. La agrupación también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, un usuario tendría que pasar por una larga lista secuencialmente para llegar al primer documento relevante. Como una estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda se ha estudiado de manera relativamente amplia [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar la agrupación en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en grupos naturales, que a menudo corresponden a diferentes subtópicos del tema de consulta general. Se generará una etiqueta para indicar de qué se trata cada clúster. Un usuario puede ver las etiquetas para decidir qué clúster buscar. Se ha demostrado que dicha estrategia es más útil que la presentación de la lista clasificada simple en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que no siempre funcionan bien: primero, los grupos descubiertos de esta manera no se corresponden necesariamente a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios a menudo están interesados en encontrar códigos telefónicos o códigos postales al ingresar los códigos de área de consulta. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales grupos no serían muy útiles para los usuarios;Incluso el mejor grupo todavía tendría una precisión baja. En segundo lugar, las etiquetas de clúster generadas no son lo suficientemente informativas como para permitir que un usuario identifique el clúster correcto. Hay dos razones para este problema: (1) los grupos no se corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles.(2) Incluso si un clúster realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un clúster, y es posible que el usuario no esté muy familiarizado con algunos delos términos. Por ejemplo, la consulta ambigua Jaguar puede significar un animal o un automóvil. Un grupo puede ser etiquetado como Panthera Onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de Jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este documento, proponemos una estrategia diferente para dividir los resultados de búsqueda, que aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, tratamos de descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos ver qué tipos de páginas vistas por los usuarios en los resultados y qué tipo de palabras se usan junto con dicha consulta. En caso de que la consulta sea ambigua, como Jaguar, podemos esperar ver algunos grupos claros que corresponden diferentes sentidos de Jaguar. Más importante aún, incluso si una palabra no es ambigua (por ejemplo, automóvil), aún podemos descubrir aspectos interesantes como el alquiler de automóviles y los precios del automóvil (que resultan ser los dos aspectos principales descubiertos en nuestros datos del registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre el automóvil. Tenga en cuenta que en el caso del automóvil, los grupos generados utilizando la agrupación regular pueden no reflejar necesariamente aspectos tan interesantes sobre el automóvil desde la perspectiva de un usuario, a pesar de que los grupos generados son coherentes y significativos de otras maneras. En segundo lugar, generaremos etiquetas de clúster más significativas utilizando palabras de consulta pasadas ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dado el tema de consulta actual, también podríamos esperar que esas palabras de consulta ingresadas por usuarios en el pasado que están asociados con la consulta actual pueden proporcionar descripciones significativas de laaspectos distintos. Por lo tanto, pueden ser mejores etiquetas que las extraídas del contenido ordinario de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en registros de motores de búsqueda y creamos una colección de historial que contiene las consultas anteriores y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas de la colección de historial y aprendemos aspectos a través de la aplicación del algoritmo de agrupación Star [2] a estas consultas y clics pasados. Luego podemos organizar los resultados de búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto por la consulta pasada más representativa en el clúster de consulta. Evaluamos nuestro método para la organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y la agrupación tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consulta pasadas son más legibles que las generadas utilizando enfoques de agrupación tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos del registro de motores de búsqueda y nuestro procedimiento para construir una recopilación de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro documento y discutimos el trabajo futuro en la Sección 7. 2. Trabajo relacionado Nuestro trabajo está estrechamente relacionado con el estudio de los resultados de búsqueda de agrupación. En [9, 15], los autores usaron algoritmo de dispersión/recopilación para agrupar los documentos superiores devueltos de un sistema de recuperación de información tradicional. Sus resultados validan la hipótesis del grupo [20] que los documentos relevantes tienden a formar grupos. El mero de sistema se describió en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basado en los fragmentos o el contenido de los documentos devueltos. Se comparan varios algoritmos de agrupación y se demostró que el algoritmo de agrupación de árboles sufijo (STC) es el más efectivo. También demostraron que usar fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante de la agrupación de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron los algoritmos de aprendizaje supervisados para extraer frases significativas de los fragmentos de resultados de búsqueda y estas frases se usaron para agrupar los resultados de búsqueda. En [13], los autores propusieron usar un algoritmo de agrupación monotética, en el que se asigna un documento a un clúster basado en una sola característica, organizar los resultados de búsqueda, y la característica única se utiliza para etiquetar el clúster correspondiente. Los resultados de búsqueda de agrupación también han atraído mucha atención en los servicios web de la industria y los comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de búsqueda. Por lo tanto, los grupos obtenidos no reflejan necesariamente las preferencias de los usuarios y las etiquetas generadas pueden no ser informativas desde el punto de vista de los usuarios. Los métodos para organizar los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, un clasificador de texto está capacitado utilizando un directorio web y los resultados de búsqueda se clasifican luego en las categorías predefinidas. Los autores diseñaron y estudiaron interfaces de categoría diferentes y descubrieron que las interfaces de categoría son más efectivas que las interfaces de la lista. Sin embargo, las categorías predefinidas a menudo son demasiado generales para reflejar los aspectos de granularidad más finos de una consulta. Los registros de búsqueda han sido explotados para varios propósitos diferentes en el pasado. Por ejemplo, las consultas de búsqueda de agrupación para encontrar esas preguntas frecuentes (preguntas frecuentes) se estudian en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño del sitio web [3], análisis semántico latente [23] y funciones de clasificación de recuperación de aprendizaje [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasada para organizar mejor los resultados de búsqueda para futuras consultas. Utilizamos el algoritmo de clúster Star [2], que es un enfoque basado en la partición gráfica, para aprender aspectos interesantes de los registros de búsqueda dada una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de manera específica de consulta y esta es otra diferencia de los trabajos anteriores, como [24, 4] en el que todas las consultas en los registros se agrupan de manera lota.3. Registros de motores de búsqueda Los registros de los motores de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses de los usuarios reales al realizar la consulta de identificación URL TIME 1 Gane zip http://www.winzip.com XXXX 1 Win Zip http: //www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Entradas de muestra de registros de motores de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda Web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL en las que hicieron clic después de enviar las consultas y el momento en que hicieron clic. Los registros de los motores de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL en las que un usuario hizo clic después de emitir la consulta [24]. En la Tabla 1 se muestra una pequeña muestra de datos de registro de búsqueda. Nuestra idea de usar registros de motores de búsqueda es tratar estos registros como historial pasado, aprender los intereses de los usuarios utilizando estos datos del historial automáticamente y representar sus intereses por consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con el automóvil y esto refleja que una gran cantidad de usuarios están interesados en la información sobre el automóvil. Probablemente diferentes usuarios estén interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un automóvil, por lo que pueden enviar una consulta como el alquiler de automóviles;Algunos están más interesados en comprar un automóvil usado, y pueden enviar una consulta como un automóvil usado;Y a otros pueden preocuparse más por comprar un accesorio para automóvil, por lo que pueden usar una consulta como Audio de automóviles. Al extraer todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente sean interesantes desde la perspectiva de los usuarios. Como ejemplo, los siguientes son algunos aspectos sobre el automóvil aprendido de nuestros datos del registro de búsqueda (ver Sección 5).1. Alquiler de automóviles, alquiler de automóviles Hertz, alquiler de automóviles empresariales, ... 2. Precios de automóviles, automóvil usado, valores de automóvil, ... 3. accidentes automovilísticos, accidentes automovilísticos, naufragios de automóviles, ... 4. Audio de automóvil, automóvilestéreo, altavoz de coche, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros sin procesar para crear una recopilación de datos del historial. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de la página web que se hace clic, junto con el momento en que el usuario hizo los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas como para decir el significado previsto de una consulta enviada con precisión. Para recopilar información rica, enriquecemos cada URL con contenido de texto adicional. Específicamente, dada la consulta en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del que obtuvimos nuestros datos de registro y extraemos los fragmentos de las URL que se hacen clic de acuerdo con la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web que hacen clic de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la escasez de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas en sí mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra recopilación de datos de historia, que se utiliza para aprender aspectos interesantes en la siguiente sección.4. Nuestro enfoque Nuestro enfoque es organizar los resultados de búsqueda por aspectos aprendidos de los registros de motores de búsqueda. Dada una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtenga su información relacionada de los registros de motores de búsqueda. Toda la información forma un conjunto de trabajo.2. Aprenda aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dada la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa.3. Clasifique y organice los resultados de búsqueda de la consulta de entrada de acuerdo con los aspectos aprendidos anteriormente. Ahora damos una presentación detallada de cada paso.4.1 Encontrar consultas anteriores relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber qué están realmente interesados a los usuarios dada esta consulta, primero recuperamos sus consultas similares en nuestra recopilación de datos del historial preprocesado. Formalmente, supongamos que tenemos n pseudo-documentos en nuestro conjunto de datos del historial: h = {q1, q2, ..., qn}. Cada Qi corresponde a una consulta única y se enriquece con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con QS en H, una forma natural es usar un algoritmo de recuperación de texto. Aquí usamos el método Okapi [17], uno de los métodos de recuperación de última generación. Específicamente, usamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi: w∈Q ¡qi c (w, q) × idf (w) × (k1 + 1) × c (w, qi) k1((1 - b) + b | qi | avdl) + c (w, qi) donde k1 y b son parámetros de okapi establecidos empíricamente, c (w, qi) y c (w, q) son el recuento de palabras w enQi y Q respectivamente, IDF (W) es la frecuencia de documento inversa de la palabra W, y AVDL es la longitud promedio del documento en nuestra colección de historia. Según los puntajes de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos que los usuarios generalmente están interesados. Cada documento en H corresponde a una consulta pasada y, por lo tanto, los documentos de alto nivel corresponden a consultas pasadas relacionadas con QS.4.2 Aspectos de aprendizaje por agrupación Dada una consulta Q, usamos HQ = {D1, ..., DN} para representar los pseudo-documentos clasificados de la colección del historial H. Estos pseudo-documentos contienen los aspectos que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupación para descubrir estos aspectos. Cualquier algoritmo de agrupación podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en la partición gráfica: el algoritmo de agrupación de estrellas [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir una buena etiqueta para cada clúster de forma natural. Describimos el algoritmo de agrupación de estrellas a continuación.4.2.1 Agrupación de estrellas Dada HQ, Star Clustering comienza con la construcción de un gráfico de similitud de pareja en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Luego, los grupos están formados por subgrafías densas que tienen forma de estrella. Estos grupos forman una cubierta del gráfico de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} En la colección HQ, calculamos un vector TF-IDF. Luego, para cada par de documentos DI y DJ (i = j), su similitud se calcula como la puntuación coseno de sus vectores correspondientes VI y VJ, es decir, sim (di, dj) = cos (vi, vj) = vi ·VJ | VI |· | VJ |. Un gráfico de similitud Gσ se puede construir de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento DI es un vértice de Gσ. Si SIM (DI, DJ)> σ, habría un borde que conecta los dos vértices correspondientes. Después de construir el gráfico de similitud Gσ, el algoritmo de agrupación de estrellas agrupa los documentos utilizando un algoritmo codicioso de la siguiente manera: 1. Asociar cada vértice en Gσ con una bandera, inicializado como sin marcar.2. De esos vértices sin marcar, encuentre el que tiene el más alto grado y deje que sea u.3. Marque la bandera de u como centro.4. Forma un grupo C que contiene U y todos sus vecinos que no están marcados como centro. Marque todos los vecinos seleccionados como satélites.5. Repita desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo tiene forma de estrella, que consiste en un solo centro y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupación de estrellas. Un gran σ aplica que los documentos conectados tienen altas similitudes y, por lo tanto, los grupos tienden a ser pequeños. Por otro lado, un pequeño σ hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de clúster Star es que genera un centro para cada clúster. En el pasado de la colección de consultas, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el clúster y, por lo tanto, proporciona una etiqueta para el clúster de forma natural. Todos los grupos obtenidos están relacionados con la consulta de entrada Q desde diferentes perspectivas, y representan los posibles aspectos de los intereses sobre la consulta Q de los usuarios.4.3 Clasificación de resultados de búsqueda Para organizar los resultados de búsqueda de acuerdo con los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas anteriores relacionadas para clasificar los resultados de búsqueda. Dadas las páginas web TOP M devueltas por un motor de búsqueda para Q: {S1, ..., SM}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización se puede usar aquí. Aquí utilizamos un método simple basado en centroides para la categorización. Naturalmente, se puede esperar que los métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basado en los pseudo-documentos en cada aspecto descubierto CI, construimos un prototipo de Centroide Pi tomando el promedio de todos los vectores de los documentos en CI: PI = 1 | CI |l∈Ci vl. Todos estos PI se utilizan para clasificar los resultados de búsqueda. Específicamente, para cualquier resultado de búsqueda SJ, construimos un vector TF-IDF. El método basado en centroides calcula la similitud cosena entre la representación vectorial de SJ y cada prototipo Centroide PI. Luego asignamos SJ al aspecto con el que tiene el puntaje de similitud de coseno más alto. Todos los aspectos finalmente se clasifican de acuerdo con el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican de acuerdo con su clasificación original del motor de búsqueda.5. Recopilación de datos Construimos nuestro conjunto de datos en función del conjunto de datos de registro de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, este registro de datos abarca 31 días desde el 01/05/2006 al 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, separamos todo el conjunto de datos en dos partes de acuerdo con la hora: los primeros 2/3 de datos se utilizan para simular los datos históricos que un motor de búsqueda acumuló, y usamos los últimos 1/3 para simular consultas futuras. En la recopilación de la historia, limpiamos los datos solo manteniendo esas consultas en inglés frecuentes, bien formadas y bien formadas (consultas que solo contienen los caracteres A, B, ..., Z y el espacio, y aparecen más de 5 veces). Después de la limpieza, obtenemos 169,057 consultas únicas en nuestra recopilación de datos de historia totalmente. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra recopilación de historial es de 129 MB. Construimos nuestros datos de prueba a partir de los últimos 1/3 de datos. Según el tiempo, separamos estos datos en dos conjuntos de pruebas por igual para la validación cruzada para establecer parámetros. Para cada conjunto de pruebas, usamos cada sesión como caso de prueba. Cada sesión contiene una sola consulta y varios clics.(Tenga en cuenta que no agregamos sesiones para casos de prueba. Diferentes casos de prueba pueden tener las mismas consultas pero posiblemente diferentes clics). Dado que es inviable pedirle al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por usar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda para ver qué tan bien estos algoritmos pueden ayudar a los usuarios a alcanzar las URL en los que se hacen clic. Se espera que la organización de los resultados de búsqueda en diferentes aspectos ayude a consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos esos casos de prueba con menos de 4 clics bajo el supuesto de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no pueden recuperar al menos 100 pseudocdocumentos de nuestra colección de historial. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente.6. Experimentos En la sección, describimos nuestros experimentos en la organización de resultados de búsqueda basados en registros de motores de búsqueda anteriores.6.1 Diseño experimental Utilizamos dos métodos de referencia para evaluar el método propuesto para organizar los resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (línea de base). El segundo método es organizar los resultados de búsqueda agrupándolos (basados en clúster). Para una comparación justa, utilizamos el mismo algoritmo de agrupación que nuestro método de base log (es decir, agrupación estelar). Es decir, tratamos cada resultado de la búsqueda como un documento, construimos el gráfico de similitud y encontramos los grupos en forma de estrella. Comparamos nuestro método (basado en log) con los dos métodos de referencia en los siguientes experimentos. Para los métodos basados en clúster y basados en log, los resultados de búsqueda dentro de cada clúster se clasifican en función de su clasificación original dada por el motor de búsqueda. Para comparar los diferentes métodos de organización de resultados, adoptamos un método similar al del documento [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como el que tiene el mayor número de documentos relevantes. Organizar los resultados de búsqueda en clústeres es ayudar a los usuarios a navegar en documentos relevantes rápidamente. La métrica anterior es simular un escenario cuando los usuarios siempre eligen el clúster correcto y lo analicen. Específicamente, descargamos y organizamos los 100 mejores resultados de búsqueda en aspectos para cada caso de prueba. Usamos precisión en 5 documentos (p@5) en el mejor grupo como medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos dice la precisión percibida cuando el usuario abre un clúster y mira los primeros 5 documentos. También utilizamos el rango recíproco medio (MRR) como otra métrica. MRR se calcula como MRR = 1 | T |q∈T 1 rq donde t es un conjunto de consultas de prueba, RQ es el rango del primer documento relevante para q. Para dar una comparación justa entre diferentes algoritmos de organización, forzamos métodos basados en clúster y basados en log para producir el mismo número de aspectos y obligar a cada resultado de la búsqueda a estar en un solo aspecto. El número de aspectos se fija en 10 en todos los siguientes experimentos. El algoritmo de agrupación STAR puede generar diferentes grupos de grupos para diferentes entradas. Para restringir el número de grupos a 10, ordenamos todos los grupos por sus tamaños, seleccione los 10 principales como candidatos de aspecto. Luego reasignamos cada resultado de la búsqueda a uno de estos 10 aspectos seleccionados que tiene la puntuación de mayor similitud con el centroide de aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores que 5, y esto asegura que P@5 sea una métrica significativa.6.2 Resultados experimentales Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una lista simple o resultados de búsqueda de clúster. A continuación, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Método de prueba Conjunto de prueba 1 Conjunto de prueba 2 mmr p@5 mmr p@5 línea de base 0.7347 0.3325 0.7393 0.3288 0.7735 0.3162 0.7666 0.2994 0.7833 0.3534 0.7697 0.33389 clúster/basal 5.28% -4.87% 3.69% --8.9.93.93% en línea6.62% 6.31% 4.10% 3.09% log/clúster 1.27% 11.76% 0.40% 13.20% Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de prueba de comparación 1 Conjunto de prueba 2 impr./DECR. Impr./Decr. Clúster/Baseleta 53/55 50/64 Log/Basora 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación por pares W.R.T El número de casos de prueba cuyos P@5s mejoran versus disminución de W.R.T la línea de base.6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica del motor de búsqueda (línea de base), método basado en la agrupación tradicional (basado en clúster) y nuestro método basado en log (basado en registros), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección individualmente en función de los valores P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que la línea de base y los métodos basados en clúster. Por ejemplo, en la primera colección de pruebas, el método de referencia de MMR es 0.734, el método basado en clúster es 0.773 y nuestro método es 0.783. Logramos una mayor precisión que el método basado en clúster (1,27% de mejora) y el método de referencia (mejora del 6,62%). Los valores P@5 son 0.332 para la línea de base, 0.316 para el método basado en clúster, pero 0.353 para nuestro método. Nuestro método mejora sobre la línea de base en un 6,31%, mientras que el método basado en clúster incluso disminuye la precisión. Esto se debe a que el método basado en clúster organiza los resultados de búsqueda solo en función del contenido. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis del sesgo del método basado en clúster. Comparando nuestro método con el método basado en clúster, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto muestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios del historial de consultas anteriores y, por lo tanto, puede organizar los resultados de búsqueda de una manera más útil a los usuarios. Mostramos los resultados óptimos anteriores. Para probar la sensibilidad del parámetro σ de nuestro método basado en log, usamos uno de los conjuntos de pruebas para ajustar el parámetro para que sea óptimo y luego usamos el parámetro sintonizado en el otro conjunto. Comparamos este resultado (registro afuera afuera) con los resultados óptimos de los métodos basados en clúster (optimizados en clúster) y basados en log (optimizado de registro) en la Figura 1. Podemos ver que, como se esperaba, el rendimiento que usa el parámetro sintonizado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método aún funciona mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas.0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de prueba 1 Conjunto de prueba 2 P@5 Registro optimizado de registro optimizado ajustado fuera de la Figura 1: Resultados usando parámetros sintonizados de la otra colección de pruebas. Lo comparamos con el rendimiento óptimo de los métodos basados en clúster y nuestros métodos logbased.0 10 20 30 30 40 50 60 1 2 3 4 Número de contenedor #Queries Mejoró la Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos comparaciones por pares de los tres métodos en términos de los números de casos de prueba para los cuales P@5 aumenta en lugar de disminuir. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden mejorarse con nuestro método.6.2.2 Análisis detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultades de consulta. Todo el análisis a continuación se basa en el conjunto de pruebas 1. Análisis de diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para esas consultas cuyos resultados son más diversos, como para tales consultas, los resultados tienden a formar dos o más grupos grandes. Para probar la hipótesis de que el método basado en log ayuda más esas consultas con diversos resultados, calculamos las relaciones de tamaño de los grupos más grandes y más grandes en nuestros resultados basados en log y utilizamos esta relación como un indicador de diversidad. Si la relación es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo que los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de relación [1, 2), [2, 3), [3, 4) y [4, +∞) respectivamente.([i, j) significa que yo ≤ relación <j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5s se mejoran en lugar de disminuir con respecto a la línea de base de clasificación y trazar los números en esta figura. Podemos observar que cuando la relación es más pequeña, el método basado en log puede mejorar más casos de prueba. Pero cuando 0 5 10 15 20 25 30 1 2 3 4 Número de contenedor #Quiteries mejoró la Figura 3: La correlación entre el cambio de rendimiento y la dificultad de la consulta.La relación es grande, el método basado en log no puede mejorar sobre la línea de base. Por ejemplo, en Bin 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en Bin 4, todos los 4 casos de prueba disminuyen. Esto confirma nuestra hipótesis de que nuestro método puede ayudar más si la consulta tiene resultados más diversos. Esto también sugiere que debemos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la relación de tamaño del clúster). Análisis de dificultad: se han estudiado consultas difíciles en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método para ayudar a consultas difíciles. Cuantificamos la dificultad de consulta por la precisión promedio media (MAP) de la clasificación del motor de búsqueda original para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de prueba 1 en un orden creciente de los valores de MAP. Participamos los casos de prueba en 4 contenedores con cada uno con un número aproximadamente igual de casos de prueba. Un pequeño mapa significa que la utilidad de la clasificación original es baja. Bin 1 contiene esos casos de prueba con los mapas más bajos y el contenedor 4 contiene esos casos de prueba con los mapas más altos. Para cada contenedor, calculamos el número de casos de prueba cuyos P@5s mejoran en lugar de disminuir. La Figura 3 muestra los resultados. Claramente, en Bin 1, la mayoría de los casos de prueba mejoran (24 frente a 3), mientras que en Bin 4, el método basado en log puede disminuir el rendimiento (3 vs 20). Esto muestra que nuestro método es más beneficioso para las consultas difíciles, lo cual es el esperado ya que agrupar los resultados de búsqueda está destinado a ayudar a consultas difíciles. Esto también muestra que nuestro método realmente no ayuda a consultas fáciles, por lo que debemos desactivar nuestra opción de organización para consultas fáciles.6.2.3 Configuración de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupación de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación de Okapi, estudiamos los parámetros K1 y B. También estudiamos el impacto del número de consultas anteriores recuperadas en nuestro método basado en log. La Figura 4 muestra el impacto del parámetro σ para los métodos basados en clúster y basados en log en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con el paso 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros OKAPI. Variamos K1 de 1.0 a 2.0 con el paso 0.2 y B de 0 a 1 con el paso 0.2. A partir de esta tabla, está claro que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores que 0.35. Los valores predeterminados K1 = 1.2 y B = 0.8 dan resultados aproximadamente óptimos. Además, estudiamos el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 p@5 Similitud TRAVEDIumbral de similitud σ en métodos basados en clúster y basados en log. Mostramos el resultado en ambas colecciones de pruebas.B 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.34104 0.3546 0.3546 1.6 0.6 0.6 0.34476 0.3448 0.3448 3 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546Tabla 4: Impacto de los parámetros de Okapi K1 y B.Información para aprender variando el número de consultas pasadas que se recuperarán para los aspectos de aprendizaje. Los resultados en ambas colecciones de prueba se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que ampliamos el número de consultas anteriores recuperadas. Por lo tanto, nuestro método podría aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, cada vez más consultas tendrán suficiente historia, por lo que podemos mejorar más y más consultas.6.2.4 Un ejemplo ilustrativo Usamos los códigos de área de consulta para mostrar la diferencia en los resultados del método basado en log y el método basado en clúster. Esta consulta puede significar códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres grupos más grandes de ambos métodos. En el método basado en clúster, los resultados se dividen en función de ubicaciones: local o internacional. En el método basado en registros, los resultados se desambigan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que usan dicha consulta a menudo están interesados en códigos telefónicos o códigos postales.Dado que los valores P@5 de los métodos basados en clúster y basados en log son 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia sus resultados deseados. Método basado en clúster Método basado en el registro Ciudad, teléfono estatal, ciudad, local internacional, teléfono de área, marcación de zip internacional, Tabla postal 5: un ejemplo que muestra la diferencia entre el método basado en clúster y nuestro método basado en log 0.16 0.18 0.2 0.220.24 0.26 0.28 0.3 1501201008050403020 P@5 #Queries Recuperado Conjunto de prueba 1 Test Set 2 Figura 5: El impacto del número de consultas anteriores recuperadas.6.2.5 Comparación de etiquetado Ahora comparamos las etiquetas entre el método basado en clúster y el método basado en log. El método basado en clúster debe confiar en las palabras clave extraídas de los fragmentos para construir la etiqueta para cada clúster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clúster, contamos la frecuencia de una palabra clave que aparece en un clúster y usamos las palabras clave más frecuentes como la etiqueta del clúster. Para el método basado en log, utilizamos el centro de cada clúster Star como la etiqueta para el clúster correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de clúster automáticamente. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clúster y los métodos basados en log. En la Tabla 6, enumeramos las etiquetas de los 5 mejores grupos para dos ejemplos Jaguar y Apple. Para el método basado en clúster, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en consultas de usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupación. Comparación de etiquetas para consulta Jaguar Método basado en log Método basado en clúster 1. Jaguar Animal 1. Jaguar, Auto, Accesorios 2. Jaguar Accesorios automáticos 2. Jaguar, Tipo, Precios 3. Jaguar Cats 3. Jaguar, Pantera, Cats 4. JaguarReparación 4. Jaguar, Servicios, Boston 5. Jaguar Animal Pictures 5. Jaguar, Colección, comparación de etiquetas de ropa para consulta Método basado en el clúster basado en el registro de Apple 1. Computadora de Apple 1. Apple, soporte, producto 2. Apple iPod 2.Apple, sitio, computadora 3. Receta de Apple Crisp 3. Apple, Mundo, Visita 4. Pastel de Apple Fresh 4. Apple, iPod, Amazon 5. Laptop de Apple 5. Apple, Productos, Noticias Tabla 6: Comparación de etiquetas de clúster.7. Conclusiones y trabajo futuro En este documento, estudiamos el problema de organizar los resultados de búsqueda de manera orientada al usuario. Para alcanzar este objetivo, confiamos en registros de motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas del historial de consultas pasado, aprendemos los aspectos agrupando las consultas pasadas y la información de clics asociada y clasificamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clúster y la línea de base de la clasificación del motor de búsqueda. Los experimentos muestran que nuestro método basado en log puede superar constantemente el método basado en clúster y mejorar la línea de base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspecto más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda cuando clúster los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: primero, aunque los resultados de nuestros experimentos han mostrado claramente la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. En segundo lugar, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información informativa de comentarios de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Por lo tanto, sería interesante estudiar cómo mejorar aún más la organización de los resultados en función de dicha información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual.8. Agradecimientos Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo está en parte respaldado por una beca de investigación de Microsoft Live Labs, una beca de investigación en Google y una subvención de carrera NSF IIS-0347933.9. Referencias [1] E. Agichtein, E. Brill y S. T. Dumais. Mejora de la clasificación de búsqueda web incorporando información de comportamiento del usuario. En Sigir, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupación de estrellas para la organización de información estática y dinámica. Journal of Graph Algorithms and Applications, 8 (1): 95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Clustering aglomerativo de un registro de consulta de motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En Sigir, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Llevar el orden a la web: categorizar automáticamente los resultados de búsqueda. En Chi, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predecir el rendimiento de la consulta. En Actas de ACM Sigir 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimización de la búsqueda mostrando resultados en contexto. En Chi, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del grupo: dispersión/recolección en los resultados de recuperación. En Sigir, páginas 76-84, 1996. [10] T. Joachims. Optimización de los motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de la recuperación utilizando datos de clics., Páginas 79-96. Physica/Springer Verlag, 2003. En J. Franke y G. Nakhaeizadeh e I. Renz, minería de texto.[12] R. Jones, B. Rey, O. Madani y W. Greiner. Generación de sustituciones de consulta. En www, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo de agrupación de documentos monotéticos jerárquicos para resumir y navegar por los resultados de búsqueda. En www, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerar la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/rfps/ Buscar 2006 rfp.aspx.[15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recopilación comunica la estructura del tema de una colección de texto muy grande. En Chi, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: Aprender a clasificarse a partir de comentarios implícitos. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples efectivas al modelo de 2 poisson para la recuperación ponderada probabilística. En Sigir, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para la indexación automática. Comun. ACM, 18 (11): 613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando comentarios implícitos. En Sigir, páginas 43-50, 2005. [20] C. J. Van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo.http://vivisimo.com/.[23] X. Wang, J.-T.Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de tipo múltiple. En Sigir, páginas 236-243, 2006. [24] J.-R.Wen, J.-Y. Nie y H. Zhang. Agrupación de consultas de usuario de un motor de búsqueda. En www, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprender a estimar la dificultad de consulta: incluidas las aplicaciones para faltar detección de contenido y recuperación de información distribuida. En Sigir, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Clustering de documentos web: una demostración de factibilidad. En Sigir, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: una interfaz de agrupación dinámica para los resultados de búsqueda web. Computer Networks, 31 (11-16): 1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma y J. Ma. Aprender a agrupar los resultados de búsqueda web. En Sigir, páginas 210-217, 2004.