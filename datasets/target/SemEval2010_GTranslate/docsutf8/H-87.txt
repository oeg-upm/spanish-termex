Robustez de los métodos de filtrado adaptativo en una evaluación transversal Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel School of Computer Science, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. Resumen Este documento informa una evaluación transversal de la evaluación de billetes de deRegresión logística regularizada (LR) y rocho incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de detección y seguimiento de temas (TDT) y las conferencias de recuperación de texto (TREC) evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Descubrimos que LR funciona con fuerza y robusta en la optimización de T11SU (una función de utilidad TREC), mientras que Rocchio es mejor para optimizar CTRK (el costo de seguimiento de TDT), una función objetivo orientada de alta recuperación. Utilizando la optimización sistemática de parámetros de Cross-Corpus con ambos métodos, obtuvimos los mejores resultados jamás informados en TDT5, TREC10 y TREC11. La retroalimentación de relevancia sobre una pequeña porción (0.05 ~ 0.2%) de los documentos de prueba TDT5 arrojaron mejoras de rendimiento significativas, midiendo una reducción de hasta 54% en CTRK y un aumento del 20.9% en T11SU (con β = 0.1), en comparación con los resultados deEl sistema de rendimiento superior en TDT2004 sin información de retroalimentación de relevancia. Categorías y descriptores de temas H.3.3 [Búsqueda y recuperación de información]: filtrado de información, comentarios de relevancia, modelos de recuperación, proceso de selección;I.5.2 [Metodología de diseño]: Diseño y evaluación del clasificador Algoritmos de términos generales, medición, rendimiento, experimentación 1. Introducción El filtrado adaptativo (AF) ha sido un tema de investigación desafiante en la recuperación de la información. La tarea es que el sistema tome una decisión de membresía del tema en línea (sí o no) para cada documento, tan pronto como llega, con respecto a cada tema de interés predefinido. A partir de 1997, en el área de detección y seguimiento del tema (TDT) y 1998 en las conferencias de recuperación de texto (TREC), las evaluaciones de referencia han sido realizadas por NIST en las siguientes condiciones [6] [7] [8] [3] [4]: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivo para cada tema en el punto de partida.• La retroalimentación de relevancia estaba disponible, pero solo para los documentos del sistema (con una decisión Sí) en las evaluaciones de TREC para FA.• La retroalimentación de relevancia (RF) no se permitió en las evaluaciones de TDT para la AF (o el seguimiento de temas en la terminología TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas TREC y TDT se usaron conjuntamente para evaluar los métodos de AF en el mismo punto de referencia (El corpus TDT5) donde dominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas en las que se utilizaría un sistema AF. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no poder proporcionar un etiquetado adicional en una pequeña porción de documentos entrantes a través de la retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y los viejos temas reducidos y disminuidos. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) Es difícil aprender modelos precisos para la predicción basadas en datos de capacitación extremadamente escasos;2) No es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación;3) No se entiende bien cómo ajustar los parámetros de manera efectiva en los métodos de AF utilizando la validación de Cross-Corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o diferentes épocas. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para la clasificación por lotes, donde todos los datos de capacitación se dan a la vez. Los dos primeros problemas se han estudiado en la literatura de filtrado adaptativo, incluida la adaptación del perfil del tema utilizando roccio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbral utilizando calibración probabilística o técnicas de ajuste locales [1 1 [1 1] [2] [9] [10] [11] [12] [13]. Aunque estos trabajos proporcionan información valiosa para comprender los problemas y las posibles soluciones, es difícil sacar conclusiones con respecto a la efectividad y la solidez de los métodos actuales porque el tercer problema no se ha investigado a fondo. Abordar el tercer problema es el enfoque principal en este documento. Argumentamos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para la afinación de parámetros en múltiples corpus. La mayoría de los métodos AF tienen parámetros previamente especificados que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Los ejemplos de capacitación disponibles, por otro lado, a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al comienzo;La optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto solo deja una opción (suponiendo que el ajuste en el conjunto de pruebas no sea una alternativa), es decir, elegir un corpus externo como conjunto de validación. Observe que los temas de conjunto de validación a menudo no se superponen con los temas de conjunto de pruebas, por lo tanto, la optimización de los parámetros se realiza en la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿Qué métodos (si los hay) son robustos bajo la condición de usar la validación de Cross-Corpus para ajustar los parámetros? La literatura actual no ofrece una respuesta porque no se ha informado una investigación exhaustiva sobre la robustez de los métodos de AF. En este documento, abordamos la pregunta anterior realizando una evaluación transversal con dos enfoques efectivos en FA: rocio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en FA, con un buen rendimiento en las evaluaciones de referencia (TREC y TDT) si se usan los parámetros apropiados y si se combinan con una estrategia de calibración de umbral efectiva [2] [4] [7] [8] [9][11] [13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15] [14]. Recientemente se evaluó en el filtrado adaptativo y se descubrió que tenía un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de usar cada método solo en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de punto cruzado para pruebas de robustez. Específicamente, nos centramos en cuánto depende el rendimiento de estos métodos de la sintonización de los parámetros, cuáles son los parámetros más influyentes en estos métodos, cuán difícil (o lo fácil) optimizar estos parámetros influyentes utilizando la validación de Corporpus, qué tan fuertes funcionan estos métodos.En múltiples puntos de referencia con el ajuste sistemático de los parámetros en otros corpus, y cuán eficientes son estos métodos en ejecución de AF en grandes corpus de referencia. La organización del documento es la siguiente: la Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La Sección 3 analiza las diferencias entre las métricas TREC y TDT (servicios públicos y costos de seguimiento) y las posibles implicaciones de esas diferencias. La Sección 4 describe los enfoques Rocchio y LR para FA, respectivamente. La Sección 5 informa los experimentos y resultados. La Sección 6 concluye los principales hallazgos en este estudio.2. Benchmark Corporal Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consiste en aproximadamente 806,791 historias de noticias de Reuters desde agosto de 1997 con 84 etiquetas de temas (categorías de temas) [7]. Las primeras dos semanas (del 20 al 31 de agosto de 1996) de los documentos son el conjunto de capacitación, y el 11 y ½ meses restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) es el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, que consiste en el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas TREC11 (50) son bastante diferentes de los de TREC10;Son consultas para la recuperación con juicios de relevancia por parte de los evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en el TDT2001 Dry Run1. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y Pri the World) en el período de octubre a diciembre a diciembre de diciembre a diciembre de diciembre.1998. También se proporcionan versiones traducidas a máquina de las historias no inglesas (Xinhua, Zaobao y VOA mandarina). El punto de división para conjuntos de pruebas de entrenamiento es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas a máquina de las historias no inglesas. Solo usamos las versiones en inglés de esos documentos en nuestros experimentos para este documento. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de sujetos genéricas y siempre duraderas (como las de TREC), los temas de TDT se definen en un nivel más fino de granularidad, para eventos que ocurren en ciertos momentos y ubicaciones, y que nacen y mueren, típicamente asociados con una distribución estalladasobre noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeños que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema TREC (guerras civiles) y dos temas de TDT (reunión de la cumbre de disparos y APEC, respectivamente) durante un período de 3 meses, donde el área bajo cada curva se normaliza a una. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen que la evaluación transversal sea interesante. Por ejemplo, los algoritmos que favorecen los temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones transversales nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales para el filtrado adaptativo en el seguimiento de las tendencias de la deriva de los temas.1 http://www.ldc.upenn.edu/projects/tdt2001/topics.html Tabla 1: Estadísticas de los corpus de referencia para evaluaciones de filtrado adaptativo n (tr) es el número de los documentos de capacitación iniciales;N (TS) es el número de documentos de prueba;N+ es el número de ejemplos positivos de un tema predefinido;* es un promedio sobre todos los temas.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P (Tema | Semana) Gunnshot (TDT5) Reunión de la Cumbre APEC (TDT3) Guerra civil (TREC10) Figura 1:La naturaleza temporal de los temas 3. Métricas Para que nuestros resultados sean comparables a la literatura, decidimos utilizar las métricas de TREC convencionales y convencionales de TDT en nuestra evaluación.3.1 TREC11 Métricas Sea A, B, C y D, respectivamente, el número de verdaderos positivos, falsas alarmas, fallas y negativos verdaderos para un tema específico, y DCBAN +++ = ser el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: precisión)/(Baa +=, recuperación)/(caa +=) (2) 21 (CABA A F +++ + += β β β () η ηηβ ηβ-- +-= =1),/() (Max 11, SUT CABA donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, los puntajes de rendimiento se calculan primero para temas individuales y luego se promedian sobre temas (macroaverables).3.2 Métricas TDT La métrica convencional de TDT para el seguimiento de temas se define como: FamisStrk PTPWPTPWTC)) (1 () () (21-+= donde P (t) es el porcentaje de documentos sobre el tema T, MISS es la tasa de fallas deEl sistema sobre ese tema, FAP es la tasa de falsa alarma, y 1W y 2W son los costos (constantes preespecificadas) para una falla y una falsa alarma, respectivamente. Las evaluaciones de referencia TDT (desde 1997) han utilizado la configuración de 11 = W, 1.02 = W y 02.0) (= TP para todos los temas. Para evaluar el rendimiento de un sistema, CTRK se calcula para cada tema primero y luego los puntajes resultantes se promedian para una sola medida (el CTRK ponderado por el tema). Para que la intuición detrás de esta medida sea transparente, sustituimos los términos en la definición de CTRK de la siguiente manera: n Ca tp + =) (, n db tp + = -) (1, ca c pmiss + =, db b pfa + = =,) (1) (21 21 BWCW N DB B N DB W CA C N CA WTCTRK + ⋅ = + ⋅ + pastorRatio de penalización para fallas frente a falsas alarmas. Además de TRKC, TDT2004 también empleó 1.011 = βsut como métrica de utilidad. Para distinguir esto del 5.011 = βSUT en TREC11, llamamos al ex TDT5SU en el resto de este documento. Corpus #Topics N (TR) N (TS) AVG N+ (TR) AVG N+ (TS) MAX N+ (TS) MIN N+ (TS) #TOPICS POR DOC (TS) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias desde un punto de optimizaciónes una función de costo. Nuestro objetivo es maximizar el primero o minimizar el segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas se pueden analizar a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU se correlacionan positivamente con los valores de A y D, y se correlacionan negativamente con los valores de B y C;La única diferencia entre ellos es en sus proporciones de penalización para las falsificaciones versus falsas alarmas, es decir, 10: 1 en TDT5SU y 2: 1 en T11SU. La función CTRK, por otro lado, se correlaciona positivamente con los valores de C y B, y se correlaciona negativamente con los valores de A y D;Por lo tanto, se correlaciona negativamente con T11SU y TDT5SU. Más importante aún, existe una diferencia sutil y importante entre CTRK y las funciones de utilidad: T11SU y TDT5SU. Es decir, CTRK tiene una relación de penalización muy diferente para Misses frente a falsas alarmas: favorece los sistemas orientados al recuerdo a un extremo. A primera vista, uno pensaría que la relación de penalización en CTRK es 10: 1 desde 11 = W y 1.02 = W. Sin embargo, esto no es cierto si 02.0) (= TP es una estimación inexacta de los documentos en el tema en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje verdadero es: 002.0 37770 3.79) (≈ = + = n n tp donde n es el tamaño promedio de los conjuntos de pruebas en TDT3, y N + es el número promedio de ejemplos positivos por tema en la pruebaconjuntos. Usando 02.0) (ˆ = TP como una estimación (inexacta) de 0.002 amplía la relación de penalización prevista de 10: 1 a 100: 1, más o menos hablando. To wit: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+× elegante =++× - ×+× • × − fue+× • = - ×+× • ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0) () (ˆ = === TP TP ρ es el factor de ampliación en la estimación de P (t) en comparación con la verdad. Comparando el resultado anterior con la Fórmula 2, podemos ver que la relación de penalización real para Misses versus falsas alarmas fue de 100: 1 en las evaluaciones en TDT3 usando CTRK. Del mismo modo, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0) () (ˆ === TP TP ρ, lo que significa la relación de penalización real para las falsificaciones en la evaluación en la evaluaciónen TDT5 usando CTRK fue de aproximadamente 583: 1. Las implicaciones del análisis anterior son bastante significativas: • CTRK definido en la misma fórmula no significa necesariamente la misma función objetivo en la evaluación;En cambio, el criterio de optimización depende del corpus de prueba.• Los sistemas optimizados para CTRK no optimizarían TDT5SU (y T11SU) porque el primero favorece la alta recuperación orientada a un extremo, mientras que el segundo no lo hace.• Los parámetros sintonizados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (por ejemplo, TDT5) a menos que tengamos en cuenta la dependencia sutil previamente desconocida de CTRK de los datos.• Los resultados en CTRK en los últimos años de evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la relación de penalización en CTRK varió. Aunque estos problemas con CTRK no se anticiparon originalmente, ofreció la oportunidad de examinar la capacidad de los sistemas en el comercio de precisión para un retiro extremo. Esta fue una parte desafiante de la evaluación TDT2004 para FA. Comparar las métricas en TDT y TREC desde un punto de vista de la utilidad o la optimización de costos es importante para comprender los resultados de la evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este problema se analiza explícitamente, hasta donde sabemos.4. Métodos 4.1 Rocchio incremental Para AF empleamos una versión común de clasificadores de estilo rocchio que calcula un vector prototipo por tema (t) de la siguiente manera: |) (|) (|) () () () (TD D TD D TQTPTddtdd - ∈+ ∈ ∑∑ -+ -+ = rr rr rr γβα El primer término en el RHS es la representación del vector ponderada de la descripción del tema cuyos elementos son los términos pesos. El segundo término es el centroide ponderado del conjunto) (TD+ de ejemplos de entrenamiento positivo, cada uno de los cuales es un vector de pesos de término con indocumento. El tercer término es el centroide ponderado del conjunto) (TD -de ejemplos de entrenamiento negativo que son los vecinos más cercanos del centroide positivo. Los tres términos reciben pesos preespecificados de βα y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión Sí en un nuevo documento para ese tema. Si la retroalimentación de relevancia está disponible (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de cualquiera de) (TD+ o) (TD−, y el prototipo se recomputa en consecuencia; si la retroalimentación de relevancia no está disponible (comoes el caso en el seguimiento de eventos TDT), la predicción de sistemas (sí) se trata como la verdad, y el nuevo documento se agrega a) (TD+ para actualizar el prototipo. Ambos casos son parte de nuestros experimentos en este documento (y parte de las evaluaciones de TDT 2004 para FA). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y el segundo caso PRF Rocchio, donde PRF representa la retroalimentación de pseudorelevancia. Las predicciones en un nuevo documento se realizan calculando la similitud coseno entre cada prototipo de tema y el vector de documento, y luego comparando las puntuaciones resultantes con un umbral: ⎩ ⎨ ⎧ - + = -) () ())), ((COS(No SÍ DTPSIGN NUEVA La calibración de umbral θ RR en Rocchio incremental es un tema de investigación desafiante. Se han desarrollado múltiples enfoques. Lo más simple es usar un umbral universal para todos los temas, sintonizado en un conjunto de validación y fijado durante la fase de prueba. Los métodos más elaborados incluyen la calibración de umbral probabilístico que convierte los puntajes de similitud no probabilísticos en probabilidades (es decir,) | (DTP R) para la optimización de servicios públicos [9] [13], y la regresión local basada en el margen para la reducción del riesgo [11]. Está más allá del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en términos de cuánto depende su rendimiento del ajuste elaborado del sistema y cuán difícil (o lo fácil) es obtener un buen rendimiento a través de la optimización de los parámetros cruzados. Por lo tanto, decidimos usar una versión relativamente simple de Rocchio como línea de base, es decir, con un umbral universal sintonizado en un corpus de validación y fijado para todos los temas en la fase de prueba. Esta versión simple de Rocchio se ha utilizado comúnmente en las evaluaciones de referencia TDT anteriores para el seguimiento de temas, y tuvo un rendimiento fuerte en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). Los resultados de variantes más complejas de Rocchio también se discuten cuando son correspondientes.4.2 Regresión logística para la regresión logística de AF (LR) estima la probabilidad posterior de un tema dado un documento que usa una función sigmoide) 1/(1), | 1 (XW EWXYP RRRR ⋅− +== donde x R es el vector de documento cuyos elementosson pesos de término, w r es el vector de coeficientes de regresión, y} 1,1 { -+∈Y es la variable de salida correspondiente a sí o no con respecto a un tema en particular. Dado un conjunto de entrenamiento de documentos etiquetados {}), (,) ,, (11 nn yxyxd r l r =, el problema de regresión estándar se define como para encontrar las estimaciones de máxima probabilidad de los coeficientes de regresión (los parámetros del modelo): {}{} {})) exp (1 (1LogMinarg) | (logmaxarg) | (maxarg ii xwyn i w wdp w wdp w mlw rr r r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que puede resolverAlgoritmo de gradiente conjugado en o (inf) tiempo para capacitación por tema, donde yo es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de capacitación y el número de características respectivamente [14]. Una vez que los coeficientes de regresión están optimizados en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: () ⎩ ⎨ ⎧ - + = -) () (), | (no sí wxypsign optnew θ rr señala que w r está constantementeActualizado cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de FA, mientras que el umbral óptimo OPTθ es constante, dependiendo solo de la función de utilidad predefinida (o costo) para la evaluación. Si T11SU es la métrica, por ejemplo, con la relación de penalización de 2: 1 para fallas y falsas alarmas (Sección 3.1), el umbral óptimo para LR es 33.0) 12/(1 =+ para todos los temas. Modificamos la versión estándar (arriba) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ ⎧ - ++ = ∑ = ⋅− 2 1) 1Log () (Minarg μλ Rrr rr r wyysw n i xwy i w mapii donde) (se considera que es α, β y γ para consulta, documentos positivos y negativos respectivamente, que son similares a los de Rocchio, dando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), ON-Documentos temáticos y documentos fuera del tema. El segundo término en la función objetivo es para la regularización, equivalente a agregar un gaussiano antes de los coeficientes de regresión con la matriz de varianza de covarianza media μ r y covarianza ι⋅λ2/1, donde ι es la matriz de identidad. La sintonización λ (≥0) está teóricamente justificada para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el ajuste en exceso en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un problema abierto para la investigación, dependiendo de la creencia de los usuarios sobre el espacio de los parámetros y el rango óptimo. La solución de la función objetivo modificada se denomina estimación máxima a posteriori (MAP), que se reduce a la solución de máxima probabilidad para LR estándar si 0 = λ.5. Evaluaciones Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial de TDT2004, los resultados de la optimización de parámetros cruzados y los resultados correspondientes a las cantidades de retroalimentación de relevancia.5.1 TDT2004 Resultados de referencia Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Participaron múltiples equipos de investigación y se permitieron múltiples carreras de cada equipo. CTRK y TDT5SU se usaron como métricas. La Figura 2 y la Figura 3 muestran los resultados;La mejor carrera de cada equipo fue seleccionada con respecto a CTRK o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero un umbral universal fijo para todos los temas) tuvo el mejor resultado en CTRK, y nuestra regresión logística tuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras carreras se sintonizaron en el corpus TDT3. Los resultados para otros sitios también se enumeran de forma anónima para la comparación. Ctrk our 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (cuanto más bajo el mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.2 0.25 0.3 0.35 0.4 Site2 Site2 Site3 Sitio4 Figura 2: TDT2004 Resultados en CTRK de CTRK de CTRK de CTRK de CTRK de CTRK de los sistemas de los sistemas.(El nuestro es el método Rocchio). También colocamos el primer y tercero cuartiles como palos para cada sitio.2 T11SU OUS 0.7328 SITIO3 0.7281 SITIO2 0.6672 SITO4 0.382 METRIC = TDT5SU (cuanto mayor es mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Site3 Site3 Site2 Figura 4 Figura 3: Figura 3:TDT2004 da como resultado TDT5SU de sistemas utilizando retroalimentación de relevancia verdadera.(El nuestro es LR con 0 = μ R y 005.0 = λ). Ctrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de trato de tema primario en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 OUUSSS Site2 Site5 Site4 Ctrk Figura 4: TDT2004 Resultados de CTRK.(El nuestro es PRF Rocchio). El filtrado adaptativo sin usar la retroalimentación de relevancia verdadera también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todos los procesos de capacitación y prueba, aunque los documentos de prueba no etiquetados podrían usarse tan pronto como se hicieron predicciones sobre ellos. Tal configuración ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra las presentaciones oficiales resumidas de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento.2 Usamos cuartiles en lugar de desviaciones estándar ya que la primera es más resistente a los valores atípicos.5.2 Optimización de parámetros de Cross-Corpus cuánto depende el rendimiento fuerte de nuestros sistemas es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados antes del proceso AF. Los parámetros compartidos incluyen la muestra de pesas α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativo (es decir,) (TD−), el esquema de peso de término y el número máximo de elementos distintos de cero en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ R, λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento TDT5, es imposible optimizar de manera efectiva estos parámetros en los datos de capacitación, y tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (C.F. Sección 2) Porque es más similar a TDT5 en términos de la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y arreglamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones a TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis posterior. Dado que las pruebas exhaustivas de todas las configuraciones de parámetros posibles son computacionalmente intratables, seguimos un procedimiento de encadenamiento hacia adelante paso a paso: prepecificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego sintonizamos un parámetro en el momento en quearreglando la configuración de los parámetros restantes. Repetimos este procedimiento para varios pases según lo permitido.0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 y TREC11 cuando el umbral de decisión varió. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima TDT3 es más cercana al TDT5-Optimal, mientras que el TREC10-Optimal y TREC1-Optimal están bastante lejos de la óptima TDT5. Si estuviéramos usando TREC10 o TREC11 en lugar de TDT3 como el corpus de validación para TDT5, o si el Corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de las puntuaciones ad-hoc (no probabilísticas) generadas por el método Rocchio: la distribución de las puntuaciones depende del corpus, lo que hace que la optimización del umbral de Corpor Corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto al ajuste del umbral porque produce puntajes probabilísticos de) | 1pr (xy = sobre el cual el umbral óptimo puede calcularse directamente si la estimación de probabilidad es precisa. Dada la relación de penalización para Misses frente a las falsas alarmas como 2: 1 en T11SU, 10: 1 en TDT5SU y 583: 1 en CTRK (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de entornos casi óptimos. Con estas configuraciones de umbral en nuestros experimentos para LR, nos centramos en la validación de CrossCorpus de los parámetros previos bayesianos, es decir, μ R y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 usando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 usando TDT5SU. A modo de comparación, también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados informados por NIST para TREC10 y TREC11. A partir de este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ = 0. Este hallazgo empírico es consistente con un informe anterior [13] para LR en TREC11, aunque nuestros resultados de LR (0.585 ~ 0.608 en T11SU) son más fuertes que los resultados (0.49 para LR estándar y 0.54 para LR usando Rocchio Prototype como el anterior)ese informe. Más importante aún, nuestra evaluación transversal da una fuerte evidencia de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de los puntajes generados por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de clasificadores de Rocchio no. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo Rocchio como media en el anterior;En cambio, el rendimiento disminuyó en algunos casos. Esta observación no es compatible con el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio son más precisos que los modelos LR para los temas en la etapa inicial del proceso AF, y creemos que el uso de un RocchioEl prototipo como la media en el Prior Gaussiano introduciría un sesgo indeseable a LR. También creemos que la reducción de la varianza (en la fase de prueba) debe controlarse por la elección de λ (pero no μ r), para los cuales realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes Priors Bayesian Corpus TDT3 TDT5 TREC10 TREC11 LR (μ = 0, λ = 0) 0.7562 0.7737 0.585 0.5715 LR (μ = 0, λ = 0.01) 0.8384 0.7812 0.6077 0.5747 LR (μ = ROC*,λ = 0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77 ~ 0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial TDT2004 (0.73) porque la optimización de los parámetros se ha mejorado después.4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, que no es directamente comparable a los puntajes en T11SU, solo indicativo.*: Se estableció μ r en el prototipo Rocchio 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Rendimiento Ctrk en TDT3 TDT5SU en TDT3 TDT5SU en TDT5 T11SU en TREC11 Figura 6: LR con lambda variando. El rendimiento de LR se resume con respecto a la sintonización λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y CTRK para las ejecuciones en TDT3,. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la LR regularizada es estable, igual o mejorado ligeramente sobre el rendimiento de la LR estándar. En el caso de minimizar CTRK, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 arrojó mejoras relativamente grandes sobre el rendimiento de la LR estándar porque el entrenamiento de un modelo para un retiro extremadamente alto es estadísticamente más complicado y, por lo tanto,Se necesita más regularización. En cualquier caso, la sintonización λ es relativamente segura y fácil de hacer con éxito mediante la sintonización cruzada. Otra opción influyente en la configuración de nuestro experimento es la ponderación de término: examinamos las elecciones de los esquemas binarios, TF y TF-IDF (la versión LTC). Encontramos el TF-IDF más efectivo tanto para Rocchio como para LR, y utilizamos este entorno en todos nuestros experimentos.5.3 Porcentajes de datos etiquetados cuánta retroalimentación de relevancia (RF) se necesitaría durante el proceso AF es una pregunta significativa en las aplicaciones del mundo real. Para responder, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación en absoluto • PRF Rocchio, actualización de perfiles de temas sin usar comentarios de relevancia verdadera;• Rocchio adaptativo, actualización de perfiles de temas utilizando comentarios de relevancia sobre documentos aceptados por el sistema más 10 documentos muestreados aleatoriamente desde el grupo de documentos SystemRected;• LR con 0 RR = μ, 01.0 = λ y umbral = 0.004;• Todos los parámetros en Rocchio sintonizados en TDT3. La Tabla 3 resume los resultados en CTRK: Rocchio adaptativo con retroalimentación de relevancia sobre el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% sobre el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. La LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que CTRK es una métrica de orientación de recuperación extremadamente alta, que causa una actualización frecuente de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón, establecemos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de cálculo inductable. El tiempo de cálculo en las horas de la máquina fue de 0.33 para la ejecución de Rocchio adaptativo y 14 para la ejecución de LR en TDT5 al optimizar CTRK. La Tabla 4 resume los resultados en TDT5SU;Adaptive LR fue el ganador en este caso, con retroalimentación de relevancia sobre el 0.05% de los documentos de prueba que mejoraron la utilidad en un 20.9% sobre los resultados de Rocchio de PRF. Tabla 3: Métodos AF en TDT5 (rendimiento en CTRK) ROC Base PRF ROC ADP ROC LR% de RF 0% 0% 0% 0.6% 0.2% CTRK 0.076 0.0707 0.0324 0.0382 ±% +7% (basal) -54% -46% Tabla4: Métodos AF en TDT5 (rendimiento en TDT5SU) Base ROC PRF ROC ADP ROC LR (λ = .01)% de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (basal) +6.9.9.9.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de usar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el objetivo principal de FA.5.4 Resumen del proceso de adaptación Después de que decidimos la configuración de los parámetros utilizando la validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/ROCCHIO utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados aleatoriamente;2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre relevancia y luego recibimos comentarios de relevancia para aquellos (predichos) documentos positivos.3) El modelo y las estadísticas de las FDI se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia.6. Observaciones finales presentamos una evaluación transversal de Rocchio incremental y LR incremental en el filtrado adaptativo, centrándonos en su robustez en términos de consistencia del rendimiento con respecto a la optimización de los parámetros cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de los parámetros en AF es un desafío abierto, pero no se ha estudiado a fondo en el pasado.• La robustez en el ajuste de los parámetros cruzados es importante para la evaluación y la comparación de métodos.• Encontramos LR más robusto que Rocchio;Tuvo los mejores resultados (en T11SU) jamás informados en TDT5, TREC10 y TREC11 sin un ajuste extenso.• Descubrimos que Rocchio funciona fuertemente cuando hay un buen corpus de validación disponible, y una opción preferida al optimizar CTRK es el objetivo, que favorece el retiro sobre precisión a un extremo. Para futuras investigaciones, queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y la deriva del contenido. Agradecimientos Este material se basa en el trabajo respaldado en piezas por la National Science Foundation (NSF) bajo la subvención IIS-0434035, por el DOD bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el No. NBCHD030010. Cualquier opinión, hallazgos y conclusiones o recomendaciones expresadas en este material son las del autor (s) y no reflejan necesariamente las opiniones de los patrocinadores.7. Referencias [1] J. Allan. Comentarios de relevancia incremental para el filtrado de información. En Sigir-96, 1996. [2] J. Callan. Aprendiendo mientras filtra documentos. En Sigir-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Detección de temas y descripción general de seguimiento. En detección y seguimiento de temas: Organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Descripción general de la evaluación y resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos de aprendizaje estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En Trec-10, 2001. [8] S. Robertson e I. Soboroff. El informe de pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En Trec-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Aumento y rocchio aplicado al filtrado de texto. En Sigir-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en el margen para el filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima probabilidad para los umbrales de filtrado. En Sigir-01, 2001. [13] Y. Zhang. Uso de antecedentes bayesianos para combinar clasificadores para el filtrado adaptativo. En Sigir-04, 2004. [14] J. Zhang e Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En Sigir-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Inf. RECR.4 (1): 5-31 (2001).