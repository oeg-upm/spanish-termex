Scouts, promotores y conectores: los roles de las clasificaciones en el Filtrado Collaborativo de Bharath Kumar Mohan del vecino más cercano, del Departamento de Bharath Kumar Mohan. del Instituto Indio de Ciencias de CSA Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Departamento de Ciencias de la Computación del Este de la Universidad de Michigan Ypsilanti, MI 48917, EE. UU.Virginia Tech, Blacksburg VA 24061, EE. UU. Naren@cs.vt.edu Resumen Los sistemas de recomendación de recomendaciones agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta de manera crucial la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en las contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las calificaciones en el filtrado colaborativo más cercano de Newbor. En particular, formulamos tres calificaciones de roles, promotores y conectores, que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas (resp.). Estos roles encuentran usos directos para mejorar las recomendaciones para los usuarios, en una mejor orientación de elementos y, lo más importante, para ayudar a monitorear la salud del sistema en su conjunto. Por ejemplo, pueden usarse para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumeren a los usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema aataques como chelines. Argumentamos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para administrar un sistema de recomendación y su comunidad. Categorías y descriptores de sujetos H.4.2 [Aplicaciones de sistemas de información]: Tipos de soporte de decisión de sistemas;J.4 [Aplicaciones informáticas]: Ciencias sociales y de comportamiento Algoritmos de términos generales, Factores humanos 1. Introducción Los sistemas de recomendación se han vuelto integrales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basado en compras anteriores o historial de calificación. El filtrado colaborativo, una forma común de recomendación, predice una calificación de usuarios para un elemento combinando (otras) calificaciones de ese usuario con las calificaciones de otros usuarios. Se han realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativos rápidos y precisos [2, 7], diseñando interfaces para presentar recomendaciones a los usuarios [1] y estudiar la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención al desentrañar el funcionamiento interno de un recomendador en términos de las calificaciones individuales y los roles que juegan al hacer (buenas) recomendaciones. Tal comprensión dará un mango importante para el monitoreo y la gestión de un sistema de recomendación, para diseñar mecanismos para mantener al recomendador y, por lo tanto, garantizar su éxito continuo. Nuestra motivación aquí es desglosar las métricas de rendimiento de recomendaciones globales en las contribuciones hechas por cada calificación individual, lo que nos permite caracterizar los muchos roles desempeñados por las clasificaciones en el filtrado colaborativo de vecino más cercano. Identificamos tres posibles roles: (Scouts) para conectar al usuario al sistema para recibir recomendaciones, (promotores) para conectar un elemento al sistema a recomendar y (conectores) para conectar las calificaciones de estos dos tipos. Viendo las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que ocurran recomendaciones como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a apoyar escenarios como: 1. Situando usuarios en mejores vecindarios: las calificaciones de los usuarios pueden conectar inadvertidamente al usuario a un vecindario para el que los gustos de los usuarios pueden no ser una coincidencia perfecta. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a ubicar al usuario en un mejor vecindario.2. Elementos de orientación: los sistemas de recomendación sufren de falta de participación del usuario, especialmente en escenarios de arranque en frío [13] que involucran artículos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendación.3. Monitoreo de la evolución del sistema de recomendación y sus partes interesadas: un sistema de recomendación está constantemente bajo cambio: crecer con nuevos usuarios y 250 elementos, reducirse con los usuarios que abandonan el sistema, los elementos se vuelven irrelevantes y partes del sistema bajo ataque. El seguimiento de los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría administrarse y mejorar. Estos incluyen poder identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían eliminarse;enumerar a los usuarios que están en peligro de irse o dejar el sistema;y para evaluar la susceptibilidad del sistema a ataques como el chelín [5]. Como mostramos, la caracterización de los roles de calificación presentados aquí proporciona primitivas amplias para administrar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. Los antecedentes sobre el filtrado colaborativo de vecino más cercano y la evaluación de algoritmos se analizan en la Sección 2. La Sección 3 define y discute los roles de una calificación, y la Sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos descritos anteriormente.2. Antecedentes 2.1 Algoritmos más cercanos Los algoritmos de filtrado colaborativo de vecino más cercano usan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno de los segundos se llama elemento [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para usuarios) o elementos (para artículos basados en elementos). Las predicciones se calculan luego agregando las calificaciones, que en un algoritmo basado en el usuario implica agregar las calificaciones del elemento objetivo por los vecinos de los usuarios y, en un algoritmo basado en elementos, implica agregar las calificaciones de los usuarios de elementos que son vecinos del elemento objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y el cálculo de las predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para Grouplens [11] con variaciones de Herlocker et al.[2], y un algoritmo basado en elementos similar al de Sarwar et al.[12]. El algoritmo utilizado por Resnick et al.[11] define la similitud de dos usuarios U y V como la correlación de Pearson de sus calificaciones comunes: Sim (U, V) = P i∈Iu∩iv (Ru, I - ¯ru) (RV, I - ¯RV)qp i∈Iu (ru, i - ¯ru) 2 qp i∈Iv (rv, i - ¯rv) 2, donde iu es el conjunto de elementos clasificados por el usuario u, ru, es la calificación del usuario para el elemento i,y ¯RU es la calificación promedio de User U (de manera similar para V). La similitud calculada de esta manera generalmente se escala por un factor proporcional al número de clasificaciones comunes, para reducir la posibilidad de hacer una recomendación hecha en conexiones débiles: Sim (U, V) = Max (| IU ∩ IV |, γ) γ· Sim (U, V), donde γ ≈ 5 es una constante utilizada como límite inferior en la escala [2]. Estas nuevas similitudes se utilizan para definir una NU de vecindad estática para cada usuario U que consta de los principales usuarios de K más similares al usuario U. Una predicción para el usuario u y el elemento I se calcula por un promedio ponderado de las calificaciones por los vecinos pu, i = ¯ru + p v∈V sim (u, v) (rv, i - ¯rv) p v∈V sim(u, v) (1) donde v = nu ∩ ui es el conjunto de usuarios más similares a los que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al.[12]. En este algoritmo, la similitud se define como la medida del coseno ajustado SIM (I, J) = P u∈Ui∩uj (ru, i - ¯ru) (ru, j - ¯ru) qp u∈Ui (ru, i -¯ru) 2 qp u∈UJ (ru, j - ¯ru) 2 (2) donde UI es el conjunto de usuarios que tienen el elemento nominal i. En cuanto al algoritmo basado en el usuario, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en SIM (i, j) = max (| ui ∩ uj |, γ) γ · SIM (I, J).(3) Dadas las similitudes, el Ni del vecindario de un artículo I se define como los mejores elementos K más similares para ese artículo. Una predicción para el usuario U y el elemento I se calcula como el promedio ponderado PU, i = ¯ri + p j∈J sim (i, j) (ru, j - ¯rj) p j∈J sim (i, j) (4) donde j = ni ∩ iu es el conjunto de elementos clasificados por u que son más similares a i.2.2 Evaluación Los algoritmos de recomendación se han evaluado típicamente utilizando medidas de precisión predictiva y cobertura [3]. Estudios sobre algoritmos de recomendación, especialmente Herlocker et al.[2] y Sarwar et al.[12], típicamente calcula la precisión predictiva dividiendo un conjunto de clasificaciones en conjuntos de entrenamiento y prueba, y calcula la predicción de un elemento en el conjunto de pruebas utilizando las clasificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de prueba T = {(u, i)} se define como, mae = p (u, i) ∈T | pu, i - ru, i i|| T |.(5) La cobertura tiene una serie de definiciones, pero generalmente se refiere a la proporción de elementos que pueden predecir el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios generalmente reciben listas de recomendaciones y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces establecido en términos de ratas de estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Entonces, en su lugar, podemos medir la recomendación o la precisión de rango, lo que indica el grado en que la lista está en el orden correcto. Herlocker et al.[3] discuten una serie de medidas de precisión de rango, que van desde Kendalls Tau hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. Kendalls Tau mide el número de inversiones al comparar pares ordenados en el verdadero pedido de usuarios de 251 Jim Tom Jeff mi primo Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en una simple recomendación de películas.ítems y el orden recomendado, y se define como τ = c - d p (c + d + tr) (c + d + tp) (6) donde c es el número de pares que el sistema predice en el orden correcto, DEl número de pares que predice el sistema en el orden incorrecto, TR el número de pares en el pedido verdadero que tienen las mismas clasificaciones, y TP es el número de pares en el orden previsto que tienen las mismas clasificaciones [3]. Una deficiencia de la métrica Tau es que es ajeno a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista tiene el mismo peso que uno al principio. Una solución es considerar las inversiones solo en los pocos elementos principales de la lista recomendada o las inversiones de peso en función de su posición en la lista.3. Los roles de una calificación de nuestra observación básica es que cada calificación juega un papel diferente en cada predicción en la que se usa. Considere un sistema de recomendación de películas simplificado con tres usuarios Jim, Jeff y Tom y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará la máscara utilizando las otras calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de la máscara utilizando las clasificaciones de usuarios que calificaron la máscara y otras películas de manera similar (por ejemplo, clasificaciones de Jims de la matriz y la máscara; y las clasificaciones de Jeff de Star Wars y la máscara). Las clasificaciones de Toms de esas películas se utilizan para hacer una predicción para la máscara.2. Un algoritmo de filtrado colaborativo basado en el usuario construiría un vecindario alrededor de Tom al rastrear a otros usuarios cuyos comportamientos de calificación son similares a Toms (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado la matriz). La predicción de la clasificación TOMS para la máscara se basa en las clasificaciones de Jeff y Tim. Aunque los algoritmos del vecino más cercano agregan las clasificaciones para formar vecindarios utilizados para calcular predicciones, podemos desglosar las similitudes para ver el cálculo de una predicción como simultáneamente siguiendo las rutas paralelas de las calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la clasificación TOMS de la máscara como caminar a través de una secuencia de clasificaciones. En Jim Tom Jeff, la Matrix Star Wars, la máscara Q1 Q2 Q2 P1 P2 P3 Figura 2: Calificaciones utilizadas para predecir la máscara para Tom. Jim Tom Jeff The Matrix Star Wars The Mask Q1 Q2 Q3 P1 P2 P3 Jerry R2 R3 Figura 3: Predicción de la máscara para Tom en la que se usa una calificación más de una vez.Este ejemplo, se usaron dos rutas para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (Q1, Q2, Q3). Tenga en cuenta que estos caminos no están dirigidos y son todos de longitud 3. Solo el orden en que se atraviesan las clasificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (Q3, Q2, Q1)) y el algoritmo basado en el usuario (por ejemplo, (P1, P2,P3), (Q1, Q2, Q3).) Una calificación puede ser parte de muchas rutas para una sola predicción como se muestra en la Fig. 3, donde se usan tres rutas para una predicción, dos de los cuales siguen P1: (P1, P2, P3) y (P1, R2, R3). Las predicciones en un algoritmos de filtrado colaborativo pueden involucrar miles de tales caminatas en paralelo, cada una jugando un papel en la influencia del valor predicho. Cada ruta de predicción consta de tres calificaciones, que juegan roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considere la ruta (p1, p2, p3) en la Fig. 2 utilizada para hacer una predicción de la máscara para Tom: 1. La calificación P1 (Tom → Star Wars) establece una conexión de Tom a otras clasificaciones que se pueden usar para predecir la calificación TOMS para la máscara. Esta calificación sirve como un explorador en el gráfico bipartito de las clasificaciones para encontrar un camino que conduzca a la máscara.2. La calificación P2 (Jeff → Star Wars) ayuda al sistema a recomendar la máscara a Tom conectando el Scout al promotor.3. La calificación P3 (Jeff → la máscara) permite conexiones a la máscara y, por lo tanto, promueve esta película a Tom. Formalmente, dada una predicción PU, un elemento objetivo A para el usuario U, un explorador para PU, A es una calificación RU, yo tal que existe un usuario V con calificaciones RV, A y RV, para algunos elementos I;Un promotor para PU, A es un RV de calificación, A para algún usuario V, de modo que existan calificaciones RV, I y Ru, I para un artículo I y;Un conector para PU, un 252 Jim Tom Jeff Jerry, mi primo Vinny, el Matrix Star Wars The Mask Jurasic Park Figura 4: Scouts, promotores y conectores.es una calificación RV, I por algún usuario V y calificación I, de modo que exista las calificaciones Ru, I y RV, a. Los exploradores, conectores y promotores para la predicción de la clasificación TOMS de la máscara son P1 y Q1, P2 y Q2, y P3 y Q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de los usuarios y el sistema en términos de permitir que se hagan recomendaciones.3.1 Roles en las clasificaciones detalladas que actúan como Scouts tienden a ayudar al sistema de recomendación sugerir más películas para el usuario, aunque la medida en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación Tom → Star Wars ayuda al sistema a recomendarle solo la máscara, mientras que Tom → la matriz ayuda a recomendar la máscara, Jurassic Park y mi primo Vinny. Tom hace una conexión con Jim, que es un usuario prolífico del sistema, calificando la matriz. Sin embargo, esto no hace que la matriz sea la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual por la máscara y la matriz, que permiten al sistema recomendarle Star Wars. Su calificación de la máscara es el mejor explorador para Jeff, y Jerrys Only Scout es su calificación de Star Wars. Esto sugiere que los buenos exploradores permiten a un usuario generar similitud con usuarios prolíficos y, por lo tanto, asegurarse de obtener más del sistema. Si bien los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus duales y son beneficiosos para los artículos. En la Fig. 4, mi primo Vinny se beneficia de la calificación de JIMS, ya que permite recomendaciones a Jeff y Tom. La máscara no depende tan de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerrys de Star Wars no ayuda a promoverla a ningún otro usuario. Concluimos que un buen promotor conecta un elemento con un vecindario más amplio de otros artículos y, por lo tanto, asegura que se recomiende a más usuarios. Los conectores cumplen un papel crucial en un sistema de recomendación que no es tan obvio. Las películas de mi primo Vinny y Jurassic Park tienen el potencial de recomendación más alto, ya que se pueden recomendar a Jeff, Jerry y Tom en función de la estructura de enlace ilustrada en la figura 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo debido a las clasificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad de los sistemas para hacer recomendaciones sin ganancia explícita para el usuario. Tenga en cuenta que cada calificación puede ser de variado beneficio en cada uno de estos roles. La calificación Jim → Mi primo Vinny es un pobre explorador y conector, pero es un muy buen promotor. La calificación Jim → la máscara es un explorador razonablemente bueno, un muy buen conector y un buen promotor. Finalmente, la calificación Jerry → Star Wars es un muy buen explorador, pero no tiene ningún valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se usa una calificación en cada rol, lo que solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varias rutas de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, cada ruta) en el sistema hacia el error general de los sistemas. Podemos entender la dinámica del sistema con una granularidad más fina al rastrear la influencia de una calificación de acuerdo con el papel jugado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol.4. Las contribuciones de las calificaciones, como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede usar cualquier medida numérica de una propiedad de la salud del sistema y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito por una calificación en cada uno de los tres roles, y también rastrear la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones definiendo primero la influencia de una calificación, y luego instanciando el enfoque para la precisión predictiva, y luego la precisión de rango. También demostramos cómo estas contribuciones pueden agregarse para estudiar el vecindario de las calificaciones involucradas en la calculación de las recomendaciones de los usuarios. Tenga en cuenta que aunque nuestra formulación general para la influencia de calificación es independiente del algoritmo, debido a las consideraciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento.4.1 La influencia de las calificaciones recuerda que un enfoque basado en elementos para el filtrado colaborativo se basa en la construcción de vecindarios de elementos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida de coseno ajustado (ecuaciones (2) y (3)). Se mantiene un conjunto de los mejores vecinos K para todos los elementos para el espacio y la eficiencia computacional. Una predicción del elemento I para un usuario U se calcula como la desviación ponderada de los elementos calificación media como se muestra en la ecuación (4). La lista de recomendaciones para un usuario es la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos el impacto (A, I, J), el impacto que tiene un usuario A para determinar la similitud entre dos ítems I y J. Este es el cambio en la similitud entre I y J cuando se elimina la calificación AS, y se define como impacto (a, i, j) = | sim (i, j) - sim¯a (i, j) |P w∈Cij | sim (i, j) - sim ¯w (i, j) |donde cij = {u ∈ U |∃ ru, i, ru, j ∈ R (u)} es el conjunto de corras 253 de los elementos I y J (usuarios que califican tanto i como j), r (u) es el conjunto de calificaciones proporcionadas por el usuario u, ySim¯a (i, j) es la similitud de I y J cuando las calificaciones del usuario A se eliminan Sim¯a (i, j) = P v∈U \ {a} (ru, i - ¯ru) (ru, j - ¯ru) qp u∈U \ {a} (ru, i - ¯ru) 2 qp u∈U \ {a} (ru, j - ¯ru) 2, y ajustado para el número de evaluadores sim¯a (i, j) = max (| ui ∩ uj | - 1, γ) γ · sim (i, j). Si todas las coratas de I y J califican de manera idéntica, definimos el impacto como impacto (A, I, J) = 1 | CIJ |Dado que p w∈Cij | sim (i, j) - sim ¯w (i, j) |= 0. La influencia de cada ruta (u, j, v, i) = [ru, j, rv, j, rv, i] en la predicción de PU, i es dada por influencia (u, j, v, i) = sim(i, j) p l∈Ni∩iu sim (i, l) · impacto (v, j) se deduce que la suma de influencias en todas esas rutas, para un conjunto determinado de puntos finales, es 1. 4.2Valores para la precisión predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción decide si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para la precisión predictiva, el error en la predicción e = | pu, i - ru, i |se asigna a un nivel de comodidad utilizando una función de mapeo M (e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores inferiores a 1.0 (para una escala de calificación de 1 a 5) [4], por lo que un error inferior a 1.0 se considera aceptable, pero cualquier cosa más grande no lo es. Por lo tanto, definimos m (e) como (1 - e) agrupado a un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción PU, I, M (e) se atribuye a todos los caminos que ayudaron al cálculo de PU, I, proporcional a sus influencias. Este tributo, m (e) ∗ influencia (u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru, j, rv, j, rv, i], con el crédito/culpaacumulando a los roles respectivos de Ru, J como Scout, RV, J como conector y RV, yo como promotor. En otras palabras, el valor Scout SF (Ru, J), el valor del conector CF (RV, J) y el valor del promotor PF (RV, I) están incrementados por el monto del tributo. En una gran cantidad de predicciones, los exploradores que han resultado repetidamente en grandes tasas de error tienen un gran valor de exploración negativa, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF].4.3 Valores de roles para la precisión de rango ahora definimos el cálculo de la contribución de las calificaciones a la precisión de rango observada. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden calcular predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de la prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error) cada vez que uno de los siguientes se mantiene: • if (ru, i <ru, j) entonces (pu, i - pu, j <);• if (ru, i> ru, j) entonces (pu, i - pu, j>);o • if (ru, i = ru, j) entonces (| pu, i - pu, j | ≤). Del mismo modo, un par (i, j) es discordante (con error) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Se acreditan todos los caminos involucrados en la predicción de los dos elementos en un par concordante, y los caminos involucrados en un par discordante están desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c (i, j) = (t t · 1 c+d if (i, j) son concordantes - t t ·1 c+d if (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de pruebas de usuarios cuyas calificaciones podrían ser predicha, t es el número de elementos calificados por el usuario u en el conjunto de pruebas, c esEl número de concordancias y D es el número de discordancias. El crédito C se divide entre todos los caminos responsables de predecir PU, I y PU, J proporcional a sus influencias. Nuevamente agregamos los valores de roles obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación.4.4 Agregando roles de calificación Después de calcular los valores de roles para las calificaciones individuales, también podemos usar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos usar los valores de roles para caracterizar la salud de un vecindario. Considere la lista de las principales recomendaciones presentadas a un usuario en un momento específico. El algoritmo de filtrado colaborativo atravesó muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario de recomendación del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los artículos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye por completo en la satisfacción de los usuarios con el sistema. Podemos caracterizar un vecindario de recomendación de usuarios agregando los valores de roles individuales de las calificaciones involucradas, ponderadas por la influencia de las calificaciones individuales para determinar su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercieron una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de elementos altamente calificados tienen una voz mayor en los elementos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios con respecto a su posicionamiento en un vecindario saludable o poco saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Puede tener un buen vecindario, pero sus malos exploradores pueden garantizar que el potencial de los vecindarios se vuelva inútil. Esperamos que los usuarios con buenos exploradores y buenos vecindarios estén más satisfechos con el sistema en el futuro. Un vecindario de usuarios se caracteriza por un triple que representa la suma ponderada de los valores de roles de las calificaciones individuales involucradas en la realización de recomendaciones. Considere un usuario U y su lista ordenada de recomendaciones L. Un elemento I 254 en la lista se ponderó inversamente, como K (i), dependiendo de su posición en la lista. En nuestros estudios usamos K (i) = P Posición (I). Varios caminos de calificaciones [Ru, J, RV, J, RV, I] están involucrados en la predicción de PU, I, que finalmente decide su posición en L, cada una con influencia (U, J, V, I). El vecindario de recomendación de un usuario U se caracteriza por el triple, [SFN (U), CFN (U), Pfn (U)] donde Sfn (u) = x i∈L p [ru, j, rv, j, rv,I] sf (ru, j) influencia (u, j, v, i) k (i)! CFN (U) y PFN (U) se definen de manera similar. Este triple estima la calidad de las recomendaciones estadounidenses basadas en el historial pasado de las calificaciones involucradas en sus respectivos roles.5. Experimentación Como hemos visto, podemos asignar valores de roles a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque de nuestro objetivo general de definir un enfoque para monitorear y administrar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de calificación de Mobielens Million. En particular, discutimos los resultados relacionados con la identificación de buenos exploradores, promotores y conectores;la evolución de los roles de calificación;y la caracterización de los vecindarios de los usuarios.5.1 Metodología Nuestros experimentos usan el conjunto de datos de calificación de Movielens Million, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las clasificaciones están en el rango 1 a 5, y están etiquetadas con el tiempo que se dio la calificación. Como se discutió anteriormente, consideramos solo el algoritmo basado en ítems aquí (con vecindarios de elementos del tamaño 30) y, debido a consideraciones de espacio, solo los resultados del valor del rol presentan para la precisión de rango. Dado que estamos interesados en la evolución de los valores de rol de calificación a lo largo del tiempo, el modelo del sistema de recomendación se basa en las calificaciones de procesamiento en su orden de llegada. La campaña de tiempo proporcionada por Movielens es, por lo tanto, crucial para los análisis presentados aquí. Hacemos evaluaciones de roles de calificación a intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de roles a medida que las clasificaciones ordenadas se fusionan en el modelo. Para mantener el experimento computacionalmente manejable, definimos un conjunto de datos de prueba para cada usuario. A medida que las clasificaciones de tiempo ordenadas se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 calificaciones, calculamos las predicciones para las calificaciones en los datos de prueba y luego calculamos los valores de roles para las calificaciones utilizadas en las predicciones. Una posible crítica de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan para sus roles. Superamos esta preocupación repitiendo el experimento, usando diferentes semillas aleatorias. La probabilidad de que cada calificación se considera para la evaluación es considerablemente alta: 1 - 0.2N, donde n es el número de veces que el experimento se repite con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativos basados en elementos fue ordinario con respecto a la precisión de rango. La figura 5 muestra una gráfica de la precisión y el recuerdo, ya que las clasificaciones se fusionaron en el orden de tiempo en el modelo. El retiro siempre fue alto, pero la precisión promedio era de aproximadamente 53%.0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Clasificaciones fusionadas en el valor del modelo Recuerdo de precisión Figura 5: Precisión y recuperación para el algoritmo de filtrado colaborativo basado en elementos.5.2 Inducir buenos exploradores Las calificaciones de un usuario que sirven como exploradores son aquellas que permiten al usuario recibir recomendaciones. Afirmamos que los usuarios con calificaciones que tienen valores de exploración respetables estarán más felices con el sistema que aquellos con calificaciones con valores de exploración bajos. Tenga en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordante de la preferencia de los usuarios. Sin embargo, no está claro si todos estos pares discordantes son observables por el usuario, sin duda, esto sugiere que es necesario poder dirigir a los usuarios a elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de exploración para la mayoría de las calificaciones de los usuarios son gaussianos con media cero. La figura 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en una instantánea dada. Observamos que una gran cantidad de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relatable es cuando Amazons Recomender hace sugerencias de libros o artículos basados en otros artículos que se compraron como regalos. Con comentarios de relevancia simple del usuario, tales calificaciones pueden aislarse como malos exploradores y descontarse de futuras predicciones. Se descubrió que la eliminación de exploradores malos valía la pena para los usuarios individuales, pero la mejora general del rendimiento fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores calificando simplemente películas populares como lo sugieren Rashid et al.[9]. Muestran que una mezcla de popularidad y entropía de calificación identifica los mejores elementos para sugerir a los nuevos usuarios cuando se evalúa usando MAE. Después de su intuición, esperaríamos ver una mayor correlación entre la popularidad de la atención y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de una película (número de veces que está clasificada);y con su popularidad*medida de varianza en diferentes instantáneas del sistema. Tenga en cuenta que los valores de los exploradores estaban inicialmente anticorrelacionados con popularidad (Fig. 7), pero se correlacionó moderadamente a medida que el sistema evolucionó. La varianza de popularidad y popularidad*se desempeñó de manera similar. Una posible explicación es que no ha habido tiempo suficiente para que las películas populares acumulen calificaciones.255-10 0 10 20 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 FRIDUENCIA DE VALOR DE SCOUT Figura 6: Distribución de valores de exploración para un usuario de muestra.-0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*VAR Figura 7: Correlación entre el valor de exploración agregado y la popularidad del artículo (calculado a diferentes intervalos).-0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor del promotor agregado y la prolificidad del usuario (calculado a diferentes intervalos). Tabla 1: Películas que forman los mejores exploradores. Siendo John Malkovich (1999) 1.00 445 Star Wars: Episodio IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69445 Run Lola Run (Lola Rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Sospechosos habituales, The (1995) 0.62 326 Aliens (1986) 0.62 385 Norte por Noroeste (1959) 0.62 245 Fugitivo, el (1993)0.62 402 Fin de los días (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Lista de Schindlers (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abys Abys, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Harold y Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Parte III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84Mujeres al borde de A ... (1988) 0.38 113 uvas de ira, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsénico y Old Lace (1944) 0.38 138 Vaquero de medianoche (1969) 0.38 137 para matar AMockingbird (1962) 0.31 195 Cuatro bodas y un funeral (1994) 0.31 271 Good, The Bad and the Ugly, The (1966) 0.31 156 Es una vida maravillosa (1946) 0.31 146 Jugador, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Verdad Acerca de Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill,El (1983) 0.31 184 256 Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen constantemente en buenos exploradores con el tiempo. Afirmamos que estas películas harán exploradores viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada uno. Se dice que una película induce un buen explorador si la película estaba en el top 100 de la lista ordenada, e induce un mal explorador si estaba en los 100 inferiores de la misma lista. Se espera que las películas que aparecen constantemente altas con el tiempo permanecen allí en el futuro. La confianza efectiva en una película m es cm = tm - bm n (8) donde tm es el número de veces que apareció en el top 100, el número de veces que apareció en las 100 inferiores, y n es el número de intervalosconsideró. Usando esta medida, las mejores películas que se esperan inducir los mejores exploradores se muestran en la Tabla 1. Las películas que serían las opciones de Scouts se muestran en la Tabla 2 con sus confidencias asociadas. También se muestran las popularidades de las películas. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada solo en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador John Malkovich-se trata de un titiritero que descubre un portal en una estrella de cine, una película que se ha descrito de manera diversa en Amazon.com como te hace sentir vertiginoso, muy extraño, comedia con profundidad, tonta, extraña, e inventivo. Indicando si a alguien le gusta esta película o no es muy útil para situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho de una película un mal explorador, como la fuerte variación en las preferencias de los usuarios en el vecindario de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence of Arabia, pero pueden diferir considerablemente sobre cómo se sintieron sobre las otras películas que vieron. Se producen exploradores malos cuando hay una desviación en el comportamiento en torno a un punto de sincronización común.5.3 La inducción de buenas clasificaciones de promotores que sirven para promover elementos en un sistema de filtrado colaborativo son fundamentales para permitir que se recomiende un nuevo elemento a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación de arranque en frío. Observamos que la distribución de frecuencia de los valores del promotor para las clasificaciones de películas de muestra también es gaussiana (similar a la Fig. 6). Esto indica que la promoción de una película se beneficia más por las calificaciones de algunos usuarios, y no se ven afectados por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de los usuarios y su valor promotor agregado. La figura 8 muestra la evolución de la correlación de Pearson coeficiente entre la prolificidad de un usuario (número de calificaciones) versus su valor promotor agregado. Esperamos que las chelines conspicuas, al recomendar películas incorrectas a los usuarios, se desacrediten con los valores negativos del promotor agregado y deben ser identificables fácilmente. Dada esta observación, la regla obvia a seguir al introducir una nueva película es tenerla clasificada directamente por usuarios prolíficos que poseen altos valores de promotores agregados. Por lo tanto, se emite una nueva película en el vecindario de muchas otras películas que mejora su visibilidad. Sin embargo, tenga en cuenta que un usuario puede haber dejado de usar el sistema. El seguimiento de los valores del promotor de manera consistente permite que solo se consideren los usuarios recientes más activos.5.4 Inducir buenos conectores dada la forma en que se caracterizan los exploradores, conectores y promotores, se deduce que las películas que son parte de los mejores exploradores también son parte de los mejores conectores. Del mismo modo, los usuarios que constituyen los mejores promotores también forman parte de los mejores conectores. Los buenos conectores se inducen al garantizar que un usuario con un alto valor del promotor califique una película con un alto valor de exploración. En nuestros experimentos, encontramos que un rol más largo de calificación a menudo es como un conector. A menudo se ve una calificación con un valor de mal conector debido a que su usuario es un mal promotor o su película es un mal explorador. Dichas calificaciones se pueden eliminar del proceso de predicción para llevar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que eliminar un conjunto de conectores que se comportan mal ayudó a mejorar el rendimiento general de los sistemas en un 1,5%. El efecto fue aún mayor en algunos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en el retiro.5.5 Monitoreo de la evolución de los roles de calificación Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, al estudiar los roles cambiantes de las calificaciones a lo largo del tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de chelín o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y insignificantes puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de administrar un sistema de recomendación. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considere a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 son parte del conjunto de pruebas. Si un explorador tiene un valor superior a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo que diremos que es bueno. Por lo tanto, se elige un umbral de 0.005 para reducir una calificación como buena, mala o insignificante en términos de su explorador, conector y valor promotor. Por ejemplo, una calificación R, en el tiempo t con el valor de rol triple [0.1, 0.001, −0.01] se clasifica como [Scout +, conector 0, promotor -], donde + indica bueno, 0 indica negligible y - indica mal. El crédito positivo en poder de una calificación es una medida de su contribución al mejoramiento del sistema, y el desacredit es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución reemplaza a su tamaño. Por ejemplo, a pesar de que solo el 1.7% de todas las calificaciones se clasificaron como buenos exploradores, ¡tienen el 79% de todo el crédito positivo en el sistema! Del mismo modo, los malos exploradores fueron solo el 1.4% de todas las calificaciones, pero poseen el 82% de todos los desacreditados. Tenga en cuenta que los exploradores buenos y malos, juntos, comprenden solo 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Del mismo modo, los buenos conectores fueron 1.2% del sistema y poseen el 30% de todo el crédito positivo. Los conectores malos (0.8% del sistema) poseen el 36% de todos los desacreditados. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los promotores incobrables (2%) tienen el 50% de todos los desacreditados. Esto reitera que algunas calificaciones influyen en la mayoría del rendimiento del sistema. Por lo tanto, es importante rastrear las transiciones entre ellas independientemente de sus pequeños números.257 En diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambio. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, los exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en los malos roles. Sin embargo, es suficiente ver que las calificaciones en roles malos se convierten en roles buenos o vestigiales. Del mismo modo, observar una gran cantidad de buenos roles se convierten en malos es un signo de falla inminente del sistema. Empleamos el principio de los episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0,+, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0];[0, +, 0]: 1 [ +, 0, 0];[0, 0, 0]: 1 [0, +, 0];[0, 0, 0]: 1 en lugar de [+, 0, 0];[0, +, 0]: 2 [ +, 0, 0];[0, 0, 0]: 2 [0, +, 0];[0, 0, 0]: 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Hacemos una simplificación adicional y utilizamos solo 9 estados, lo que indica si la calificación es un explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven múltiples propósitos se cuentan utilizando instancias de múltiples episodios, pero los estados mismos no están duplicados más allá de los 9 estados restringidos. En este modelo, una transición como [ +, 0, +];[0, +, 0]: 1 se cuenta como [Scout +];[Scout0]: 1 [Scout+];[conector+]: 1 [Scout+];[promotor0]: 1 [conector0];[Scout0]: 1 [conector0];[Scout+]: 1 [conector0];[promotor0]: 1 [promotor+];[Scout0]: 1 [promotor+];[conector+]: 1 [promotor+];[Promoter0]: 1 de estos, transiciones como [PX];[Q0] donde p = q, x ∈ {+, 0, -} se consideran poco interesantes, y solo se cuentan el resto. La figura 9 muestra las principales transiciones contadas mientras procesan las primeras 200,000 calificaciones del conjunto de datos Movielens. Solo se muestran transiciones con frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican el número de calificaciones que se encontró en esos estados.