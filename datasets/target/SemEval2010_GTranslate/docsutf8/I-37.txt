Un marco para el aprendizaje automático y la minería de datos distribuidos basados en agentes Jan Tezicka Gerstner Laboratorio de la Universidad Técnica Checa Technick «A 2, Prague, 166 27 República Checa tezicka@labe.felk.cvut.cz Michael Rovatsos School of Informatics La Universidad de Edinburgh Edinburgh EH8 EH8 EH89le Reino Unido mrovatso@inf.ed.ac.uk Michal Pechoucek Gerstner Laboratorio Czech Technical University Technick «A 2, Praga, 166 27 República Checa Pechouc@labe.felk.cvut.cz Resumen Este documento propone un marco para agentes distribuido distribuido distribuidoAprendizaje automático y minería de datos basada en (i) el intercambio de descripciones de nivel meta de procesos de aprendizaje individuales entre agentes y (ii) razonamiento en línea sobre el éxito del aprendizaje y el progreso del aprendizaje por los agentes de aprendizaje. Presentamos una arquitectura abstracta que permite a los agentes intercambiar modelos de sus procesos de aprendizaje locales e introduce varios métodos diferentes para integrar estos procesos. Esto nos permite aplicar mecanismos de interacción de agentes existentes a tareas de aprendizaje automático distribuido, aprovechando así los poderosos métodos de coordinación disponibles en la informática basada en agentes, y permite a los agentes participar en metaderazos sobre sus propias decisiones de aprendizaje. Aplicamos esta arquitectura a una aplicación de agrupación distribuida en el mundo real para ilustrar cómo el marco conceptual se puede utilizar en sistemas prácticos en los que diferentes alumnos pueden usar diferentes conjuntos de datos, hipótesis y algoritmos de aprendizaje. Reportamos sobre los resultados experimentales obtenidos utilizando este sistema, revisamos el trabajo relacionado sobre el tema y discutimos las posibles extensiones futuras al marco. Términos generales Categorías de teoría y descriptores de sujetos I.2.11 [Inteligencia artificial]: inteligencia artificial distribuida Sistemas-Multiagentes 1. Introducción En las áreas de aprendizaje automático y minería de datos (cf. [14, 17] para descripción general), durante mucho tiempo se ha reconocido que la paralelización y la distribución pueden usarse para mejorar el rendimiento del aprendizaje. Se han sugerido varias técnicas a este respecto, que van desde la integración de bajo nivel de hipótesis de aprendizaje derivadas de forma independiente (por ejemplo, combinando diferentes clasificadores para tomar decisiones de clasificación óptimas [4, 7], promedio de modelos de clasificadores bayesianos [8] o métodos basados en consenso paraIntegrando diferentes agrupaciones [11]), a la combinación de alto nivel de resultados de aprendizaje obtenidos por agentes de aprendizaje heterogéneos que usan meta-aprendizaje (por ejemplo, [3, 10, 21]). Todos estos enfoques asumen la homogeneidad del diseño de agentes (todos los agentes aplican el mismo algoritmo de aprendizaje) y/o objetivos de agentes (todos los agentes están tratando de resolver cooperativamente un solo problema de aprendizaje global). Por lo tanto, las técnicas que sugieren no son aplicables en sociedades de estudiantes autónomos que interactúan en sistemas abiertos. En tales sistemas, los alumnos (agentes) pueden no ser capaces de integrar sus conjuntos de datos o resultados de aprendizaje (debido a diferentes formatos de datos y representaciones, algoritmos de aprendizaje o restricciones legales que prohíben dicha integración [11]) y no siempre se puede garantizar que interactúe enUna moda estrictamente cooperativa (el conocimiento descubierto y los datos recopilados pueden ser activos económicos que solo deben compartirse cuando esto se considera rentable; los agentes maliciosos pueden intentar influir negativamente en los resultados de los demás aprendiendo, etc.). Ejemplos para aplicaciones de este tipo abundan. Muchos dominios de aprendizaje distribuido implican el uso de datos confidenciales y prohíben el intercambio de estos datos (por ejemplo, el intercambio de datos del paciente en el diagnóstico de tumor cerebral distribuido [2]); sin embargo, pueden permitir el intercambio de hipótesis de aprendizaje local entre diferentes alumnos. En otras áreas, los datos de capacitación pueden ser comercialmente valiosos, de modo que los agentes solo lo pongan a disposición de otras si esos agentes pudieran proporcionar algo a cambio (por ejemplo, en la vigilancia y el seguimiento remotos de los barcos, donde las diferentes agencias involucradas son proveedores de servicios comerciales [1]). Además, los agentes pueden tener un interés personal en afectar negativamente el rendimiento de aprendizaje de otros agentes. Un ejemplo para esto es el de los agentes fraudulentos en eBay, que pueden tratar de evitar que los agentes de aprendizaje de la reputación sean de la construcción de modelos útiles para detectar fraude. Ver a los alumnos como agentes autónomos y autodirigidos es la única visión apropiada que uno puede adoptar en el modelado de estos entornos de aprendizaje distribuidos: la metáfora del agente se convierte en una necesidad como opuesta a las preferencias de escalabilidad, selección dinámica de datos, interactividad [13], que también puede serlogrado a través de la distribución (no agente) y la paralelización en principio. A pesar de la autonomía y la autodirección de los agentes de aprendizaje, muchos de estos sistemas exhiben una superposición suficiente en términos de objetivos de aprendizaje individual, por lo que estaba disponible una cooperación beneficiosa si estuviera disponible un modelo para la interacción flexible entre los alumnos autónomos que permitió a los agentes 1. intercambiar 1.Información sobre diferentes aspectos de su propio mecanismo de aprendizaje en diferentes niveles de detalle sin verse obligado a revelar información privada que no debe divulgarse, 2. Decide en qué medida quieran compartir información sobre sus propios procesos de aprendizaje y utilizar la información proporcionada por otros alumnos.y 3. Razón sobre cómo esta información se puede utilizar mejor para mejorar su propio rendimiento de aprendizaje. Nuestro modelo se basa en la simple idea de que los alumnos autónomos deben mantener metadescripciones de sus propios procesos de aprendizaje (ver también [3]) para poder intercambiar información y razonar sobre ellos de manera racional (es decir, con el objetivo generalde mejorar sus propios resultados de aprendizaje). Nuestra hipótesis es muy simple: si podemos idear una visión abstracta suficientemente general de describir los procesos de aprendizaje, podremos utilizar toda la gama de métodos para (i) razonamiento racional y (ii) comunicación y coordinación ofrecida por el agentetecnología para construir agentes de aprendizaje autónomos efectivos. Para probar esta hipótesis, introducimos dicha arquitectura abstracta (Sección 2) e implementamos una instancia simple y concreta de ella en un dominio del mundo real (Sección 3). Reportamos sobre los resultados empíricos obtenidos con este sistema implementado que demuestra la viabilidad de nuestro enfoque (Sección 4). Finalmente, revisamos el trabajo relacionado (Sección 5) y concluyemos con un resumen, discusión de nuestro enfoque y perspectiva para el trabajo futuro sobre el tema (Sección 6).2. Arquitectura abstracta Nuestro marco se basa en proporcionar descripciones formales (de nivel meta) de los procesos de aprendizaje, es decir, representaciones de todos los componentes relevantes de la maquinaria de aprendizaje utilizada por un agente de aprendizaje, junto con información sobre el estado del proceso de aprendizaje. Para garantizar que este marco sea suficientemente general, consideramos la siguiente descripción general de un problema de aprendizaje: dados los datos D ⊆ D tomados de un espacio de instancia D, un espacio de hipótesis H y una función objetivo (desconocida) C ∈ H1, derivan una funciónH ∈ H que se aproxima a C lo mejor posible de acuerdo con alguna medida de rendimiento G: H → Q donde Q es un conjunto de niveles posibles de rendimiento del aprendizaje.1 Al requerir esto, estamos asegurando que el problema de aprendizaje pueda resolverse en principio utilizando el espacio de hipótesis dado. Esta definición muy amplia incluye una serie de componentes de un problema de aprendizaje para el cual se pueden proporcionar especificaciones más concretas si queremos ser más precisos. Para los casos de clasificación y agrupación, por ejemplo, podemos especificar aún más lo anterior de la siguiente manera: los datos de aprendizaje se pueden describir en ambos casos como d = × n i = 1 [ai] donde [ai] es el dominio del atributo ésimo yEl conjunto de atributos es a = {1 ,..., n}. Para el espacio de hipótesis, obtenemos h ⊆ {h | h: d → {0, 1}} en el caso de clasificación (es decir, un subconjunto del conjunto de todos los clasificadores posibles, cuya naturaleza depende de la expresividad del algoritmo de aprendizajeusado) y H ⊆ {H | H: D → N, H es total con el rango {1 ,..., k}} en el caso de la agrupación (es decir, un subconjunto de todos los conjuntos de posibles asignaciones de clúster que mapean puntos de datos a un número finito de grupos numerados 1 a k). Para la clasificación, G podría definirse en términos de los números de falsos negativos y falsos positivos con respecto a algún conjunto de validación V ⊆ D, y la agrupación podría usar varias medidas de validez del clúster para evaluar la calidad de una hipótesis actual, de modo que Q =R en ambos casos (pero se pueden imaginar otros conjuntos de niveles de calidad de aprendizaje). A continuación, presentamos una noción de paso de aprendizaje, que impone una estructura básica uniforme en todos los procesos de aprendizaje que se supone que intercambian información utilizando nuestro marco. Para esto, suponemos que a cada alumno se le presenta un conjunto finito de datos d = D1 ,...DK en cada paso (este es un conjunto ordenado para expresar que el orden en el que las muestras se usan para asuntos de capacitación) y emplea una función de capacitación/actualización F: H × D ∗ → H que actualiza H dada una serie de muestras D1,..., dk. En otras palabras, un paso de aprendizaje siempre consiste en aplicar la función de actualización a todas las muestras en D exactamente una vez. Definimos un paso de aprendizaje como tupla L = D, H, F, G, H donde requerimos que H ⊆ H y H ∈ H. La intuición detrás de esta definición es que cada paso de aprendizaje describe completamente una iteración de aprendizaje como se muestra en la figura1: En el paso T, el alumno actualiza la hipótesis actual HT - 1 con Data DT, y evalúa la nueva hipótesis HT resultante de acuerdo con la medida de rendimiento actual GT. Tal paso de aprendizaje es equivalente a los siguientes pasos de cálculo: 1. Entrena el algoritmo en todas las muestras en D (una vez), es decir, calcule FT (HT - 1, DT) = HT, 2. Calcule la calidad GT de la hipótesis resultanteGT (HT). Denotamos el conjunto de todos los pasos de aprendizaje posibles por L. Para facilitar la notación, denotamos los componentes de cualquier l ∈ L por D (L), H (L), F (L) y G (L), respectivamente. La razón por la cual tales especificaciones de paso de aprendizaje usan un subconjunto H de h en lugar de h en sí mismo es que los alumnos a menudo tienen un conocimiento explícito sobre qué hipótesis se descartan efectivamente por F dada H en el futuro (si este no es el caso, aún podemos establecerH = H). Un proceso de aprendizaje es una secuencia finita y no vacía L = L1 → L2 →...→ ln de pasos de aprendizaje tal que ∀1 ≤ i <n .h (li+1) = f (li) (h (li), d (li)) el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 679 Función de entrenamiento HT Medida de rendimiento Calidad QTGTFT Conjunto de capacitación DT Hipótesis HT-1 Figura 1: Un modelo genérico de un paso de aprendizaje, es decir, el único requisito de la relación de transición → ⊆ L × L ×L hace que la nueva hipótesis es el resultado de capacitar a la antigua hipótesis en todos los datos de muestra disponibles que pertenece al paso actual. Denotamos el conjunto de todos los procesos de aprendizaje posibles por L (ignorando, para facilitar la notación, el hecho de que este conjunto depende de H, D y los espacios de posibles funciones de capacitación y evaluación F y G). La traza de rendimiento asociada con un proceso de aprendizaje L es la secuencia Q1 ,..., qn ∈ Qn donde qi = g (li) (h (li)), es decir, la secuencia de valores de calidad calculados por las medidas de rendimiento de los pasos de aprendizaje individuales en las hipótesis respectivas. Dichas especificaciones permiten a los agentes proporcionar una autodescripción de su proceso de aprendizaje. Sin embargo, en la comunicación entre los agentes de aprendizaje, a menudo es útil proporcionar información parcial sobre el proceso de aprendizaje interno en lugar de sus detalles completos, p.Al anunciar esta información para ingresar negociaciones de intercambio de información con otros. Para este propósito, asumiremos que los alumnos describen su estado interno en términos de conjuntos de procesos de aprendizaje (en el sentido de elección disyuntiva) que llamamos descripciones de procesos de aprendizaje (LPD) en lugar de dar descripciones precisas sobre un solo proceso de aprendizaje concreto. Esto nos permite describir las propiedades de un proceso de aprendizaje sin especificar sus detalles exhaustivamente. Como ejemplo, el conjunto {l ∈ L | ∀l = l [i] .d (l) ≤ 100} describe todos los procesos que tienen un conjunto de entrenamiento de la mayoría de las 100 muestras (donde todos los demás elementos son arbitrarios). Del mismo modo, {l ∈ L | ∀l = l [i] .d (l) = {d}} es equivalente a proporcionar información sobre una sola muestra {d} y ningún otro detalle sobre el proceso (esto puede ser útil paramodelo, por ejemplo, datos recibidos del entorno). Por lo tanto, usamos ℘ (l), es decir, el conjunto de todos los LPD, como base para diseñar lenguajes de contenido para la comunicación en los protocolos que especificamos a continuación. En la práctica, el lenguaje de contenido real elegido, por supuesto, estará más restringido y permitirá solo un tipo especial de subconjuntos de L de forma compacta, y su elección será crucial para las interacciones que pueden ocurrir entre los agentes de aprendizaje. Para nuestros ejemplos a continuación, simplemente asumimos la enumeración explícita de todos los elementos posibles de los espacios de funciones y espacios de funciones respectivos (D, H, etc.) extendidos por el uso de símbolos comodín ∗ (de modo que nuestro segundo ejemplo anterior se convierta ({D}, ∗, ∗, ∗, ∗)).2.1 Agentes de aprendizaje En nuestro marco, un agente de aprendizaje es esencialmente una función de metetrazonamiento que opera en información sobre procesos de aprendizaje y está situado en un entorno co-habitado por otros agentes de aprendizaje. Esto significa que no solo es capaz de controlar meta a nivel sobre cómo aprender, sino que al hacerlo puede tener en cuenta la información que proporciona otros agentes o el medio ambiente. Aunque son posibles casos puramente cooperativos o híbridos, para los fines de este documento supondremos que los agentes son puramente egoístas, y que, si bien puede haber un potencial de cooperación considerando cómo los agentes pueden mejorar mutuamente el rendimiento de aprendizaje de los demás, no hayMecanismo global que puede hacer cumplir dicho comportamiento cooperativo.2 Formalmente hablando, una función de aprendizaje de los agentes es una función que, dada un conjunto de historias de procesos de aprendizaje anteriores (de uno mismo y potencialmente de procesos de aprendizaje sobre los cuales otros agentes han proporcionado información) y produce unaPaso de aprendizaje, que es su próxima acción de aprendizaje. En el sentido más general, la actualización del proceso de aprendizaje interno de nuestros agentes de aprendizaje puede verse como una función λ: ℘ (l) → L × ℘ (l) que toma un conjunto de historias de aprendizaje de uno mismo y de otros como insumos y calcula un nuevoPaso de aprendizaje que se ejecutará mientras se actualiza el conjunto de historiales de procesos de aprendizaje conocidos (por ejemplo, al agregar la nueva acción de aprendizaje al proceso de aprendizaje propio y deja toda la información sobre los procesos de aprendizaje de los demás intactos). Tenga en cuenta que en λ ({l1, ... ln}) = (l, {l1, ... ln}) Algunos elementos li del conjunto de procesos de aprendizaje de entrada pueden ser descripciones de nuevos datos de aprendizaje recibidos del entorno. La función λ puede ser elegida esencialmente por el agente siempre que se cumpla un requisito, a saber, que los datos de aprendizaje que se usan siempre provienen de lo que se ha observado previamente. Más formalmente, ∀ {L1 ,...ln} ∈ ℘ (l) .λ ({l1, ... ln}) = (l, {l1, ... ln}) ⇒ „d (l) ∪ [l = li [j] d (l)«⊆ [l = li [j] d (l), es decir, cualquier λ que salga como un nuevo paso de aprendizaje y un conjunto actualizado de historias de aprendizaje, no puede inventar nuevos datos;Tiene que funcionar con las muestras que se han puesto a disposición antes en el proceso a través del entorno o de otros agentes (y, por supuesto, puede volver a entrenar en los datos utilizados anteriormente). El objetivo del agente es generar un paso de aprendizaje óptimo en cada iteración dada la información que tiene. Una posibilidad de especificar esto es exigir que ∀ {L1 ,...ln} ∈ ℘ (l) .λ ({l1, ... ln}) = (l, {l1, ... ln}) ⇒ l = arg max l ∈L g (l) (h (l))Pero dado que generalmente no será realista calcular el siguiente paso de aprendizaje óptimo en cada situación, es más útil 2 Tenga en cuenta que nuestra perspectiva no solo es diferente de los modelos comunes y cooperativos de aprendizaje automático distribuido y minería de datos, sino que también delinea nuestro enfoque desdeSistemas de aprendizaje multiagente en los que los agentes aprenden sobre otros agentes [25], es decir, el objetivo de aprendizaje en sí no se ve afectado por el comportamiento de los agentes en el medio ambiente.680 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) I J DJ HJ FJ GJ HJ DI PD → D 1 (DI, DJ)... PD → D KD → D (DI, DJ)......n / A ... Hola ... ... n/a fi... ... n/a gi... N/A PG → H 1 (GI, HJ)... PG → H KG → H (GI, HJ) HI... N/A ... Tabla 1: Matriz de funciones de integración para los mensajes enviados desde el alumno I a J para usar simplemente G (L) (H (L)) como una medida de rendimiento en ejecución para evaluar qué tan bien está funcionando el agente. Esto es demasiado abstracto e inespecífico para nuestros propósitos: si bien describe lo que los agentes deben hacer (transformar la configuración para el próximo paso de aprendizaje de una manera óptima), no especifica cómo se puede lograr esto en la práctica.2.2 Integración de la información del proceso de aprendizaje Para especificar cómo un proceso de aprendizaje de los agentes puede verse afectado por la integración de la información recibida de otros, necesitamos desarrollar los detalles de cómo los pasos de aprendizaje que realizará se pueden modificar utilizando información entrante sobre procesos de aprendizaje descritos por otros agentes(Esto incluye la adquisición de nuevos datos de aprendizaje del entorno como un caso especial). En el caso más general, podemos especificar esto en términos de las posibles modificaciones a la información existente sobre los historiales de aprendizaje que se pueden realizar utilizando nueva información. Para facilitar la presentación, asumiremos que los agentes son procesos de aprendizaje estacionarios que solo pueden registrar el paso de aprendizaje ejecutado previamente e solo intercambiar información sobre este paso de aprendizaje individual (nuestro modelo puede extenderse fácilmente para atender a configuraciones más complejas). Sea LJ = DJ, HJ, FJ, GJ, HJ el estado actual del Agente J Al recibir una descripción del proceso de aprendizaje Li = Di, Hi, Fi, Gi, Hi del Agente I (por el momento, suponemos que esto esun paso de aprendizaje específico y no una descripción más vaga y disyuntiva de las propiedades del paso de aprendizaje de I). Teniendo en cuenta todas las interacciones posibles a nivel abstracto, básicamente obtenemos una matriz de posibilidades para las modificaciones de la especificación de paso de aprendizaje JS como se muestra en la Tabla 1. En esta matriz, cada entrada especifica una familia de funciones de integración PC → C 1 ,..., PC → C KC → C donde C, C ∈ {D, H, F, G, H} y que definen cómo el componente de Agente JS CJ se modificará utilizando la información CI proporcionada sobre (el mismo o un componente diferente de)Paso de aprendizaje aplicando PC → C R (CI, CJ) para algunos r ∈ {1 ,..., KC → C}. Para decirlo más en pocas palabras, las colecciones de las funciones P que utiliza un agente j especifica cómo modificará su propio comportamiento de aprendizaje utilizando la información obtenida de i. Para la diagonal de esta matriz, que contiene las formas más comunes de integrar nueva información en el modelo de aprendizaje propio, las formas obvias de modificar el proceso de aprendizaje propio incluyen reemplazar a CJ por CI o ignorar CI por completo. Las formas más complejas/sutiles de integración de procesos de aprendizaje incluyen: • Modificación de DJ: Agregar DI a DJ;Filtrar todos los elementos de DJ que también aparecen en DI;Agregue DI para que DJ descarte todos los elementos con atributos fuera de rangos que afectan a GJ, o esos elementos ya clasificados correctamente por HJ;• Modificación de HI: use la unión/intersección de HI y HJ;Alternativamente, descarte elementos de HJ que son inconsistentes con DJ en el proceso de intersección o unión, o filtren elementos que no pueden obtenerFJ utilizando información sobre FI;Evalúe su relevancia simulando pasos de aprendizaje anteriores en DJ usando GJ y descarte aquellos que no ayudan a mejorar el rendimiento propio • Modificación de HJ: Combine HJ con HI usando (digamos) operadores lógicos o matemáticos;Haga el uso de HI contingente en una evaluación previa a la integración de su calidad utilizando propios datos de datos y GJ Si bien esta lista no incluye operaciones de integración concreta y completamente incrustadas para los procesos de aprendizaje, es indicativo de la amplia gama de interacciones entre los agentes individuales que aprendenprocesos que nuestro marco habilita. Tenga en cuenta que la lista no incluye ninguna modificación a GJ. Esto se debe a que no permitimos modificaciones en la medida de calidad de los agentes, ya que esto haría que el modelo de acción racional (aprendizaje) sea inútil (si la medida de calidad es relativa y volátil, no podemos juzgar objetivamente el rendimiento del aprendizaje). También tenga en cuenta que algunos de los ejemplos anteriores requieren consultar otros elementos de LJ que los que aparecen como argumentos de las operaciones P;Los omitimos para facilitar la notación, pero enfatizamos que las operaciones ricas en información implicarán consultar muchos aspectos diferentes de LJ. Además de las operaciones a lo largo de la diagonal de la matriz, se pueden concebir operaciones de integración más exóticas que combinan información sobre diferentes componentes. En teoría, podríamos llenar la mayoría de la matriz con entradas para ellos, pero por falta de espacio enumeramos solo unos pocos ejemplos: • Modificación de DJ usando FI: muestras de preproceso en FI, p.Para lograr representaciones intermedias a las que FJ se puede aplicar a • Modificación de DJ usando HI: Filtro de muestras de DJ que están cubiertas por HI y construyen HJ usando FJ solo en las muestras restantes • Modificación de HJ usando FI: Filtro de hipótesis de HJ que sonNo es realizable usando FI • Modificación de HJ usando GI: si HJ está compuesto por varios subcomponentes, filtre los subcomponentes que no funcionan bien según GI •... Finalmente, muchos mensajes recibidos de otros que describen las propiedades de sus procesos de aprendizaje contendrán información sobre varios elementos de un paso de aprendizaje, lo que dará lugar a operaciones aún más complejas que dependen de qué tipos de información están disponibles. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 681 Figura 2: Captura de pantalla de nuestro sistema de simulación, que muestra datos de seguimiento de embarcaciones en línea para la Región 3 del Mar del Norte. Ejemplo de aplicación 3.1 Descripción del dominio Como ilustración de nuestro marco, presentamos un sistema de minería de datos basado en agente para la vigilancia basada en la agrupación utilizando datos AIS (sistema de identificación automática [1]). En nuestro dominio de la aplicación, diferentes agencias comerciales y gubernamentales rastrean los viajes de los barcos con el tiempo utilizando datos AIS que contienen información estructurada proporcionada automáticamente por barcos equipados con estaciones de AIS móviles de barcos a estaciones de tierra, otros barcos y aviones. Estos datos contienen la identidad, el tipo, la posición, el curso, la velocidad, el estado de navegación y otra información relacionada con la seguridad. La Figura 2 muestra una captura de pantalla de nuestro sistema de simulación. Es la tarea de las agencias de AIS detectar el comportamiento anómalos para alarmar a las unidades de policía/guardia costera para investigar más a fondo el comportamiento inusual y potencialmente sospechoso. Tal comportamiento puede incluir cosas como la desviación de las rutas estándar entre el origen declarado y el destino del viaje, encuentros inesperados cercanos entre diferentes buques en el mar o patrones inusuales en la elección del destino en múltiples viajes, tomando el tipo de embarcación e informes informadosflete en cuenta. Si bien las razones de tal comportamiento inusual pueden variar desde una coincidencia pura o problemas técnicos hasta actividades criminales (como contrabando, piratería, ataques terroristas/militares), es obviamente útil preprocesar la gran cantidad de datos de embarcaciones (seguimiento) que están disponiblesAntes de realizar un análisis posterior de expertos humanos. Para admitir esta tarea de preprocesamiento automatizada, el software utilizado por estas agencias aplica métodos de agrupación para identificar valores atípicos y marcarlos como entidades potencialmente sospechosas para el usuario humano. Sin embargo, muchas agencias activas en este dominio son empresas competidoras y usan sus conjuntos de datos (parcialmente superpuestos, pero distintos) e hipótesis de aprendizaje (modelos) como activos y, por lo tanto, no se puede esperar que colaboren de una manera totalmente cooperativa para mejorar los resultados generales del aprendizaje. Teniendo en cuenta que esta es la realidad del dominio en el mundo real, es fácil ver que un marco como el que hemos sugerido anteriormente podría ser útil para explotar el potencial de cooperación que no es explotado por los sistemas actuales.3.2 Diseño del sistema de aprendizaje distribuido basado en agentes Para describir un diseño concreto para el dominio AIS, necesitamos especificar los siguientes elementos del sistema general: 1. Los conjuntos de datos y los algoritmos de agrupación disponibles para los agentes individuales, 2. El mecanismo de interacción utilizado para intercambiar descripciones de los procesos de aprendizaje y 3. Los agentes del mecanismo de decisión se aplican para tomar decisiones de aprendizaje. Con respecto a 1., nuestros agentes están equipados con sus propios conjuntos de datos privados en forma de descripciones de embarcaciones. Las muestras de aprendizaje están representadas por tuplas que contienen datos sobre vasos individuales en términos de atributos a = {1 ,..., n} incluyendo cosas como ancho, longitud, etc. con dominios de valores reales ([ai] = r para todo i). En términos de algoritmo de aprendizaje, consideramos la agrupación con un número fijo de clústeres k que usan los algoritmos de agrupación de k-means y k-medoides [5] (se fijó el significado de que el algoritmo de aprendizaje siempre emitirá clusters K; sin embargo, permitimos que los agentes cambienel valor de k sobre diferentes ciclos de aprendizaje). Esto significa que el espacio de hipótesis puede definirse como H = {C1 ,..., ck | ci ∈ R | a |} es decir, el conjunto de todos los conjuntos posibles de k centroides de clúster en | a | espacio euclidiano de dimensiones. Para cada hipótesis H = C1 ,..., ck y cualquier punto de datos d ∈ × n i = 1 [ai] dominio dado [ai] Para el atributo ésimo de cada muestra, la asignación a los grupos viene dada por C (c1, .., ck, d) = arg min1≤j≤k | d - cj |es decir, D se asigna a ese clúster cuyo centroide está más cerca del punto de datos en términos de distancia euclidiana. Para fines de evaluación, cada conjunto de datos relacionados con un agente en particular I se divide inicialmente en un conjunto de capacitación DI y una validación VI. Luego, generamos un conjunto de embarcaciones falsas fi tal que | fi |= | Vi |. Estos dos conjuntos evalúan la capacidad de los agentes para detectar vasos sospechosos. Para esto, asignamos un valor de confianza R (H, D) a cada barco D: R (H, D) = 1 | D - CC (H, D) |donde C (H, D) es el índice del centroide más cercano. Según esta medida, clasificamos cualquier recipiente en Fi ∪ VI como falso si su valor R está por debajo de la mediana de todas las confidencias R (H, D) para d ∈ Fi ∪ VI. Con esto, podemos calcular la calidad Gi (h) ∈ R como la relación entre todos los vasos clasificados correctamente y todos los vasos en Fi ∪ VI. En lo que respecta a las preocupaciones 2., utilizamos un mecanismo de comercio de hipótesis basado en el protocolo de red de contrato simple (CNP) [20]: antes de cada iteración de aprendizaje, el problema de los agentes (transmitidos públicamente) llamadas para proposiciones (CFP), anunciando su propio modelo numérico de calidad de la calidad numérica. En otras palabras, el iniciador de un CNP describe su propio estado de aprendizaje actual como (∗, ∗, ∗, gi (h), ∗) donde h es su hipótesis/modelo actual. Suponemos que los agentes son sinceros cuando anuncian la calidad de su modelo, pero tenga en cuenta que esta calidad podría ser de relevancia limitada para otros agentes, ya que pueden especializarse en regiones específicas del espacio de datos no relacionado con el conjunto de pruebas del remitente del CFP. Posteriormente, (algunos) agentes pueden emitir ofertas en las que anuncian, a su vez, la calidad de su propio modelo. Si el 682 el sexto intl. Conf.En los agentes autónomos y los sistemas de múltiples agentes (AAMAS 07) son aceptadas por el iniciador del protocolo que emitió el PCP, los agentes intercambian sus hipótesis y la próxima iteración de aprendizaje. Para describir lo que es necesario para 3., tenemos que especificar (i) bajo las condiciones de las condiciones que los agentes presentan ofertas en respuesta a un PCP, (ii) cuando aceptan ofertas en el proceso de negociación de CNP y (iii) cómo integran el recibidoinformación en su propio proceso de aprendizaje. Con respecto a (i) y (ii), empleamos una regla muy simple que es idéntica en ambos casos: dejemos que sean la calidad del modelo propio y que se anuncie el CFP (o la oferta más alta, respectivamente). Si G> G respondemos a la CFP (acepte la oferta), de lo contrario, responda al CFP (aceptar la oferta) con probabilidad P (G /G) e ignorarla (rechazar) más. Si dos agentes llegan a un acuerdo, intercambian sus hipótesis de aprendizaje (modelos). En nuestros experimentos, G y G son calculados por un agente adicional que actúa como un mecanismo de validación global para todos los agentes (en un entorno más realista, tendrá que proporcionar un mecanismo de comparación para diferentes funciones G). En cuanto a (iii), cada agente utiliza un operador de fusión de modelo único tomado de las siguientes dos clases de operadores (HJ es el modelo de receptores propios y HI es el modelo de proveedores): • Ph → H (HI, HJ): - M-Únase: los mejores grupos (en términos de cobertura de DJ) de la hipótesis Hi se agregan a HJ.- M-Select: el conjunto de los mejores grupos (en términos de cobertura de DJ) de la Unión Hi ∪HJ se elige como un nuevo modelo.(A diferencia de M-Join, este método no prefiere los grupos propios sobre otros.) • Ph → D (HI, DJ):-M-Filter: Los mejores grupos (como anteriormente) de HI se identifican y se adhieren a un nuevo modelo formadoAl usar esas muestras no cubiertas por estos grupos que aplican el propio algoritmo de aprendizaje FJ. Cada vez que M es lo suficientemente grande como para abarcar todos los grupos, simplemente escribimos unir o filtrar para ellos. En la Sección 4 analizamos el rendimiento de cada una de estas dos clases para diferentes opciones de m.Es de destacar que este sistema de minería de datos distribuidos basado en agentes es una de las instancias concebibles más simples de nuestra arquitectura abstracta. Si bien anteriormente lo hemos aplicado también a una arquitectura basada en el mercado más compleja utilizando estudiantes de programación de lógica inductiva en un dominio de logística de transporte [22], creemos que el sistema descrito aquí es lo suficientemente complejo como para ilustrar las decisiones clave de diseño involucradas en el uso de nuestro marcoy proporciona soluciones de ejemplo simples para estos problemas de diseño.4. Resultados experimentales La Figura 3 muestra los resultados obtenidos de las simulaciones con tres agentes de aprendizaje en el sistema anterior utilizando los métodos de agrupación de K-means y K-Medoides respectivamente. Participamos el conjunto de datos total de 300 barcos en tres conjuntos de disjunto de 100 muestras cada una y asignamos cada una de estas a un agente de aprendizaje. El agente único está aprendiendo de todo el conjunto de datos. El parámetro K se establece en 10, ya que este es el valor óptimo para el conjunto de datos total de acuerdo con el índice Davies-Bouldin [9]. Para M-Select asumimos M = K que logra una Figura 3 constante 3: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades de alumnos homogéneas que utilizan el tamaño del modelo del modelo de métodos K-means (TOP) y K-Medoides (inferior). Para M-Join y M-Filter asumimos que M = 3 limita el grado en que los modelos aumentan con el tiempo. Durante cada experimento, los agentes de aprendizaje reciben descripciones de barcos en lotes de 10 muestras. Entre estos lotes, hay suficiente tiempo para intercambiar los modelos entre los agentes y recomputar los modelos si es necesario. Cada barco se describe utilizando atributos de ancho, longitud, borrador y velocidad con el objetivo de aprender a detectar qué buques han proporcionado descripciones falsas de sus propias propiedades. El conjunto de validación contiene 100 barcos falsos reales y 100 generados al azar. Para generar propiedades suficientemente realistas para los barcos falsos, sus valores de atributos individuales se toman de los barcos seleccionados al azar en el conjunto de validación (de modo que cada muestra falsa es una combinación de valores de atributo de varias naves existentes). En estos experimentos, estamos interesados principalmente en investigar si una forma simple de intercambio de conocimientos entre los agentes de aprendizaje egoístas podría mejorar el rendimiento del agente en comparación con una configuración de alumnos aislados. De este modo, distinguimos entre sociedades de alumnos homogéneas donde todos los agentes usan el mismo algoritmo de agrupación y los heterogéneos donde diferentes agentes usan diferentes algoritmos. Como se puede ver en los gráficos de rendimiento en la Figura 3 (caso homogéneo) y 4 (caso heterogéneo, dos agentes usan el mismo método y un agente usa el otro) Este es claramente el caso de las operaciones de integración de unión y filtro (sin restricciones) (m = k) En ambos casos. Esto es bastante natural, ya que estas operaciones equivalen a compartir todo el conocimiento del modelo disponible entre los agentes (bajo restricciones apropiadas dependiendo de cuán beneficioso parezca el intercambio a los agentes). Podemos ver que la calidad de estas operaciones está muy cerca del agente único que tiene acceso a todos los datos de capacitación. Para los métodos restringidos (m <k) m-unes, m-filter y m-select, también podemos observar una distinción interesante, la sexta intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 683 Figura 4: Resultados de rendimiento obtenidos para diferentes operaciones de integración en sociedades heterogéneas con la mayoría de los alumnos que usan los métodos de K-means (Top) y K-Medoides (abajo) que son, a saber, que estosActuar de manera similar al caso de alumno aislado en grupos de agentes homogéneos, pero mejor que los alumnos aislados en sociedades más heterogéneas. Esto sugiere que los alumnos heterogéneos pueden beneficiarse incluso del intercambio de conocimientos bastante limitado (y esto es lo que el uso de un M = 3 bastante pequeño equivale a que K = 10), mientras que esto no siempre es cierto para los agentes homogéneos. Esto ilustra bien cómo los diferentes algoritmos de aprendizaje o minería de datos pueden especializarse en diferentes partes del espacio de problemas y luego integrar sus resultados locales para lograr un mejor rendimiento individual. Además de estos obvios beneficios de rendimiento, la integración de los resultados de aprendizaje parcial también puede tener otras ventajas: la operación M-Filter, por ejemplo, disminuye el número de muestras de aprendizaje y, por lo tanto, puede acelerar el proceso de aprendizaje. El número relativo de ejemplos filtrados medidos en nuestros experimentos se muestra en la siguiente tabla.K-means K-Medoids que filtran 30-40 % 10-20 % M-filtering 20-30 % 5-15 % La conclusión general de la que podemos sacar de estos experimentos iniciales con nuestra arquitectura es que, dado que una aplicación muy simplista de sus principios haProbado de mejorar el rendimiento de los agentes de aprendizaje individuales, vale la pena investigar formas más complejas de intercambio de información sobre los procesos de aprendizaje entre los alumnos autónomos.5. Trabajo relacionado ya hemos mencionado el trabajo sobre el aprendizaje automático distribuido (no agente) y la minería de datos en el capítulo introductorio, por lo que en esta sección nos restringiremos a los enfoques que están más estrechamente relacionados con nuestra perspectiva de los sistemas de aprendizaje distribuidos. Muy a menudo, los enfoques que supuestamente están basados en el agente ignoran la autonomía del agente y prescriben procedimientos locales de toma de decisiones a priori. Un ejemplo típico para este tipo de sistema es el sugerido por Caragea et al.[6] que se basa en un enfoque de máquina de vectores de soporte distribuido donde los agentes unen incrementalmente sus conjuntos de datos de acuerdo con un algoritmo distribuido fijo. Un ejemplo similar es el trabajo de Weiss [24], donde los grupos de agentes clasificadores aprenden a organizar su actividad para optimizar el comportamiento del sistema global. La diferencia entre este tipo de sistemas de aprendizaje basados en agentes colaborativos [16] y nuestro propio marco es que estos enfoques asumen un objetivo de aprendizaje conjunto que todos los agentes perseguen en colaboración. Muchos enfoques dependen en gran medida de una suposición de homogeneidad: Plaza y Ontanon [15] sugieren métodos para la reutilización inteligente basada en agentes en el razonamiento basado en casos, pero solo son aplicables a sociedades de alumnos homogéneos (y acuñado hacia un método de aprendizaje específico). Klusch et al. Presentan un método basado en agente para integrar procesos de análisis de clúster distribuidos utilizando una estimación de densidad.[13] que también está diseñado específicamente para un algoritmo de aprendizaje particular. Lo mismo es cierto para [22, 23] que presentan mecanismos basados en el mercado para agregar la producción de múltiples agentes de aprendizaje, a pesar de que estos enfoques consideran mecanismos de interacción más interesantes entre los alumnos. También se han propuesto varios enfoques para compartir datos de aprendizaje [18]: Grecu y Becker [12] sugieren un intercambio de muestras de aprendizaje entre los agentes y Ghosh et al.[11] es un paso en la dirección correcta en términos de revelar solo información parcial sobre el proceso de aprendizaje, ya que trata el intercambio de información limitado en la agrupación distribuida. Papyrus [3] es un sistema que proporciona un lenguaje de marcado para la metadescripción de datos, hipótesis y resultados intermedios y permite un intercambio de toda esta información entre diferentes nodos, sin embargo, con un objetivo estrictamente cooperativo de distribuir la carga para datos distribuidos masivamentetareas mineras. El sistema masculino [19] era un sistema de aprendizaje multiagente muy temprano en el que los agentes utilizaron un enfoque de pizarra para comunicar sus hipótesis. Los agentes pudieron criticar hipótesis de los demás hasta llegar a un acuerdo. Sin embargo, todos los agentes en este sistema eran idénticos y el sistema era estrictamente cooperativo. El sistema de animales [10] se utilizó para simular el aprendizaje multiestrategia mediante la combinación de dos o más técnicas de aprendizaje (representadas por agentes heterogéneos) para superar las debilidades en los algoritmos individuales, sin embargo, también fue un sistema estrictamente cooperativo. Como muestran estos ejemplos y, según nuestro conocimiento, no ha habido intentos previos de proporcionar un marco que pueda acomodar agentes de aprendizaje independientes y heterogéneos y esto puede considerarse como la principal contribución de nuestro trabajo.6. Conclusión En este documento, describimos un marco genérico y abstracto para el aprendizaje automático distribuido y la minería de datos. Este marco constituye, hasta donde sabemos, el primer intento 684 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) para capturar formas complejas de interacción entre los alumnos heterogéneos y/o interesados en una arquitectura que puede usarse como base para implementar sistemas que utilizan mecanismos de interacción y razonamiento complejos para permitir agentesPara informar y mejorar sus habilidades de aprendizaje con la información proporcionada por otros alumnos en el sistema, siempre que todos los agentes participen en una actividad de aprendizaje suficientemente similar. Para ilustrar que los principios abstractos de nuestra arquitectura pueden convertirse en sistemas concretos y computacionales, describimos un sistema de agrupación distribuido basado en el mercado que se evaluó en el dominio del seguimiento de los vasos para identificar el comportamiento desviado o sospechoso. Aunque nuestros resultados experimentales solo insinúan el potencial de usar nuestra arquitectura, subrayan que lo que estamos proponiendo es factible en principio y puede tener efectos beneficiosos incluso en su instanciación más simple. Sin embargo, hay una serie de problemas que no hemos abordado en la presentación de la arquitectura y su evaluación empírica: en primer lugar, no hemos considerado el costo de la comunicación y suponemos la suposición implícita de que la comunicación requerida es de forma gratuita. Por supuesto, esto es inadecuado si queremos evaluar nuestro método en términos del esfuerzo total requerido para producir una cierta calidad de resultados de aprendizaje. En segundo lugar, no hemos experimentado con agentes que usan algoritmos de aprendizaje completamente diferentes (por ejemplo, simbólicos y numéricos). En los sistemas compuestos por agentes completamente diferentes, las circunstancias bajo las cuales se puede lograr el intercambio de información exitoso puede ser muy diferente de los descritos aquí, y pueden ser necesarios métodos de comunicación y razonamiento mucho más complejos para lograr una integración útil de los diferentes procesos de aprendizaje de los agentes. Finalmente, se deben desarrollar criterios de evaluación más sofisticados para tales arquitecturas de aprendizaje distribuido para arrojar algo de luz sobre cuáles deberían ser las medidas correctas de optimización para los agentes de razonamiento y comunicación de forma autónoma. Estos problemas, junto con una investigación más sistemática y exhaustiva de los mecanismos avanzados de interacción y comunicación para agentes distribuidos, colaboradores y competidores serán objeto de nuestro trabajo futuro sobre el tema. Reconocimiento: Agradecemos el apoyo de la investigación presentada por el Proyecto del Laboratorio de Investigación del Ejército N62558-03-0819 y la Oficina del Proyecto de Investigación Naval N00014-06-1-0232.7. Referencias [1] http://www.aislive.com.[2] http://www.healthagents.com.[3] S. Bailey, R. Grossman, H. Sivakumar y A. Turinsky. Papiro: un sistema para la minería de datos sobre grupos de áreas locales y amplias y super clústeres. En Proc.de la conferencia sobre supercomputación.1999. [4] E. Bauer y R. Kohavi. Una comparación empírica de los algoritmos de clasificación de votación: bolsas, impulso y variantes. Aprendizaje automático, 36, 1999. [5] P. Berkhin. Encuesta de técnicas de minería de datos de agrupación, Informe técnico, Software de Acrue, 2002. [6] D. Caragea, A. Silvescu y V. Honavar. Agentes que aprenden de las fuentes de datos dinámicas distribuidas. En Proc.del taller sobre agentes de aprendizaje, 2000. [7] N. Chawla y S. E. Abd L. O. Sala. Creación de conjuntos de clasificadores. En Actas de ICDM 2001, páginas 580-581, San José, CA, EE. UU., 2001. [8] D. Dash y G. F. Cooper. Promedio del modelo para la predicción con redes bayesianas discretas. Journal of Machine Learning Research, 5: 1177-1203, 2004. [9] D. L. Davies y D. W. Bouldin. Una medida de separación de clúster. Transacciones IEEE sobre análisis de patrones e inteligencia de máquinas, 4: 224-227, 1979. [10] P. Edwards y W. Davies. Un sistema de aprendizaje heterogéneo de múltiples agentes. En Actas del grupo de interés especial sobre sistemas basados en el conocimiento cooperante, páginas 163-184, 1993. [11] J. Ghosh, A. Strehl y S. Merugu. Un marco de consenso para integrar clúster distribuidos bajo un intercambio de conocimientos limitado. En el taller de NSF sobre la minería de datos de próxima generación, 99-108, 2002. [12] D. L. Grecu y L. A. Becker. Aprendizaje coactivo para minería de datos distribuidos. En Actas de KDD-98, páginas 209-213, Nueva York, NY, agosto de 1998. [13] M. Klusch, S. Lodi y G. Moro. Minería de datos distribuidos basados en agentes: el esquema KDEC. En AgentLink, número 2586 en LNC. Springer, 2003. [14] T. M. Mitchell. Aprendizaje automático, páginas 29-36. McGraw-Hill, Nueva York, 1997. [15] S. Ontanon y E. Plaza. Datos de reciclaje para aprendizaje de múltiples agentes. En Proc.de ICML-05, 2005. [16] L. Panait y S. Luke. Aprendizaje cooperativo de múltiples agentes: el estado del arte. Agentes autónomos y sistemas de múltiples agentes, 11 (3): 387-434, 2005. [17] B. Park y H. Kargupta. Minería de datos distribuidos: algoritmos, sistemas y aplicaciones. En N. Ye, editor, Manual de minería de datos, páginas 341-358, 2002. [18] F. J. Provost y D. N. Hennessy. Escala: aprendizaje automático distribuido con cooperación. En Proc.de AAAI-96, páginas 74-79. AAAI Press, 1996. [19] S. Sian. Extender el aprendizaje a múltiples agentes: problemas y un modelo para el aprendizaje automático de múltiples agentes (MA-ML). En Y. Kodratoff, editor, Machine LearningeWSL-91, páginas 440-456. Springer-Verlag, 1991. [20] R. Smith. El protocolo de red de contrato: comunicación y control de alto nivel en un solucionador de problemas distribuido. Transacciones IEEE en computadoras, C-29 (12): 1104-1113, 1980. [21] S. J. Stolfo, A. L. Prodromidis, S. Tselepis, W. Lee, D. W. Fan y P. K. Chan. Jam: agentes de Java para meta-aprendizaje sobre bases de datos distribuidas. En Proc.del KDD-97, páginas 74-81, EE. UU., 1997. [22] J. Toˇziˇcka, M. Jakob y M. Pˇechouˇcek. Enfoque inspirado en el mercado para el aprendizaje colaborativo. En los agentes de información cooperativa X (CIA 2006), volumen 4149 de LNCS, páginas 213-227. Springer, 2006. [23] Y. Z. Wei, L. Moreau y N. R. Jennings. Sistemas de recomendación: un diseño basado en el mercado. En Actas de AAMAS-03), páginas 600-607, 2003. [24] G. Weiß. Una perspectiva múltiple de aprendizaje automático paralelo y distribuido. En Actas de Agentes98, páginas 226-230, 1998. [25] G. Weiss y P. Dillenbourg. ¿Qué es Multi en el aprendizaje de múltiples agentes? Learning colaborativo: enfoques cognitivos y computacionales, 64-80, 1999. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 685