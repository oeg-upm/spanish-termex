Un estudio del modelo de generación de consultas de Poisson para la recuperación de información Qiaozhu Mei, Hui Fang, Chengxiang Zhai Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign Urbana, IL 61801 {QMEI2, HFANG, CZHAI}@uiuc.edu Abstract muchas variadores de idiomas modelos de idiomashan sido propuestos para la recuperación de información. La mayoría de los modelos existentes se basan en la distribución multinomial y obtendrían documentos basados en la probabilidad de consulta calculada en función de un modelo probabilístico de generación de consultas. En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en la distribución de Poisson. Mostramos que, si bien en sus formas más simples, la nueva familia de modelos y los modelos multinomiales existentes son equivalentes, se comportan de manera diferente para muchos métodos de suavizado. Mostramos que el modelo Poisson tiene varias ventajas sobre el modelo multinomial, que incluye un suavizado de acomodación natural por término y que permite un modelado de fondo más preciso. Presentamos varias variantes del nuevo modelo correspondiente a diferentes métodos de suavizado, y las evaluamos en cuatro colecciones representativas de pruebas TREC. Los resultados muestran que, si bien sus modelos básicos funcionan de manera comparable, el modelo Poisson puede superar al modelo multinomial con suavizado por término. El rendimiento se puede mejorar aún más con un suavizado de dos etapas. Categorías y descriptores de asignaturas: H.3.3 [Búsqueda y recuperación de información]: Modelos de recuperación Términos generales: Algoritmos 1. Introducción Como nuevo tipo de modelos de recuperación probabilística, se ha demostrado que los modelos de lenguaje son efectivos para muchas tareas de recuperación [21, 28, 14, 4]. Entre muchas variantes de modelos de idiomas propuestos, el más popular y fundamental es el modelo de lenguaje de generación de consultas [21, 13], lo que conduce al método de puntuación de la probabilidad de consulta para clasificar documentos. En dicho modelo, dada una consulta q y un documento D, calculamos la probabilidad de generar consultas Q con un modelo estimado en base al documento D, es decir, la probabilidad condicional P (Q | D). Luego podemos clasificar los documentos basados en la probabilidad de generar la consulta. Prácticamente todos los modelos de lenguaje de generación de consultas existentes se basan en distribución multinomial [19, 6, 28] o distribución multivariada de Bernoulli [21, 18]. La distribución multinomial es especialmente popular y también se muestra bastante efectiva. El uso intensivo de la distribución multinomial se debe en parte al hecho de que se ha utilizado con éxito en el reconocimiento de voz, donde la distribución multinomial es una opción natural para modelar la aparición de una palabra particular en una posición particular en el texto. En comparación con Bernoulli multivariado, la distribución multinomial tiene la ventaja de poder modelar la frecuencia de los términos en la consulta;Por el contrario, Bernoulli multivariado solo modela la presencia y la ausencia de términos de consulta, por lo que no pueden capturar diferentes frecuencias de términos de consulta. Sin embargo, Bernoulli multivariado también tiene una ventaja potencial sobre multinomial desde el punto de vista de la recuperación: en una distribución multinomial, las probabilidades de todos los términos deben sumar a 1, lo que hace que sea difícil acomodar el suavizado por plazo, mientras que en un Bernoulli multivariado, elLas probabilidades de presencia de diferentes términos son completamente independientes entre sí, acomodando fácilmente el suavizado y la ponderación por término. Tenga en cuenta que la ausencia del término también se captura indirectamente en un modelo multinomial a través de la restricción de que todas las probabilidades del término deben sumar a 1. En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en la distribución de Poisson. En esta nueva familia de modelos, modelamos la frecuencia de cada término independientemente con una distribución de Poisson. Para calificar un documento, primero estimaríamos un modelo de Poisson multivariado basado en el documento, y luego lo puntuaríamos en función de la probabilidad de la consulta dada por el modelo estimado de Poisson. En cierto sentido, el modelo Poisson combina la ventaja de la frecuencia multinomial en el modelado del término y la ventaja de los Bernoulli multivariados para acomodar el suavizado por término. De hecho, de manera similar a la distribución multinomial, los modelos de distribución de Poisson a términos frecuencias, pero sin la restricción de que todas las probabilidades del término deben sumar a 1, y similar a Bernoulli multivariado, modela cada término de forma independiente, por lo tanto, puede acomodar fácilmente el suavizado por término. Al igual que en el trabajo existente en modelos de lenguaje multinomial, el suavizado es fundamental para esta nueva familia de modelos. Derivamos varios métodos de suavizado para el modelo Poisson en paralelo a los utilizados para distribuciones multinomiales, y comparamos los modelos de recuperación correspondientes con los basados en distribuciones multinomiales. Encontramos que, si bien con algunos métodos de suavizado, el nuevo modelo y el modelo multinomial conducen a exactamente la misma fórmula, con algunos otros métodos de suavizado que divergen, y el modelo Poisson brinda más flexibilidad para suavizar. En particular, una diferencia clave es que el modelo de Poisson puede acomodar naturalmente el suavizado perterno, lo cual es difícil de lograr con un modelo multinomial sin giro heurístico de la semántica de un modelo generativo. Explotamos esta ventaja potencial para desarrollar un nuevo algoritmo de suavizado dependiente del término para el modelo Poisson y mostramos que este nuevo algoritmo de suavizado puede mejorar el rendimiento sobre los algoritmos de suavizado independientes del término utilizando el modelo Poisson o multinomial. Esta ventaja se ve tanto para el suavizado de una etapa como en dos etapas. Otra ventaja potencial del modelo Poisson es que su modelo de fondo correspondiente para suavizado se puede mejorar mediante el uso de un modelo de mezcla que tiene una fórmula de forma cerrada. Se muestra que este nuevo modelo de fondo supera al modelo de fondo estándar y reduce la sensibilidad del rendimiento de la recuperación al parámetro de suavizado. El resto del documento está organizado de la siguiente manera. En la Sección 2, presentamos a la nueva familia de modelos de generación de consultas con distribución de Poisson, y presentamos varios métodos de suavizado que conducen a diferentes funciones de recuperación. En la Sección 3, comparamos analíticamente el modelo de lenguaje Poisson con el modelo de lenguaje multinomial, desde la perspectiva de la recuperación. Luego diseñamos experimentos empíricos para comparar las dos familias de los modelos de idiomas en la Sección 4. Discutimos el trabajo relacionado en 5 y concluimos en 6. 2. Generación de consultas con el proceso Poisson En el marco de generación de consultas, una suposición básica es que se genera una consulta con un modelo estimado en base a un documento. En la mayoría de los trabajos existentes [12, 6, 28, 29], las personas suponen que cada palabra de consulta se muestrean independientemente de una distribución multinomial. Alternativamente, suponemos que se genera una consulta muestreando la frecuencia de las palabras de una serie de procesos de Poisson independientes [20].2.1 El proceso de generación Sea v = {w1, ..., wn} un conjunto de vocabulario. Sea w una pieza de texto compuesta por un autor y c (w1), ..., c (wn) un vector de frecuencia que representa w, donde c (wi, w) es el recuento de frecuencia del término wi en el texto w.En recuperación, W podría ser una consulta o un documento. Consideramos los recuentos de frecuencia de los N Términos N únicos en W As N diferentes tipos de eventos, muestreados a partir de n procesos de Poisson homogéneos independientes, respectivamente. Supongamos que T es el período de tiempo durante el cual el autor compuso el texto. Con un proceso de Poisson homogéneo, el recuento de frecuencia de cada evento, es decir, el número de ocurrencias de WI, sigue una distribución de Poisson con el parámetro asociado λit, donde λi es un parámetro de velocidad que caracteriza el número esperado de WI en un tiempo de unidad. La función de densidad de probabilidad de dicha distribución de Poisson viene dada por p (c (wi, w) = k | λit) = e - λit (λit) k k! Sin perder la generalidad, establecemos T a la longitud del texto w (las personas escriben una palabra en un tiempo de unidad), es decir, t = | w |. Con n tales procesos de Poisson independientes, cada uno de los cuales explica la generación de un término en el vocabulario, la probabilidad de que se genere W a partir de dichos procesos de Poisson se puede escribir como p (w | λ) = n i = 1 p (c (wi, w, w, w) | Λ) = n i = 1 e - λi · | w |(λi · | w |) c (wi, w) c (wi, w)!donde λ = {λ1, ..., λn} y | w |= n i = 1 c (wi, w). Nos referimos a estos n procesos Poisson independientes con el parámetro λ como modelo de lenguaje de Poisson. Sea d = {d1, ..., dm} un conjunto observado de muestras de documentos generadas a partir del proceso de Poisson anterior. La estimación de máxima probabilidad (MLE) de λi es ˆλi = d∈D C (wi, d) d∈D w ∈V c (w, d) Tenga en cuenta que este MLE es diferente de la MLE para la distribución de Poisson sin considerar el documentolongitudes, que aparecen en [22, 24]. Dado un documento D, podemos estimar un modelo de lenguaje Poisson λd usando D como muestra. La probabilidad de que una consulta q se genere a partir del modelo de lenguaje de documento λd se puede escribir como p (q | d) = w∈V p (c (w, q) | λd) (1) Esta representación es claramente diferente de la multinomialModelo de generación de consultas AS (1) La probabilidad incluye todos los términos en el vocabulario V, en lugar de solo aquellos que aparecen en Q, y (2) en lugar de la aparición de términos, el espacio de eventos de este modelo son las frecuencias de cada término. En la práctica, tenemos la flexibilidad de elegir el vocabulario v. En un extremo, podemos usar el vocabulario de toda la colección. Sin embargo, esto puede generar ruido y un costo computacional considerable. En el otro extremo, podemos centrarnos en los términos en la consulta e ignorar otros términos, pero se puede perder cierta información útil ignorando los términos que no son Query. Como compromiso, podemos combinar todos los términos no quirales como un solo término pseudo. En otras palabras, podemos suponer que hay exactamente un término no cuidante en el vocabulario para cada consulta. En nuestros experimentos, adoptamos esta estrategia de término pseudo no cuidante. Se puede calificar un documento con la probabilidad de la ecuación 1. Sin embargo, si un término de consulta no se ve en el documento, el MLE de la distribución de Poisson asignaría una probabilidad cero al término, lo que causa la probabilidad de que la consulta sea cero. Al igual que en los enfoques de modelado de idiomas existentes, el principal desafío de construir un modelo de recuperación razonable es encontrar un modelo de lenguaje suavizado para P (· | D).2.2 suavizado en el modelo de recuperación de Poisson En general, queremos asignar tarifas no cero para los términos de consulta que no se ven en el documento d.Se han propuesto muchos métodos de suavizado para modelos de lenguaje multinomial [2, 28, 29]. En general, tenemos que descartar las probabilidades de algunas palabras vistas en el texto para dejar una masa de probabilidad adicional para asignar a las palabras invisibles. Sin embargo, en los modelos de lenguaje de Poisson, no tenemos la misma restricción que en un modelo multinomial (es decir, w∈V P (w | d) = 1). Por lo tanto, no tenemos que descartar la probabilidad de las palabras vistas para dar una tarifa distinta de cero a una palabra invisible. En cambio, solo necesitamos garantizar que K = 0,1,2, ... P (C (W, D) = K | D) = 1. En esta sección, presentamos tres estrategias diferentes para suavizar un modelo de lenguaje de Poisson y mostramos cómo conducen a diferentes funciones de recuperación.2.2.1 suavizado bayesiano utilizando gamma antes del marco de minimización de riesgo en [11], suponemos que un documento se genera mediante la llegada de los términos en un período de tiempo de | D |De acuerdo con el modelo de lenguaje de documento, que esencialmente consiste en un vector de tarifas de Poisson para cada término, es decir, λd = λd, 1, ..., λd, | V |. Se supone que un documento se genera a partir de un modelo potencialmente diferente. Dado un documento en particular D, queremos estimar λd. La tasa de un término se estima independientemente de otros términos. Utilizamos la estimación bayesiana con el siguiente gamma anterior, que tiene dos parámetros, α y β: gamma (λ | α, β) = βα γ (α) λα - 1 e - βλ para cada término W, los parámetros αW y βW sonElegido para ser αW = µ ∗ λc, W y βW = µ, donde µ es un parámetro y λc, W es la velocidad de W estimada a partir de algún modelo de lenguaje de antecedentes, generalmente el modelo de lenguaje de recolección. La distribución posterior de λd viene dada por p (λd | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d | d |V |Distribuciones gamma con parámetros c (w, d) + µλc, w y | d |+ µ para cada palabra w.Dado que la media gamma es α β, tenemos ˆλd, w = λd, w λd, wp (λd, w | d, c) dλd, w = c (w, d) + µλc, w | d |+ µ Esta es precisamente la estimación suavizada del modelo de lenguaje multinomial con Dirichlet Prior [28].2.2.2 Interpolación (Jelinek-Mercer) suavizando otro método directo es descomponer el modelo de generación de consultas como una mezcla de dos modelos de componentes. Uno es el modelo de lenguaje de documento estimado con un estimador de máxima verosimilitud, y el otro es un modelo estimado a partir de los antecedentes de la colección, P (· | C), que asigna una tasa distinta de cero a w.Por ejemplo, podemos usar un coeficiente de interpolación entre 0 y 1 (es decir, Δ ∈ [0, 1]). Con esta interpolación simple, podemos obtener un documento con puntaje (d, q) = w∈V log ((1 - Δ) P (C (W, Q) | D) + ΔP (C (W, Q) | C) (2) Usando el estimador de máxima verosimilitud para P (· | d), tenemos λd, w = c (w, d) | d |, por lo tanto, la ecuación 2 se convierte en la puntuación (d, q) ∝ w∈D∩q [log (1 + 1 - Δ δ e - λd, w | q | (λd, w | q |) c (w, q) c (w, q)! · P (c (w, q) | c)) - log (1 - δ) e - λd, w | q |+ ΔP (C (W, Q) = 0 | C) 1 - Δ + ΔP (C (W, Q) = 0 | C)] + W∈D log (1 - δ) E - λd, W | Q |+ ΔP (C (W, Q) = 0 | C) 1 - Δ + ΔP (C (W, Q) = 0 | C) También podemos usar un modelo de lenguaje Poisson para P (· | C), o usar algunosOtros modelos basados en frecuencia. En la fórmula de recuperación anterior, la primera suma se puede calcular de manera eficiente. La segunda suma puede tratarse como un documento anterior, que penaliza los documentos largos. Como la segunda suma es difícil de calcular de manera eficiente, combinamos todos los términos no quirados como un pseudo no quirado, denotado como N. utilizando la formulación de pseudo a término y un modelo de colección de Poisson, podemos reescribir la fórmula de recuperación como puntaje (D, q) ∝ w∈D∩q log (1 + 1 - Δ δ e - λd, w (λd, w | q |) c (w, q) e - λd, c | q | (λd, c) c) c)(w, q)) + log (1 - δ) e - λd, n | q |+ ΔE - λc, n | Q |1 - Δ + ΔE - λc, n | Q |(3) donde λd, n = | d | - w∈Q c (w, d) | d |y λc, n = | c | - w∈Q c (w, c) | c |.2.2.3 suavizado de dos etapas Como se discutió en [29], el suavizado juega dos roles en la recuperación: (1) para mejorar la estimación del modelo de lenguaje de documentos y (2) explicar los términos comunes en la consulta. Para distinguir el contenido y las palabras no discriminativas en una consulta, seguimos [29] y asumimos que se genera una consulta mediante el muestreo de una mezcla de dos componentes de modelos de lenguaje Poisson, siendo un componente el modelo de documento λd y elOtro es un modelo de lenguaje de antecedentes de consulta P (· | U).P (· | U) Modela las frecuencias de término típicas en las consultas de los usuarios. Luego podemos obtener cada documento con la probabilidad de consulta calculada utilizando el siguiente modelo de suavizado de dos etapas: P (C (W, Q) | λd, U) = (1-δ) P (C (W, Q) | λd)+ ΔP (C (W, Q) | U) (4) donde δ es un parámetro, lo que indica aproximadamente la cantidad de ruido en q. Esto se parece al suavizado de interpolación, excepto que P (· | λd) ahora debería ser un modelo de lenguaje suavizado, en lugar del estimado con MLE. Sin conocimiento previo sobre P (· | U), podríamos establecerlo en P (· | C). Cualquier método de suavizado para el modelo de lenguaje de documento se puede utilizar para estimar P (· | D) como el suavizado gamma como se discute en la Sección 2.2.1. El estudio empírico de los métodos de suavizado se presenta en la Sección 4. 3. Análisis del modelo de lenguaje Poisson De la sección anterior, notamos que el modelo de lenguaje Poisson tiene una fuerte conexión con el modelo de lenguaje multinomial. Esto se espera ya que ambos pertenecen a la familia exponencial [26]. Sin embargo, hay muchas diferencias cuando estas dos familias de modelos se aplican con diferentes métodos de suavizado. Desde la perspectiva de la recuperación, ¿funcionarán estos dos modelos de idiomas de manera equivalente? Si no, ¿qué modelo proporciona más beneficios para la recuperación o proporciona flexibilidad que podría conducir a posibles beneficios? En esta sección, analizamos analíticamente las características de recuperación de los modelos de lenguaje Poisson, comparando su comportamiento con el de los modelos de lenguaje multinomial.3.1 La equivalencia de los modelos básicos comencemos con la suposición de que todos los términos de consulta aparecen en cada documento. Bajo esta suposición, no se necesita suavizado. Un documento puede ser calificado por la probabilidad log de la consulta con la estimación de máxima probabilidad: puntaje (d, q) = w∈V log e - λd, w | q |(λd, w | q |) c (w, q) c (w, q)!(5) Usando el MLE, tenemos λd, w = c (w, d) w∈V c (w, d). Así, puntaje (d, q) ∝ c (w, q)> 0 c (w, q) log c (w, d) w∈V c (w, d) Esta es exactamente la probabilidad log de la consulta si el documentoEl modelo de idioma es un multinomial con una estimación de máxima verosimilitud. De hecho, incluso con suavizado de gamma, al conectar λd, w = c (w, d)+µλc, w | d |+µ y λc, w = c (w, c) | c |En la ecuación 5, es fácil mostrar que la puntuación (d, q) ∝ w∈Q∩d c (w, q) log (1 + c (w, d) µ · c (w, c) | c |) + +| P |log µ | d |+ µ (6) que es exactamente la fórmula de recuperación de Dirichlet en [28]. Tenga en cuenta que esta equivalencia se mantiene solo cuando la variación de la longitud del documento se modela con el proceso Poisson. Esta derivación indica la equivalencia del poisson básico y los modelos de lenguaje multinomial para la recuperación. Sin embargo, con otras estrategias de suavizado, los dos modelos serían diferentes. Sin embargo, con esta equivalencia en modelos básicos, podríamos esperar que el modelo de lenguaje Poisson realice comparablemente el modelo de lenguaje multinomial en recuperación, si solo se explora un suave simple. Basado en este análisis de equivalencia, uno puede preguntar por qué debemos perseguir el modelo de lenguaje Poisson. En las siguientes secciones, mostramos que a pesar de la equivalencia en sus modelos básicos, el modelo de lenguaje Poisson aporta una flexibilidad adicional para explorar técnicas avanzadas en varias características de recuperación, que no se pueden lograr con modelos de lenguaje multinomial.3.2 suavizante dependiente de términos Una flexibilidad del modelo de lenguaje Poisson es que proporciona un marco natural para acomodar el suavizado dependiente del término (por período). El trabajo existente en el suavizado del modelo de lenguaje ya ha demostrado que diferentes tipos de consultas deben suavizarse de manera diferente de acuerdo con cuán discriminatorios son los términos de consulta.[7] también predijo que diferentes términos deberían tener un peso de suavizado diferente. Con modelos de generación de consultas multinomiales, las personas generalmente usan un coeficiente de suavizado único para controlar la combinación del modelo de documento y el modelo de fondo [28, 29]. Este parámetro se puede hacer específico para diferentes consultas, pero siempre tiene que ser una constante para todos los términos. Esto es obligatorio ya que un modelo de lenguaje multinomial tiene la restricción de que w∈V p (w | d) = 1. Sin embargo, desde la perspectiva de recuperación, pueden necesitar diferentes términos para suavizar de manera diferente, incluso si están en la misma consulta. Por ejemplo, se espera que un término no discriminativo (por ejemplo, el, IS) se explique más con el modelo de fondo, mientras que un término de contenido (por ejemplo, recuperación, arbusto) en la consulta debe explicarse con el modelo de documento. Por lo tanto, una mejor manera de suavizar sería establecer el coeficiente de interpolación (es decir, δ en la fórmula 2 y la fórmula 3) específicamente para cada término. Dado que el modelo de lenguaje Poisson no tiene la restricción de suma a uno en los términos, puede acomodar fácilmente el suavizado por término sin necesidad de torcer heurísticamente la semántica de un modelo generativo como en el caso de los modelos de lenguaje multinomial. A continuación presentamos una posible forma de explorar un suavizado dependiente de términos con modelos de lenguaje Poisson. Esencialmente, queremos usar un coeficiente de suavizado específico de término δ en la combinación lineal, denotada como ΔW. Este coeficiente debe ser intuitivamente mayor si W es una palabra común y más pequeña si es una palabra de contenido. El problema clave es encontrar un método para asignar valores razonables a ΔW. La sintonización empírica es inviable para tantos parámetros. En su lugar, podemos estimar los parámetros ∆ = {Δ1, ..., Δ | V |} maximizando la probabilidad de la consulta dado el modelo de mezcla de P (Q | λq) y P (Q | U), donde λq es elEl verdadero modelo de consulta para generar la consulta y P (Q | U) es un modelo de fondo de consulta como se discute en la Sección 2.2.3. Con el modelo P (q | λq) oculto, la probabilidad de consulta es p (q | ∆, u) = λq w∈V ((1 - ΔW) p (c (w, q) | λq) + ΔWp (c (c (c (c (c (c (w, q) | u)) p (λq | u) dλq Si tenemos documentos relevantes para cada consulta, podemos aproximar el espacio del modelo de consulta con los modelos de lenguaje de todos los documentos relevantes. Sin documentos relevantes, optamos por aproximar el espacio del modelo de consulta con los modelos de todos los documentos de la colección. Configuración P (· | U) Como P (· | C), la probabilidad de consulta se convierte en P (Q | ∆, U) = d∈C πd w∈V ((1 - porˆΛd)+ΔWp (c (w, q) | c)) donde πd = p (ˆλd | u).P (· | ˆλd) es un modelo de lenguaje Poisson estimado para el documento d.Si tenemos conocimiento previo sobre p (ˆλd | u), como qué documentos son relevantes para la consulta, podemos establecer πd en consecuencia, porque lo que queremos es encontrar ∆ que pueda maximizar la probabilidad de la consulta dada los documentos relevantes. Sin este conocimiento previo, podemos dejar πd como parámetros libres y usar el algoritmo EM para estimar πd y ∆. Las funciones de actualización se dan como π (k + 1) d = πd w∈V ((1 - ΔW) p (c (w, q) | ˆλd) + ΔWp (c (w, q) | c)) d∈C πd w∈V ((1 - ΔW) p (c (w, q) | ˆλd) + ΔWp (c (w, q) | c)) y Δ (k + 1) w = d∈C πd ΔWP (c (w, q) | c)) (1 - ΔW) P (c (w, q) | ˆλd) + ΔWp (c (w, q) | c)) como se discute en [29], solo necesitamosEjecute el algoritmo EM para varias iteraciones, por lo que el costo computacional es relativamente bajo. Nuevamente asumimos que nuestro vocabulario contiene todos los términos de consulta más un término pseudo no cuidante. Tenga en cuenta que la función no ofrece una forma explícita de estimar el coeficiente para el término no considerado no considerado. En nuestros experimentos, lo establecemos en el promedio de más de ΔW de todos los términos de consulta. Con esta flexibilidad, esperamos que los modelos de lenguaje de Poisson puedan mejorar el rendimiento de la recuperación, especialmente para las consultas detalladas, donde los términos de la consulta tienen varios valores discriminativos. En la Sección 4, utilizamos experimentos empíricos para probar esta hipótesis.3.3 Modelos de fondo de mezcla Otra flexibilidad es explorar diferentes modelos de fondo (colección) (es decir, P (· | U) o P (· | C)). Una suposición común hecha en la recuperación de información de modelado de idiomas es que el modelo de fondo es un modelo homogéneo de los modelos de documentos [28, 29]. Del mismo modo, también podemos suponer que el modelo de recolección es un modelo de lenguaje de Poisson, con las tasas λc, w = d∈C c (w, d) | c |. Sin embargo, esta suposición generalmente no se mantiene, ya que la colección es mucho más compleja que un solo documento. De hecho, la colección generalmente consiste en una mezcla de documentos con varios géneros, autores y temas, etc. Tratar el modelo de recolección como una mezcla de modelos de documentos, en lugar de un solo modelo de pseudo-documento es más razonable. El trabajo existente de modelado de lenguaje multinomial ya ha demostrado que un mejor modelado de antecedentes mejora el rendimiento de la recuperación, como los grupos [15, 10], los documentos vecinos [25] y los aspectos [8, 27]. Todos los enfoques se pueden adoptar fácilmente utilizando modelos de lenguaje Poisson. Sin embargo, un problema común de estos enfoques es que todos requieren un cálculo pesado para construir el modelo de fondo. Con el modelado de idiomas de Poisson, mostramos que es posible modelar el fondo de la mezcla sin pagar el costo computacional pesado. Se ha propuesto la mezcla de Poisson [3] para modelar una colección de documentos, que pueden adaptarse a los datos mucho mejor que un solo Poisson. La idea básica es suponer que la colección se genera a partir de una mezcla de modelos Poisson, que tiene la forma general de p (x = k | pm) = λ p (λ) p (x = k | λ) dλ p (·| λ) es un modelo de Poisson único y P (λ) es una función de densidad de probabilidad arbitraria. Hay tres mezclas de Poisson bien conocidas [3]: 2 poisson, binomial negativo y la mezcla Katzs K [9]. Tenga en cuenta que el modelo de 2 poisson se ha explorado en los modelos de recuperación probabilística, lo que condujo a la conocida fórmula BM25 [22]. Todas estas mezclas tienen formas cerradas y se pueden estimar a partir de la recopilación de documentos de manera eficiente. Esta es una ventaja sobre los modelos de mezcla multinomial, como PLSI [8] y LDA [1], para la recuperación. Por ejemplo, la función de densidad de probabilidad de la mezcla K de Katzs se da como p (c (w) = k | αW, βW) = (1-αW) ηk, 0 + αW βW + 1 (βW βW + 1) K dondeηk, 0 = 1 cuando k = 0 y 0 de lo contrario. Con la observación de una colección de documentos, αW y βW se pueden estimar como βW = cf (w) - df (w) df (w) y αw = cf (w) nβw donde cf (w) y df (w) sonLa frecuencia de recopilación y la frecuencia de documentos de W, y N es el número de documentos en la colección. Para tener en cuenta las diferentes longitudes de documentos, suponemos que βW es una estimación razonable para generar un documento de la longitud promedio, y usar β = βW avdl | Q |para generar la consulta. Este modelo de mezcla de Poisson se puede usar fácilmente para reemplazar P (· | C) en las funciones de recuperación 3 y 4. 3.4 Otras flexibilidades posibles además del suavizado dependiente del término y el fondo de mezcla eficiente, un modelo de lenguaje de Poisson también tiene otras ventajas potenciales. Por ejemplo, en la Sección 2, vemos que la Fórmula 2 introduce un componente que realiza la penalización de la longitud del documento. Intuitivamente, cuando el documento tiene palabras más únicas, será penalizado más. Por otro lado, si un documento es exactamente n copias de otro documento, no se superaría penalizado. Esta característica es deseable y no se logra con el modelo Dirichlet [5]. Potencialmente, este componente podría penalizar un documento de acuerdo con los tipos de términos que contiene. Con la configuración específica a término de δ, podríamos obtener aún más flexibilidad para la normalización de la longitud del documento. La pseudo-retbalda es otra dirección interesante en la que el modelo Poisson podría mostrar su ventaja. Con la retroalimentación basada en modelos, podríamos relajar nuevamente los coeficientes de combinación del modelo de retroalimentación y el modelo de fondo, y permitir que diferentes términos contribuyan de manera diferente al modelo de retroalimentación. También podríamos utilizar los documentos relevantes para aprender mejores coeficientes de suavizado por término.4. Evaluación En la Sección 3, comparamos analíticamente los modelos de lenguaje Poisson y los modelos de lenguaje multinomial desde la perspectiva de la generación y recuperación de consultas. En esta sección, comparamos estas dos familias de modelos empíricamente. Los resultados del experimento muestran que el modelo de Poisson con suavizado Perter supera el modelo multinomial, y el rendimiento puede mejorarse aún más con un suavizado de dos etapas. El uso de la mezcla de Poisson como modelo de fondo también mejora el rendimiento de la recuperación.4.1 conjuntos de datos Dado que el rendimiento de la recuperación podría variar significativamente de una colección de pruebas a otra, y de una consulta a otra, seleccionamos cuatro colecciones representativas de pruebas TREC: AP, TREC7, TREC8 y WT2G (Web). Para cubrir diferentes tipos de consultas, seguimos [28, 5], y construimos una palabra corta (SK, título de palabras clave), consultas de verbose (SV, una descripción de una oración) y consultas de larga data (LV, múltiples oraciones). Los documentos se encuentran con los porteros Stemmer, y no eliminamos ninguna palabra de parada. Para cada parámetro, variamos su valor para cubrir un rango razonablemente amplio.4.2 Comparación con multinomial Comparamos el rendimiento de los modelos de recuperación de Poisson y los modelos de recuperación multinomial utilizando el suavizado de interpolación (Jelinekmercer, JM) y suavizado bayesiano con antugados. La Tabla 1 muestra que los dos modelos suavizado por JM funcionan de manera similar en todos los conjuntos de datos. Dado que el suavizado de Dirichlet para el modelo de lenguaje multinomial y el suavizado de gamma para el modelo de lenguaje Poisson conducen a la misma fórmula de recuperación, el rendimiento de estos dos modelos se presenta conjuntamente. Vemos que los métodos de suavizado de Dirichlet/gamma superan a los métodos de suavizado Jelinek-Mercer. Las curvas de sensibilidad del parámetro para dos Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 DataSet: TREC8 Parámetro: Δ Precisión promedio JM-Multinomial: VI JM-Multinomial: SV JM-Multinomio: SK JM JM JM− Poisson: SK JM-Poisson: SV JM-Poisson: LV Figura 1: Poisson y multinomial se realizan de manera similar con los métodos de suavizado de suavizado Jelinek-Mercer se muestran en la Figura 1. Claramente, estos dos métodos funcionan de manera similar en términos de consulta de datos de optimización JM-Multinomial JM-Poisson Dirichlet/Gamma por período de 2 etapas Poisson Map initpr pr@5d Map initpr pr@5d Map Initpr Pr@5D Map PR@5D 5DAP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 470 0.291 0.710 0.496 0.304* 0.695 0.510 TREC7 SK 0.167 0.635 0.400 0.1680.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 VV 0.2223 0.730 0.496 0.215 0.766 0.488888. 738 0.512 TREC8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web 0.650 0.650 302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.2730.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Tabla 1: Comparación de rendimiento entre Poisson y Modelos de recuperación multinomial: Modelos básicos funcionan comparablemente;El suave de dos etapas dependiente del término mejora significativamente Poisson, un asterisco (*) indica que la diferencia entre el rendimiento del suavizado de dos etapas dependiente y el del suavizado único de Dirichlet/gamma es estadísticamente significativa de acuerdo con la prueba de rango firmada de Wilcoxon en la prueba de rango en lanivel de 0.05.o sensibilidad. Se espera esta similitud de rendimiento, como discutimos en la Sección 3.1. Aunque el modelo Poisson y el modelo multinomial son similares en términos del modelo básico y/o con métodos de suavizado simples, el modelo Poisson tiene un gran potencial y flexibilidad para mejorar aún más. Como se muestra en la columna más a la derecha de la Tabla 1, el modelo Poisson de dos etapas dependiente del término supera constantemente los modelos de suavizado básicos, especialmente para las consultas detalladas. Este modelo se da en la Fórmula 4, con un suavizado gamma para el modelo de documento P (· | D), y ΔW, que depende del término. El parámetro µ del suavizado gamma de la primera etapa se ajusta empíricamente. Los coeficientes de combinación (es decir, ∆), se estiman con el algoritmo EM en la Sección 3.2. Las curvas de sensibilidad de parámetros para Dirichlet/Gamma y el modelo de suavizado de dos etapas por plazo se representan en la Figura 2. El método de suavizado de dos etapas por término es menos sensible al parámetro µ que Dirichlet/gamma, y produce un mejor rendimiento óptimo.0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 conjunto de datos: AP;Tipo de consulta: Parámetro SV: µ Precisión promedio Dirichlet/Gamma suave término dependiente 2-etapa Figura 2: suave dependiente de dos etapas de dos etapas de Poisson supera a Dirichlet/gamma En las siguientes subsecciones, realizamos experimentos para demostrar cómo la flexibilidad del modelo Poisson podríaSe utilizará para lograr un mejor rendimiento, que no podemos lograr con modelos de lenguaje multinomial.4.3 suavizante dependiente del término Para probar la efectividad del suave dependiente del término, realizamos los siguientes dos experimentos. En el primer experimento, relajamos el coeficiente constante en la simple fórmula de suavizado de Jelinek-Mercer (es decir, Fórmula 3), y usamos el algoritmo EM propuesto en la Sección 3.2 para encontrar un ΔW para cada término único. Dado que estamos utilizando el algoritmo EM para estimar iterativamente los parámetros, generalmente no queremos que la probabilidad de P (· | D) sea cero. Luego usamos un método de Laplace simple para suavizar ligeramente el modelo de documento antes de entrar en las iteraciones de emergencias. Los documentos todavía se califican con la Fórmula 3, pero utilizan ΔW erudito. Los resultados están etiquetados con JM+L.en la Tabla 2. Datos Q JM JM JM+L.2 etapas de 2 etapas (MAP) PT: No Sí Sí No Sí AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* TREC7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.1960.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* web Sk 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261*entre el jm+l.El método y el método JM es estadísticamente significativo;Un asterisco (*) en la columna 5 significa que la diferencia entre el método de dos etapas dependiente del término y el método de dos etapas dependiente de la consulta es estadísticamente significativa;PT significa por período. Con coeficientes dependientes del término, el rendimiento del modelo Jelinek-Mercer Poisson mejora en la mayoría de los casos. Sin embargo, en algunos casos (por ejemplo, TREC7/SV), funciona mal. Esto podría ser causado por el problema de la estimación de EM con modelos de documentos sin liso. Una vez que se asigna una probabilidad no cero a todos los términos antes de ingresar la iteración EM, el rendimiento de las consultas detalladas se puede mejorar significativamente. Esto indica que todavía hay espacio para encontrar mejores métodos para estimar ΔW. Tenga en cuenta que ni el método JM Perter ni el JM+L.El método tiene un parámetro que sintonizar. Como se muestra en la Tabla 1, el suave suave dependiente de dos etapas depende puede mejorar significativamente el rendimiento de la recuperación. Para comprender si la mejora es contribuida por el suave suave dependiente o el marco de suavizado de dos etapas, diseñamos otro experimento para comparar el suavizado de dos etapas por término con el método de suavizado de dos etapas propuesto en [29]. Su método logró encontrar coeficientes específicos para la consulta, por lo que una consulta detallada usaría un δ más alto. Sin embargo, dado que su modelo se basa en el modelado de lenguaje multinomial, no pudieron obtener coeficientes por término. Adoptamos su método para el suavizado de dos etapas de Poisson, y también estimamos un coeficiente por QUERERY para todos los términos. Comparamos el rendimiento de dicho modelo con el modelo de suavizado de dos etapas por período, y presentamos los resultados en las dos columnas correctas en la Tabla 2. Una vez más, vemos que el suavizado de dos etapas por período supera al suavizado de dos etapas por QUERERY, especialmente para consultas verbosas. La mejora no es tan grande como la forma en que el método de suavizado de término mejora sobre Dirichlet/gamma. Se espera que esto, ya que el suavizado por QUIERY ya ha abordado el problema de discriminación de consultas hasta cierto punto. Este experimento muestra que incluso si el suavizado ya es por QUERERY, lo que lo hace por plazo sigue siendo beneficioso. En resumen, el suavizado por término mejoró el rendimiento de la recuperación del método de suavizado de una etapa y dos etapas.4.4 Modelo de fondo de mezcla En esta sección, realizamos experimentos para examinar los beneficios de usar un modelo de fondo de mezcla sin un costo computacional adicional, que no se puede lograr para modelos multinomiales. Específicamente, en la fórmula 3 de recuperación, en lugar de usar una distribución de Poisson única para modelar el fondo P (· | C), usamos el modelo Katzs K-Micte, que es esencialmente una mezcla de distribuciones de Poisson.P (· | c) se puede calcular de manera eficiente con estadísticas de recolección simples, como se discutió en la Sección 3.3. Consulta de datos JM. Poisson JM. K-MEXCTURA AP SK 0.203 0.204 SV 0.183 0.188* TREC-7 SK 0.168 0.169 SV 0.176 0.178* TREC-8 SK 0.239 0.239 SV 0.234 0.238* Web SK 0.250 0.250 SV 0.217 0.223* Tabla 3: el modelo de fondo K-MIZCE mejora la recuperación de la recuperación mejoraEl rendimiento del modelo de recuperación JM con fondo de Poisson único y con el modelo de fondo de la mezcla K Katzs se compara en la Tabla 3. Claramente, el uso de la mezcla K para modelar el modelo de fondo supera al modelo de fondo de Poisson único en la mayoría de los casos, especialmente para consultas verbosas donde la mejora es estadísticamente significativa. La Figura 3 muestra que el rendimiento cambia en diferentes parámetros para consultas verbosas cortas. El modelo que usa el fondo de la mezcla K es menos sensible que el que usa fondo de Poisson único. Dado que este tipo de mezcla 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Datos: TREC8;Consulta: Parámetro SV: Δ Precision Precision Poisson Background K-Mezcre Figura Figura 3: El modelo de fondo de la mezcla K desvía la sensibilidad del modelo de fondo de consultas verbosas no requiere ningún costo de cálculo adicional, sería interesante estudiar si utilizando otros modelos Poisson de mezcla,como el 2 poisson y el binomial negativo, podría ayudar al rendimiento.5. Trabajo relacionado según nuestro lo mejor de nuestro conocimiento, no ha habido un estudio de los modelos de generación de consultas basados en la distribución de Poisson. Se ha demostrado que los modelos de lenguaje son efectivos para muchas tareas de recuperación [21, 28, 14, 4]. El más popular y fundamental es el modelo de lenguaje de generación de consultas [21, 13]. Todos los modelos de lenguaje de generación de consultas existentes se basan en distribución multinomial [19, 6, 28, 13] o distribución multivariada de Bernoulli [21, 17, 18]. Presentamos una nueva familia de modelos de idiomas, basado en la distribución de Poisson. La distribución de Poisson se ha estudiado previamente en los modelos de generación de documentos [16, 22, 3, 24], lo que lleva al desarrollo de una de las fórmula de recuperación más efectiva BM25 [23].[24] Estudia la derivación paralela de tres modelos de recuperación diferentes relacionados con nuestra comparación de Poisson y multinomial. Sin embargo, el modelo Poisson en su documento aún está bajo el marco de generación de documentos, y tampoco tiene en cuenta la variación de la longitud del documento.[26] introduce una forma de buscar empíricamente un modelo exponencial para los documentos. Las mezclas de Poisson [3] como 2-poisson [22], multinomial negativo y katzs kmixture [9] ha demostrado ser efectivo para modelar y recuperar documentos. Una vez más, ninguno de este trabajo explora la distribución de Poisson en el marco de generación de consultas. El suavizado del modelo de lenguaje [2, 28, 29] y las estructuras de fondo [15, 10, 25, 27] se han estudiado con modelos de lenguaje multinomial.[7] muestra analíticamente que el suavizado de término específico podría ser útil. Mostramos que el modelo de lenguaje Poisson es natural para acomodar el suavizado por término sin giro heurístico de la semántica de un modelo generativo, y es capaz de modelar mejor el fondo de la mezcla, tanto analíticamente como empíricamente.6. Conclusiones presentamos una nueva familia de modelos de lenguaje de generación de consultas para la recuperación basados en la distribución de Poisson. Derivamos varios métodos de suavizado para esta familia de modelos, incluido el suavizado de una etapa y el suavizado de dos etapas. Comparamos los nuevos modelos con los populares modelos de recuperación multinomial tanto analíticamente como experimentalmente. Nuestro análisis muestra que si bien nuestros nuevos modelos y modelos multinomiales son equivalentes bajo algunos supuestos, generalmente son diferentes con algunas diferencias importantes. En particular, mostramos que Poisson tiene una ventaja sobre multinomial en suavizado por término naturalmente acomodado. Explotamos esta propiedad para desarrollar un nuevo algoritmo de suavizado por término para los modelos de lenguaje Poisson, que se muestra superando el suavizado independiente del término para los modelos Poisson y multinomiales. Además, mostramos que se puede utilizar un modelo de fondo de mezcla para Poisson para mejorar el rendimiento y la robustez sobre el modelo de fondo de Poisson estándar. Nuestro trabajo abre muchas direcciones interesantes para una mayor exploración en esta nueva familia de modelos. Explorar aún más las flexibilidades sobre los modelos de lenguaje multinomial, como la normalización de la longitud y la pseudo-retroalimentación podría ser un buen trabajo futuro. También es atractivo encontrar métodos robustos para aprender los coeficientes de suavizado por término sin un costo de cálculo adicional.7. Agradecimientos Agradecemos a los revisores anónimos Sigir 07 por sus útiles comentarios. Este material se basa en parte en el trabajo apoyado por la National Science Foundation bajo los números de premios IIS-0347933 y 0425852. 8. Referencias [1] D. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Journal of Machine Learning Research, 3: 993-1022, 2003. [2] S. F. Chen y J. Goodman. Un estudio empírico de las técnicas de suavizado para el modelado de idiomas. Informe técnico TR-10-98, Universidad de Harvard, 1998. [3] K. Church y W. Gale. Mezclas de Poisson. Nat. Lang. Eng., 1 (2): 163-190, 1995. [4] W. B. Croft y J. Lafferty, editores. Modelado de idiomas y recuperación de información. Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao y C. Zhai. Un estudio formal de heurística de recuperación de información. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 49-56, 2004. [6] D. Hiemstra. Uso de modelos de idiomas para la recuperación de información. Tesis doctoral, Universidad de Twente, Enschede, Países Bajos, 2001. [7] D. Hiemstra. Suavizado de término específico para el enfoque de modelado de idiomas para la recuperación de información: la importancia de un término de consulta. En Actas de la 25ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 35-41, 2002. [8] T. Hofmann. Indexación semántica latente probabilística. En Actas de ACM Sigir99, páginas 50-57, 1999. [9] S. M. Katz. Distribución de palabras y frases de contenido en el modelado de texto y lenguaje. Nat. Lang. Eng., 2 (1): 15-59, 1996. [10] O. Kurland y L. Lee. Estructura del corpus, modelos de idiomas y recuperación de información ad-hoc. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 194-201, 2004. [11] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En Actas de Sigir01, páginas 111-119, septiembre de 2001. [12] J. Lafferty y C. Zhai. Modelos IR probabilísticos basados en la consulta y la generación de documentos. En Actas del modelado de idiomas y el taller IR, páginas 1-5, 31 de mayo - 1 de junio de 2001. [13] J. Lafferty y C. Zhai. Modelos de relevancia probabilística basados en la generación de documentos y consultas. En W. B. Croft y J. Lafferty, editores, modelado de idiomas y recuperación de información. Kluwer Academic Publishers, 2003. [14] V. Lavrenko y B. Croft. Modelos de idiomas basados en relevancia. En Actas de Sigir01, páginas 120-127, septiembre de 2001. [15] X. Liu y W. B. Croft. Recuperación basada en clúster utilizando modelos de lenguaje. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 186-193, 2004. [16] E. L. margulis. Modelado de documentos con múltiples distribuciones de Poisson. Inf. Proceso. Manage., 29 (2): 215-227, 1993. [17] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto de Naive Bayes. En Actas del Taller de AAAI-98 sobre el aprendizaje para la categorización de texto, 1998. [18] D. Metzler, V. Lavrenko y W. B. Croft. Modelos formales de múltiples bernoulli para modelado de idiomas. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 540-541, 2004. [19] D. H. Miller, T. Leek y R. Schwartz. Un sistema de recuperación de información del modelo de Markov oculto. En Actas de la Conferencia ACM Sigir sobre investigación y desarrollo de 1999 en recuperación de información, páginas 214-221, 1999. [20] A. papulis. Probabilidad, variables aleatorias y procesos estocásticos. Nueva York: McGraw-Hill, 1984, 2ª ed., 1984. [21] J. M. Ponte y W. B. Croft. Un enfoque de modelado de idiomas para la recuperación de información. En Actas de la 21a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 275-281, 1998. [22] S. Robertson y S. Walker. Algunas aproximaciones simples efectivas al modelo de 2 poisson para la recuperación ponderada probabilística. En Actas de Sigir94, páginas 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M.Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En D. K. Harman, editor, la tercera conferencia de recuperación de texto (TREC-3), páginas 109-126, 1995. [24] T. Roelleke y J. Wang. Una derivación paralela de modelos de recuperación de información probabilística. En Actas de la 29a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de idioma con expansión de documentos. En Actas de HLT/NAACL 2006, páginas 407-414, 2006. [26] J. Teevan y D. R. Karger. Desarrollo empírico de un modelo probabilístico exponencial para la recuperación de texto: utilizando el análisis textual para construir un mejor modelo. En Actas de la 26ª Conferencia Anual de Investigación y Desarrollo de Información de ACM ACM en recuperación de información, páginas 18-25, 2003. [27] X. Wei y W. B. Croft. Modelos de documentos basados en LDA para recuperación ad-hoc. En Actas de la 29a Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 178-185, 2006. [28] C. Zhai y J. Lafferty. Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. En Actas de ACM Sigir01, páginas 334-342, septiembre de 2001. [29] C. Zhai y J. Lafferty. Modelos de lenguaje de dos etapas para la recuperación de información. En Actas de ACM Sigir02, páginas 49-56, agosto de 2002.