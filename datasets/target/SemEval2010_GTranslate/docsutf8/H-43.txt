Combinación de contenido y enlace para la clasificación utilizando la factorización de matriz Shenghuo Zhu Kai Yu Yun Chi Yihong Gong {Zsh, Kyu, Ychi, Ygong}@sv.nec-labs.com NEC Laboratories America, Inc. 10080 North Wolfe Road SW3-350 Cupertino, CA CA95014, EE. UU. Resumen La World Wide Web contiene contenidos textuales ricos que están interconectados a través de hipervínculos complejos. Esta enorme base de datos viola la suposición de la mayoría de los métodos estadísticos convencionales que cada página web se considera una muestra independiente e idéntica. Por lo tanto, es difícil aplicar métodos tradicionales de minería o aprendizaje para resolver problemas de minería web, por ejemplo, clasificación de la página web, explotando tanto el contenido como la estructura del enlace. La investigación en esta dirección ha recibido recientemente una atención considerable, pero todavía está en una etapa temprana. Aunque algunos métodos explotan tanto la estructura del enlace como la información de contenido, algunos de ellos combinan la única información de autoridad con la información de contenido, y los otros primero descomponen la estructura del enlace en características de HUB y autoridad, luego aplicanlos como características adicionales de documentos. Siendo prácticamente atractivo para su gran simplicidad, este documento tiene como objetivo diseñar un algoritmo que explote tanto la información de contenido como de enlace, realizando una factorización conjunta tanto en la matriz de adyacencia de enlace como en la matriz a plazo de documentos, y deriva una nueva representación para la webPáginas en un espacio de factores de baja dimensión, sin separarlas explícitamente como contenido, centro o factores de autoridad. Se puede realizar un análisis adicional en función de la representación compacta de las páginas web. En los experimentos, el método propuesto se compara con los métodos de última generación y demuestra una excelente precisión en la clasificación de hipertexto en los puntos de referencia WebKB y Cora. Categorías y descriptores de sujetos: H.3.3 [Sistemas de información]: Búsqueda y recuperación de información Términos generales: Algoritmos, Experimentación 1. Introducción Con el avance de la World Wide Web, cada vez más documentos de hipertexto están disponibles en la web. Algunos ejemplos de dichos datos incluyen páginas web organizacionales y personales (por ejemplo, el conjunto de datos de referencia de WebKB, que contiene páginas web de la universidad), trabajos de investigación (por ejemplo, datos en CitaSeer), artículos de noticias en línea y medios generados por clientes (por ejemplo, blogs). En comparación con los datos en la gestión de la información tradicional, además del contenido, estos datos en la web también contienen enlaces: por ejemplo, hipervínculos de una página de inicio de los estudiantes que señalan la página de inicio de su asesor, citas en papel, fuentes de un artículo de noticias, comentarios de un bloggeren publicaciones de otro blogger, y así sucesivamente. Realizar tareas de gestión de información sobre dichos datos estructurados plantea muchos desafíos de investigación nuevos. En la siguiente discusión, utilizamos la tarea de la clasificación de la página web como un ejemplo de ilustración, mientras que las técnicas que desarrollamos en secciones posteriores son aplicables igualmente bien a muchas otras tareas en la recuperación de información y la minería de datos. Para el problema de clasificación de las páginas web, un enfoque simple es tratar las páginas web como documentos independientes. La ventaja de este enfoque es que muchas herramientas de clasificación estándar se pueden aplicar directamente al problema. Sin embargo, este enfoque se basa solo en el contenido de las páginas web e ignora la estructura de los enlaces entre ellos. Las estructuras de enlace proporcionan información invaluable sobre las propiedades de los documentos, así como las relaciones entre ellos. Por ejemplo, en el conjunto de datos WebKB, la estructura de enlaces proporciona información adicional sobre la relación entre documentos (por ejemplo, enlaces que a menudo apuntan de un estudiante a su asesor o de un miembro de la facultad a sus proyectos). Dado que algunos enlaces entre estos documentos implican la interdependencia entre los documentos, el habitual I.I.D.(Distribuido independiente e idéntico) La suposición de documentos ya no se mantiene más. Desde este punto de vista, los métodos de clasificación tradicionales que ignoran la estructura del enlace pueden no ser adecuados. Por otro lado, algunos estudios, por ejemplo [25], dependen únicamente de las estructuras de enlace. Sin embargo, es un caso muy raro que la información de contenido puede ser ignorable. Por ejemplo, en el conjunto de datos CORA, el contenido de un resumen del artículo de investigación determina en gran medida la categoría del artículo. Para mejorar el rendimiento de la clasificación de la página web, por lo tanto, se debe tener en cuenta tanto la estructura del enlace como la información del contenido. Para lograr este objetivo, un enfoque simple es convertir un tipo de información al otro. Por ejemplo, en la clasificación de blogs spam, Kolari et al.[13] Concatenate Outlink características con las características de contenido del blog. En la clasificación de documentos, Kurland y Lee [14] convierten la similitud de contenido entre documentos en pesos de enlaces. Sin embargo, la información del enlace y el contenido tienen diferentes propiedades. Por ejemplo, un enlace es una evidencia real que representa una relación asimétrica, mientras que la similitud de contenido generalmente se define conceptualmente para cada par de documentos de manera simétrica. Por lo tanto, convertir directamente un tipo de información en el otro generalmente degrada la calidad de la información. Por otro lado, existen algunos estudios, como discutiremos en detalle en el trabajo relacionado, que consideren la información del enlace y la información de contenido por separado y luego los combinen. Argumentamos que este enfoque ignora la consistencia inherente entre el enlace y la información de contenido y, por lo tanto, no combina los dos sin problemas. Algunos trabajan, como [3], incorpora información de enlaces utilizando la similitud de cocitación, pero esto puede no capturar completamente la estructura del enlace global. En la Figura 1, por ejemplo, las páginas web V6 y V7 CO-Cite Web Página V8, lo que implica que V6 y V7 son similares entre sí. En los turnos, V4 y V5 deben ser similares entre sí, ya que V4 y V5 citan páginas web similares V6 y V7, respectivamente. Pero utilizando la similitud de cocitación, la similitud entre V4 y V5 es cero sin considerar otra información.V1 V2 V3 V4 V5 V6 V7 V8 Figura 1: Un ejemplo de estructura de enlace En este documento, proponemos una técnica simple para analizar documentos interconectados, como las páginas web, utilizando el análisis factorial [18]. En la técnica propuesta, tanto la información de contenido como las estructuras de enlaces se combinan perfectamente a través de un solo conjunto de factores latentes. Nuestro modelo contiene dos componentes. El primer componente captura la información de contenido. Este componente tiene una forma similar a la de los temas latentes en la indexación semántica latente (LSI) [8] en la recuperación de información tradicional. Es decir, los documentos se descomponen en temas/factores latentes, que a su vez se representan como vectores de término. El segundo componente captura la información contenida en la estructura de enlace subyacente, como los enlaces de las páginas de inicio de los estudiantes a los de los miembros de la facultad. Un factor puede considerarse libremente como un tipo de documentos (por ejemplo, esas páginas de inicio que pertenecen a los estudiantes). Vale la pena señalar que no definimos explícitamente el semántico de un factor a priori. En cambio, similar a LSI, los factores se aprenden de los datos. El análisis factorial tradicional modela las variables asociadas con las entidades a través de los factores. Sin embargo, en el análisis de las estructuras de enlace, necesitamos modelar la relación de dos extremos de los enlaces, es decir, los bordes entre los pares de vértices. Por lo tanto, el modelo debe involucrar factores de ambos vértices del borde. Esta es una diferencia clave entre el análisis factorial tradicional y nuestro modelo. En nuestro modelo, conectamos dos componentes a través de un conjunto de factores compartidos, es decir, los factores latentes en el segundo componente (para contenido) están vinculados a los factores en el primer componente (para enlaces). Al hacer esto, buscamos un conjunto unificado de factores latentes que mejor explique las estructuras de contenido y enlace simultáneamente y sin problemas. En la formulación, realizamos un análisis factorial basado en la factorización de la matriz: la solución al primer componente se basa en factorizar la matriz de documento de término derivada de las características de contenido;La solución al segundo componente se basa en factorizar la matriz de adyacencia derivada de los enlaces. Debido a que las dos factores comparten una base común, las bases descubiertas (factores latentes) explican tanto la información del contenido como las estructuras de enlaces, y luego se utilizan en tareas de gestión de información más como la clasificación. Este artículo está organizado de la siguiente forma: La Sección 2 revisa el trabajo relacionado. La Sección 3 presenta el enfoque propuesto para analizar la página web en función de la información combinada de enlaces y contenido. La Sección 4 extiende el marco básico y algunas variantes para la melodía. La Sección 5 muestra los resultados del experimento. La Sección 6 analiza los detalles de este enfoque y la Sección 7 concluye.2. Trabajo relacionado En la parte de análisis de contenido, nuestro enfoque está estrechamente relacionado con la indexación semántica latente (LSI) [8]. LSI mapea los documentos en un espacio latente dimensional inferior. El espacio latente captura implícitamente una gran parte de la información de los documentos, por lo tanto, se llama espacio semántico latente. La similitud entre los documentos podría definirse mediante los productos DOT de los vectores correspondientes de documentos en el espacio latente. Las tareas de análisis, como la clasificación, podrían realizarse en el espacio latente. El método de descomposición del valor singular (SVD) comúnmente utilizado garantiza que los puntos de datos en el espacio latente puedan reconstruir de manera óptima los documentos originales. Aunque nuestro enfoque también usa espacio latente para representar páginas web (documentos), consideramos la estructura de enlaces y el contenido de las páginas web. En el enfoque de análisis de enlaces, el marco de centros y autoridades (hits) [12] pone la página web en dos categorías, centros y autoridades. Utilizando la noción recursiva, un centro es una página web con muchos enlaces salientes a las autoridades, mientras que una autoridad es una página web con muchos enlaces entrantes de los centros. En lugar de usar dos categorías, PageRank [17] usa una sola categoría para la noción recursiva, una autoridad es una página web con muchos enlaces entrantes de las autoridades. Él et al.[9] proponer un algoritmo de agrupación para la agrupación de documentos web. El algoritmo incorpora la estructura del enlace y los patrones de co-citación. En el algoritmo, todos los enlaces se tratan como el borde no dirigido del gráfico de enlace. La información de contenido solo se usa para sopesar los enlaces por la similitud textual de ambos extremos de los enlaces. Zhang et al.[23] utiliza el marco de regularización de gráficos no dirigido para la clasificación de documentos. Achlioptas et al [2] descomponen la web en los atributos de Hub y Autoridad y luego los combina con contenido. Zhou et al.[25] y [24] proponen un marco de regularización de gráficos dirigido para el aprendizaje semi-supervisado. El marco combina la información del centro y la autoridad de las páginas web. Pero es difícil combinar la información de contenido en ese marco. Nuestro enfoque considere el contenido y el enlace dirigido entre los temas de las páginas web de origen y destino en un solo paso, lo que implica que el tema combina la información de la página web como autoridades y como centros en un solo conjunto de factores. Cohn y Hofmann [6] construyen el espacio latente a partir de la información de contenido y enlace, utilizando análisis de contenido basado en LSI probabilístico (PLSI) [10] y análisis de enlaces basados en PHIT [5]. La principal diferencia entre el enfoque de [6] (PLSI+PHIT) y nuestro enfoque es por parte del análisis de enlaces. En PLSI+PHIT, el enlace se construye con el enlace desde el tema de la página web de origen a la página web de destino. En el modelo, los enlaces salientes de la página web de destino no tienen ningún efecto en la página web de origen. En otras palabras, la estructura de enlace general no se utiliza en PHIT. En nuestro enfoque, el enlace se construye con el enlace entre el factor de la página web de origen y el factor de la página web de destino, en lugar de la página web de destino. El factor de la página web de destino contiene información de sus enlaces salientes. A su vez, dicha información se pasa al factor de la página web de origen. Como resultado de la factorización de la matriz, el factor forma un gráfico de factor, una miniatura del gráfico original, preservando la estructura principal del gráfico original. Taskar et al.[19] propone redes relacionales de Markov (RMN) para la clasificación de entidades, describiendo una distribución condicional de las clases de entidad dadas los atributos y las relaciones de la entidad. El modelo se aplicó a la clasificación de la página web, donde las páginas web son entidades y los hipervínculos se tratan como relaciones. Los RMN aplican campos aleatorios condicionales para definir un conjunto de funciones potenciales en camarillas de variables aleatorias, donde la estructura de enlace proporciona sugerencias para formar las camarillas. Sin embargo, el modelo no ofrece una solución estándar, porque el éxito depende en gran medida de las artes de diseñar las funciones potenciales. Por otro lado, la inferencia por RMN es intratable y requiere propagación de creencias. Los siguientes son algunos trabajos para combinar documentos y enlaces, pero los métodos están relacionado libremente con nuestro enfoque. Los experimentos de [21] muestran que el uso de términos del documento vinculado mejora la precisión de la clasificación. Chakrabarti et al.[3] Use información de co-citación en su modelo de clasificación. Joachims et al.[11] Combine los núcleos de texto y los núcleos de co-citación para la clasificación. Oh et al [16] usan el marco bayesiano ingenuo para combinar la información de enlace con el contenido.3. Nuestro enfoque en esta sección primero introduciremos un nuevo método de factorización de matriz, que es más adecuado que los métodos de factorización de matriz convencionales para el análisis de enlaces. Luego presentaremos nuestro enfoque que factoriza conjuntamente la matriz de documento a plazo y la matriz de enlace y obtiene factores compactos y altamente indicativos para representar documentos o páginas web.3.1 Factorización de la matriz de enlace Suponga que tenemos un gráfico dirigido g = (v, e), donde el vértice establece v = {vi} n i = 1 representa las páginas web y el conjunto de borde E representa los hipervínculos entre páginas web. Sea A = {ASD} denota la matriz de adyacencia N × N de G, que también se llama matriz de enlaces en este documento. Para un par de vértices, VS y VD, deje que ASD = 1 cuando haya un borde de VS a VD, y ASD = 0, de lo contrario. Tenga en cuenta que A es una matriz asimétrica, porque se dirigen los hipervínculos. La mayoría de los algoritmos de aprendizaje automático asumen una representación de vectores de características de las instancias. Sin embargo, para la clasificación de la página web, el gráfico de enlaces no proporciona fácilmente dicha representación vectorial para páginas web. Si uno usa directamente cada fila o columna de A para el trabajo, sufrirá un costo computacional muy alto porque la dimensionalidad es igual a la cantidad de páginas web. Por otro lado, producirá una precisión de clasificación deficiente (ver nuestros experimentos en la Sección 5), porque A es extremadamente escaso1. La idea de la factorización de la matriz de enlaces es derivar una representación de características de alta calidad Z de páginas web basadas en el análisis de la matriz de enlace A, donde Z es una matriz N × L, siendo cada fila el vector de características ldimensionales de una página web. La nueva representación de las páginas web captura los factores principales de la estructura del enlace y hace que el procesamiento adicional sea más eficiente. Uno puede usar un método similar a LSI, para aplicar el conocido análisis de componentes principales (PCA) para derivar Z de A. El problema de optimización correspondiente 2 es Min Z, U A - Zu 2 F + γ U 2 F (1) donde γ es un pequeño número positivo, U es una matriz L × N y · F es la norma Frobenius. La optimización tiene como objetivo aproximar A por Zu, un producto de dos matrices de bajo rango, con una regularización en U. Al final, el vector de la fila I-Th of Z puede considerarse como el vector de características del cubo de Vertex VI, y el vector de fila de U puede considerarse como la autoridad presenta. Un modelo de generación de enlaces propuesto en [2] es similar al enfoque PCA. Dado que A es una matriz no negativa aquí, también se puede considerar poner restricciones no negativas en U y Z, que produce un algoritmo similar al PLSA [10] y NMF [20].1 Debido a la escasez de A, los enlaces de dos páginas similares pueden no compartir ninguna página objetivo común, lo que hace que parezcan diferentes. Sin embargo, las dos páginas pueden estar indirectamente vinculadas a muchas páginas comunes a través de sus vecinos.2 Otra forma equivalente es Minz, U A - Zu 2 F, s.t.U u = I. La solución z está sujeta idéntica a un factor de escala. Sin embargo, a pesar de su popularidad en el análisis de matriz, PCA (u otros métodos similares como PLSA) es restrictivo para la factorización de la matriz de enlace. El principal problema es que PCA ignora el hecho de que las filas y las columnas de A están indexadas exactamente por el mismo conjunto de objetos (es decir, páginas web). La matriz aproximada ˜a = zu no muestra evidencia de que los enlaces estén dentro del mismo conjunto de objetos. Para ver el inconveniente, consideremos una situación de transitividad de enlace VI → VS → VJ, donde la página I está vinculada a la página S que está vinculada a la página j. Dado que ˜a = zu trata A como enlaces desde las páginas web {vi} a un conjunto diferente de objetos, que se denote por {oi}, ˜a = zu realmente divide un sistema operativo de objetos vinculado de VS y descompone la ruta del enlace haciaDos partes VI → OS y VS → OJ. Obviamente, esto es una interpretación de Miss en la ruta del enlace original. Para superar el problema de PCA, en este documento sugerimos usar una factorización diferente: Min Z, U A - Zuz 2 F + γ U 2 F (2) donde U es una matriz completa L × L. Tenga en cuenta que U no es simétrico, por lo tanto, Zuz produce una matriz asimétrica, que es el caso de A. Nuevamente, cada vector de fila de z corresponde a un vector de características de páginas web. La nueva forma de aproximación ˜A = ZUZ pone un claro significado de que los enlaces están entre el mismo conjunto de objetos, representados por las características Z. El modelo de factor en realidad mapea cada vértice, vi, en un vector zi = {zi, k;1 ≤ k ≤ l} en el espacio RL. Llamamos al espacio RL el espacio del factor. Luego, {Zi} codifica la información de conectividad entrante y saliente de los vértices {vi}. Las cargas de factores, U, explican cómo se produjeron estas conexiones observadas en función de {Zi}. Una vez que tenemos el vector Zi, podemos usar muchos métodos de clasificación tradicionales (como SVM) o herramientas de agrupación (como K-means) para realizar el análisis. Ilustración basada en un problema sintético para ilustrar aún más las ventajas de la factorización de matriz de enlace propuesta Eq.(2), consideremos el gráfico en la Figura 1. Dado V1 V1 V3 V3 V5 V6 V6 V7 V8 Figura 2: Resumir la Figura 1 con un gráfico de factor estas observaciones, podemos resumir el gráfico agrupando como gráfico de factor representado en la Figura 2. En el siguiente realizamos los dos métodos de factorización Eq.(2) y la ecuación.(1) En esta matriz de enlace. Una buena representación de bajo rango debe revelar la estructura del gráfico de factores. Primero probamos la descomposición similar a PCA, resolviendo la ecuación.(1) y obteniendo z = u = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 1. 0 0 0 0 0 0 −.6 −.7 .1 0 0 .0 .6−.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 .7 .7 .7 0 0 0 0 0 0 0 0 3 7 7 7 7 7 7 7 7 77 7 7 7 7 7 7 7 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 0 0 0 0 0 .5 −.5 0 0 0 .5 −.5 0 0 0 0 0 0−0.6 −.7 .1 0 0 .0 .6 −.0 0 0 .8 −.4 .3 0 0 .2 −.2 −.9 .7 .7 0 0 0 0 3 7 7 7 7 7 7 7 77 7 7 7 7 7 7 7 7 5 Podemos ver que los vectores de fila de V6 y V7 son los mismos en z, lo que indica que V6 y V7 tienen los mismos atributos de cubo. Los vectores de fila de V2 y V3 son los mismos en U, lo que indica que V2 y V3 tienen los mismos atributos de autoridad. No está claro ver la similitud entre V4 y V5, porque sus incendios (y los ataques) son diferentes. Luego, factorizamos A por zuz mediante la resolución de la ecuación.(2), y obtenga los resultados z = u = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 −.8 −.5 .3 −.1 −.0 −.0 .4.6 −.1 −.4 −.0 .4 .6 −.1 −.4 .3 −.2 .3 −.4 .3 .3 −.2 .3 −.4 .3 −.4 .5.0 −.2 .6 −.4 .5 .0 −.2 .6 −.1 .1 −.4 −.8 −.4 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 52 6 6 6 6 6 6 6 6 6 4 −.1 −.2 −.4 .6 .7 .2 −.5 −.5 −.5 .0 .1 .1 .4 −.4 .3 .1 −.2 −.0 .3 −.1 −.3 .3 −.5 −.4 −.2 3 7 7 7 7 7 7 7 7 5 La z resultante es muy consistente con la estructura de agrupación de los vértices: los vectores de filade V2 y V3 son los mismos, los de V4 y V5 son los mismos, los de V6 y V7 son los mismos. Incluso curiosamente, si agregamos restricciones para garantizar que Z y U no sean negativas, tenemos z = u = 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 4 1. 0 0 0 0 0 .9 0 0 0 00 0 .9 0 0 0 0 0 .7 0 0 0 0 .7 0 0 0 0 0 .9 0 0 0 0 .9 0 0 0 0 0 0 1. 3 7 7 7 7 7 7 7 7 7 7 7 77 7 7 7 5 2 6 6 6 6 6 6 6 6 6 4 0 1. 0 0 0 0 0 .7 0 0 0 0 0 .7 0 0 0 0 0 0 1. 0 0 0 0 0 0 7 7 7 7 7 77 7 7 5 que claramente indica la asignación de vértices a grupos de Z y los enlaces del gráfico Factor de U. Cuando la interpretabilidad no es crítica en algunas tareas, por ejemplo, la clasificación, encontramos que logra mejores precisiones sin las restricciones no negativas. Dado nuestro análisis anterior, está claro que la factorización Zuz es más expresiva que Zu al representar la matriz de enlace A. 3.2 Factorización de la matriz de contenido Ahora consideremos la información de contenido en los vértices. Para combinar la información del enlace y la información de contenido, queremos usar el mismo espacio latente para aproximar el contenido que el espacio latente para los enlaces. Usando el enfoque de la bolsa de palabras, denotamos el contenido de las páginas web por una matriz C N × M, cada una de cuyas filas representa un documento, cada columna representa una palabra clave, donde M es el número de palabras clave. Al igual que la indexación semántica latente (LSI) [8], el espacio latente L-dimensional para palabras se denota mediante una matriz M × L v. Por lo tanto, usamos ZV para aproximar la matriz C, Min V, Z C - ZV 2 F + β V 2 F, (3) donde β es un pequeño número positivo, β V 2 F sirve como un término de regularización para mejorar la robustez.3.3 Factorización de matriz de contenido de enlace conjunto Hay muchas formas de emplear la información de contenido y enlace para la clasificación de la página web. Nuestra idea en este documento no es simplemente combinarlos, sino más bien fusionarlos en una sola representación de características compacta y compacta. Para lograr este objetivo, resolvemos el siguiente problema, Min U, V, Z N J (U, V, Z) def = A - Zuz 2 F + α C - ZV 2 F + γ U 2 F + β V 2 Fo.(4) Eq.(4) es la factorización de matriz unida de A y C con regularización. La nueva representación z se garantiza para capturar las estructuras de la matriz de enlace A y la matriz de contenido C. Una vez que encontramos la Z óptima, podemos aplicar los métodos de clasificación o agrupación tradicionales en los datos vectoriales Z. La relación entre estas matrices se puede representar como la Figura 3. A Y C U Z V Figura 3: Relación entre las matrices. El nodo Y es el objetivo de clasificación. Ec.(4) se puede resolver utilizando métodos de gradiente, como el método de gradiente conjugado y los métodos cuasi-newton. Entonces el cálculo principal de los métodos de gradiente es evaluar la función del objeto j y sus gradientes contra las variables, ∂j ∂u = z zuz z - z az + γu, ∂j ∂v = α v z - c z + βV, ∂j ∂z = zuZ zu + zuz zu - a zu - azu + α zv v - cv. Debido a la escasez de A, la complejidad computacional de la multiplicación de A y Z es O (µal), donde µA es el número de entradas distintas de cero en A. Del mismo modo, la complejidad computacional de C Z y CV es O (µC L), donde µC es el número de entradas distintas de cero en C. La complejidad computacional de las multiplicaciones REST en el cálculo de gradiente es O (NL2). Por lo tanto, la complejidad computacional total en una iteración es O (µal + µC L + NL2). El número de enlaces y el número de palabras en una página web son relativamente pequeñas en comparación con el número de páginas web, y son casi constantes a medida que aumenta el número de páginas/documentos web, es decir, µA = o (n) y µc = o ((norte). Por lo tanto, teóricamente el tiempo de cálculo es casi lineal para el número de páginas web/documentos, n.4. Factorización de matriz supervisada Considere un problema de clasificación de la página web. Podemos resolver la ecuación.(4) Para obtener Z como Sección 3, luego use un clasificador tradicional para realizar la clasificación. Sin embargo, este enfoque no tiene en cuenta las etiquetas de datos en el primer paso. Creyendo que el uso de etiquetas de datos mejora la precisión al obtener una Z mejor para la clasificación, consideramos usar las etiquetas de datos para guiar la factorización de la matriz, llamada factorización de matriz supervisada [22]. Debido a que algunos datos utilizados en la factorización de la matriz no tienen información de etiqueta, la factorización de matriz supervisada cae en la categoría de aprendizaje semi-supervisado. Sea C el conjunto de clases. Para simplificar, primero consideramos el problema de la clase binaria, es decir, C = {−1, 1}. Suponga que conocemos las etiquetas {yi} para los vértices en T ⊂ V. Queremos encontrar una hipótesis H: V → R, de modo que asignamos vi a 1 cuando h (vi) ≥ 0, −1 de lo contrario. Asumimos que una transformación del espacio latente a R es lineal, es decir, H (VI) = W φ (VI) + B = W Zi + B, (5) Departamento de curso escolar.Facultad Otro Estudiante del Estudiante del Proyecto Total Cornell 44 1 34 581 18 21 128 827 Texas 36 1 46 561 20 2 148 814 Washington 77 1 30 907 18 10 123 1166 Wisconsin 85 0 38 894 25 12 156 1210 Tabla 1: DataSet of WebKB donde Wy B son parámetros para estimar. Aquí, W es la norma del límite de decisión. Similar a las máquinas vectoriales de soporte (SVM) [7], podemos usar la pérdida de bisagra para medir la pérdida, x I: Vi∈T [1 - yih (vi)]+, donde [x]+ es x si x ≥ 0, 0 si x <0. Sin embargo, la pérdida de bisagra no es suave en el punto de bisagra, lo que dificulta la aplicación de métodos de gradiente sobre el problema. Para superar la dificultad, usamos una versión suavizada de la pérdida de bisagra para cada punto de datos, g (yih (vi)), (6) donde g (x) = 8> <>: 0 cuando x ≥ 2, 1 - x cuandox ≤ 0, 1 4 (x - 2) 2 cuando 0 <x <2. Reducimos un problema multiclase en múltiples binarios. Un simple esquema de reducción es el esquema de codificación de un solo descanso. En el esquema de un solo restablecimiento, asignamos un vector de etiqueta para cada etiqueta de clase. El elemento de un vector de etiqueta es 1 si el punto de datos pertenece a la clase correspondiente, −1, si el punto de datos no pertenece a la clase correspondiente, 0, si el punto de datos no está etiquetado. Sea Y la matriz de etiqueta, cada columna de la cual es un vector de etiqueta. Por lo tanto, y es una matriz de N × C, donde C es el número de clases, | C |. Entonces los valores de la ecuación.(5) Formar una matriz H = ZW + 1B, (7) donde 1 es un vector de tamaño N, cuyos elementos son todos uno, W es una matriz de parámetros C × L, y B es un vector de parámetro de tamaño c.La pérdida total es proporcional a la suma de la ecuación.(6) Sobre todos los puntos de datos etiquetados y las clases, ly (w, b, z) = λ x i: vi∈T, j∈C g (yijhij), donde λ es el parámetro para escalar el término. Para obtener una solución robusta, también usamos la regularización de tikhonov para w, ωw (w) = ν 2 w 2 f, donde ν es el parámetro para escalar el término. Luego, el problema de factorización de la matriz supervisada se convierte en Min U, V, Z, W, B JS (U, V, Z, W, B) (8) donde JS (U, V, Z, W, B) = J (U,V, Z) + ly (W, B, Z) + Ωw (W). También podemos usar métodos de gradiente para resolver el problema de la ecuación.(8). Los gradientes son ∂js ∂u = ∂j ∂u, ∂js ∂v = ∂j ∂v, ∂js ∂z = ∂j ∂z + λgw, ∂js ∂w = λg z + νw, ∂js ∂B =λg 1, donde G es una matriz n × C, cuyo elemento ik-th es yikg (yikhik), y g (x) = 8> <>: 0 cuando x ≥ 2, −1 cuando x ≤ 0, 1 2 (x - 2) cuando 0 <x <2. Una vez que obtenemos W, B y Z, podemos aplicar H en los vértices con etiquetas de clase desconocidas, o aplicar algoritmos de clasificación tradicionales en z para obtener los resultados de clasificación.5. Experimentos 5.1 Descripción de los datos En esta sección, realizamos la clasificación en dos conjuntos de datos, para demostrar el enfoque de nuestro enfoque. Los dos conjuntos de datos son el conjunto de datos WebKB [1] y el conjunto de datos CORA [15]. El conjunto de datos WebKB consta de aproximadamente 6000 páginas web de los departamentos de informática de cuatro escuelas (Cornell, Texas, Washington y Wisconsin). Las páginas web se clasifican en siete categorías. Los números de páginas en cada categoría se muestran en la Tabla 1. El conjunto de datos CORA consta de los resúmenes y referencias de aproximadamente 34,000 trabajos de investigación de informática. Utilizamos parte de ellos para clasificar en uno de los subcampos de la estructura de datos (DS), el hardware y la arquitectura (HA), el aprendizaje automático (ML) y el lenguaje de programación (PL). Eliminamos esos artículos sin referencia a otros artículos en el conjunto. El número de documentos y el número de subcampos en cada área se muestran en la Tabla 2. Área # de documentos # de subcampos Estructura de datos (DS) 751 9 Hardware y arquitectura (HA) 400 7 Aprendizaje automático (ML) 1617 7 Lenguaje de programación (PL) 1575 9 Tabla 2: DataSet of Cora 5.2 Métodos La tarea de los experimentos es clasificar los datos en función de su información de contenido y/o estructura de enlaces. Utilizamos los siguientes métodos: 65 70 75 80 85 90 95 100 Wisconsin WashingTontexascornell Precisión (%) DataSet SVM en Content SVM en Link SVM en el contenido de enlace Reg. PLSI+PHITS Link-Content MF-Content-Content Sup. MF method Cornell Texas Washington Wisconsin SVM on content 81.00 ± 0.90 77.00 ± 0.60 85.20 ± 0.90 84.30 ± 0.80 SVM on links 70.10 ± 0.80 75.80 ± 1.20 80.50 ± 0.30 74.90 ± 1.00 SVM on link-content 80.00 ± 0.80 78.30 ± 1.00 85.20 ± 0.7084.00 ± 0.90 Directed graph regularization 89.47 ± 1.41 91.28 ± 0.75 91.08 ± 0.51 89.26 ± 0.45 PLSI+PHITS 80.74 ± 0.88 76.15 ± 1.29 85.12 ± 0.37 83.75 ± 0.87 link-content MF 93.50 ± 0.80 96.20 ± 0.50 93.60 ± 0.50 92.60 ± 0.60 link-Content sup. MF 93.80 ± 0.70 97.07 ± 1.11 93.70 ± 0.60 93.00 ± 0.30 Tabla 3: Precisión de clasificación (media ± STD-ERR %) en el conjunto de datos de WebKB • SVM en el contenido Aplicamos las máquinas vectoriales de soporte (SVM) en el contenido de los documentos. Las características son las palabras de la bolsa y todas las palabras son vistas. Este método ignora la estructura del enlace en los datos. Se usa SVM lineal. El parámetro de regularización de SVM se selecciona utilizando el método de validación cruzada. La implementación de SVM utilizada en los experimentos es libsvm [4].• SVM en enlaces tratamos los enlaces como las características de cada documento, es decir, la característica I-Th es Link to Pagei. Aplicamos SVM en las funciones de enlace. Este método utiliza información de enlaces, pero no la estructura del enlace.• SVM en el contenido del enlace Combinamos las características de los dos métodos anteriores. Usamos diferentes pesos para estos dos conjuntos de características. Los pesos también se seleccionan mediante validación cruzada.• Regularización de gráficos dirigidos Este método se describe en [25] y [24]. Este método se basa únicamente en la estructura del enlace.• PLSI+Phits Este método se describe en [6]. Este método combina información de contenido de texto y estructura de enlaces para el análisis. El algoritmo PHITS es en un espíritu similar a la ecuación 1, con una restricción no negativa adicional. Modela las estructuras salientes y entrantes por separado.• El contenido de enlace MF Este es nuestro enfoque de la factorización de la matriz descrita en la Sección 3. Usamos 50 factores latentes para Z. Después de calcular la Z, entrenamos un SVM lineal usando Z como vectores de características, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase.• Sup. Link-Content Sup. MF Este método es nuestro enfoque de la factorización de matriz supervisada en la Sección 4. Usamos 50 factores latentes para Z. Después de calcular la Z, entrenamos un SVM lineal en la parte de entrenamiento de Z, luego aplicamos SVM en la parte de prueba de Z para obtener el resultado final, debido a la salida multiclase. Divendemos aleatoriamente los datos en cinco pliegues y repitemos el experimento por cinco veces, por cada vez que usamos un pliegue para la prueba, otros cuatro pliegues para el entrenamiento. Durante el proceso de capacitación, utilizamos la validación cruzada para seleccionar todos los parámetros del modelo. Medimos los resultados por precisión de clasificación, es decir, el porcentaje del número de documentos clasificados correctos en todo el conjunto de datos. Los resultados se muestran como las precisiones de clasificación promedio y la desviación estándar de TI en las cinco repeticiones.5.3 Resultados Las precisiones de clasificación promedio para el conjunto de datos WebKB se muestran en la Tabla 3. Para esta tarea, las precisiones de SVM en los enlaces son peores que las de SVM en el contenido. Pero la regularización del gráfico dirigido, que también se basa solo en el enlace, logra una precisión mucho mayor. Esto implica que la estructura de enlaces juega un papel importante en la clasificación de este conjunto de datos, pero los enlaces individuales en una página web dan poca información. La combinación de enlaces y contenido que usa SVM logra una precisión similar a la de SVM solo en el contenido, lo que confirma que los enlaces individuales en una página web brindan poca información. Dado que nuestro enfoque considere la estructura del enlace y la información del contenido, nuestros dos métodos dan a los resultados una mayor precisión entre estos enfoques. La diferencia entre los resultados de nuestros dos métodos no es significativa. Sin embargo, en los experimentos a continuación, mostramos la diferencia entre ellos. Las precisiones de clasificación para el conjunto de datos CORA se muestran en la Tabla 4. En este experimento, las precisiones de SVM en la combinación de enlaces y contenido son más altas que SVM en contenido o SVM en enlaces. Esto indica que tanto el contenido como los enlaces son infor45 50 55 60 65 70 75 80 PLMLHADS Precisión (%) DataSet SVM en Content SVM en el enlace SVM en el contenido de enlace Reg. PLSI+PHITS Link-Content MF-Content-Content Sup. Método MF DS HA ML PL SVM en el contenido 53.70 ± 0.50 67.50 ± 1.70 68.30 ± 1.60 56.40 ± 0.70 SVM en enlaces 48.90 ± 1.70 65.80 ± 1.40 60.70 ± 1.10 58.20 ± 0.70 SVM en el enlace 63.70 ± 5.50 70.5.50. 062.35 ± 1.00 Regularización de gráficos dirigidos 46.07 ± 0.82 65.50 ± 2.30 59.37 ± 0.96 56.06 ± 0.84 PLSI+PHITS 53.60 ± 1.78 67.40 ± 1.48 67.51 ± 1.13 57.45 ± 0.68 Enlace-Content MF 61.00 00 00 ± 7.20 7.20 7.20 7.20 7.20 7.20 7.20 7.20 7.20 7.20 74. 62.50 ± 0.80 enlace-Content sup. MF 69.38 ± 1.80 74.20 ± 0.70 78.70 ± 0.90 68.76 ± 1.32 Tabla 4: Precisión de clasificación (media ± STD-ERR %) en el conjunto de datos CORA mativo para clasificar los artículos en subfields. El método de regularización de gráficos dirigidos no funciona tan bien como SVM en el contenido de enlace, lo que confirma la importancia del contenido del artículo en esta tarea. Aunque nuestro método de factorización de matriz de contenido de enlace funciona ligeramente mejor que otros métodos, nuestro método de factorización de matriz supervisada de LinkContent superan significativamente.5.4 El número de factores Como discutimos en la Sección 3, la complejidad computacional de cada iteración para resolver el problema de optimización es cuadrático al número de factores. Realizamos experimentos para estudiar cómo el número de factores afecta la precisión de la predicación. Utilizamos diferentes números de factores para los datos de Cornell del conjunto de datos WebKB y los datos de aprendizaje automático (ML) del conjunto de datos CORA. El resultado que se muestra en la Figura 4 (a) y 4 (b). Las cifras muestran que la precisión 88 89 90 91 92 93 94 95 0 10 20 30 30 40 50 Precisión (%) Número de factores Sup. MF Link-Content MF (A) Cornell Datos 62 64 66 68 70 72 74 76 78 80 0 10 20 30 40 50 Precisión (%) Número de factores Link-Content Sup. MF Contento de enlace MF (B) ML Datos Figura 4: Precisión frente a los factores aumenta a medida que aumenta el número de factores. Es un concepto diferente de elegir el número óptimo de grupos en la aplicación de agrupación. Es cuánta información representar en las variables latentes. Hemos considerado la regularización sobre los factores, lo que evita el problema de sobre fáctica para una gran cantidad de factores. Para elegir el número de factores, debemos considerar la compensación entre la precisión y el tiempo de cálculo, lo cual es cuadrático para el número de factores. La diferencia entre el método de factorización de la matriz y la supervisada disminuye a medida que aumenta el número de factores. Esto indica que la utilidad de la factorización de la matriz supervisada a un número menor de factores.6. Discusiones Las funciones de pérdida en la ecuación.(2) y LC en la ecuación.(3) Use la pérdida al cuadrado debido a la conveniencia computacional. En realidad, la pérdida al cuadrado no describe con precisión el modelo de ruido subyacente, porque los pesos de la matriz de adyacencia solo pueden tomar valores no negativos, en nuestro caso, cero o uno solo, y los componentes de la matriz de contenido C solo pueden tomar enteros no negativos. Por lo tanto, podemos aplicar otros tipos de pérdida, como pérdida de bisagra o pérdida de bisagra suavizada, p. La (u, z) = µH (a, zuz), donde h (a, b) = p i, j [1 - aijbij]+. En nuestro artículo, discutimos principalmente la aplicación de la clasificación. Una entrada de Matrix Z significa la relación de una página web y un factor. Los valores de las entradas son los pesos del modelo lineal, en lugar de las probabilidades de las páginas web que pertenecen a temas latentes. Por lo tanto, permitimos que los componentes tomen posibles valores reales. Cuando llegamos a la aplicación de agrupación, podemos usar este modelo para encontrar Z, luego aplicar K-Means para dividir las páginas web en grupos. En realidad, podemos usar la idea de factorización de matriz no negativa para la agrupación [20] para agrupar las páginas web directamente. Como el ejemplo con restricciones no negativas que se muestran en la Sección 3, representamos cada grupo por un tema latente, es decir, la dimensionalidad del espacio latente se establece en el número de grupos que queremos. Entonces el problema de la ecuación.(4) se convierte en Min U, V, Z J (U, V, Z), S.T.Z ≥ 0. (9) Resolviendo la ecuación.(9), podemos obtener resultados más interpretables, que podrían usarse para la agrupación.7. Conclusiones En este documento, estudiamos el problema de cómo combinar la información de contenido y enlaces para el análisis de la página web, principalmente en la aplicación de clasificación. Proponemos un enfoque simple utilizando factores para modelar el contenido de texto y la estructura de enlace de las páginas/documentos web. Los enlaces dirigidos se generan a partir de la combinación lineal de vinculación entre los factores de origen y de destino. Al compartir factores entre el contenido de texto y la estructura del enlace, es fácil combinar tanto la información del contenido como la estructura del enlace. Nuestros experimentos muestran que nuestro enfoque es efectivo para la clasificación. También discutimos una extensión para la aplicación de agrupación. Reconocimiento Nos gustaría agradecer al Dr. Dengyong Zhou por compartir su código de su algoritmo. Además, gracias a los revisores por los comentarios constructivos.8. REFERENCIAS [1] Proyecto de la Base de Conocimientos Mundial de CMU (WEBKB). Disponible en http://www.cs.cmu.edu/∼webkb/.[2] D. Achlioptas, A. Fiat, A. R. Karlin y F. McSherry. Búsqueda web a través de la síntesis de centros. En el Simposio IEEE sobre Fundamentos de la Informática, Páginas 500-509, 2001. [3] S. Chakrabarti, B. E. Dom y P. Indyk. Categorización mejorada de hipertexto utilizando hipervínculos. En L. M. Haas y A. Tiwary, editores, Actas de Sigmod-98, Conferencia Internacional de ACM sobre gestión de datos, páginas 307-318, Seattle, EE. UU., 1998. ACM Press, Nueva York, EE. UU.[4] C.-C.Chang y C.-J. Lin. Libsvm: una biblioteca para máquinas de soporte vectorial, 2001. Software disponible en http://www.csie.ntu.edu.tw/∼cjlin/libsvm.[5] D. Cohn y H. Chang. Aprender a identificar probabilísticamente documentos autorizados. Proc. ICML 2000. pp.167-174., 2000. [6] D. Cohn y T. Hofmann. El enlace faltante: un modelo probabilístico de contenido de documentos y conectividad de hipertexto. En T. K. Leen, T. G. Dietterich y V. Tresp, Editores, Avances en Sistemas de Procesamiento de Información Neural 13, páginas 430-436. MIT Press, 2001. [7] C. Cortes y V. Vapnik. Redes de soporte-vector. Machine Learning, 20: 273, 1995. [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas y R. A. Harshman. Indexación por análisis semántico latente. Journal of the American Society of Information Science, 41 (6): 391-407, 1990. [9] X. Él, H. Zha, C. Ding y H. Simon. Agrupación de documentos web utilizando estructuras de hipervínculos. Estadísticas computacionales y análisis de datos, 41 (1): 19-45, 2002. [10] T. Hofmann. Indexación semántica latente probabilística. En Actas de la Vetuenta Second International Sigir Conference, 1999. [11] T. Joachims, N. Cristianini y J. Shawe-Taylor. Kernels compuestos para la categorización de hipertexto. En C. Brodley y A. Danyluk, editores, Actas de ICML-01, 18ª Conferencia Internacional de Aprendizaje Machine, páginas 250-257, Williams College, EE. UU., 2001. Morgan Kaufmann Publishers, San Francisco, EE. UU.[12] J. M. Kleinberg. Fuentes autorizadas en un entorno hipervínculos. J. ACM, 48: 604-632, 1999. [13] P. Kolari, T. Finin y A. Joshi. SVMS para la blogósfera: identificación de blog y detección de Splog. En AAAAI Spring Simposio sobre enfoques computacionales para analizar los weblogs, marzo de 2006. [14] O. Kurland y L. Lee. PageRank sin hipervínculos: reanimiento estructural utilizando enlaces inducidos por modelos de idiomas. En Sigir 05: Actas de la 28ª Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 306-313, Nueva York, NY, EE. UU., 2005. ACM Press.[15] A. McCallum, K. Nigam, J. Rennie y K. Seymore. Automatizar la construcción de portales de Internet con aprendizaje automático. Revista de recuperación de información, 3 (127-163), 2000. [16] H.-J. Oh, S. H. Myaeng y M.-H.Sotavento. Un método práctico de comergorización de hipertexto utilizando enlaces e información de clase disponible incrementalmente. En Sigir 00: Actas de la 23a Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 264-271, Nueva York, NY, EE. UU., 2000. ACM Press.[17] L. Page, S. Brin, R. Motowani y T. Winograd. Ranking de citas de PageRank: traiga orden a la web. Documento de trabajo de la Biblioteca Digital de Stanford 1997-0072, 1997. [18] C. Spearman. Inteligencia general, determinada y medida objetivamente. The American Journal of Psychology, 15 (2): 201-292, abril de 1904. [19] B. Taskar, P. Abbeel y D. Koller. Modelos probabilísticos discriminativos para datos relacionales. En Actas de la 18ª Conferencia Internacional de UAI, 2002. [20] W. Xu, X. Liu e Y. Gong. La agrupación de documentos basada en la factorización de la matriz no negativa. En Sigir 03: Actas de la 26ª Conferencia Internacional de Investigación y Desarrollo ACM anual de ACM en recuperación de información, páginas 267-273. ACM Press, 2003. [21] Y. Yang, S. Slattery y R. Ghani. Un estudio de enfoques para la categorización de hipertexto. Journal of Intelligent Information Systems, 18 (2-3): 219-241, 2002. [22] K. Yu, S. Yu y V. Tresp. Indexación semántica latente informada con múltiples etiquetas. En Sigir 05: Actas de la 28ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 258-265, Nueva York, NY, EE. UU., 2005. ACM Press.[23] T. Zhang, A. Popescul y B. Dom. Modelos de predicción lineal con regularización de gráficos para la categorización de la página web. En KDD 06: Actas de la 12ª Conferencia Internacional de ACM SIGKDD sobre descubrimiento de conocimiento y minería de datos, páginas 821-826, Nueva York, NY, EE. UU., 2006. ACM Press.[24] D. Zhou, J. Huang y B. Sch¨olkopf. Aprender de datos etiquetados y sin etiquetar en un gráfico dirigido. En Actas de la 22ª Conferencia Internacional sobre Aprendizaje Machine, Bonn, Alemania, 2005. [25] D. Zhou, B. Sch¨olkopf y T. Hofmann. Aprendizaje semi-supervisado en gráficos dirigidos. Proc. Información neural. Sistemas de procesamiento, 2004.