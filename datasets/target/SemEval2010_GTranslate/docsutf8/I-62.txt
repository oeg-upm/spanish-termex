Un enfoque de Q-Decomposition y RTDP limitado para la asignación de recursos Pierrick Plamondon y Brahim Chaib-Draa Informática y Ingeniería de Ingeniería de Ingeniería de Software Laval Universidad de Laval Quebec, Canadá {PLAMON, CHAIBTHATR&D Canadá-Valcartier Québec, Canadá abderrezak.benaskeur@drdc-rddc.gc.ca Resumen Este documento contribuye a resolver problemas de asignación de recursos estocásticos efectivos que se sabe que son NP-completas. Para abordar este complejo problema de gestión de recursos, se propone un enfoque QDecomposition cuando los recursos que ya se comparten entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. La decomposición Q permite coordinar estos agentes separados por recompensas y, por lo tanto, permite reducir el conjunto de estados y acciones a considerar. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna QDecomposición y utilizamos la búsqueda heurística. En particular, se utiliza la programación dinámica en tiempo real limitada (RTDP limitado). RTDP limitado concentra la planificación solo en estados significativos y poda el espacio de acción. La poda se realiza proponiendo límites superiores e inferiores en la función de valor. Categorías y descriptores de sujetos I.2.8 [Inteligencia artificial]: resolución de problemas, métodos de control y búsqueda;I.2.11 [Inteligencia artificial]: inteligencia artificial distribuida. Algoritmos de términos generales, rendimiento, experimentación.1. Introducción Este documento tiene como objetivo contribuir a resolver problemas de asignación de recursos estocásticos complejos. En general, se sabe que los problemas de asignación de recursos son NP completos [12]. En tales problemas, un proceso de programación sugiere que la acción (es decir, los recursos para asignar) se comprometen a realizar ciertas tareas, de acuerdo con el estado del medio ambiente perfectamente observable. Al ejecutar una acción para realizar un conjunto de tareas, la naturaleza estocástica del problema induce probabilidades en el próximo estado visitado. En general, el número de estados es la combinación de todos los estados específicos posibles de cada tarea y recursos disponibles. En este caso, el número de acciones posibles en un estado es la combinación de cada posible asignación de recursos de cada individuo a las tareas. El número muy alto de estados y acciones en este tipo de problema lo hace muy complejo. Puede haber muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya se comparten entre los agentes, y las acciones realizadas por un agente no influyen en el estado de otro agente, la política globalmente óptima puede calcularse mediante la planificación por separado para cada agente. Un segundo tipo de problema de asignación de recursos es donde los recursos ya se comparten entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Para resolver este problema de manera eficiente, adaptamos Qdecomposicion propuesta por Russell y Zimdars [9]. En nuestro enfoque de Q-Decomposition, un agente de planificación gestiona cada tarea y todos los agentes tienen que compartir los recursos limitados. El proceso de planificación comienza con el estado inicial S0. En S0, cada agente calcula su respectivo valor Q. Luego, los agentes de planificación se coordinan a través de un árbitro para encontrar el valor Q global más alto agregando los respectivos valores Q de cada agente. Cuando se implementa con búsqueda heurística, dado que el número de estados y acciones a considerar al calcular la política óptima se reduce exponencialmente en comparación con otros enfoques conocidos, Q-Decomposition permite formular el primer algoritmo de búsqueda heurística descompuesta óptima en entornos estocásticos. Por otro lado, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. Una forma común de abordar este gran problema estocástico es mediante el uso de los procesos de decisión de Markov (MDP), y en particular la búsqueda en tiempo real donde se han desarrollado muchos algoritmos recientemente. Por ejemplo, la programación dinámica en tiempo real (RTDP) [1], LRTDP [4], HDP [3] y LAO [5] son todos los enfoques de búsqueda heurística de última generación en un entorno estocástico. Debido a su calidad de tiempo, un enfoque interesante es RTDP introducido por Barto et al.[1] que actualiza establece en trayectorias de un estado inicial S0 a un estado de estado SG.RTDP se usa en este documento para resolver eficientemente un problema de asignación de recursos restringido.RTDP es mucho más efectivo si el espacio de acción puede ser podado de acciones subóptimas. Para hacer esto, McMahan et 1212 978-81-904262-7-5 (RPS) c 2007 Ifaamas al.[6], Smith y Simmons [11], y Singh y Cohn [10] propusieron resolver un problema estocástico utilizando una búsqueda heurística tipo RTDP con límites superiores e inferiores en el valor de los estados. McMahan et al.[6] y Smith y Simmons [11] sugirieron, en particular, una trayectoria eficiente de actualizaciones de estado para acelerar aún más la convergencia, cuando se administran límites superiores e inferiores. Esta trayectoria eficiente de actualizaciones de estado se puede combinar al enfoque propuesto aquí, ya que este documento se centra en la definición de límites estrechos y una actualización de estado eficiente para un problema de asignación de recursos restringido. Por otro lado, el enfoque de Singh y Cohn es adecuado para nuestro caso, y se extiende en este documento utilizando, en particular, el concepto de ingresos marginales [7] para elaborar límites estrechos. Este documento propone nuevos algoritmos para definir los límites superior e inferior en el contexto de un enfoque de búsqueda heurística RTDP. Nuestros límites de ingresos marginales se comparan teórica y empíricamente con los límites propuestos por Singh y Cohn. Además, incluso si el algoritmo utilizado para obtener la política óptima es RTDP, nuestros límites se pueden usar con cualquier otro algoritmo para resolver un MDP. La única condición sobre el uso de nuestros límites es estar en el contexto de la asignación de recursos restringidos estocásticos. El problema ahora está modelado.2. Formulación de problemas Un problema de asignación de recursos simple es aquella en la que hay las siguientes dos tareas para realizar: ta1 = {lavar los platos} y ta2 = {limpiar el piso}. Estas dos tareas están en estado realizado o no realizado. Para realizar las tareas, se suponen dos tipos de recursos: res1 = {pincel}, y res2 = {detergente}. Una computadora tiene que calcular la asignación óptima de estos recursos para limpiar los robots para realizar sus tareas. En este problema, un estado representa una conjunción del estado particular de cada tarea y los recursos disponibles. Los recursos pueden estar limitados por la cantidad que puede usarse simultáneamente (restricción local) y en total (restricción global). Además, cuanto más alto es el número de recursos asignados para realizar una tarea, mayor es la expectativa de realizar la tarea. Por esta razón, cuando cambian los estados específicos de las tareas, o cuando el número de recursos disponibles cambia, el valor de este estado puede cambiar. Al ejecutar una acción A en el estado s, los estados específicos de las tareas cambian estocásticamente, y el recurso restante se determina con el recurso disponible en S, restados de los recursos utilizados por la acción A, si el recurso es consumible. De hecho, nuestro modelo puede considerar tipos de recursos consumibles y no consumo. Un tipo de recurso consumible es aquel en el que la cantidad de recursos disponibles disminuye cuando se usa. Por otro lado, un tipo de recurso no consumo es uno en el que la cantidad de recursos disponibles no cambia cuando se usa. Por ejemplo, un pincel es un recurso no consumo, mientras que el detergente es un recurso de consumo.2.1 Asignación de recursos como MDP en nuestro problema, la función de transición y la función de recompensa son conocidas. Un marco del proceso de decisión de Markov (MDP) se utiliza para modelar nuestro problema de asignación de recursos estocásticos.Los investigadores han adoptado ampliamente los MDP para modelar un proceso estocástico. Esto se debe al hecho de que los MDP proporcionan un modelo de mundo bien estudiado y simple pero muy expresivo. Un MDP en el contexto de un problema de asignación de recursos con recursos limitados se define como una tuple res, t a, s, a, p, w, r ,, donde: • res = res1, ..., res | res |es un conjunto finito de tipos de recursos disponibles para un proceso de planificación. Cada tipo de recurso puede tener una restricción de recursos local LRES sobre el número que puede usarse en un solo paso, y una restricción de recursos global sobre el número que puede usarse en total. La restricción global solo se aplica para los tipos de recursos consumibles (rescate) y las limitaciones locales siempre se aplican a los tipos de recursos consumibles y no consumo.• T A es un conjunto finito de tareas con ta ∈ T a para ser logrado.• S es un conjunto finito de estados con s ∈ S. un estado s es una tuple t a, res1, ..., res | rescá, que es la característica de cada tarea no confidencial ta ∈ T a en el entorno y los recursos consumibles disponibles.STA es el estado específico de la tarea TA. Además, S contiene un conjunto no vacío SG de estados de meta. Un estado de gol es un estado de sumidero donde un agente permanece para siempre.• A es un conjunto finito de acciones (o tareas). Las acciones a ∈ A (S) aplicable en un estado son la combinación de todas las tareas de recursos que pueden ejecutarse, de acuerdo con el estado s.En particular, A es simplemente una asignación de recursos a las tareas actuales, y ATA es la asignación de recursos a la tarea TA. Las posibles acciones están limitadas por Lres y Gres.• Probabilidades de transición PA (S | S) para S ∈ S y A ∈ A (S).• W = [WTA] es el peso relativo (criticidad) de cada tarea.• Estado recompensa r = [rs]: ta∈T a rsta ← sta × wta. La recompensa relativa del estado de una tarea RSTA es el producto de un número real STA por el factor de peso WTA. Para nuestro problema, se da una recompensa de 1 × WTA cuando el estado de una tarea (STA) está en un estado logrado y 0 en todos los demás casos.• Un factor de descuento (preferencia) γ, que es un número real entre 0 y 1. Una solución de un MDP es una política de mapeo π establece s en acciones a ∈ A (s). En particular, πta (s) es la acción (es decir, recursos para asignar) que deben ejecutarse en la tarea TA, considerando el estado global s.En este caso, una política óptima es la que maximiza la recompensa total esperada por realizar todas las tareas. El valor óptimo de un estado, v (s), viene dado por: v (s) = r (s) + max a∈A (s) γ s ∈S pa (s | s) v (s) (1)donde los recursos consumibles restantes en los estados son rescate \ res (a), donde res (a) son los recursos consumibles utilizados por la acción a. De hecho, dado que una acción A es una asignación de recursos, Resc \ Res (a) es el nuevo conjunto de recursos disponibles después de la ejecución de la acción a. Además, uno puede calcular los valores Q (A, S) de cada par de acciones estatales utilizando el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1213 Siguiente ecuación: Q (A, S) = R (S) + γ S ∈S Pa (S | S) Max a ∈A (S) Q (A, S) (2) donde el valor óptimo de un estado es v (s) = max a∈A (s) q (a, s). La política se somete a las restricciones de recursos locales Res (π (s)) ≤ lres∀ s ∈ S, y ∀ res ∈ Res. La restricción global se define de acuerdo con todas las trayectorias del sistema tra ∈ T ra. Una trayectoria del sistema TRA es una posible secuencia de pares de acción estatal, hasta que se alcanza un estado de objetivo bajo la política óptima π. Por ejemplo, se ingresa el estado S, que puede transitar a S o a S, de acuerdo con la acción a. Las dos posibles trayectorias del sistema son (s, a), (s) y (s, a), (s). La restricción global de recursos es Res (TRA) ≤ Gres∀ Tra ∈ T ra, y ∀ res ∈ Rescente donde Res (TRA) es una función que devuelve los recursos utilizados por trayectoria TRA. Dado que los recursos consumibles disponibles están representados en el espacio estatal, esta condición se verifica por sí misma. En otras palabras, el modelo es Markovian ya que la historia no debe considerarse en el espacio estatal. Además, el tiempo no se considera en la descripción del modelo, pero también puede incluir un horizonte temporal mediante el uso de un MDP de horizonte finito. Dado que la asignación de recursos en un entorno estocástico es completado NP, se deben emplear heurísticas. La decomposición Q que descompone un problema de planificación para muchos agentes para reducir la complejidad computacional asociada al estado y/o los espacios de acción ahora se introduce.2.2 Decomposición Q Para la asignación de recursos puede haber muchos tipos de problemas de asignación de recursos. En primer lugar, si los recursos ya se comparten entre los agentes, y las acciones realizadas por un agente no influyen en el estado de otro agente, la política globalmente óptima puede calcularse mediante la planificación por separado para cada agente. Un segundo tipo de problema de asignación de recursos es donde los recursos ya se comparten entre los agentes, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por ejemplo, un grupo de agentes que administra el petróleo consumado por un país cae en este grupo. Estos agentes desean maximizar su recompensa específica consumiendo la cantidad correcta de petróleo. Sin embargo, todos los agentes son penalizados cuando un agente consume petróleo debido a la contaminación que genera. Otro ejemplo de este tipo proviene de nuestro problema de interés, explicado en la Sección 3, que es una plataforma naval que debe contrarrestar los misiles entrantes (es decir, tareas) mediante el uso de sus recursos (es decir, armas, movimientos). En algunos escenarios, puede suceder que los misiles se pueden clasificar en dos tipos: aquellos que requieren un conjunto de recursos RES1 y aquellos que requieren un conjunto de recursos RES2. Esto puede suceder dependiendo del tipo de misiles, su rango, etc. En este caso, dos agentes pueden planificar ambos conjuntos de tareas para determinar la política. Sin embargo, hay interacción entre el recurso de Res1 y Res2, de modo que no se puede asignar cierta combinación de recursos. En particular, si un agente asigno los recursos RESI al primer conjunto de tareas t ai, y el agente asigno recursos resi al segundo conjunto de tareas t ai, la política resultante puede incluir acciones que no se pueden ejecutar juntas. Para dar como resultado estos conflictos, utilizamos la decomposición Q propuesta por Russell y Zimdars [9] en el contexto del aprendizaje de refuerzo. La suposición principal subyacente a QDecomposición es que la función de recompensa general R se puede descomponer aditivamente en recompensas separadas RI para cada agente distinto I ∈ Ag, donde | Ag |es el número de agentes. Es decir, r = i∈Ag ri. Requiere que cada agente calcule un valor, desde su perspectiva, para cada acción. Para coordinar entre sí, cada agente I informa sus valores de acción Qi (Ai, Si) para cada estado Si ∈ Si a un árbitro en cada iteración de aprendizaje. Luego, el árbitro elige una acción que maximiza la suma de los valores Q del Agente para cada estado global s ∈ S. La próxima vez que se actualice el estado s, un agente que considera el valor como su contribución respectiva, o valor Q, al globalvalor Q máximo. Es decir, qi (ai, si) es el valor de un estado tal que maximiza maxa∈A (s) i∈Ag qi (ai, si). El hecho de que los agentes usen un valor Q determinado como el valor de un estado es una extensión del algoritmo SARSA sobre política [8] a la descomposición Q. Russell y Zimdars llamaron a este enfoque local Sarsa. De esta manera, se puede encontrar un compromiso ideal para que los agentes alcancen un óptimo global. De hecho, en lugar de permitir que cada agente elija la acción sucesora, cada agente I utiliza la acción ai ejecutada por el árbitro en el estado sucesor Si: qi (ai, si) = ri (si) + γ si∈Si pai (Si |Si) qi (ai, Si) (3) donde los recursos consumibles restantes en el estado SI son resci \ resi (ai) para un problema de asignación de recursos. Russell y Zimdars [9] demostraron que el Sarsa local converge a lo óptimo. Además, en algunos casos, esta forma de descomposición del agente permite que las funciones Q locales se expresen por un espacio de estado y acción muy reducido. Para nuestro problema de asignación de recursos descrito brevemente en esta sección, la decomposición Q se puede aplicar para generar una solución óptima. De hecho, una copia de seguridad óptima de Bellman se puede aplicar en un estado como en el algoritmo 1. En la línea 5 de la función QDEC-Backup, cada agente que administra una tarea calcula su respectivo valor Q. Aquí, Qi (AI, S) determina el valor Q óptimo del agente I en el estado s. Un agente I utiliza como el valor de una posible transición de estado s El valor Q para este agente que determina el valor Q global máximo para los estados como en el enfoque original de la decomposición Q. En resumen, para cada estado visitado S ∈ S, cada agente calcula sus respectivos valores Q con respecto al estado global s.Entonces el espacio estatal es el espacio estatal conjunto de todos los agentes. Parte de la ganancia en la complejidad para usar Q-Decomposition reside en la parte de la ecuación Si∈Si PAI (Si | s). Un agente considera una posible transición de estado solo los posibles estados del conjunto de tareas que administra. Dado que el número de estados es exponencial con el número de tareas, el uso de Q-Decomposition debería reducir significativamente el tiempo de planificación. Además, el espacio de acción de los agentes tiene en cuenta solo sus recursos disponibles, que es mucho menos complejo que un espacio de acción estándar, que es la combinación de todas las posibles asignación de recursos en un estado para todos los agentes. Luego, las funcionalidades del árbitro están en las líneas 8 a 20. El valor Q global es la suma de los valores Q producidos por cada agente que administra cada tarea como se muestra en la línea 11, considerando la acción global a. En este caso, cuando una acción de un agente no puedo ser ejecutada simultáneamente con una acción de otro agente I, la acción global simplemente se descarta del espacio de acción A (s). La línea 14 simplemente asigne el valor actual con respecto al mayor valor Q global, como en una copia de seguridad estándar de Bellman. Luego, la política óptima y el valor Q de cada agente se actualizan en las líneas 16 y 17 a las sub-acciones AI y los valores Q específicos Qi (AI, S) de cada agente 1214 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) para la acción a.Algoritmo 1 La copia de seguridad de Bellman Q-Decomposition.1: Función QDEC-Backup (S) 2: V (S) ← 0 3: Para todos los i ∈ Ag do 4: para todos los ai ∈ Ai (s) do 5: Qi (Ai, S) ← Ri (s) + +γ si ∈Si pai (si | s) qi (ai, s) {donde qi (ai, s) = hi (s) cuando s aún no se ha visitado, y s tiene recursos de consumibles restantes para cadaAgente I} 6: Fin para 7: Fin para 8: para todos A ∈ A (S) Do 9: Q (A, S) ← 0 10: Para todos los I ∈ Ag do 11: Q (A, S) ← Q(a, s) + qi (ai, s) 12: final para 13: si q (a, s)> v (s) luego 14: v (s) ← q (a, s) 15: para todos los i ∈AG do 16: πi (s) ← Ai 17: Qi (AI, S) ← Qi (AI, S) 18: Fin para 19: Fin si 20: Fin para una copia de seguridad estándar de Bellman tiene una complejidad de O (| A |× | SAG |), donde | SAG |es el número de estados conjuntos para todos los agentes que excluyen los recursos, y | a |es el número de acciones conjuntas. Por otro lado, la copia de seguridad de Bellman Q-Decomposition tiene una complejidad de o ((| ag | × | ai | × | si) |) + (| a | × | ag |)), donde | si |es el número de estados para un agente I, excluyendo los recursos y | ai |es el número de acciones para un agente i. Desde | SAG |es combinatorial con el número de tareas, entonces | si || S |. Además, | a |es combinatorial con el número de tipos de recursos. Si los recursos ya se comparten entre los agentes, el número de tipo de recursos para cada agente generalmente será más bajo que el conjunto de todos los tipos de recursos disponibles para todos los agentes. En estas circunstancias, | ai || A |. En una copia de seguridad estándar de Bellman, | A |se multiplica por | sag |, que es mucho más complejo que multiplicar | a |por | ag |con el Q-Decomposition Bellman Backup. Por lo tanto, la copia de seguridad de Bellman Q-Decomposition es mucho menos compleja que una copia de seguridad estándar de Bellman. Además, el costo de comunicación entre los agentes y el árbitro es nulo ya que este enfoque no considera un problema separado geográficamente. Sin embargo, cuando los recursos están disponibles para todos los agentes, no es posible ninguna descomposición Q. En este caso, la programación dinámica en tiempo real limitada (RTDP limitada) permite centrar la búsqueda en los estados relevantes y podar el espacio de acción A utilizando un límite más bajo y más alto en el valor de los estados.ahora se introduce RTDP limitado.2.3 Bonet RTDP limitado y Geffner [4] propuso LRTDP como una mejora para RTDP [1].LRTDP es un algoritmo de programación dinámico simple que implica una secuencia de ejecuciones de prueba, cada uno que comienza en el estado inicial S0 y termina en un objetivo o en un estado resuelto. Cada prueba LRTDP es el resultado de simular la política π al actualizar los valores V (S) utilizando una copia de seguridad de Bellman (Ecuación 1) sobre los estados que se visitan.H (S) es una heurística que define un valor inicial para el estado s.Esta heurística debe ser admisible: el valor dado por la heurística tiene que sobreestimar (o subestimar) el valor óptimo V (s) cuando la función objetivo se maximiza (o se minimiza). Por ejemplo, una heurística admisible para un problema de ruta más corta estocástica es la solución de un problema de ruta más corta determinista. De hecho, dado que el problema es estocástico, el valor óptimo es más bajo que para la versión determinista. Se ha demostrado que LRTDP, dada una heurística inicial admisible sobre el valor de los estados, no puede quedarse atrapado en bucles, y finalmente produce valores óptimos [4]. La convergencia se logra mediante un procedimiento de etiquetado llamado control (S,). Este procedimiento intenta etiquetar como resuelto cada estado atravesado en la trayectoria actual. Cuando el estado inicial se etiqueta como se resuelve, el algoritmo ha convergido. En esta sección, se presenta una versión limitada de RTDP (BoundEDRTDP) en el Algoritmo 2 para podar el espacio de acción de las acciones subóptimas. Esta poda permite acelerar la convergencia de LRTDP.RTDP limitado es similar a RTDP, excepto que hay dos heurísticas iniciales distintas para estados no visitados s ∈ S;HL (S) y HU (S). Además, el procedimiento de verificación (S) se puede omitir porque los límites pueden proporcionar el etiquetado de un estado como se resuelve. Por un lado, HL (S) define un límite inferior en el valor de S de modo que el valor óptimo de S es mayor que HL (S). Para su parte, Hu (S) define un límite superior en el valor de S de modo que el valor óptimo de S es más bajo que HU (S). Los valores de los límites se calculan en las líneas 3 y 4 de la función de respaldo limitado. La calculación de estos dos valores Q se realiza simultáneamente, ya que las transiciones estatales son las mismas para ambos valores Q. Solo cambian los valores de las transiciones de estado. Por lo tanto, tener que calcular dos valores Q en lugar de uno no aumenta la complejidad del enfoque. De hecho, Smith y Simmons [11] afirman que el tiempo adicional para calcular una copia de seguridad de Bellman para dos límites, en lugar de uno, no es más del 10%, que también es lo que obtuvimos. En particular, L (s) es el límite inferior de estado s, mientras que U (S) es el límite superior del estado s.Del mismo modo, Ql (A, S) es el valor Q del límite inferior de la acción A en el estado S, mientras que Qu (A, S) es el valor Q del límite superior de acción A en el estado s.El uso de estos dos límites permite reducir significativamente el espacio de acción A. De hecho, en las líneas 5 y 6 de la función de respaldo limitado, si qu (a, s) ≤ l (s) entonces la acción A puede ser podada del espacio de acción de s.En la línea 13 de esta función, un estado puede marcarse como se resuelve si la diferencia entre los límites inferiores y superiores es más bajo que. Cuando la ejecución se remonta a la función RTDP limitada, el siguiente estado en la línea 10 tiene un número fijo de recursos consumibles disponibles rescate, determinado en la línea 9. En resumen, PickNextState (RES) selecciona un estado no resuelto accesible bajo la política actual que tiene el error más alto de Bellman (| U (S)-L (S) |). Finalmente, en las líneas 12 a 15, una copia de seguridad se realiza de manera atrasada en todos los estados visitados de una trayectoria, cuando se ha realizado esta trayectoria. Esta estrategia se ha demostrado como eficiente [11] [6]. Según lo discutido por Singh y Cohn [10], este tipo de algoritmo tiene una serie de características deseables en cualquier momento: si se debe elegir una acción en el estado antes de que el algoritmo haya convergente (mientras que múltiples acciones competitivas permanecen), la acción con la mayorEl límite inferior está recogido. Dado que se conoce el límite superior para el estado, se puede estimar el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Algoritmo 1215 2 El algoritmo RTDP limitado. Adaptado de [4] y [10].1: Función Limitada-RTDP (s) 2: Devuelve un valor Función V 3: Repita 4: S ← S0 5: Visitado ← NULL 6: Repita 7: Visitado.Resc ← Resc \ {π (s)} 10: S ← S.PickNextState (resc) 11: Hasta que S sea un objetivo 12: mientras se lo visite = NULL DO 13: S ← Visited.Pop () 14: Backup (S limitado (S S) 15: Fin mientras 16: hasta que se resuelva S0 o | A (s) |= 1 ∀ s ∈ S accesible de S0 17: return v Algoritmo 3 La copia de seguridad de Bellman acotada.1: Función Limitada (s) 2: para todos a ∈ A (s) do 3: qu (a, s) ← r (s) + γ s ∈S pa (s | s) u (s) 4:Ql (a, s) ← r (s) + γ s ∈S pa (s | s) l (s) {donde l (s) ← hl (s) y u (s) ← hu (s) cuando s es esaún no visitado y S tiene rescate \ res (a) recursos consumibles restantes} 5: Si qu (a, s) ≤ l (s) entonces 6: a (s) ← a (s) \ res (a) 7: finSi 8: finalice para 9: l (s) ← max a∈A (s) ql (a, s) 10: u (s) ← max a∈A (s) qu (a, s) 11: π (s) ← arg max a∈A (s) ql (a, s) 12: if | u (s) - l (s) |<Entonces 13: S ← Resuelto 14: Fin si hasta qué punto el límite inferior está de lo óptimo. Si la diferencia entre el límite inferior y superior es demasiado alta, uno puede optar por usar otro algoritmo codicioso de opciones, que genera una solución rápida y casi óptima. Además, si una nueva tarea llega dinámicamente al entorno, se puede acomodar redefiniendo los límites inferiores y superiores que existen en el momento de su llegada. Singh y Cohn [10] demostraron que un algoritmo que utiliza límites inferiores y superiores admisibles para podar el espacio de acción está asegurado de convergir a una solución óptima. Las siguientes secciones describen dos métodos separados para definir HL (s) y Hu (S). En primer lugar, el método de Singh y Cohn [10] se describe brevemente. Luego, nuestro propio método propone límites más estrictos, lo que permite una poda más efectiva del espacio de acción.2.4 Los límites de Singh y Cohns Singh y Cohn [10] definieron los límites inferiores y superiores para podar el espacio de acción. Su enfoque es bastante sencillo. En primer lugar, se calcula una función de valor para que todas las tareas se den cuenta, utilizando un enfoque RTDP estándar. Luego, se puede definir estas funciones de valor de tarea, se puede definir un HL de límite inferior y Hu límite superior. En particular, hl (s) = max ta∈T a vta (sta), y hu (s) = ta∈T a vta (sta). Para la legibilidad, el límite superior de Singh y Cohn se llama Singhu, y el límite inferior se llama Singhl. Singh y Cohn han demostrado la admisibilidad de estos límites, de modo que el límite superior siempre sobreestima el valor óptimo de cada estado, mientras que el límite inferior siempre subestima el valor óptimo de cada estado. Para determinar la política óptima π, Singh y Cohn implementaron un algoritmo muy similar a RTDP limitado, que utiliza los límites para inicializar L (s) y U (S). La única diferencia entre RTDP limitado y la versión RTDP de Singh y Cohn está en los criterios de detención. Singh y Cohn propusieron que el algoritmo termina cuando solo queda una acción competitiva para cada estado, o cuando el rango de todas las acciones competitivas para cualquier estado está limitado por un parámetro de indiferencia.Etiquetas de rtdp limitadas para los cuales | u (s)-l (s) |<como se resuelve y la convergencia se alcanza cuando S0 se resuelve o cuando solo queda una acción competitiva para cada estado. Este criterio de detención es más efectivo ya que es similar al utilizado por Smith y Simmons [11] y McMahan et al.BRTDP [6]. En este artículo, los límites definidos por Singh y Cohn e implementados utilizando RTDP limitado definen el enfoque Singh-RTDP. Las siguientes secciones proponen apretar los límites de Singh-RTDP para permitir una poda más efectiva del espacio de acción.2.5 Reducir el Singhu de límite superior incluye acciones que pueden no ser posibles ejecutar debido a limitaciones de recursos, que sobreestima el límite superior. Para considerar solo acciones posibles, se introduce nuestro límite superior, llamado maxu: hu (s) = max a∈A (s) ta∈T a qta (ata, sta) (4) donde qta (ata, sta) es la q-Value de la tarea TA para el estado STA, y la acción ATA calculada utilizando un enfoque LRTDP estándar. Teorema 2.1. El límite superior definido por la ecuación 4 es admisible. Prueba: Las restricciones de recursos locales se cumplen porque el límite superior se calcula utilizando todas las acciones posibles globales a. Sin embargo, Hu (S) todavía sobreestima V (s) porque la restricción de recursos globales no se aplica. De hecho, cada tarea puede usar todos los recursos consumibles para su propio propósito. Hacer esto produce un valor más alto para cada tarea que el obtenido al planificar todas las tareas a nivel mundial con los recursos limitados compartidos. Calcular el máximo en un estado tiene una complejidad de o (| a | × | t a |), y o (| t a |) para Singhu. Una copia de seguridad estándar de Bellman tiene una complejidad de o (| a | × | s |). Desde | a | × | t a || A | × | s |, el tiempo de cálculo para determinar el límite superior de un estado, que se realiza una vez para cada estado visitado, es mucho menor que el tiempo de cálculo requerido para calcular una copia de seguridad de Bellman estándar para un estado, que esPor lo general, se hace muchas veces para cada estado visitado. Por lo tanto, el tiempo de cálculo del límite superior es insignificante.1216 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 2.6 Aumento del límite inferior La idea de aumentar SingHL es asignar los recursos a priori entre las tareas. Cuando cada tarea tiene su propio conjunto de recursos, cada tarea puede resolverse de forma independiente. El límite inferior del estado s es hl (s) = ta∈T un lowta (STA), donde Lowta (STA) es una función de valor para cada tarea Ta ∈ T a, de modo que los recursos se han asignado a priori. La asignación a priori de los recursos se realiza utilizando ingresos marginales, que es un concepto altamente utilizado en microeconomía [7], y recientemente se ha utilizado para la coordinación de un MDP descentralizado [2]. En resumen, los ingresos marginales son los ingresos adicionales que una unidad adicional de producto traerá a una empresa. Por lo tanto, para un problema de asignación de recursos estocásticos, el ingreso marginal de un recurso es el valor esperado adicional que implica. El ingreso marginal de un recurso resero para una tarea TA en un estado STA se define como siguiendo: MRTA (STA) = Max ATA∈A (STA) QTA (ATA, STA) - MAX ATA∈A (STA) QTA (ATA |Res /∈ AtA, STA) (5) El concepto de ingresos marginales de un recurso se usa en el Algoritmo 4 para asignar los recursos a priori entre las tareas que permiten definir el valor límite más bajo de un estado. En la línea 4 del algoritmo, se calcula una función de valor para todas las tareas en el entorno utilizando un enfoque estándar de LRTDP [4]. Estas funciones de valor, que también se utilizan para el límite superior, se calculan considerando que cada tarea puede usar todos los recursos disponibles. La línea 5 inicializa la variable Valueta. Esta variable es el valor estimado de cada tarea ta ∈ T a. Al comienzo del algoritmo, no se asignan recursos a una tarea específica, por lo tanto, la variable valueta se inicializa a 0 para todos Ta ∈ T a. Luego, en la línea 9, se selecciona un tipo de recursos (consumible o no consumo) para que se asigne. Aquí, un experto en dominio puede separar todos los recursos disponibles en muchos tipos o piezas que se asignarán. Los recursos se asignan en el orden de su especialización. En otras palabras, cuanto más un recurso es eficiente en un pequeño grupo de tareas, más se asigna temprano. La asignación de los recursos en este orden mejora la calidad del límite inferior resultante. La línea 12 calcula los ingresos marginales de un recurso consumible Res para cada tarea Ta ∈ T a. Para un recurso no consumo, dado que el recurso no se considera en el espacio estatal, todos los demás estados accesibles de STA consideran que el recurso RES aún se puede usar. El enfoque aquí es sumar la diferencia entre el valor real de un estado al valor Q máximo de este estado si los recursos RES no se pueden usar para todos los estados en una trayectoria dada por la política de tarea TA. Esta heurística demostró obtener buenos resultados, pero otros pueden ser juzgados, por ejemplo, simulación de Monte-Carlo. En la línea 21, los ingresos marginales se actualizan en función de los recursos ya asignados a cada tarea. R (SGTA) es la recompensa para realizar la tarea TA. Por lo tanto, VTA (STA) −valueta R (SGTA) es el valor esperado residual que queda por lograr, conociendo la asignación actual a la tarea TA y se normaliza por la recompensa de realizar las tareas. El ingreso marginal se multiplica por este término para indicar que, cuanto más una tarea tenga un alto valor residual, más sus ingresos marginales serán altos. Luego, una tarea TA se selecciona en la línea 23 con los ingresos marginales más altos, ajustados con valor residual. En la línea 24, el tipo de recurso RES se asigna al grupo de recursos RESTA de la tarea TA. Posteriormente, la línea 29 Recomalgoritmo 4 El algoritmo de límite inferior de ingresos marginales.1: FUNCIÓN DE INGRESOS BUENTES 2: Devuelve un límite inferior bajo a 3: para todos los ta ∈ T a do 4: VTA ← LRTDP (STA) 5: Valueta ← 0 6: Fin para 7: S ← S0 8:Repita 9: Res ← Seleccione un tipo de recurso Res ∈ Res 10: para todos Ta ∈ T A Do 11: Si Res es consumible, entonces 12: MRTA (STA) ← VTA (STA) - VTA (STA (RES) 13: else 14: mrta (sta) ← 0 15: repite 16: mrta (sta) ← mrta (sta) + vta (sta) max (ata∈A (sta) | res/∈Ata) Qta (ATA, STA) 17: Sta ← Sta.PickNextState (rescate) 18: Hasta que Sta sea un objetivo 19: S ← S0 20: Fin si 21: Mrrvta (STA) ← Mrta (STA) × VTA (STA) −Valueta R (SGTA) 22: Endpara 23: ta ← tarea ta ∈ T a que maximice mrrvta (sta) 24: RESTA ← RESTA {Res} 25: Temp ← ∅ 26: Si Res es consumible, entonces 27: Temp ← Res 28: Fin si 29: Valueta ← Valueta+ ((VTA (STA) - Valueta) × max ata∈A (STA, Res) Qta (ATA, STA (TEMP)) VTA (STA)) 30: Hasta que todos los tipos de recursos se asignen 31: para todos los TA∈ T A DO 32: Lowta ← LRTDP (STA, RESA) 33: Fin para 34: return Lowt A Punes Valueta. La primera parte de la ecuación para calcular valueta representa el valor residual esperado para la tarea TA. Este término se multiplica por max ata∈A (STA) QTA (ATA, STA (RES)) VTA (STA), que es la relación de la eficiencia del tipo de recurso RES. En otras palabras, Valueta se asigna a Valueta + (el valor residual × la relación de valor del tipo de recurso res). Para un recurso consumible, el valor Q considera solo recursos en el espacio estatal, mientras que para un recurso no consumo, no hay recursos disponibles. Todos los tipos de recursos se asignan de esta manera hasta que RES esté vacío. Todos los tipos de recursos consumibles y no consumo se asignan a cada tarea. Cuando se asignan todos los recursos, los componentes de límite inferior LowTA de cada tarea se calculan en la línea 32. Cuando se calcula la solución global, el límite inferior es el siguiente: hl (s) = max (singhl, max a∈A (s) ta∈T a lowta (sta)) (6) Usamos el máximo del límite singhly la suma de los componentes límite inferiores Lowta, por lo tanto, marginalRevenue ≥ Singhl. En particular, el singhl límite puede el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1217, sea mayor cuando quedan un pequeño número de tareas. Como los componentes Lowta se calculan considerando S0;Por ejemplo, si en un estado posterior solo queda una tarea, el límite de SingHL será más alto que cualquiera de los componentes LowTA. La principal diferencia de complejidad entre SingHL y los ingresos es en la línea 32, donde se debe calcular un valor para cada tarea con el recurso compartido. Sin embargo, dado que el recurso se comparte, el espacio de estado y el espacio de acción se reducen considerablemente para cada tarea, reduciendo en gran medida el cálculo en comparación con las funciones de valor calculadas en la línea 4 que se realiza tanto para SINGHL como para los ingresos. Teorema 2.2. El límite inferior de la ecuación 6 es admisible. Prueba: Lowta (STA) se calcula con el recurso que se comparte. Sumar las funciones de valor LowTA (STA) para cada ta ∈ T A no viola las limitaciones de recursos locales y globales. De hecho, a medida que se comparten los recursos, las tareas no pueden usarlos en exceso. Por lo tanto, HL (S) es una política realizable y un límite inferior admisible.3. Discusión y experimentos El dominio de los experimentos es una plataforma naval que debe contrarrestar los misiles entrantes (es decir, tareas) mediante el uso de sus recursos (es decir, armas, movimientos). Para los experimentos, se generaron 100 problemas de asignación de recursos aleatorios para cada enfoque y posible número de tareas. En nuestro problema, | Sta |= 4, por lo tanto, cada tarea puede estar en cuatro estados distintos. Hay dos tipos de estados;En primer lugar, los estados donde las acciones modifican las probabilidades de transición;Y luego, hay estados meta. Las transiciones estatales son estocásticas porque cuando un misil está en un estado dado, siempre puede transitar en muchos estados posibles. En particular, cada tipo de recurso tiene una probabilidad de contrarrestar un misil entre 45% y 65% dependiendo del estado de la tarea. Cuando no se contrarresta un misil, transita a otro estado, que puede preferirse o no al estado actual, donde el estado más preferido para una tarea es cuando se contrarresta. La efectividad de cada recurso se modifica al azar en ± 15% al comienzo de un escenario. También hay limitaciones de recursos locales y globales sobre la cantidad que se puede utilizar. Para las limitaciones locales, como máximo 1 recurso de cada tipo se puede asignar para ejecutar tareas en un estado específico. Esta restricción también está presente en una plataforma naval real debido a las limitaciones de sensores y lanzadores y políticas de compromiso. Además, para los recursos de consumo, la cantidad total de recursos consumibles disponibles es entre 1 y 2 para cada tipo. La restricción global se genera aleatoriamente al comienzo de un escenario para cada tipo de recurso consumible. El número de tipos de recursos se ha fijado a 5, donde hay 3 tipos de recursos consumibles y 2 tipos de recursos no consumen. Para este problema se ha implementado un enfoque LRTDP estándar. Se ha utilizado una heurística simple donde se asigna el valor de un estado no visitado como el valor de un estado objetivo de tal manera que se logren todas las tareas. De esta manera, se asegura el valor de cada estado no visitado para sobreestimar su valor real ya que el valor de lograr una tarea TA es el más alto que el planificador puede obtener para TA. Dado que esta heurística es bastante sencilla, las ventajas de usar una mejor heurística son más evidentes. Sin embargo, incluso si el enfoque LRTDP utiliza una heurística simple, aún no se visita una gran parte del espacio estatal al calcular la política óptima. Los enfoques descritos en este documento se comparan en las Figuras 1 y 2. Resumamos estos enfoques aquí: • QDEC-LRTDP: Las copias de seguridad se calculan utilizando la función QDEC-Backup (Algoritmo 1), pero en un contexto LRTDP. En particular, las actualizaciones realizadas en la función resuelta de verificación también se realizan utilizando la función QDECBACKUP.• LRTDP-UP: el límite superior de MaxU se usa para LRTDP.• Singh-RTDP: los límites de Singhl y Singhu se utilizan para RTDP limitado.• MR-RTDP: los límites de los ingresos y MAXU se utilizan para RTDP limitado. Para implementar QDEC-LRTDP, dividimos el conjunto de tareas en dos partes iguales. El conjunto de tareas, administrado por el Agente I, se puede lograr con el conjunto de recursos RESI, mientras que el segundo conjunto de tareas, administrado por el Agente AGI, se puede lograr con el conjunto de recursos RESI. RESI tenía un tipo de recurso consumible y un tipo de recurso no consumo, mientras que ResI tenía dos tipos de recursos consumibles y un tipo de recurso no consumo de recursos. Cuando el número de tareas es impar, se asignó una tarea más a t ai. Existen una restricción entre el grupo de RESI y RESI de recursos de modo que algunas tareas no son posibles. Estas restricciones son administradas por el árbitro como se describe en la Sección 2.2. Q-Decomposition permite disminuir el tiempo de planificación significativamente en la configuración de nuestro problema, y parece un enfoque muy eficiente cuando un grupo de agentes tiene que asignar recursos que solo están disponibles para sí mismas, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida poral menos otro agente. Para calcular el límite inferior de los ingresos, todos los recursos disponibles deben separarse en muchos tipos o piezas para asignarse. Para nuestro problema, asignamos cada recurso de cada tipo en el orden de su especialización como dijimos al describir la función de los ingresos. En términos de experimentos, observe que el LRTDP LRTDP-UP y los enfoques para la asignación de recursos, que no podan el espacio de acción, son mucho más complejos. Por ejemplo, tardó un promedio de 1512 segundos en planificar el enfoque LRTDP-Up con seis tareas (ver Figura 1). El enfoque SINGH-RTDP disminuyó el tiempo de planificación mediante el uso de una parte inferior y superior para podar el espacio de acción.MR-RTDP reduce aún más el tiempo de planificación al proporcionar límites iniciales muy ajustados. En particular, Singh-RTDP necesitaba 231 segundos en promedio para resolver problemas con seis tareas y MR-RTDP requirió 76 segundos. De hecho, la reducción del tiempo es bastante significativa en comparación con LRTDP-UP, lo que demuestra la eficiencia del uso de límites para podar el espacio de acción. Además, implementamos MR-RTDP con el Singhu Bound, y esto fue ligeramente menos eficiente que con el Bound Bound. También implementamos MR-RTDP con el Bound SingHL, y esto fue un poco más eficiente que Singh-RTDP. A partir de estos resultados, concluimos que la diferencia de eficiencia entre MR-RTDP y Singh-RTDP es más atribuible al límite inferior de los ingresos marginales al límite superior MAXU. De hecho, cuando el número de tareas de ejecución es alto, los límites inferiores de Singh-RTDP toman los valores de una sola tarea. Por otro lado, el límite inferior de MR-RTDP tiene en cuenta el valor de los 1218 el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 TIEMPLESECONDS Número de tareas LRTDP QDEC-LRTDP Figura 1: Eficiencia de Q-Decomposition LRTDPy LRTDP.0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 TimeSeconds Número de tareas LRTDP LRTDP-UP SINGH-RTDP MR-RTDP Figura 2: Eficiencia de MR-RTDP en comparación con SingH-RTDP.tarea utilizando una heurística para distribuir los recursos. De hecho, una asignación óptima es aquella en la que los recursos se distribuyen de la mejor manera a todas las tareas, y nuestro límite inferior heurísticamente lo hace.4. Conclusión Los experimentos han demostrado que la decomposición Q parece un enfoque muy eficiente cuando un grupo de agentes tiene que asignar recursos que solo están disponibles para sí mismas, pero las acciones realizadas por un agente pueden influir en la recompensa obtenida por al menos otro agente. Por otro lado, cuando se comparten el recurso disponible, no es posible la descomposición Q y propusimos límites ajustados para la búsqueda heurística. En este caso, el tiempo de planificación de RTDP limitado, que poda el espacio de acción, es significativamente más bajo que para LRTDP. Además, los ingresos marginales vinculados propuestos en este documento se comparan favorablemente con el enfoque Singh y Cohn [10].LimitEDRTDP con nuestros límites propuestos puede aplicarse a una amplia gama de entornos estocásticos. La única condición para el uso de nuestros límites es que cada tarea posee recursos limitados consumibles y/o no consumo. Una avenida de investigación interesante sería experimentar nuestros límites con otros algoritmos de búsqueda heurística. Por ejemplo, FRTDP [11] y BRTDP [6] son algoritmos de búsqueda heurísticos eficientes. En particular, ambos enfoques propusieron actualizaciones de trayectoria de estado eficientes, cuando se les dan límites superiores e inferiores. Nuestros límites apretados habilitarían, tanto para FRTDP como para BRTDP, para reducir el número de copias de seguridad para realizar antes de la convergencia. Finalmente, la función RTDP limitada poda el espacio de acción cuando qu (a, s) ≤ l (s), como sugirieron Singh y Cohn [10].FRTDP y BRTDP también podrían podar el espacio de acción en estas circunstancias para reducir aún más su tiempo de planificación.5. Referencias [1] A. Barto, S. Bradtke y S. Singh. Aprender a actuar usando la programación dinámica en tiempo real. Inteligencia Artificial, 72 (1): 81-138, 1995. [2] A. Beynier y A. I. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión descentralizados de Markov restringidos. En procedimiento de la vigésima primera conferencia nacional sobre inteligencia artificial (AAAI-06), 2006. [3] B. Bonet y H. Geffner. Algoritmos de búsqueda heurísticos más rápidos para planificar con incertidumbre y comentarios completos. En Actas de la Decimoctavo Conferencia Internacional Conjunta sobre Inteligencia Artificial (IJCAI-03), agosto de 2003. [4] B. Bonet y H. Geffner. Enfoque LRTDP etiquetado: Mejora de la convergencia de la programación dinámica en tiempo real. En el procedimiento de la Decimotercera Conferencia Internacional sobre Planificación y Programación Automatizada (ICAPS-03), Páginas 12-21, Trento, Italia, 2003. [5] E. A. Hansen y S. Zilberstein.Lao: un algoritmo de búsqueda heurística que encuentra soluciones con bucles. Inteligencia Artificial, 129 (1-2): 35-62, 2001. [6] H. B. McMahan, M. Likhachev y G. J. Gordon. Programación dinámica en tiempo real limitada: RTDP con límites superiores monótonos y garantías de rendimiento. En ICML 05: Actas de la Vigésimo Segunda Conferencia Internacional sobre Aprendizaje Autor, páginas 569-576, Nueva York, NY, EE. UU., 2005. ACM Press.[7] R. S. Pindyck y D. L. Rubinfeld. Microeconomía. Prentice Hall, 2000. [8] G. A. Rummery y M. Niranjan. Q-learning en línea utilizando sistemas Connectionist. Informe técnico Cud/Finfeng/TR 166, Departamento de Ingeniería de la Universidad de Cambridge, 1994. [9] S. J. Russell y A. Zimdars. Q-Decomposition para agentes de aprendizaje de refuerzo. En ICML, páginas 656-663, 2003. [10] S. Singh y D. Cohn. Cómo fusionar dinámicamente los procesos de decisión de Markov. En Avances en Sistemas de Procesamiento de Información Neural, Volumen 10, páginas 1057-1063, Cambridge, MA, EE. UU., 1998. MIT Press.[11] T. Smith y R. Simmons. Programación dinámica en tiempo real enfocada para MDP: exprimiendo más de una heurística. En Actas de la vigésima primera conferencia nacional sobre inteligencia artificial (AAAI), Boston, EE. UU., 2006. [12] W. Zhang. Modelado y resolución de un problema de asignación de recursos con técnicas de restricción suave. Informe técnico: WUCS-2002-13, Universidad de Washington, Saint-Louis, Missouri, 2002. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1219