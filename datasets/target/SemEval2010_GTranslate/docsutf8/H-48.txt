Un nuevo enfoque para evaluar la expansión de la consulta: consulta Documento Término Mismatch Tonya Custis Thomson Corporation 610 Opperman Drive St. Paul, MN Tonya.custis@thomson.com Khalid al-Kofahi Thomson Corporation 610 Opperman Drive St. Paul, Mn Khalid.al--kofahi@thomson.com Resumen La efectividad de los sistemas de recuperación de información (IR) está influenciado por el grado de superposición de términos entre consultas de usuario y documentos relevantes. La falta de coincidencia del término del documento de consulta, ya sea parcial o total, es un hecho que los sistemas IR deben tratarse. La expansión de la consulta (QE) es un método para lidiar con la falta de coincidencia de términos. Los sistemas IR que implementan la expansión de consultas se evalúan típicamente ejecutando cada consulta dos veces, con y sin expansión de la consulta, y luego comparando los dos conjuntos de resultados. Si bien esto mide un cambio general en el rendimiento, no mide directamente la efectividad de los sistemas IR para superar el problema inherente del malhechor de términos entre la consulta y los documentos relevantes, ni proporciona ninguna idea de cómo tales sistemas se comportarían en presencia deCONSIDAD DEL Documento MISMACH. En este documento, proponemos un nuevo enfoque para evaluar las técnicas de expansión de consultas. El enfoque propuesto es atractivo porque proporciona una estimación del rendimiento del sistema bajo diversos grados de desajuste de término de documento de consulta, utiliza colecciones de pruebas fácilmente disponibles y no requiere ningún juicio de relevancia adicional o ninguna forma de procesamiento manual. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información Términos generales Medición, Experimentación 1. Introducción en nuestro dominio, 1 y, a diferencia de la búsqueda web, es muy importante que los abogados encuentren todos los documentos (por ejemplo, casos) que sean relevantes para un problema. Los documentos relevantes faltantes pueden tener consecuencias no triviales en el resultado de un procedimiento judicial. Los abogados están especialmente preocupados por la falta de documentos relevantes al investigar un tema legal que es nuevo para ellos, ya que pueden no ser conscientes de todas las variaciones de idiomas en tales temas. Por lo tanto, es importante desarrollar sistemas de recuperación de información que sean robustos con respecto a las variaciones de lenguaje o el malhechado de término entre consultas y documentos relevantes. Durante nuestro trabajo sobre el desarrollo de dichos sistemas, concluimos que los métodos de evaluación actuales no son suficientes para este propósito.{Tos ferina, tos ferina}, {ataque cardíaco, infarto de miocardio}, {lavado de autos, limpieza de automóviles}, {abogado, asesor legal, abogado} son ejemplos de cosas que comparten el mismo significado. A menudo, los términos elegidos por los usuarios en sus consultas son diferentes a los que aparecen en los documentos relevantes para sus necesidades de información. Este término del término de consulta del documento surge de dos fuentes: (1) La sinonimia que se encuentra en el lenguaje natural, tanto en el término como en el nivel de la frase, y (2) el grado en que el usuario es un experto en la búsqueda y/o tiene expertoConocimiento en el dominio de la colección que se busca. Las evaluaciones IR son de naturaleza comparativa (cf. Trec). En general, las evaluaciones IR muestran cómo el sistema A lo hizo en relación con el sistema B en la misma colección de pruebas basada en varias métricas basadas en precisión y retiro. Del mismo modo, los sistemas IR con capacidades QE se evalúan típicamente ejecutando cada búsqueda dos veces, una y una vez sin expansión de la consulta, y luego comparando los dos conjuntos de resultados. Si bien este enfoque muestra qué sistema puede haber tenido un mejor desempeño en general con respecto a una colección de pruebas particular, no mide directa o sistemáticamente la efectividad de los sistemas IR para superar la falta de coincidencia del término de documento de consulta. Si el objetivo de QE es aumentar el rendimiento de la búsqueda mediante la mitigación de los efectos del desajuste del término del documento de consulta, entonces el grado en que un sistema lo hace debe ser medible en la evaluación. Un método de evaluación efectivo debe medir el rendimiento de los sistemas IR bajo diversos grados de desajuste de término de documento de consulta, no solo en términos de rendimiento general en una colección en relación con otro sistema.1 Thomson Corporation desarrolla soluciones basadas en información a los mercados profesionales, incluidos los legales, financieros, de atención médica, científica e fiscal y contabilidad. Para medir que un sistema IR en particular puede superar la falta de coincidencia del término del documento de consulta al recuperar documentos que son relevantes para una consulta de usuarios, pero que no necesariamente contienen los términos de consulta en sí mismos, introducimos sistemáticamente el desajuste de términos en la recopilación de pruebas deEliminar términos de consulta de documentos relevantes conocidos. Debido a que estamos induciendo a propósito el malhechor de términos entre las consultas y los documentos relevantes conocidos en nuestras colecciones de pruebas, el marco de evaluación propuesto puede medir la efectividad de QE de una manera que las pruebas en toda la colección no lo son. Si un método de búsqueda QE encuentra un documento que se sabe que es relevante, pero que no obstante se pierde los términos de consulta, muestra que la técnica QE es realmente robusta con respecto a la desajuste del término de consulta.2. Trabajo relacionado que contabiliza a término desajuste entre los términos en consultas de usuarios y los documentos relevantes para las necesidades de información de los usuarios ha sido un problema fundamental en la investigación de IR durante casi 40 años [38, 37, 47]. La expansión de la consulta (QE) es una técnica utilizada en IR para mejorar el rendimiento de la búsqueda al aumentar la probabilidad de superposición de términos (ya sea explícita o implícitamente) entre consultas y documentos que son relevantes para las necesidades de información de los usuarios. La expansión de consultas explícitas se produce en tiempo de ejecución, en función de los resultados de búsqueda iniciales, como es el caso con la retroalimentación de relevancia y la retroalimentación de pseudo relevancia [34, 37]. La expansión de consultas implícitas puede basarse en propiedades estadísticas de la recopilación de documentos, o puede confiar en fuentes de conocimiento externas como un tesauro o una ontología [32, 17, 26, 50, 51, 2]. Independientemente del método, los algoritmos QE que son capaces de recuperar documentos relevantes a pesar de la falta de coincidencia parcial o total entre consultas y documentos relevantes deberían aumentar el retiro de los sistemas IR (recuperando documentos que se habrían perdido anteriormente), así como su precisión (al recuperardocumentos más relevantes). En la práctica, QE tiende a mejorar el rendimiento promedio de la recuperación general, haciéndolo mejorando el rendimiento en algunas consultas al tiempo que lo empeora en otras. Las técnicas QE se consideran efectivas en el caso de que ayudan más de lo que duelen en general en una colección particular [47, 45, 41, 27]. A menudo, los términos de expansión agregados a una consulta en la fase de expansión de la consulta terminan perjudicando el rendimiento general de la recuperación porque introducen ruido semántico, lo que hace que el significado de la consulta se deriva. Como tal, se ha realizado mucho trabajo con respecto a diferentes estrategias para elegir términos QE semánticamente relevantes para incluir para evitar la deriva de la consulta [34, 50, 51, 18, 24, 29, 30, 32, 3, 4, 5]. La evaluación de los sistemas IR ha recibido mucha atención en la comunidad de investigación, tanto en términos de desarrollo de colecciones de pruebas para la evaluación de diferentes sistemas [11, 12, 13, 43] como en términos de la utilidad de las métricas de evaluación como el retiro, la precisión, la precisión, precisión promedio media, precisión en rango, bpref, etc. [7, 8, 44, 14]. Además, ha habido evaluaciones comparativas de diferentes técnicas de QE en varias colecciones de prueba [47, 45, 41]. Además, la comunidad de investigación IR ha prestado atención a las diferencias entre el desempeño de las consultas individuales. Se han realizado esfuerzos de investigación para predecir qué consultas serán mejoradas con QE y luego aplicarlo selectivamente solo a esas consultas [1, 5, 27, 29, 15, 48], para lograr un rendimiento general óptimo. Además, se ha realizado el trabajo relacionado para predecir la dificultad de consultas, o qué consultas probablemente se realizan mal [1, 4, 5, 9]. Existe un interés general en la comunidad de investigación para mejorar la robustez de los sistemas IR al mejorar el rendimiento de la recuperación en consultas difíciles, como lo demuestra la pista robusta en las competiciones TREC y las nuevas medidas de evaluación como GMAP. GMAP (precisión promedio media geométrica) da más peso al extremo inferior de la precisión promedio (en lugar de MAP), enfatizando así el grado en que las consultas difíciles o de bajo rendimiento contribuyen a la puntuación [33]. Sin embargo, no se presta atención a la evaluación de la robustez de los sistemas IR que implementan QE con respecto al desajuste del término de consulta en términos cuantificables. Al inducir deliberadamente la falta de coincidencia entre los términos en consultas y documentos relevantes, nuestro marco de evaluación nos permite una manera controlada para degradar la calidad de las consultas con respecto a sus documentos relevantes, y luego medir tanto el grado de dificultad (inducida)de la consulta y el grado en que QE mejora el rendimiento de recuperación de la consulta degradada. El trabajo más similar al nuestro en la literatura consiste en un trabajo en el que las colecciones o consultas de documentos se alteran de manera sistemática para medir las diferencias de rendimiento de la consulta.[42] se introduce en la recopilación de documentos Pseudowords que son ambiguos con respecto al sentido de las palabras, para medir el grado en que la desambiguación del sentido de las palabras es útil en IR.[6] Experimentos con la alteración de la recopilación de documentos agregando términos de expansión semánticamente relacionados a los documentos en el tiempo de indexación. En el IR cruzado, [28] explora diferentes técnicas de expansión de consultas al tiempo que degrada deliberadamente sus recursos de traducción, en lo que equivale a expandir una consulta con solo un porcentaje controlado de sus términos de traducción. Aunque similar en la introducción de una cantidad controlada de varianza en sus colecciones de pruebas, estos trabajos difieren del trabajo que se presenta en este documento, ya que el trabajo que se presenta aquí se mide explícita y sistemáticamente la efectividad de la consulta en presencia de la falta de coincidencia de términos de consulta.3. Metodología Para medir con precisión el rendimiento del sistema IR en presencia de desajuste a término de consultas, necesitamos poder ajustar el grado de desajuste a término en un corpus de prueba de manera principalmente. Nuestro enfoque es introducir la falta de coincidencia del término deldocumento en un corpus de manera controlada y luego medir el rendimiento de los sistemas IR a medida que cambia el grado de desajuste del término. Eliminamos sistemáticamente los términos de consulta de documentos relevantes conocidos, creando versiones alternativas de una colección de pruebas que difieren solo en cuántos o qué términos de consulta se han eliminado de los documentos relevantes para una consulta en particular. La introducción de la falta de desajuste del término del documento de consulta en la colección de pruebas de esta manera nos permite manipular el grado de desajuste del término entre documentos y consultas relevantes de manera controlada. Este proceso de eliminación afecta solo a los documentos relevantes en la colección de búsqueda. Las consultas mismas permanecen inalteradas. Los términos de consulta se eliminan de los documentos uno por uno, por lo que las diferencias en el rendimiento del sistema IR se pueden medir con respecto a los términos faltantes. En el caso más extremo (es decir, cuando la longitud de la consulta es menor o igual al número de términos de consulta eliminados de los documentos relevantes), no habrá superposición de término entre una consulta y sus documentos relevantes. Observe que, para una consulta dada, solo se modifican documentos relevantes. Los documentos no relevantes se dejan sin cambios, incluso en el caso de que contengan términos de consulta. Aunque, en la superficie, estamos cambiando la distribución de los términos entre los conjuntos de documentos relevantes y no relevantes al eliminar los términos de consulta de los documentos relevantes, ya que no cambia la relevancia conceptual de estos documentos. Eliminar sistemáticamente los términos de consulta de documentos relevantes conocidos introduce una cantidad controlada de desajuste de término de documento de consulta por el cual podemos evaluar el grado en que las técnicas QE particulares pueden recuperar documentos conceptualmente relevantes, a pesar de la falta de superposición de término real. Eliminar un término de consulta de documentos relevantes simplemente enmascara la presencia de ese término de consulta en esos documentos. De ninguna manera cambia la relevancia conceptual de los documentos. El marco de evaluación presentado en este documento consta de tres elementos: una colección de pruebas, C;una estrategia para seleccionar qué términos de consulta eliminar de los documentos relevantes en esa colección, s;y una métrica para comparar el rendimiento de los sistemas IR, m.La recopilación de pruebas, C, consiste en una recopilación de documentos, consultas y juicios de relevancia. La estrategia, s, determina el orden y la forma en que los términos de la consulta se eliminan de los documentos relevantes en C. Este marco de evaluación no es específico de la métrica;Cualquier métrica (mapa, p@10, retirada, etc.) se puede utilizar para medir el rendimiento del sistema IR. Aunque las colecciones de pruebas son difíciles de encontrar, debe tenerse en cuenta que este marco de evaluación se puede usar en cualquier colección de pruebas disponible. De hecho, el uso de este marco estira el valor de las colecciones de pruebas existentes en que una colección se convierte en varios cuando los términos de consulta se eliminan de los documentos relevantes, lo que aumenta la cantidad de información que se puede obtener al evaluar una colección en particular. En otras evaluaciones de efectividad de QE, la variable controlada es simplemente si las consultas se han expandido o no, comparado en términos de alguna métrica. Por el contrario, la variable controlada en este marco es el término de consulta que se ha eliminado de los documentos relevantes para esa consulta, según lo determinado por la estrategia de eliminación, los términos de consulta de S. se eliminan uno por uno, de una manera y orden determinado por S, de modo que las colecciones difieren solo con respecto al un término que se ha eliminado (o enmascarado) en los documentos relevantes para esa consulta. Es de esta manera que podemos medir explícitamente el grado en que un sistema IR supera el desajuste del término del documento de consulta. La elección de una estrategia de eliminación de términos de consulta es relativamente flexible;La única restricción para elegir una estrategia s es que los términos de consulta deben eliminarse uno a la vez. Se deben tomar dos decisiones al elegir una estrategia de eliminación S. El primero es el orden en el que S elimina los términos de los documentos relevantes. Los posibles pedidos de eliminación podrían basarse en métricas como IDF o la probabilidad global de un término en una recopilación de documentos. Según el propósito de la evaluación y el algoritmo de recuperación que se está utilizando, podría tener más sentido elegir una orden de eliminación para S basada en el término de consulta IDF o tal vez basado en una medida de la probabilidad de término de consulta en la recopilación de documentos. Una vez que se ha decidido un pedido de eliminación, se debe decidir una forma de extracción/enmascaramiento a plazo. Debe determinarse si S eliminará los términos individualmente (es decir, eliminar solo un término diferente cada vez) o aditivamente (es decir, eliminar un término primero, entonces ese término además de otro, y así sucesivamente). La eliminación aditiva incremental de los términos de consulta de los documentos relevantes permite que la evaluación muestre el grado en que el rendimiento del sistema IR se degrada a medida que faltan más y más términos de consulta, lo que aumenta el grado de desajuste de término de documento de consulta. Eliminar términos individualmente permite una comparación clara de la contribución de QE en ausencia de cada término de consulta individual.4. Configuración experimental 4.1 Sistemas IR Utilizamos el marco de evaluación propuesto para evaluar cuatro sistemas IR en dos colecciones de pruebas. De los cuatro sistemas utilizados en la evaluación, dos técnicas de expansión de consultas implementan: OKAPI (con pseudo-retroalimentación para QE) y un motor de búsqueda conceptual patentado (bien, llame TCS, para Thomson Concept Search). TCS es un motor de recuperación basado en modelos de idiomas que utiliza un corpus externo apropiado para el tema (es decir, legal o noticias) como fuente de conocimiento. Esta fuente de conocimiento externo es un corpus separado, pero temáticamente relacionado con la recopilación de documentos a buscar. Las probabilidades de traducción para QE [2] se calculan a partir de estos grandes corpus externos. OKAPI (sin retroalimentación) y un modelo de probabilidad de consulta del modelo de idioma (QL) (implementado con Indri) se incluyen como líneas de base de solo palabras clave. Okapi sin retroalimentación se pretende como una línea de base análoga para OKAPI con comentarios, y el modelo QL está destinado a ser una línea de base apropiada para TCS, ya que ambos implementan algoritmos de recuperación basados en modelos de lenguaje. Elegimos esto como líneas de base porque solo dependen de las palabras que aparecen en las consultas y no tienen capacidades QE. Como resultado, esperamos que cuando los términos de la consulta se eliminen de los documentos relevantes, el rendimiento de estos sistemas debería degradarse más dramáticamente que sus contrapartes que implementan QE. Los resultados del modelo OKAPI y QL se obtuvieron utilizando el kit de herramientas Lemur.2 OKAPI se ejecutó con los parámetros K1 = 1.2, B = 0.75 y K3 = 7. Cuando se ejecutan con comentarios, los parámetros de retroalimentación utilizados en Okapi se establecieron en 10 documentos y 25 términos. El modelo QL utilizó el suavizado Jelinek-Mercer, con λ = 0.6.4.2 COLECCIONES DE PRUEBA Evaluamos el rendimiento de los cuatro sistemas IR descritos anteriormente en dos colecciones de prueba diferentes. Las dos colecciones de prueba utilizadas fueron la colección TREC AP89 (disco de Tipster 1) y la colección FSupp. La colección FSUPP es una colección patentada de 11,953 documentos de jurisprudencia para los cuales tenemos 44 consultas (que van de cuatro a veintidós palabras después de la eliminación de palabras de parada) con juicios de relevancia total.3 La longitud promedio de los documentos en la colección FSUPP es 3444 palabras 3444 palabras.2 www.lemuRproject.org 3 Cada uno de los 11,953 documentos fue evaluado por expertos en dominios con respecto a cada una de las 44 consultas. La colección de pruebas TREC AP89 contiene 84,678 documentos, con un promedio de 252 palabras de longitud. En nuestra evaluación, utilizamos tanto el título como los campos de descripción de los temas 151200 como consultas, por lo que tenemos dos conjuntos de resultados para la colección AP89. Después de la eliminación de palabras de parada, las consultas de título varían de dos a once palabras y las consultas de descripción varían de cuatro a veintiséis términos.4.3 Estrategia de eliminación de términos de consulta En nuestros experimentos, elegimos eliminar secuencial y aditivamente los términos de consulta de la frecuencia de documentos inversa (IDF) más alta a la más baja con respecto a toda la recopilación de documentos. Los términos con altos valores de IDF tienden a influir en la clasificación de documentos más que aquellos con valores de IDF más bajos. Además, los términos de IDF altos tienden a ser términos específicos de dominio que son menos propensos a ser conocidos por un usuario no experto, por lo tanto, comenzamos eliminándolos primero. Para la colección FSupp, las consultas se evaluaron de manera incremental con uno, dos, tres, cinco y siete términos eliminados de sus documentos relevantes correspondientes. Las consultas de descripción más largas de TREC TEMICS 151-200 también se evaluaron en la colección AP89 con uno, dos, tres, cinco y siete términos de consulta eliminados de sus documentos relevantes. Para las consultas de título de TREC más cortas, eliminamos uno, dos, tres y cinco términos de los documentos relevantes.4.4 Métricas En esta implementación del marco de evaluación, elegimos tres métricas para comparar el rendimiento del sistema IR: precisión promedio media (MAP), precisión en 10 documentos (P10) y recuperación en 1000 documentos. Aunque estas son las métricas que elegimos para demostrar este marco, cualquier métrica IR apropiada podría usarse dentro del marco.5. Los resultados 5.1 FSUPP COLECCIÓN Las Figuras 1, 2 y 3 muestran el rendimiento (en términos de MAP, P10 y retiro, respectivamente) para los cuatro motores de búsqueda en la colección FSUPP. Como se esperaba, el rendimiento de los sistemas IR de solo palabra clave, QL y OKAPI, cae rápidamente a medida que los términos de consulta se eliminan de los documentos relevantes en la colección. El rendimiento de Okapi con comentarios (Okapi FB) es algo sorprendente en el que en la colección original (es decir, antes de la eliminación del término de consulta), su rendimiento es peor que el de Okapi sin comentarios sobre las tres medidas. TCS supera la línea de base de palabras clave QL en cada medida, excepto el mapa en la colección original (es decir, antes de eliminar cualquier término de consulta). Debido a que TCS emplea la expansión de consultas implícitas utilizando una base de conocimiento específica de dominio externo, es menos sensible a la eliminación de términos (es decir, desajuste) que el OKAPI FB, que se basa en términos de los documentos mejor clasificados recuperados mediante una búsqueda de palabras clave inicial. Debido a que el rendimiento general del motor de búsqueda se mide con frecuencia en términos de MAP, y debido a que otras evaluaciones de QE a menudo solo consideran el rendimiento en toda la colección (es decir, no consideran la falta de coincidencia de término), el QE implementado en TCS se consideraría (en AN0 12 3 4 5 6 7 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 MediaPrecision (MAP) OKAPI FB OKAPI TCS QL FSUPP: Precisión promedio media con términos de consulta Figura 1: el rendimiento de los cuatroSistemas de recuperación en la colección FSupp en términos de precisión promedio media (MAP) y en función del número de términos de consulta eliminados (el eje horizontal). 0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de documentos relevantes 0 0.050.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 PrecisionAt10Documentos (P10) OKAPI FB OKAPI TCS QL FSUPP: P10 Con términos de consulta eliminado Figura 2: El rendimiento de los cuatro sistemas de recuperación en la colección FSUPP en términos de precisión en 10 y como unfunción del número de términos de consulta eliminados (el eje horizontal).Otra evaluación) para dañar el rendimiento en la colección FSupp. Sin embargo, cuando observamos la comparación de TCS con QL cuando los términos de consulta se eliminan de los documentos relevantes, podemos ver que el QE en TCS está contribuyendo positivamente a la búsqueda.5.2 La colección AP89: utilizando la descripción consultas Figuras 4, 5 y 6 muestran el rendimiento de los cuatro sistemas IR en la colección AP89, utilizando las descripciones de temas de TREC como consultas. La diferencia más interesante entre el rendimiento en la colección FSupp y la colección AP89 es la reversión de Okapi FB y TCS. En FSUPP, TCS superó a los otros motores de manera consistente (ver Figuras 1, 2 y 3);En la colección AP89, Okapi FB es claramente el mejor artista (ver Figuras 4, 5 y 6). Esto es aún más interesante, según el hecho de que QE en Okapi FB tiene lugar después de la primera iteración de búsqueda, que 0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de documentos relevantes 0 0.1 0.2 0.3 0.4 0.6 0.7 0.8 0.80.9 1 Recuerde el Okapi FB Okapi TCS Indri Fsupp: Recuerde a 1000 documentos con términos de consulta eliminado Figura 3: El retiro (a 1000) de los cuatro sistemas de recuperación en la colección FSupp en su función del número de términos de consulta eliminado (el eje horizontal).0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 Media VerageRecision (MAP) OKAPI FB OKAPI TCS QL AP89: Precisión promedio media con términos de consulta (Queras Descripción) Figura 4: MAPde los cuatro sistemas IR en la colección AP89, utilizando consultas de descripción TREC. El mapa se mide en función del número de términos de consulta eliminados.0 1 2 3 4 5 6 7 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 PrecisionAt10Documentos (P10) OKAPI FB OKAPI TCS QL AP89: P10 Con Términos de consulta eliminados (Descripción Consultas) Figura 5: Precisión: Precisióna 10 de los cuatro sistemas IR en la colección AP89, utilizando consultas de descripción de TREC. P en 10 se mide en función del número de términos de consulta eliminados.0 1 2 3 4 5 6 7 Number of Query Terms Removed from Relevant Documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Okapi FB Okapi TCS QL AP89: Recall at 1000 documents with Query Terms Removed (description queries) Figure 6: Recall (a 1000) de los cuatro sistemas IR en la colección AP89, utilizando consultas de descripción TREC y en función del número de términos de consulta eliminados.Esperamos ser discapacitados cuando se eliminen los términos de la consulta. Mirando P10 en la Figura 5, podemos ver que TCS y Okapi FB obtienen de manera similar en P10, comenzando en el punto donde se elimina un término de consulta de los documentos relevantes. En dos términos de consulta eliminados, TCS comienza a superar a Okapi FB. Si modela esto en términos de usuarios expertos versus no expertos, podríamos concluir que TCS podría ser un mejor motor de búsqueda para que los no expertos se utilicen en la colección AP89, mientras que Okapi FB sería mejor para un buscador de expertos. Es interesante observar que en cada métrica para las consultas de descripción AP89, TCS funciona más mal que todos los demás sistemas en la colección original, pero supera rápidamente los sistemas de referencia y se acerca al rendimiento de OKAPI FBS a medida que se eliminan los términos. Este es nuevamente un caso en el que el rendimiento de un sistema en toda la colección no es necesariamente indicativo de cómo maneja el desajuste del término del documento de consulta.5.3 La colección AP89: Uso de las consultas de título Figuras 7, 8 y 9 muestran el rendimiento de los cuatro sistemas IR en la colección AP89, utilizando los títulos de temas de TREC como consultas. Al igual que con las consultas de descripción AP89, Okapi FB es nuevamente el mejor desempeño de los cuatro sistemas en la evaluación. Como antes, el rendimiento de los sistemas OKAPI y QL, los sistemas de referencia que no son de QE, se degrada bruscamente a medida que se eliminan los términos de consulta. En las consultas más cortas, TCS parece tener más dificultades para ponerse al día con el rendimiento de Okapi FB a medida que se eliminan los términos. Quizás el resultado más interesante de nuestra evaluación es que, aunque las líneas de base de solo palabras clave funcionaban de manera consistente y como se esperaba en ambas colecciones con respecto a la eliminación de términos de consulta de documentos relevantes, el rendimiento de los motores que implementan técnicas QE diferían dramáticamente entre las colecciones.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 MediaSaverageRecision (MAP) OKAPI FB OKAPI TCS QL AP89: Precisión promedio media con términos de consulta (Título Queries) Figura 7: Mapa deLos cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.4 0.45 0.5 Precisión10Documentos (P10) OKAPI FB OKAPI TCS QL AP89: P10 Con Términos de consulta eliminados (Consultas de título) Figura 8: Precisión AT10 de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.0 1 2 3 4 5 Número de términos de consulta eliminados de los documentos relevantes 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recuerde Okapi FB Okapi TCS QL AP89: Recuerde a 1000 documentos con términos de consulta eliminados (consultas de título) Figura 9: Recuerdos (a 1000 a 1000) de los cuatro sistemas IR en la colección AP89, utilizando consultas de título TREC y en función del número de términos de consulta eliminados.6. Discusión La intuición detrás de este marco de evaluación es medir el grado en que varias técnicas de QE superan el desajuste de términos entre consultas y documentos relevantes. En general, es fácil evaluar el rendimiento general de diferentes técnicas para QE en comparación entre sí o contra una variante que no es de QE en cualquier colección de prueba completa. Tal enfoque nos dice qué sistemas funcionan mejor en una colección de prueba completa, pero no mide la capacidad de una técnica QE particular para recuperar documentos relevantes a pesar de la falta de coincidencia de término parcial o completo entre consultas y documentos relevantes. Una evaluación sistemática de los sistemas IR como se describe en este documento es útil no solo con respecto a la medición del éxito general o el fracaso de técnicas de QE particulares en presencia de desajuste del término de documento de consulta, sino que también proporciona información sobre cómo será un sistema IR particularRealice cuando los usuarios expertos versus no expertos en una colección en particular. Cuanto menos conoce el usuario sobre el dominio de la recopilación de documentos en la que están buscando, es probable que sea más prevaleciente desajuste del término del documento de consulta. Esta distinción es especialmente relevante en el caso de que la colección de pruebas es específica del dominio (es decir, médica o legal, en oposición a un dominio más general, como las noticias), donde la distinción entre expertos y no expertos puede ser más marcada. Por ejemplo, un no experto en el dominio médico podría buscar tos ferina, pero los documentos relevantes podrían contener el término médico. Dado que los términos de consulta están enmascarados solo en los documentos relevantes, este marco de evaluación está realmente sesgado contra la recuperación de documentos relevantes. Esto se debe a que los documentos no relevantes también pueden contener términos de consulta, lo que puede hacer que un sistema de recuperación clasifique dichos documentos más altos de lo que hubiera sido antes de que los términos se enmascararon en documentos relevantes. Aún así, creemos que este es un escenario más realista que eliminar los términos de todos los documentos, independientemente de la relevancia. El grado en que una técnica QE se adapta bien a una colección particular se puede evaluar en términos de su capacidad para aún encontrar los documentos relevantes, incluso cuando faltan términos de consulta, a pesar del sesgo de este enfoque contra los documentos relevantes. Sin embargo, dado que Okapi FB y TCS superaron entre sí en dos conjuntos de recolección diferentes, probablemente esté justificada una mayor investigación sobre el grado de compatibilidad entre el enfoque de expansión de QE y la recolección de objetivos. Además, la investigación de otras estrategias de eliminación de términos podría proporcionar información sobre el comportamiento de las diferentes técnicas QE y su impacto general en la experiencia del usuario. Como se mencionó anteriormente, nuestra elección de la estrategia de eliminación del término fue motivada por (1) nuestro deseo de ver el mayor impacto en el rendimiento del sistema a medida que se eliminan los términos y (2) porque los altos términos de las FDI, en nuestro contexto de dominio, tienen más probabilidades de serDominio específico, que nos permite comprender mejor el rendimiento de un sistema IR experimentado por usuarios expertos y no expertos. Aunque no se intenta en nuestros experimentos, otra aplicación de este marco de evaluación sería eliminar los términos de consulta individualmente, en lugar de incrementalmente, para analizar qué términos (o posiblemente qué tipos de términos) están siendo ayudados más por una técnica QE en una colección de pruebas particular.. Esto podría conducir a una idea de cuándo deberíamos y no debemos aplicarse. Este marco de evaluación nos permite ver cómo funcionan los sistemas IR en presencia de desajuste del término del documento de consulta. En otras evaluaciones, el rendimiento de un sistema se mide solo en toda la colección, en la que no se conoce el grado de desajuste de documentos de consulta a plazo. Al introducir sistemáticamente este desajuste, podemos ver que incluso si un sistema IR no es el mejor desempeño en toda la colección, su rendimiento puede ser más robusto para el desajuste de términos de documentos de consulta que otros sistemas. Tal robustez hace que un sistema sea más fácil de usar, especialmente para usuarios no expertos. Este artículo presenta un marco novedoso para la evaluación del sistema IR, cuyas aplicaciones son numerosas. Los resultados presentados en este documento no están destinados a ser exhaustivos o completamente representativos de las formas en que se podría aplicar esta evaluación. Sin duda, hay mucho trabajo futuro que podría hacerse utilizando este marco. Además de observar el rendimiento promedio de los sistemas IR, los resultados de las consultas individuales podrían examinarse y compararse más de cerca, tal vez dando más información sobre la clasificación y la predicción de consultas difíciles, o tal vez mostrando qué técnicas QE mejoran (o degradan) la consulta individualrendimiento bajo diferentes grados de coincidencia de término de consultas. De hecho, este marco también se beneficiaría de más pruebas en una colección más grande.7. Conclusión El marco de evaluación propuesto nos permite medir el grado en que los diferentes sistemas de IR superan (o no superan) el malhechor del término entre consultas y documentos relevantes. Las evaluaciones de los sistemas IR que emplean QE realizados solo en toda la colección no tienen en cuenta que el propósito de QE es mitigar los efectos del desajuste a término en la recuperación. Al eliminar sistemáticamente los términos de consulta de los documentos relevantes, podemos medir el grado en que QE contribuye a una búsqueda al mostrar la diferencia entre el rendimiento de un sistema QE y su línea de base de palabras clave cuando los términos de consulta se han eliminado de documentos relevantes conocidos. Además, podemos modelar el comportamiento de usuarios expertos versus no expertos manipulando la cantidad de desajuste de término de documento de consulta introducido en la colección. El marco de evaluación propuesto en este documento es atractivo por varias razones. Lo más importante es que proporciona una manera controlada para medir el rendimiento de QE con respecto al desajuste del término del documento de consulta. Además, este marco aprovecha y estira la cantidad de información que podemos obtener de las colecciones de pruebas existentes. Además, este marco de evaluación no es específico de la métrica: se puede obtener información en términos de cualquier métrica (mapa, p@10, etc.) para evaluar un sistema IR de esta manera. También se debe tener en cuenta que este marco es generalizable para cualquier sistema IR, ya que evalúa qué tan bien los sistemas IR evalúan las necesidades de información de los usuarios como lo representan sus consultas. Un sistema IR que es fácil de usar debe ser bueno para recuperar documentos que sean relevantes para las necesidades de información de los usuarios, incluso si las consultas proporcionadas por los usuarios no contienen las mismas palabras clave que los documentos relevantes.8. Referencias [1] Amati, G., C. Carpineto y G. Romano. Dificultad de consulta, robustez y aplicación selectiva de la expansión de consultas. En Actas de la 25ª Conferencia Europea sobre Recuperación de Información (ECIR 2004), pp. 127-137.[2] Berger, A. y J.D. Lafferty.1999. Recuperación de información como traducción estadística. En investigación y desarrollo en recuperación de información, páginas 222-229.[3] Billerbeck, B., F. Scholer, H. E. Williams y J. Zobel.2003. Expansión de consulta utilizando consultas asociadas. En Actas de CIKM 2003, pp. 2-9.[4] Billerbeck, B. y J. Zobel.2003. Cuando la expansión de la consulta falla. En Actas de Sigir 2003, pp. 387-388.[5] Billerbeck, B. y J. Zobel.2004. Expansión de consultas de consulta: un examen del comportamiento y los parámetros. En Actas de la 15ª Conferencia de la Base de datos de Australasia (ADC2004), pp. 69-76.[6] Billerbeck, B. y J. Zobel.2005. Expansión del documento versus expansión de consulta para la recuperación ad-hoc. En Actas del Décimo Simposio de Computación de Documentos de Australia.[7] Buckley, C. y E.M. Voorhees.2000. Evaluación de la estabilidad de la medida de evaluación. En Actas de Sigir 2000, pp. 33-40.[8] Buckley, C. y E.M. Voorhees.2004. Evaluación de recuperación con información incompleta. En Actas de Sigir 2004, pp. 25-32.[9] Carmel, D., E. Yom-Tov, A. Darlow, D. Pelleg.2006. ¿Qué hace que una consulta sea difícil? En Actas de Sigir 2006, pp. 390-397.[10] Carpineto, C., R. Mori y G. Romano.1998. Selección de término informativo para la expansión automática de consultas. En la séptima conferencia de recuperación de texto, pp.363: 369.[11] Carterette, B. y J. Allan.2005. Colecciones de pruebas incrementales. En Actas de CIKM 2005, pp. 680-687.[12] Carterette, B., J. Allan y R. Sitaraman.2006. Colecciones de pruebas mínimas para la evaluación de recuperación. En Actas de Sigir 2006, pp. 268-275.[13] Cormack, G.V., C. R. Palmer y C.L. Clarke.1998. Construcción eficiente de grandes colecciones de pruebas. En Actas de Sigir 1998, pp. 282-289.[14] Cormack, G. y T.R. Lynam.2006. Precisión estadística de la evaluación de recuperación de información. En Actas de Sigir 2006, pp. 533-540.[15] Cronen-Townsend, S., Y. Zhou y W.B. Granja pequeña.2004. Un marco de modelado de idiomas para la expansión de consultas selectivas, informe técnico de CIIR.[16] Efthimiadis, E.N. Expansión de consulta.1996. En Martha E. Williams (ed.), Revisión anual de Sistemas de Información y Tecnología (ARIST), V31, pp 121-187. [17] Evans, D.A.y Lefferts, R.G.1995. Experimentos de Clarit-TREC. Procesamiento y gestión de la información.31 (3): 385-295.[18] Fang, H. y C.X. Zhai.2006. Matriota de término semántico en enfoques axiomáticos para la recuperación de información. En Actas de Sigir 2006, pp. 115-122.[19] Gao, J., J. Nie, G. Wu y G. Cao.2004. Modelo de lenguaje de dependencia para la recuperación de información. En Actas de Sigir 2004, pp. 170-177.[20] Harman, D.K.1992. Comentarios de relevancia revisitado. En Actas de ACM Sigir 1992, pp. 1-10.[21] Harman, D.K., ed.1993. La primera conferencia de recuperación de texto (TREC-1): 1992. [22] Harman, D.K., ed.1994. La segunda conferencia de recuperación de texto (TREC-2): 1993. [23] Harman, D.K., ed.1995. La tercera conferencia de recuperación de texto (TREC-3): 1994. [24] Harman, D.K., 1998. Hacia la expansión de consultas interactivas. En Actas de Sigir 1998, pp. 321-331.[25] Hofmann, T. 1999. Indexación semántica latente probabilística. En Actas de Sigir 1999, pp 50-57.[26] Jing, Y. y W.B. Granja pequeña.1994. El Tesauro de la Asociación para la recuperación de información. En Actas de Riao 1994, pp. 146-160 [27] Lu, X.A.y R.B. Keefer. Expansión/reducción de consulta y su impacto en la efectividad de la recuperación. En: D.K. Harman, ed. La tercera conferencia de recuperación de texto (TREC-3). Gaithersburg, MD: Instituto Nacional de Normas y Tecnología, 1995,231-239.[28] McNamee, P. y J. Mayfield.2002. Comparación de técnicas de expansión de consultas de consulta cruzada mediante la degradación de los recursos de traducción. En Actas de Sigir 2002, pp. 159-166.[29] Mitra, M., A. Singhal y C. Buckley.1998. Mejora de la expansión automática de consultas. En Actas de Sigir 1998, pp. 206-214.[30] Peat, H. J. y P. Willett.1991. Las limitaciones de los datos de concurrencia de término para la expansión de la consulta en los sistemas de recuperación de documentos. Journal of the American Society for Information Science, 42 (5): 378-383.[31] Ponte, J.M. y W.B. Granja pequeña.1998. Un enfoque de modelado de idiomas para la recuperación de información. En Actas de Sigir 1998, pp.275-281.[32] Qiu Y. y Frei H. 1993. Expansión de consultas basada en el concepto. En Actas de Sigir 1993, pp. 160-169.[33] Robertson, S. 2006. En GMAP y otras transformaciones. En Actas de CIKM 2006, pp. 78-83.[34] Robertson, S.E.y K. Sparck Jones.1976. Ponderación de relevancia de los términos de búsqueda. Journal of the American Society for Information Science, 27 (3): 129-146.[35] Robertson, S.E., S. Walker, S. Jones, M.M. Hancock-Beaulieu y M. Gatford.1994. Okapi en TREC-2. En D.K. Harman (ed).1994. La segunda conferencia de recuperación de texto (TREC-2): 1993, pp. 21-34.[36] Robertson, S.E., S. Walker, S. Jones, M.M. Hancock-Beaulieu y M. Gatford.1995. Okapi en TREC-3. En D.K. Harman (ed).1995. La tercera conferencia de recuperación de texto (TREC-2): 1993, pp. 109-126 [37] Rocchio, J.J.1971. Comentarios de relevancia en la recuperación de información. En G. Salton (ed.), El sistema de recuperación inteligente. Prentice-Hall, Inc., Englewood Cliffs, NJ, pp. 313-323.[38] Salton, G. 1968. Organización y recuperación de información automática. McGraw-Hill.[39] Salton, G. 1971. El sistema de recuperación inteligente: experimentos en el procesamiento automático de documentos. Englewood Cliffs NJ;Prentice Hall.[40] Salton, g.1980. Constructor de clase de término automático utilizando Relevance-A Resumen del trabajo en la clasificación de pseudo automático. Procesamiento y gestión de la información.16 (1): 1-15.[41] Salton, G. y C. Buckley.1988. Sobre el uso de métodos de activación de propagación en la recuperación de información automática. En Actas de Sigir 1998, pp. 147-160.[42] Sanderson, M. 1994. La desambiguación del sentido de la palabra y la recuperación de la información. En Actas de Sigir 1994, pp. 161-175.[43] Sanderson, M. y H. Joho.2004. Formando colecciones de pruebas sin agrupación de sistemas. En Actas de Sigir 2004, pp. 186-193.[44] Sanderson, M. y Zobel, J. 2005. Evaluación del sistema de recuperación de información: esfuerzo, sensibilidad y confiabilidad. En Actas de Sigir 2005, pp. 162-169.[45] Smeaton, A.F. y C.J. Van Rijsbergen.1983. Los efectos de recuperación de la expansión de la consulta en un sistema de recuperación de documentos de retroalimentación. Revista de computadora.26 (3): 239-246.[46] Song, F. y W.B. Granja pequeña.1999. Un modelo de idioma general para la recuperación de información. En Actas de la Octava Conferencia Internacional sobre Gestión de Información y Conocimiento, páginas 316-321.[47] Sparck Jones, K. 1971. Clasificación automática de palabras clave para recuperación de información. Londres: Butterworths.[48] Terra, E. y C. L. Clarke.2004. Puntuación de términos faltantes en tareas de recuperación de información. En Actas de CIKM 2004, pp. 50-58.[49] Tortuga, Howard.1994. Lenguaje natural vs. Evaluación de consultas booleanas: una comparación del rendimiento de la recuperación. En Actas de Sigir 1994, pp. 212-220.[50] Voorhees, E.M. 1994a. Al expandir los vectores de consulta con palabras relacionadas con léxicamente. En Harman, D. K., ed. Conferencia de recuperación de texto (TREC-1): 1992. [51] Voorhees, E.M. 1994b. Expansión de consulta utilizando relaciones léxicas semánticas. En Actas de Sigir 1994, pp. 61-69.