Descarga de contenido web oculto textual a través de consultas de palabras clave Alexandros ntoulas UCLA Computer Science ntoulas@cs.ucla.edu Petros Zerfos UCLA Computer Science pzerfos@cs.ucla.edu junghoo cho ucla informática científica cho@cla.ucla.edu Resumen una cantidad crecienteLa información en la web hoy está disponible solo a través de interfaces de búsqueda: los usuarios deben escribir un conjunto de palabras clave en un formulario de búsqueda para acceder a las páginas desde ciertos sitios web. Estas páginas a menudo se conocen como la web oculta o la web profunda. Dado que no hay enlaces estáticos a las páginas web ocultas, los motores de búsqueda no pueden descubrir e indexar tales páginas y, por lo tanto, no los devuelven en los resultados. Sin embargo, según estudios recientes, el contenido proporcionado por muchos sitios web ocultos a menudo es de muy alta calidad y puede ser extremadamente valioso para muchos usuarios. En este documento, estudiamos cómo podemos crear un rastreador web oculto efectivo que pueda descubrir y descargar páginas de forma autónoma de la web oculta. Dado que el único punto de entrada a un sitio web oculto es una interfaz de consulta, el principal desafío que tiene que enfrentar un rastreador web oculto es cómo generar automáticamente consultas significativas para emitir en el sitio. Aquí, proporcionamos un marco teórico para investigar el problema de generación de consultas para la web oculta y proponemos políticas efectivas para generar consultas automáticamente. Nuestras políticas proceden iterativamente, emitiendo una consulta diferente en cada iteración. Evaluamos experimentalmente la efectividad de estas políticas en 4 sitios web ocultos reales y nuestros resultados son muy prometedores. Por ejemplo, en un experimento, una de nuestras políticas descargó más del 90% de un sitio web oculto (que contiene 14 millones de documentos) después de emitir menos de 100 consultas. Categorías y descriptores de asignaturas: H.3.7 [Sistemas de información]: Bibliotecas digitales;H.3.1 [Sistemas de información]: análisis e indexación de contenido;H.3.3 [Sistemas de información]: Búsqueda y recuperación de información. Términos generales: algoritmos, rendimiento, diseño.1. Introducción Estudios recientes muestran que no se puede alcanzar una fracción significativa del contenido web siguiendo los enlaces [7, 12]. En particular, una gran parte de la web está oculta detrás de los formularios de búsqueda y solo se puede accesible cuando los usuarios escriben un conjunto de palabras clave o consultas a los formularios. Estas páginas a menudo se denominan la web oculta [17] o la web profunda [7], porque los motores de búsqueda generalmente no pueden indexar las páginas y no devolverlas en sus resultados (por lo tanto, las páginas están esencialmente ocultas de un usuario web típico). Según muchos estudios, el tamaño de la web oculta aumenta rápidamente a medida que más organizaciones ponen su valioso contenido en línea a través de una interfaz web fácil de usar [7]. En [12], Chang et al.Estima que actualmente existen más de 100,000 sitios de Widden-Web en la web. Además, el contenido proporcionado por muchos sitios de Widden-Web a menudo es de muy alta calidad y puede ser extremadamente valioso para muchos usuarios [7]. Por ejemplo, PubMed alberga muchos documentos de alta calidad en investigaciones médicas que fueron seleccionadas de cuidadosos procesos de revisión por pares, mientras que el sitio de la Oficina de Patentes y Marcas de los Estados Unidos hace que los documentos de patentes existentes estén disponibles, ayudando a los posibles inventores a examinar la arte anterior. En este documento, estudiamos cómo podemos construir un Crawler2 oculto de WEB que puede descargar automáticamente páginas de la web oculta, para que los motores de búsqueda puedan indexarlas. Los rastreadores convencionales dependen de los hipervínculos en la web para descubrir páginas, por lo que los motores de búsqueda actuales no pueden indexar las páginas de Widden-Web (debido a la falta de enlaces). Creemos que un rastreador efectivo de Widden-Web puede tener un tremendo impacto en cómo los usuarios buscan información en la web: • aprovechando la información inexplorada: el rastreador de Widden-Web permitirá que un usuario web promedio explore fácilmente la gran cantidad de información que essobre todo oculto en la actualidad. Dado que la mayoría de los usuarios de la web confían en los motores de búsqueda para descubrir páginas, cuando los motores de búsqueda no están indexadas en las páginas, es poco probable que muchos usuarios web las vean. A menos que los usuarios vayan directamente a los sitios de Widden-Web y emitan consultas allí, no pueden acceder a las páginas en los sitios.• Mejora de la experiencia del usuario: incluso si un usuario conoce una serie de sitios de WEB Hidden-Web, el usuario aún tiene que perder una cantidad significativa de tiempo y esfuerzo, visitando todos los sitios potencialmente relevantes, consultando cada uno de ellos y explorando el resultado. Al hacer las páginas de Widden-Web que se pueden buscar en una ubicación central, podemos reducir significativamente los usuarios desperdiciados el tiempo y el esfuerzo en la búsqueda de la web oculta.• Reducción del sesgo potencial: debido a la gran dependencia de muchos usuarios web en los motores de búsqueda para localizar información, los motores de búsqueda influyen en cómo los usuarios perciben la web [28]. Los usuarios no necesariamente perciben lo que realmente existe en la web, sino lo que los motores de búsqueda indexan [28]. Según un artículo reciente [5], varias organizaciones han reconocido la importancia de traer información de sus sitios web ocultos a la superficie y cometieron recursos considerables para este esfuerzo. Nuestra Oficina de Patentes de 1 EE. UU.: Http://www.uspto.gov 2 rastreadores son los programas que atraviesan la web automáticamente y descargan páginas para los motores de búsqueda.100 Figura 1: Una interfaz de búsqueda de un solo atributo Hidden-Web Crawler intenta automatizar este proceso para sitios web ocultos con contenido textual, minimizando así los costos y el esfuerzo asociados requeridos. Dado que la única entrada a las páginas web ocultas es consultar un formulario de búsqueda, hay dos desafíos básicos para implementar un rastreador web oculto efectivo: (a) El rastreador debe ser capaz de comprender y modelar una interfaz de consulta, y (b)El rastreador tiene que presentar consultas significativas para emitir la interfaz de consulta. Raghavan y Garcia-Molina abordaron el primer desafío en [29], donde se presentó un método para aprender interfaces de búsqueda. Aquí, presentamos una solución al segundo desafío, es decir, cómo un rastreador puede generar automáticamente consultas para que pueda descubrir y descargar las páginas web ocultas. Claramente, cuando los formularios de búsqueda enumeran todos los valores posibles para una consulta (por ejemplo, a través de una lista desplegable), la solución es sencilla. Examinablemente emitimos todas las consultas posibles, una consulta a la vez. Sin embargo, cuando los formularios de consulta tienen una entrada de texto libre, es posible un número infinito de consultas, por lo que no podemos emitir exhaustivamente todas las consultas posibles. En este caso, ¿qué consultas debemos elegir? ¿Puede el rastreador encontrar automáticamente consultas significativas sin comprender la semántica del formulario de búsqueda? En este artículo, proporcionamos un marco teórico para investigar el problema de rastreo de Widden-Web y proponer formas efectivas de generar consultas automáticamente. También evaluamos nuestras soluciones propuestas a través de experimentos realizados en sitios reales de Widden-Web. En resumen, este documento hace las siguientes contribuciones: • Presentamos un marco formal para estudiar el problema del rastreo oculto.(Sección 2).• Investigamos una serie de políticas de rastreo para la web oculta, incluida la política óptima que puede descargar el número máximo de páginas a través del número mínimo de interacciones. Desafortunadamente, mostramos que la política óptima es NP-Hard y no se puede implementar en la práctica (Sección 2.2).• Proponemos una nueva política adaptativa que se aproxima a la política óptima. Nuestra política adaptativa examina las páginas devueltas de consultas anteriores y adapta su política de selección de consultas automáticamente en función de ellas (Sección 3).• Evaluamos varias políticas de rastreo a través de experimentos en sitios web reales. Nuestros experimentos mostrarán las ventajas relativas de varias políticas de rastreo y demostrarán su potencial. Los resultados de nuestros experimentos son muy prometedores. En un experimento, por ejemplo, nuestra política adaptativa descargó más del 90% de las páginas dentro de PubMed (que contiene 14 millones de documentos) después de emitir menos de 100 consultas.2. Marco En esta sección, presentamos un marco formal para el estudio del problema de rastreo de Widden-Web. En la Sección 2.1, describimos nuestras suposiciones en los sitios de Widden-Web y explicamos cómo los usuarios interactúan con los sitios. Según este modelo de interacción, presentamos un algoritmo de alto nivel para un rastreador de WEB oculto en la Sección 2.2. Finalmente en la Sección 2.3, formalizamos el problema de rastreo de Widden-Web.2.1 Modelo de base de datos Hidden-Web Existe una variedad de fuentes web ocultas que proporcionan información sobre una multitud de temas. Dependiendo del tipo de información, podemos clasificar un sitio de Widden-Web como una base de datos textual o una base de datos estructurada. Una base de datos textual es un sitio que Figura 2: una interfaz de búsqueda de atributos múltiples contiene principalmente documentos de texto sencillo, como PubMed y Lexisnexis (una base de datos en línea de documentos legales [1]). Dado que los documentos de texto sin formato no suelen tener una estructura bien definida, la mayoría de las bases de datos textuales proporcionan una interfaz de búsqueda simple donde los usuarios escriben una lista de palabras clave en un solo cuadro de búsqueda (Figura 1). Por el contrario, una base de datos estructurada a menudo contiene datos relacionales multi-atributos (por ejemplo, un libro en el sitio web de Amazon puede tener el título de los campos = Harry Potter, autor = J.K. Rowling e ISBN = 0590353403) y admite interfaces de búsqueda de múltiples atributos (Figura 2). En este documento, nos centraremos principalmente en bases de datos textuales que admiten consultas de palabras clave de atribución única. Discutimos cómo podemos extender nuestras ideas para las bases de datos textuales a bases de datos estructuradas de atribución múltiple en la Sección 6.1. Por lo general, los usuarios deben tomar los siguientes pasos para acceder a las páginas en una base de datos de Widden-Web: 1. Paso 1. Primero, el usuario emite una consulta, por ejemplo, el hígado, a través de la interfaz de búsqueda proporcionada por el sitio web (como el que se muestra en la Figura 1).2. Paso 2. Poco después de que el usuario emite la consulta, se le presenta una página de índice de resultados. Es decir, el sitio web devuelve una lista de enlaces a páginas web potencialmente relevantes, como se muestra en la Figura 3 (a).3. Paso 3. Desde la lista en la página del índice de resultados, el usuario identifica las páginas que parecen interesantes y siguen los enlaces. Al hacer clic en un enlace, lleva al usuario a la página web real, como la que se muestra en la Figura 3 (b), que el usuario desea ver.2.2 Un algoritmo genérico de rastreo web oculto dado que la única entrada a las páginas en un sitio de Widden-Web es su búsqueda, un rastreador de Widden-Web debe seguir los tres pasos descritos en la sección anterior. Es decir, el rastreador tiene que generar una consulta, emitirla en el sitio web, descargar la página de índice de resultados y seguir los enlaces para descargar las páginas reales. En la mayoría de los casos, un rastreador tiene tiempo limitado y recursos de red, por lo que el rastreador repite estos pasos hasta que utiliza sus recursos. En la Figura 4 mostramos el algoritmo genérico para un rastreador de Widden-Web. Para simplificar, suponemos que el rastreador oculto de WEB emite consultas solo a un solo término.3 El rastreador primero decide qué término de consulta va a usar (paso (2)), emite la consulta y recupera la página del índice de resultados (paso (paso(3)). Finalmente, según los enlaces que se encuentran en la página del índice de resultados, descarga las páginas web ocultas del sitio (paso (4)). Este mismo proceso se repite hasta que se usan todos los recursos disponibles (paso (1)). Dado este algoritmo, podemos ver que la decisión más crítica que un rastreador tiene que tomar es qué consulta emitir a continuación. Si el rastreador puede emitir consultas exitosas que devolverán muchas páginas coincidentes, el rastreador puede terminar su rastreo desde el principio utilizando recursos mínimos. Por el contrario, si el rastreador emite consultas completamente irrelevantes que no devuelven ninguna página coincidente, puede desperdiciar todos sus recursos simplemente emitiendo consultas sin recuperar páginas reales. Por lo tanto, cómo el rastreador selecciona la siguiente consulta puede afectar en gran medida su efectividad. En la siguiente sección, formalizamos este problema de selección de consultas.3 Para la mayoría de los sitios web que suponen y para consultas de palabras múltiples, las consultas de un solo término devuelven el número máximo de resultados. Extender nuestro trabajo a consultas de palabras múltiples es sencillo.101 (a) Lista de páginas coincidentes para el hígado de consulta.(b) La primera página coincidente para el hígado. Figura 3: Páginas del sitio web de PubMed. Algoritmo 2.1. Rastreando un procedimiento de sitio web oculto (1) mientras (hay recursos disponibles) Do // Seleccione un término para enviar al sitio (2) qi = selectterm () // Enviar consultas y adquirir la página del índice de resultados (3) R (Qi) = QueryWebSite (Qi) // Descargar las páginas de interés (4) Descargar (R (Qi)) (5) Hecho Figura 4: Algoritmo para rastrear un sitio web oculto. S Q1 Q QQ 2 34 Figura 5: Una formalización del conjunto del problema óptimo de selección de consultas.2.3 Formalización de problemas Teóricamente, el problema de la selección de consultas se puede formalizar de la siguiente manera: suponemos que el rastreador descarga páginas de un sitio web que tiene un conjunto de páginas (el rectángulo en la Figura 5). Representamos cada página web en S como punto (puntos en la Figura 5). Cada Qi de consulta potencial que podamos emitir se puede ver como un subconjunto de S, que contiene todos los puntos (páginas) que se devuelven cuando emitimos Qi al sitio. Cada subconjunto está asociado con un peso que representa el costo de emitir la consulta. Bajo esta formalización, nuestro objetivo es encontrar qué subconjuntos (consultas) cubren el número máximo de puntos (páginas web) con el peso total mínimo (costo). Este problema es equivalente al problema de cobertura de conjunto en la teoría de gráficos [16]. Hay dos dificultades principales que debemos abordar en esta formalización. Primero, en una situación práctica, el rastreador no sabe qué páginas web serán devueltas por qué consultas, por lo que los subconjuntos de S no se conocen de antemano. Sin conocer estos subconjuntos, el rastreador no puede decidir qué consultas elegir para maximizar la cobertura. En segundo lugar, se sabe que el problema de cobertura del conjunto es NP-Hard [16], por lo que aún no se ha encontrado un algoritmo eficiente para resolver este problema de manera óptima en el tiempo polinomial. En este artículo, presentaremos un algoritmo de aproximación que puede encontrar una solución casi óptima a un costo computacional razonable. Nuestro algoritmo aprovecha la observación de que aunque no sabemos qué páginas serán devueltas por cada Qi de consulta que emitamos, podemos predecir cuántas páginas se devolverán. Según esta información, nuestro algoritmo de selección de consultas puede seleccionar las mejores consultas que cubren el contenido del sitio web. Presentamos nuestro método de predicción y nuestro algoritmo de selección de consultas en la Sección 3. 2.3.1 Métrica de rendimiento antes de presentar nuestras ideas para el problema de selección de consultas, discutimos brevemente parte de nuestra notación y las métricas de costo/rendimiento. Dada una consulta Qi, usamos P (Qi) para denotar la fracción de páginas que recuperaremos si emitimos Qi de consulta al sitio. Por ejemplo, si un sitio web tiene 10,000 páginas en total, y si se devuelven 3.000 páginas para la consulta qi = medicamento, entonces p (qi) = 0.3. Usamos P (Q1 ∧ Q2) para representar la fracción de páginas que se devuelven tanto de Q1 como Q2 (es decir, la intersección de P (Q1) y P (Q2)). Del mismo modo, usamos P (Q1 ∨ Q2) para representar la fracción de páginas que se devuelven de Q1 o Q2 (es decir, la unión de P (Q1) y P (Q2)). También utilizamos Costo (Qi) para representar el costo de emitir la consulta Qi. Dependiendo del escenario, el costo se puede medir en el tiempo, el ancho de banda de la red, el número de interacciones con el sitio, o puede ser una función de todos estos. Como veremos más adelante, nuestros algoritmos propuestos son independientes de la función de costo exacto. En el caso más común, el costo de la consulta consta de una serie de factores, incluido el costo de enviar la consulta al sitio, recuperar la página del índice de resultados (Figura 3 (a)) y descargar las páginas reales (Figura 3 (b)). Suponemos que enviar una consulta incurre en un costo fijo de CQ. El costo para descargar la página del índice de resultados es proporcional al número de documentos coincidentes con la consulta, mientras que el CD de costo para descargar un documento coincidente también se soluciona. Entonces el costo total de la consulta Qi es el costo (Qi) = CQ + CRP (Qi) + CDP (Qi).(1) En ciertos casos, algunos de los documentos de Qi ya pueden haberse descargado de consultas anteriores. En este caso, el rastreador puede omitir la descarga de estos documentos y el costo de Qi puede ser costo (Qi) = CQ + CRP (Qi) + CDPNew (Qi).(2) Aquí, usamos PNEW (Qi) para representar la fracción de los nuevos documentos de Qi que no se han recuperado de consultas anteriores. Más adelante en la Sección 3.1 estudiaremos cómo podemos estimar P (Qi) y PNEW (Qi) para estimar el costo de Qi. Dado que nuestros algoritmos son independientes de la función de costo exacto, asumiremos un costo de función de costo genérico (QI) en este documento. Sin embargo, cuando necesitamos una función de costo concreto, usaremos la Ecuación 2. Dada la notación, podemos formalizar el objetivo de un rastreador de Widden-Web de la siguiente manera: 102 Problema 1. Encuentre el conjunto de consultas Q1 ,..., Qn que maximiza P (Q1 ∨ · · · ∨ Qn) bajo la restricción n i = 1 costo (qi) ≤ t.Aquí, T es el recurso de descarga máximo que tiene el rastreador.3. Selección de palabras clave ¿Cómo debe un rastreador seleccionar las consultas para emitir? Dado que el objetivo es descargar el número máximo de documentos únicos de una base de datos textual, podemos considerar una de las siguientes opciones: • aleatorio: seleccionamos palabras clave aleatorias de, por ejemplo, un diccionario en inglés y emitirlas en la base de datos. La esperanza es que una consulta aleatoria devuelva un número razonable de documentos coincidentes.• Frequencia genérica: analizamos un corpus de documentos genérico recopilado en otro lugar (por ejemplo, de la web) y obtenemos la distribución de frecuencia genérica de cada palabra clave. Según esta distribución genérica, comenzamos con la palabra clave más frecuente, emitirla a la base de datos Hidden-WEB y recuperar el resultado. Luego continuamos con la segunda palabra clave más frecuente y repitemos este proceso hasta que agotamos todos los recursos de descarga. La esperanza es que las palabras clave frecuentes en un corpus genérico también sean frecuentes en la base de datos de Widden-Web, devolviendo muchos documentos coincidentes.• Adaptativo: analizamos los documentos devueltos de las consultas anteriores emitidas a la base de datos Hidden-WEB y estimamos qué palabras clave es más probable que devuelva la mayoría de los documentos. Según este análisis, emitimos la consulta más prometedora y repetimos el proceso. Entre estas tres políticas generales, podemos considerar la política aleatoria como el punto de comparación base, ya que se espera que funcione lo peor. Entre la frecuencia genérica y las políticas adaptativas, ambas políticas pueden mostrar un rendimiento similar si la base de datos rastreada tiene una recopilación de documentos genéricos sin un tema especializado. Sin embargo, la política adaptativa puede funcionar significativamente mejor que la política de frecuencia genérica si la base de datos tiene una colección muy especializada que es diferente del corpus genérico. Compararemos experimentalmente estas tres políticas en la Sección 4. Si bien las dos primeras políticas (políticas aleatorias y de frecuencia genérica) son fáciles de implementar, necesitamos comprender cómo podemos analizar las páginas descargadas para identificar la consulta más prometedora para implementar la política adaptativa. Abordamos este problema en el resto de esta sección.3.1 Estimación del número de páginas coincidentes Para identificar la consulta más prometedora, debemos estimar cuántos documentos nuevos descargaremos si emitimos la consulta Qi como la siguiente consulta. Es decir, suponiendo que hemos emitido consultas Q1 ,..., Qi - 1 necesitamos estimar P (Q1∨ · · · ∨qi - 1∨qi), para cada posible consulta de la siguiente consulta Qi y comparar este valor. Al estimar este número, observamos que podemos reescribir P (Q1 ∨ · · · ∨ Qi - 1 ∨ Qi) AS: P ((Q1 ∨ · · · ∨ Qi - 1) ∨ Qi) = P (Q1 ∨ · ·· ∨ Qi - 1) + P (Qi) - P ((Q1 ∨ · · · · ∨ Qi - 1) ∧ Qi) = P (Q1 ∨ · · · ∨ Qi - 1) + P (Qi) - P (Q1∨ · · · ∨ Qi - 1) P (Qi | Q1 ∨ · · · ∨ Qi - 1) (3) En la fórmula anterior, tenga en cuenta que podemos medir con precisión P (Q1 ∨ · · · ∨ Qi - 1) yP (Qi | Q1 ∨ · · · ∨ Qi - 1) Analizando páginas de desactividad previamente cargadas: sabemos P (Q1 ∨ · · · ∨ Qi - 1), la unión de todas las páginas descargadas de Q1 ,..., Qi - 1, ya que ya hemos emitido Q1 ,..., Qi - 1 y descargamos las páginas coincidentes.4 También podemos medir P (Qi | Q1 ∨ · · · ∨ Qi - 1), la probabilidad de que Qi aparezca en las páginas de Q1 ,..., Qi - 1, contando cuántas veces aparece Qi en las páginas de Q1 ,..., Qi - 1. Por lo tanto, solo necesitamos estimar P (Qi) para evaluar P (Q1 ∨ · · · ∨ Qi). Podemos considerar varias formas diferentes de estimar P (Qi), incluida la siguiente: 1. Estimador de independencia: Suponemos que la aparición del término Qi es independiente de los términos Q1 ,..., Qi - 1. Es decir, suponemos que p (qi) = p (qi | q1 ∨ · · · ∨ qi - 1).2. Estimador ZIPF: en [19], Ipeirotis et al.propuso un método para estimar cuántas veces se produce un término particular en todo el corpus basado en un subconjunto de documentos del corpus. Su método explota el hecho de que la frecuencia de los términos dentro de las colecciones de texto sigue una distribución de la ley de potencia [30, 25]. Es decir, si clasificamos todos los términos en función de su frecuencia de ocurrencia (con el término más frecuente que tiene un rango de 1, segundo más frecuente un rango de 2, etc.), entonces la frecuencia f de un término dentro de la colección de texto viene dada por: F = α (R + β) −γ (4) donde R es el rango del término y α, β y γ son constantes que dependen de la colección de texto. Su idea principal es (1) estimar los tres parámetros, α, β y γ, en función del subconjunto de documentos que hemos descargado de consultas anteriores, y (2) usar los parámetros estimados para predecir f dada la clasificación R de A A A A Atérmino dentro del subconjunto. Para una descripción más detallada de cómo podemos usar este método para estimar P (Qi), remitimos al lector a la versión extendida de este documento [27]. Después de estimar los valores P (Qi) y P (Qi | Q1 ∨ · · · · ∨ Qi - 1), podemos calcular P (Q1 ∨ · · · · ∨ Qi). En la Sección 3.3, explicamos cómo podemos calcular eficientemente P (Qi | Q1 ∨ · · · ∨ Qi - 1) manteniendo una tabla de resumen sucinta. En la siguiente sección, primero examinamos cómo podemos usar este valor para decidir qué consulta debemos emitir junto al sitio web oculto.3.2 Algoritmo de selección de consultas El objetivo del rastreador de Widden-Web es descargar el número máximo de documentos únicos de una base de datos utilizando sus recursos de descarga limitados. Dado este objetivo, el rastreador de Widden-Web tiene que tener en cuenta dos factores.(1) El número de nuevos documentos que se pueden obtener de la consulta Qi y (2) el costo de emitir la consulta Qi. Por ejemplo, si dos consultas, Qi y QJ, incurren en el mismo costo, pero Qi devuelve más páginas nuevas que QJ, Qi es más deseable que QJ. Del mismo modo, si Qi y QJ devuelven el mismo número de documentos nuevos, pero Qi incurre en menos costo que QJ, Qi es más deseable. Según esta observación, el rastreador de Widden-Web puede usar la siguiente métrica de eficiencia para cuantificar la conveniencia de la consulta Qi: eficiencia (Qi) = PNEW (Qi) Costo (Qi) aquí, PNEW (Qi) representa la cantidad de nuevos documentosdevuelto para Qi (las páginas que no han sido devueltas para consultas anteriores). El costo (Qi) representa el costo de emitir la consulta Qi. Intuitivamente, la eficiencia de las medidas de Qi cuántos documentos nuevos se recuperan por costo unitario, y pueden usarse como un indicador de 4 para una estimación exacta, necesitamos saber el número total de páginas en el sitio. Sin embargo, para comparar solo valores relativos entre consultas, esta información no es realmente necesaria.103 Algoritmo 3.1. Parámetros de selección Greedy SelectM (): T: La lista de Procedimiento de palabras clave de consulta potencial (1) foreach Tk en t do (2) estimación de eficiencia (TK) = PNEW (TK) Costo (TK) (3) Listo (4) TK con TK conEficiencia máxima (TK) Figura 6: Algoritmo para seleccionar el siguiente término de consulta.Qué tan bien se gastan nuestros recursos al emitir Qi. Por lo tanto, el rastreador web oculto puede estimar la eficiencia de cada Qi candidato y seleccionar el que tiene el valor más alto. Al utilizar sus recursos de manera más eficiente, el rastreador puede eventualmente descargar el número máximo de documentos únicos. En la Figura 6, mostramos la función de selección de consultas que utiliza el concepto de eficiencia. En principio, este algoritmo adopta un enfoque codicioso e intenta maximizar la ganancia potencial en cada paso. Podemos estimar la eficiencia de cada consulta utilizando el método de estimación descrito en la Sección 3.1. Es decir, el tamaño de los nuevos documentos de la consulta Qi, PNEW (Qi), es PNEW (Qi) = P (Q1 ∨ · · · · Qi - 1 ∨ Qi) - P (Q1 ∨ · · · ∨ Qi−1) = P (Qi) - P (Q1 ∨ · · · ∨ Qi - 1) P (Qi | Q1 ∨ · · · ∨ Qi - 1) de la Ecuación 3, donde P (Qi) puede estimarse usando uno de losMétodos descritos en la Sección 3. También podemos estimar el costo (Qi) de manera similar. Por ejemplo, si el costo (qi) es costo (qi) = cq + CRP (qi) + cdpnew (qi) (ecuación 2), podemos estimar el costo (qi) estimando p (qi) y pNew (qi).3.3 Cálculo eficiente de las estadísticas de consulta En la estimación de la eficiencia de las consultas, encontramos que necesitamos medir P (Qi | Q1∨ · · · ∨qi - 1) para cada consulta potencial Qi. Este cálculo puede llevar mucho tiempo si lo repitemos desde cero para cada Qi de consulta en cada iteración de nuestro algoritmo. En esta sección, explicamos cómo podemos calcular P (Qi | Q1 ∨ · · · ∨ Qi - 1) de manera eficiente manteniendo una tabla pequeña que llamamos una tabla de estadísticas de consulta. La idea principal de la tabla de estadísticas de consulta es que P (Qi | Q1 ∨ · · · ∨ Qi - 1) se puede medir contando cuántas veces la palabra clave Qi aparece dentro de los documentos descargados de Q1 ,..., Qi - 1. Grabamos estos recuentos en una tabla, como se muestra en la Figura 7 (a). La columna izquierda de la tabla contiene todos los términos de consulta potencial y la columna derecha contiene el número de documentos previamente descargados que contienen el término respectivo. Por ejemplo, la tabla en la Figura 7 (a) muestra que hasta ahora hemos descargado 50 documentos, y el término modelo aparece en 10 de estos documentos. Dado este número, podemos calcular que P (modelo | Q1 ∨ · · · ∨ Qi - 1) = 10 50 = 0.2. Observamos que la tabla de estadísticas de consulta debe actualizarse cada vez que emitamos una nueva consulta Qi y descargamos más documentos. Esta actualización se puede hacer de manera eficiente como ilustramos en el siguiente ejemplo. EJEMPLO 1. Después de examinar la tabla de estadísticas de consulta de la Figura 7 (a), hemos decidido usar el término computadora como nuestra próxima consulta Qi. Desde la nueva consulta Qi = computadora, descargamos 20 páginas nuevas más. De estos, 12 contienen el modelo de palabras clave Término TK N (TK) Modelo 10 Computadora 38 Digital 50 Término TK N (TK) Modelo 12 Computadora 20 Disco 18 Páginas totales: 50 Páginas nuevas: 20 (a) Después de Q1 ,..., Qi - 1 (b) Nuevo de Qi = Término de la computadora TK N (TK) Modelo 10+12 = 22 Computadora 38+20 = 58 Disco 0+18 = 18 Digital 50+0 = 50 Páginas totales: 50+20 = 70(c) Después de Q1 ,..., Qi Figura 7: Actualización de la tabla de estadísticas de consulta.Q I1 I - 1 Q \/ ... \/ Q Q I/ S Figura 8: Un sitio web que no devuelve todos los resultados.y 18 el disco de palabras clave. La tabla en la Figura 7 (b) muestra la frecuencia de cada término en las páginas recién descendentes. Podemos actualizar la tabla anterior (Figura 7 (a)) para incluir esta nueva información simplemente agregando las entradas correspondientes en las Figuras 7 (a) y (b). El resultado se muestra en la Figura 7 (c). Por ejemplo, el modelo de palabras clave existe en 10 + 12 = 22 páginas dentro de las páginas recuperadas de Q1 ,..., Qi. De acuerdo con esta nueva tabla, P (modelo | Q1∨ · · · ∨qi) es ahora 22 70 = 0.3.3.4 Sitios de rastreo que limitan el número de resultados en ciertos casos, cuando una consulta coincide con una gran cantidad de páginas, el sitio web oculto devuelve solo una parte de esas páginas. Por ejemplo, el proyecto Open Directory [2] permite a los usuarios ver solo hasta 10, 000 resultados después de emitir una consulta. Obviamente, este tipo de limitación tiene un efecto inmediato en nuestro rastreador web oculto. Primero, dado que solo podemos recuperar hasta un número específico de páginas por consulta, nuestro rastreador necesitará emitir más consultas (y potencialmente utilizará más recursos) para descargar todas las páginas. En segundo lugar, el método de selección de consultas que presentamos en la Sección 3.2 supone que para cada posible Qi de consulta, podemos encontrar P (Qi | Q1 ∨ · · · ∨ Qi - 1). Es decir, para cada consulta Qi podemos encontrar la fracción de documentos en toda la base de datos de texto que contiene Qi con al menos uno de Q1 ,..., Qi - 1. Sin embargo, si la base de datos de texto devolvía solo una parte de los resultados para cualquiera de los Q1 ,..., Qi - 1 entonces el valor P (Qi | Q1 ∨ · · · ∨ Qi - 1) no es preciso y puede afectar nuestra decisión para la siguiente consulta Qi, y potencialmente el rendimiento de nuestro rastreador. Dado que no podemos recuperar más resultados por consulta que el número máximo que permite el sitio web, nuestro rastreador no tiene otra opción además de enviar más consultas. Sin embargo, hay una manera de estimar el valor correcto para P (Qi | Q1 ∨ · · · ∨ Qi - 1) en el caso de que el sitio web devuelva solo una parte de los resultados.104 Nuevamente, suponga que el sitio web oculto que estamos rastreando actualmente se representa como el rectángulo en la Figura 8 y sus páginas como puntos en la figura. Suponga que ya hemos emitido consultas Q1 ,..., Qi - 1 que devolvió una serie de resultados menores que el número máximo de lo que permite el sitio, y por lo tanto hemos descargado todas las páginas para estas consultas (Big Circle en la Figura 8). Es decir, en este punto, nuestra estimación para P (qi | q1 ∨ · · · ∨qi - 1) es precisa. Ahora suponga que enviamos Qi de consulta al sitio web, pero debido a una limitación en el número de resultados que recuperamos, recuperamos el Qi establecido (círculo pequeño en la Figura 8) en lugar del Qi establecido (círculo discontinuo en la Figura 8). Ahora necesitamos actualizar nuestra tabla de estadísticas de consulta para que tenga información precisa para el siguiente paso. Es decir, aunque recuperamos el Qi, por cada consulta potencial Qi+1 necesitamos encontrar P (Qi+1 | Q1 ∨ · · · ∨ Qi): P (Qi+1 | Q1 ∨ · · · ∨ Qi) = 1 P (Q1 ∨ · · · ∨ Qi) · [P (Qi+1 ∧ (Q1 ∨ · · · ∨ Qi - 1))+P (Qi+1 ∧ Qi) - P (Qi+1 ∧ Qi∧ (Q1 ∨ · · · ∨ Qi - 1))] (5) En la ecuación anterior, podemos encontrar P (Q1 ∨ · · · ∨qi) estimando P (Qi) con el método que se muestra en la Sección 3. Además, podemos calcular p (qi+1 ∧ (Q1 ∨ · · · ∨ qi - 1)) y p (qi+1 ∧ qi ∧ (Q1 ∨ · · · ∨ qi - 1)) examinando directamente los documentos queHemos descargado de las consultas Q1 ,..., Qi - 1. Sin embargo, el término P (qi+1 ∧ qi) es desconocido y necesitamos estimarlo. Suponiendo que Qi es una muestra aleatoria de Qi, entonces: P (Qi+1 ∧ Qi) P (Qi+1 ∧ Qi) = P (Qi) P (Qi) (6) De la ecuación 6 podemos calcular p (Qi+1 ∧ qi) y después de reemplazar este valor a la ecuación 5 podemos encontrar p (qi+1 | q1 ∨ · · · ∨ qi).4. Evaluación experimental En esta sección evaluamos experimentalmente el rendimiento de los diversos algoritmos para el rastreo web oculto presentados en este documento. Nuestro objetivo es validar nuestro análisis teórico a través de los experimentos de RealWorld, rastreando los populares sitios web ocultos de bases de datos textuales. Dado que el número de documentos que se descubren y descargan desde una base de datos textual depende de la selección de las palabras que se emitirán como consultas5 con la interfaz de búsqueda de cada sitio, comparamos las diversas políticas de selección que se describieron en la Sección 3, a saber, elAlgoritmos aleatorios, de frecuencia genérica y adaptativa. El algoritmo adaptativo aprende nuevas palabras clave y términos de los documentos que descarga, y su proceso de selección está impulsado por un modelo de costo como se describe en la Sección 3.2. Para mantener nuestro experimento y su análisis simple en este punto, asumiremos que el costo de cada consulta es constante. Es decir, nuestro objetivo es maximizar el número de páginas descargadas emitiendo el menor número de consultas. Más tarde, en la Sección 4.4 presentaremos una comparación de nuestras políticas basadas en un modelo de costo más elaborado. Además, utilizamos el estimador de independencia (Sección 3.1) para estimar P (Qi) de las páginas descargadas. Aunque el estimador de independencia es un estimador simple, nuestros experimentos mostrarán que puede funcionar muy bien en la práctica.6 Para la política de frecuencia genérica, calculamos la distribución de frecuencia de palabras que aparecen en un Corpus 5 de 5.5 millones de webs 5A lo largo de nuestros experimentos, una vez que un algoritmo ha presentado una consulta a una base de datos, excluyimos la consulta de los envíos posteriores a la misma base de datos del mismo algoritmo.6 Diferimos el informe de resultados basados en la estimación ZIPF a un trabajo futuro.Descargado de 154 sitios web de varios temas [26]. Las palabras clave se seleccionan en función de su frecuencia decreciente con la que aparecen en este conjunto de documentos, y el más frecuente se selecciona primero, seguido de la segunda palabra clave más frecuente, etc.7 con respecto a la política aleatoria, usamos el mismo conjunto de palabrasRecopilado del Corpus web, pero en este caso, en lugar de seleccionar palabras clave en función de su frecuencia relativa, las elegimos al azar (distribución uniforme). Para investigar más a fondo cómo la calidad de la lista potencial de medidas de consulta afecta el algoritmo basado al azar, construimos dos conjuntos: uno con las 16, 000 palabras más frecuentes del término colección utilizada en la política de frecuencia genérica (en adelante, en adelante,, en adelante,,La política aleatoria con el conjunto de 16,000 palabras se denominará Random-16k), y otro conjunto con las 1 millón de palabras más frecuentes de la misma colección que anteriormente (en adelante, denominado Random-1m). El set anterior tiene palabras frecuentes que aparecen en una gran cantidad de documentos (al menos 10, 000 en nuestra colección) y, por lo tanto, pueden considerarse términos de alta calidad. Sin embargo, el último conjunto contiene una colección mucho mayor de palabras, entre las cuales algunas podrían ser falsas y sin sentido. Los experimentos se realizaron empleando cada uno de los algoritmos antes mencionados (adaptativo, frecuencia genérica, random16k y random-1m) para rastrear y descargar contenidos de tres sitios web ocultos: la Biblioteca Médica PubMed, 8 Amazon, 9 y el directorio abiertoProyecto [2]. Según la información en el sitio web de PubMeds, su colección contiene aproximadamente 14 millones de resúmenes de artículos biomédicos. Consideramos estos resúmenes como los documentos en el sitio, y en cada iteración de la política adaptativa, utilizamos estos resúmenes como entrada al algoritmo. Por lo tanto, nuestro objetivo es descubrir tantos resúmenes únicos como sea posible mediante la consulta repetidamente de la interfaz de consulta web proporcionada por PubMed. El rastreo web oculto en el sitio web de PubMed puede considerarse como específico del tema, debido al hecho de que todos los resúmenes dentro de PubMed están relacionados con los campos de la medicina y la biología. En el caso del sitio web de Amazon, estamos interesados en descargar todas las páginas ocultas que contienen información sobre los libros. La consulta a Amazon se realiza a través del kit de desarrolladores de software que Amazon proporciona para interactuar en su sitio web, y que devuelve los resultados en forma XML. El campo de palabras clave genéricas se usa para buscar, y como entrada a la política adaptativa extraemos la descripción del producto y el texto de las revisiones de los clientes cuando está presente en la respuesta XML. Dado que Amazon no proporciona ninguna información sobre cuántos libros tiene en su catálogo, utilizamos un muestreo aleatorio en el número ISBN de 10 dígitos de los libros para estimar el tamaño de la colección. De los 10, 000 números de ISBN aleatorios consultados, 46 se encuentran en el catálogo de Amazon, por lo tanto, el tamaño de su colección de libros se estima en 46 10000 · 1010 = 4.6 millones de libros. También vale la pena señalar aquí que Amazon plantea un límite superior en el número de resultados (libros en nuestro caso) devuelto por cada consulta, que se establece en 32, 000. En cuanto al tercer sitio web oculto, el proyecto Open Directory (en adelante, también conocido como DMOZ), el sitio mantiene los enlaces a 3.8 millones de sitios junto con un breve resumen de cada sitio enumerado. Los enlaces se pueden buscar a través de una interfaz de búsqueda de palabras clave. Consideramos cada enlace indexado junto con su breve resumen como documento del sitio DMOZ, y proporcionamos los breves resúmenes al algoritmo adaptativo para impulsar la selección de nuevas palabras clave para consultar. En el sitio web de DMOZ, realizamos dos rastreos web ocultos: el primero está en su colección genérica de 3.8 millones indexados 7 No excluyimos manualmente las palabras de parada (por ejemplo, el, es, de, etc.) de la lista de palabras clave. Resulta que todos los sitios web, excepto los documentos coincidentes de PubMed, para las palabras de parada, como el.8 Biblioteca médica de PubMed: http://www.pubmed.org 9 Amazon Inc.: Http://www.amazon.com 105 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 FRACCIONES Número de consultoría Fracción cumulativa Cumulativa Cumulativade documentos únicos-Sitio web de PubMed Adaptativo Generic-Frequencia Random-16K Random-1m Figura 9: Cobertura de las políticas para PubMed 0 0.1 0.2 0.2 0.4 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 FractionOfdocuments Número de consultas Fracción Cumulativa de un únicoDocumentos-Sitio web de Amazon Adaptativo Generic-Frequency Random-16k Random-1m Figura 10: Cobertura de las políticas para sitios de Amazon, independientemente de la categoría en la que caen. El otro rastreo se realiza específicamente en la sección de artes de DMOZ (http: // dmoz.org/arts), que consta de aproximadamente 429, 000 sitios indexados que son relevantes para las artes, lo que hace que este temas de rastreo sea específico, como en PubMed. Al igual que Amazon, DMOZ también aplica un límite superior en el número de resultados devueltos, que son 10, 000 enlaces con sus resúmenes.4.1 Comparación de las políticas La primera pregunta que buscamos responder es la evolución de la métrica de cobertura a medida que presentamos consultas a los sitios. Es decir, ¿qué fracción de la colección de documentos almacenados en el sitio web oculto podemos descargar a medida que consultamos continuamente las nuevas palabras seleccionadas utilizando las políticas descritas anteriormente? Más formalmente, estamos interesados en el valor de P (Q1 ∨ · · · ∨ Qi - 1 ∨ Qi), después de enviar Q1 ,..., Qi consultas, y a medida que aumenta. En las Figuras 9, 10, 11 y 12 presentamos la métrica de cobertura para cada política, en función del número de consulta, para los sitios web de PubMed, Amazon, DMOZ general y el DMOZ específico de ART, respectivamente. En el eje Y, la fracción del total de documentos descargados del sitio web se traza, mientras que el eje X representa el número de consulta. Una primera observación de estos gráficos es que, en general, la frecuencia genérica y las políticas adaptativas funcionan mucho mejor que los algoritmos basados al azar. En todas las figuras, los gráficos para el Random-1M y el Random-16k están significativamente por debajo de los de otras políticas. Entre la frecuencia genérica y las políticas adaptativas, podemos ver que este último supera al primero cuando el sitio es tema SPE0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 100 200 300 400 500 600 700 FRACCIONES Número de consultas Número de fracción Cumulativa de un únicoDocumentos-Sitio web de DMOZ Adaptación Generic-Frequencia Random-16K Random-1m Figura 11: Cobertura de las políticas para DMOZ general 0 0.1 0.2 0.2 0.4 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200 250 300 350 400 450 FRACCIONES DESCOMENTOS Número de consultoría Fracción cumulativa de deDocumentos únicos: sitio web de DMOZ/Arts adaptativo-frecuencia genérica aleatoria-16k aleatoria-1m Figura 12: Cobertura de las políticas para la sección de artes de DMOZ CIFF. Por ejemplo, para el sitio de PubMed (Figura 9), el algoritmo adaptativo emite solo 83 consultas para descargar casi el 80% de los documentos almacenados en PubMed, pero el algoritmo de frecuencia genérica requiere 106 consultas para la misma cobertura,. Para el DMOZ/Arts Crawl (Figura 12), la diferencia es aún más sustancial: la política adaptativa puede descargar el 99.98% del total de sitios indexados en el directorio emitiendo 471 consultas, mientras que el algoritmo basado en la frecuencia es mucho menos efectivoutilizando el mismo número de consultas, y descubre solo el 72% del número total de sitios indexados. El algoritmo adaptativo, al examinar el contenido de las páginas que descarga en cada iteración, puede identificar el tema del sitio expresado por las palabras que aparecen con mayor frecuencia en el conjunto de resultados. En consecuencia, es capaz de seleccionar palabras para consultas posteriores que sean más relevantes para el sitio, que las preferidas por la política de frecuencia genérica, que se extraen de una colección grande y genérica. La Tabla 1 muestra una muestra de 10 palabras clave de 211 elegidas y enviadas al sitio web de PubMed por el algoritmo adaptativo, pero no por las otras políticas. Para cada palabra clave, presentamos el número de la iteración, junto con el número de resultados que regresó. Como se puede ver en la tabla, estas palabras clave son muy relevantes para los temas de la medicina y la biología de la Biblioteca Médica Pública, y coinciden con numerosos artículos almacenados en su sitio web. En ambos casos examinados en las Figuras 9 y 12, las políticas basadas en el azar funcionan mucho peor que el algoritmo adaptativo y la frecuencia genérica. Sin embargo, vale la pena señalar que la política randombada con el conjunto pequeño y cuidadosamente seleccionado de 16, 000 palabras de calidad logra descargar una fracción considerable de 42.5% 106 Número de palabras clave de iteración de resultados 23 Departamento 2, 719, 031 34 pacientes 1, 934, 428 53 clínica 1, 198, 322 67 tratamiento 4, 034, 565 69 Medical 1, 368, 200 70 Hospital 503, 307 146 Enfermedad 1, 520, 908 172 Proteína 2, 620, 938 Tabla 1: Muestra de palabras clave consideradas aPubMed exclusivamente por la política adaptativa del sitio web de PubMed después de 200 consultas, mientras que la cobertura de la sección de artes de DMOZ alcanza el 22.7%, después de 471 palabras clave consultadas. Por otro lado, el enfoque basado en el azar que hace uso de la gran colección de 1 millón de palabras, entre las cuales un gran número es una falsa palabras clave, no se descarga incluso solo el 1% de la colección total, después de enviar el mismo número de número dePalabras de consulta. Para las colecciones genéricas de Amazon y los sitios DMOZ, que se muestran en las Figuras 10 y 11, respectivamente, obtenemos resultados mixtos: la política de frecuencia genérica muestra un rendimiento ligeramente mejor que la política adaptativa para el sitio de Amazon (Figura 10), y el método adaptativo supera claramenteLa frecuencia genérica para el sitio DMOZ general (Figura 11). Una mirada más cercana a los archivos de registro de los dos rastreadores web ocultos revela la razón principal: Amazon estaba funcionando de una manera muy escamosa cuando el rastreador adaptativo lo visitó, lo que resultó en una gran cantidad de resultados perdidos. Por lo tanto, sospechamos que el rendimiento ligeramente pobre de la política adaptativa se debe a esta varianza experimental. Actualmente estamos ejecutando otro experimento para verificar si este es realmente el caso. Además de esta varianza experimental, el resultado de Amazon indica que si la colección y las palabras que contiene un sitio web oculto son lo suficientemente genéricos, entonces el enfoque de frecuencia genérica puede ser un buen algoritmo candidato para un rastreo efectivo. Como en el caso de los sitios web ocultos específicos del tema, las políticas de Randombased también exhiben un bajo rendimiento en comparación con los otros dos algoritmos cuando se arrastran los sitios genéricos: para el sitio web de Amazon, Random-16k tiene éxito en descargar casi 36.7% después de emitir 775 Queridas,Por desgracia, para la colección genérica de DMOZ, la fracción de la colección de enlaces descargados es del 13.5% después de la consulta 770. Finalmente, como se esperaba, Random-1M es aún peor que Random16k, descargando solo el 14.5% de Amazon y el 0.3% del DMOZ genérico. En resumen, el algoritmo adaptativo funciona notablemente bien en todos los casos: es capaz de descubrir y descargar la mayoría de los documentos almacenados en sitios web ocultos emitiendo el menor número de consultas. Cuando la recopilación se refiere a un tema específico, puede identificar las palabras clave más relevantes para el tema del sitio y, en consecuencia, solicitar términos que lo más probable sea que devuelva una gran cantidad de resultados. Por otro lado, la política de frecuencia genérica demuestra ser bastante efectiva también, aunque menos que la adaptativa: es capaz de recuperar relativamente rápido una gran parte de la colección, y cuando el sitio no es específico del tema, su efectividad puedealcanzar el de adaptativo (p. Ej. Amazonas). Finalmente, la política aleatoria funciona mal en general y no debe preferirse.4.2 Impacto de la consulta inicial Una cuestión interesante que merece un examen más detallado es si la elección inicial de la palabra clave utilizada como la primera consulta emitida por el algoritmo adaptativo afecta su efectividad en las iteraciones posteriores. La elección de esta palabra clave no se realiza mediante la selección del 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 20 30 50 60 FractionOfdocuments Convergencia de número de consulta de adaptativa bajo diferentes consultas iniciales - Sitio web PubMed Información de datos PubMed Figura 13: Convergencia del algoritmo adaptativo utilizando diferentes consultas iniciales para rastrear el algoritmo adaptativo del sitio web de PubMed y debe establecerse manualmente, ya que sus tablas de estadísticas de consulta aún no se han poblado. Por lo tanto, la selección es generalmente arbitraria, por lo que a los fines de automatizar completamente todo el proceso, parece necesaria alguna investigación adicional. Por esta razón, iniciamos tres rastreadores web ocultos adaptativos dirigidos al sitio web de PubMed con diferentes palabras de semillas: los datos de Word, que devuelve 1.344,999 resultados, la información que informa 308, 474 documentos y el retorno de la palabra que recupera 29, 707páginas, de 14 millones. Estas palabras clave representan diversos grados de popularidad a término en PubMed, la primera es de alta popularidad, el segundo de medio y el tercero de Low. También mostramos resultados para la palabra clave PubMed, utilizada en los experimentos para la cobertura de la Sección 4.1, y que devuelve 695 artículos. Como podemos ver en la Figura 13, después de un pequeño número de consultas, los cuatro rastreadores descargan aproximadamente la misma fracción de la colección, independientemente de su punto de partida: sus coberturas son aproximadamente equivalentes a partir de la consulta 25. Finalmente, los cuatro rastreadores usan el mismo conjunto de términos para sus consultas, independientemente de la consulta inicial. En el experimento específico, desde la consulta 36 en adelante, los cuatro rastreadores usan los mismos términos para sus consultas en cada iteración, o los mismos términos se usan mediante uno o dos números de consulta. Nuestro resultado confirma la observación de [11] que la elección de la consulta inicial tiene un efecto mínimo en el rendimiento final. Podemos explicar esto intuitivamente de la siguiente manera: nuestro algoritmo se aproxima al conjunto óptimo de consultas para usar para un sitio web en particular. Una vez que el algoritmo ha emitido un número significativo de consultas, tiene una estimación precisa del contenido del sitio web, independientemente de la consulta inicial. Dado que esta estimación es similar para todas las carreras del algoritmo, los rastreadores usarán aproximadamente las mismas consultas.4.3 Impacto del límite en el número de resultados Mientras que los sitios de Amazon y DMOZ tienen el límite respectivo de 32,000 y 10,000 en sus tamaños de resultados, estos límites pueden ser mayores que los impuestos por otros sitios web ocultos. Para investigar cómo un límite más ajustado en el tamaño de los resultados afecta el rendimiento de nuestros algoritmos, realizamos dos rastreos adicionales al sitio genérico-DMOZ: ejecutamos la frecuencia genérica y las políticas adaptativas, pero recuperamos solo los 1,000 resultados principales.para cada consulta. En la Figura 14, trazamos la cobertura de las dos políticas en función del número de consultas. Como cabría esperar, al comparar el nuevo resultado en la Figura 14 con el de la Figura 11, donde el límite de resultados fue de 10,000, concluimos que el límite más estrecho requiere un mayor número de consultas para lograr la misma cobertura. Por ejemplo, cuando el límite de resultados fue de 10,000, el Pol107 adaptativo 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 FRACCIÓN DE LA FRACEPAGES Número de consulta Fracción acumulada de páginas únicas Descargados Persona - Sitio web DMOZ (límite de tapa (límite de tapa1000) FRISIÓN GENÉRICA ADAPCITIVA Figura 14: Cobertura de DMOZ general después de limitar el número de resultados a 1,000 helados podría descargar el 70% del sitio después de emitir 630 consultas, mientras que tuvo que emitir 2.600 consultas para descargar el 70% del sitio cuando el sitio cuando era el sitio cuando el sitio cuando el sitioEl límite fue de 1,000. Por otro lado, nuestro nuevo resultado muestra que incluso con un límite de resultados apretado, aún es posible descargar la mayor parte de un sitio web oculto después de emitir un número razonable de consultas. La política adaptativa podría descargar más del 85% del sitio después de emitir 3.500 consultas cuando el límite era de 1,000. Finalmente, nuestro resultado muestra que nuestra política adaptativa supera constantemente la política de frecuencia genérica, independientemente del límite de resultados. Tanto en la Figura 14 como en la Figura 11, nuestra política adaptativa muestra una cobertura significativamente mayor que la política de frecuencia genérica para el mismo número de consultas.4.4 Incorporación del costo de descarga de documentos para la brevedad de la presentación, los resultados de la evaluación de desempeño proporcionados hasta ahora asumieron un modelo de costo simplificado donde cada consulta involucraba un costo constante. En esta sección presentamos resultados con respecto al rendimiento de los algoritmos adaptativos y de frecuencia genérica utilizando la Ecuación 2 para impulsar nuestro proceso de selección de consultas. Como discutimos en la Sección 2.3.1, este modelo de costo de consulta incluye el costo de enviar la consulta al sitio, recuperar la página del índice de resultados y también descargar las páginas reales. Para estos costos, examinamos el tamaño de cada resultado en la página de índice y los tamaños de los documentos, y elegimos CQ = 100, CR = 100 y CD = 10000, como valores para los parámetros de la ecuación 2, y para elExperimento particular que ejecutamos en el sitio web de PubMed. Los valores que seleccionamos implican que el costo de emitir una consulta y recuperar un resultado de la página del índice de resultados es aproximadamente el mismo, mientras que el costo para descargar una página real es 100 veces mayor. Creemos que estos valores son razonables para el sitio web de PubMed. La Figura 15 muestra la cobertura de los algoritmos de frecuencia adaptativa y genérica en función de las unidades de recursos utilizadas durante el proceso de descarga. El eje horizontal es la cantidad de recursos utilizados, y el eje vertical es la cobertura. Como es evidente en el gráfico, la política adaptativa hace un uso más eficiente de los recursos disponibles, ya que puede descargar más artículos que la frecuencia genérica, utilizando la misma cantidad de unidades de recursos. Sin embargo, la diferencia en la cobertura es menos dramática en este caso, en comparación con el gráfico de la Figura 9. La menor diferencia se debe al hecho de que bajo la métrica de costo actual, el costo de descarga de los documentos constituye una parte significativa del costo. Por lo tanto, cuando ambas políticas descargaron el mismo número de documentos, el ahorro de la política adaptativa no es tan dramático como antes. Ese 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5000 10000 15000 20000 25000 30000 FRACCIÓN DE LA CUNICIPACIONES Costo total (CQ = 100, CR = 100, CD = 10000) Fracción acumulada de páginas únicas descargadas por unidad de costo - Sitio web PubMed Frecuencia adaptativa de adaptaciónFigura 15: La cobertura de PubMed después de incorporar el costo de descarga del documento es, los ahorros en el costo de consulta y el costo de descarga del índice de resultados es solo una parte relativamente pequeña del costo total. Aún así, observamos ahorros notables de la política adaptativa. A un costo total de 8000, por ejemplo, la cobertura de la política adaptativa es aproximadamente 0.5, mientras que la cobertura de la política de frecuencia es solo 0.3.5. Trabajo relacionado en un estudio reciente, Raghavan y García-Molina [29] presentan un modelo arquitectónico para un rastreador web oculto. El enfoque principal de este trabajo es aprender interfaces de consulta de Widden-Web, no generar consultas automáticamente. Las consultas potenciales son proporcionadas manualmente por usuarios o recopiladas de las interfaces de consulta. En contraste, nuestro enfoque principal es generar consultas automáticamente sin ninguna intervención humana. La idea de emitir consultas automáticamente a una base de datos y examinar los resultados se ha utilizado previamente en diferentes contextos. Por ejemplo, en [10, 11], Callan y Connel intentan adquirir un modelo de lenguaje preciso mediante la recopilación de una muestra aleatoria uniforme de la base de datos. En [22] Lawrence y Giles emiten consultas aleatorias a varios motores de búsqueda web para estimar la fracción de la web que ha sido indexada por cada uno de ellos. De manera similar, Bharat y Broder [8] emiten consultas aleatorias a un conjunto de motores de búsqueda para estimar el tamaño relativo y la superposición de sus índices. En [6], Barbosa y Freire evalúan experimentalmente métodos para construir consultas de palabras múltiples que pueden devolver una gran fracción de una recopilación de documentos. Nuestro trabajo difiere de los estudios anteriores de dos maneras. Primero, proporciona un marco teórico para analizar el proceso de generación de consultas para una base de datos y examinar los resultados, lo que puede ayudarnos a comprender mejor la efectividad de los métodos presentados en el trabajo anterior. En segundo lugar, aplicamos nuestro marco al problema del rastreo web oculto y demostramos la eficiencia de nuestros algoritmos. Cope et al.[15] propone un método para detectar automáticamente si una página web en particular contiene un formulario de búsqueda. Este trabajo es complementario a el nuestro;Una vez que detectamos interfaces de búsqueda en la web utilizando el método en [15], podemos usar nuestros algoritmos propuestos para descargar páginas automáticamente de esos sitios web. Referencia [4] informa métodos para estimar qué fracción de una base de datos de texto puede adquirirse eventualmente emitiendo consultas a la base de datos. En [3] los autores estudian técnicas basadas en consultas que pueden extraer datos relacionales de grandes bases de datos de texto. Nuevamente, estos trabajos estudian problemas ortogonales y son complementarios a nuestro trabajo. Para hacer documentos en múltiples bases de datos textuales que se pueden buscar en un lugar central, se han propuesto varios enfoques de cosecha (por ejemplo, OAI [21], DP9 [24]). Estos enfoques asumen esencialmente bases de datos de documentos cooperativos que comparten voluntariamente algunos de sus metadatos y/o documentos para ayudar a un motor de búsqueda de terceros a indexar los documentos. Nuestro enfoque asume bases de datos no cooperativas que no comparten sus datos públicamente y cuyos documentos son accesibles solo a través de interfaces de búsqueda. Existe un gran cuerpo de trabajo que estudia cómo identificar la base de datos más relevante dada una consulta de usuario [20, 19, 14, 23, 18]. Este cuerpo de trabajo a menudo se conoce como un problema de selección de metaes de búsqueda o base de datos sobre la web oculta. Por ejemplo, [19] sugiere el uso de sondeo enfocado para clasificar las bases de datos en una categoría de actualidad, de modo que, dada una consulta, se puede seleccionar una base de datos relevante en función de su categoría de actualidad. Nuestra visión es diferente de este cuerpo de trabajo, ya que tenemos la intención de descargar e indexar las páginas ocultas en una ubicación central por adelantado, para que los usuarios puedan acceder a toda la información a su conveniencia desde una sola ubicación.6. Conclusión y trabajo futuro Los rastreadores tradicionales normalmente siguen enlaces en la web para descubrir y descargar páginas. Por lo tanto, no pueden llegar a las páginas web ocultas a las que solo se pueden acceder a través de interfaces de consulta. En este documento, estudiamos cómo podemos crear un rastreador web oculto que pueda consultar automáticamente un sitio web oculto y descargar páginas de él. Propusimos tres políticas de generación de consultas diferentes para la web oculta: una política que elige consultas al azar de una lista de palabras clave, una política que elige consultas basadas en su frecuencia en una colección de texto genérico y una política que elige adaptativamente una buena consulta basada en la consulta.en el contenido de las páginas descargadas desde el sitio web oculto. La evaluación experimental en 4 sitios web ocultos reales muestra que nuestras políticas tienen un gran potencial. En particular, en ciertos casos, la política adaptativa puede descargar más del 90% de un sitio web oculto después de emitir aproximadamente 100 consultas. Dados estos resultados, creemos que nuestro trabajo proporciona un mecanismo potencial para mejorar la cobertura del motor de búsqueda de la web y la experiencia del usuario de la búsqueda web.6.1 Trabajo futuro discutimos brevemente algunas vías de investigación futura. Bases de datos de atribución múltiple. Actualmente estamos investigando cómo extender nuestras ideas a bases de datos estructuradas de atribución múltiple. Si bien la generación de consultas para bases de datos de atributos múltiples es claramente un problema más difícil, podemos explotar la siguiente observación para abordar este problema: cuando un sitio admite consultas de atributo múltiple, el sitio a menudo devuelve páginas que contienen valores para cada uno de los atributos de consulta. Por ejemplo, cuando una librería en línea admite consultas sobre el título, el autor e ISBN, las páginas devueltas de una consulta generalmente contienen el título, el autor e ISBN de los libros correspondientes. Por lo tanto, si podemos analizar las páginas devueltas y extraer los valores para cada campo (por ejemplo, Title = Harry Potter, Author = J.K. Rowling, etc.), podemos aplicar la misma idea que utilizamos para la base de datos textual: estimar la frecuencia deCada valor del atributo y elige el más prometedor. El principal desafío es segmentar automáticamente las páginas devueltas para que podamos identificar las secciones de las páginas que presentan los valores correspondientes a cada atributo. Dado que muchos sitios web siguen estilos de formato limitado en la presentación de múltiples atributos, por ejemplo, la mayoría de los títulos de libros están precedidos por el título de la etiqueta: - Creemos que podemos aprender reglas de segmentación de página automáticamente de un pequeño conjunto de ejemplos de capacitación. Otros problemas prácticos Además del problema de generación de consultas automáticas, hay muchos problemas prácticos que se abordarán para construir un rastreador de WEB oculto totalmente automático. Por ejemplo, en este documento asumimos que el rastreador ya conoce todas las interfaces de consulta para sitios de Widden-Web. Pero, ¿cómo puede el rastreador descubrir las interfaces de consulta? El método propuesto en [15] puede ser un buen punto de partida. Además, algunos sitios de Widden-Web devuelven sus resultados en lotes de, digamos, 20 páginas, por lo que el usuario debe hacer clic en un botón siguiente para ver más resultados. En este caso, un rastreador de WEB oculto totalmente automático debe saber que la primera página de índice de resultados contiene solo un resultado parcial y presiona el botón siguiente automáticamente. Finalmente, algunos sitios web ocultos pueden contener un número infinito de páginas web ocultas que no contribuyen con mucho contenido significativo (por ejemplo, un calendario con enlaces para cada día). En este caso, el rastreador de Widden-Web debería poder detectar que el sitio no tiene mucho más contenido nuevo y dejar de descargar páginas del sitio. Los algoritmos de detección de similitud de página pueden ser útiles para este propósito [9, 13].7. Referencias [1] LexisNexis http://www.lexisnexis.com.[2] El proyecto Open Directory, http://www.dmoz.org.[3] E. Agichtein y L. Gravano. Consulta de bases de datos de texto para una extracción de información eficiente. En ICDE, 2003. [4] E. Agichtein, P. ipeirotis y L. Gravano. Modelado de acceso basado en consultas a bases de datos de texto. En WebDB, 2003. [5] Artículo sobre New York Times. El viejo motor de búsqueda, la biblioteca, intenta encajar en un mundo de Google. Disponible en: http: //www.nytimes.com/2004/06/21/technology/21libr.html, junio de 2004. [6] L. Barbosa y J. Freire. Syphoning Hidden-Web Data a través de interfaces basadas en palabras clave. En SBBD, 2004. [7] M. K. Bergman. The Deep Web: Surfacing Hidden Value, http: //www.press.umich.edu/jep/07-01/bergman.html.[8] K. Bharat y A. Broder. Una técnica para medir el tamaño relativo y la superposición de los motores de búsqueda web públicos. En www, 1998. [9] A. Z. Broder, S. C. Glassman, M. S. Manasse y G. Zweig. Clustering sintáctico de la web. En www, 1997. [10] J. Callan, M. Connell y A. Du. Descubrimiento automático de modelos de lenguaje para bases de datos de texto. En Sigmod, 1999. [11] J. P. Callan y M. E. Connell. Muestreo basado en consultas de bases de datos de texto. Information Systems, 19 (2): 97-130, 2001. [12] K. C.-C.Chang, B. Él, C. Li y Z. Zhang. Bases de datos estructuradas en la web: observaciones e implicaciones. Informe técnico, UIUC.[13] J. Cho, N. Shivakumar y H. García-Molina. Encontrar colecciones web replicadas. En Sigmod, 2000. [14] W. Cohen e Y. Cantante. Aprender a consultar la web. En AAAAI Workshop en Sistemas de información basados en Internet, 1996. [15] J. Cope, N. Craswell y D. Hawking. Descubrimiento automatizado de interfaces de búsqueda en la web. En la 14ª Conferencia de Australia sobre Tecnologías de Base de Datos, 2003. [16] T. H. Cormen, C. E. Leiserson y R. L. Rivest. Introducción a los algoritmos, 2ª edición. MIT Press/McGraw Hill, 2001. [17] D. Florescu, A. Y. Levy y A. O. Mendelzon. Técnicas de base de datos para la web mundial: una encuesta. Sigmod Record, 27 (3): 59-74, 1998. [18] B. Él y K. C.-C.Chang. Combinación de esquema estadístico en las interfaces de consultas web. En la Conferencia Sigmod, 2003. [19] P. ipeirotis y L. Gravano. Búsqueda distribuida sobre la web oculta: muestreo y selección de bases de datos jerárquicas. En VLDB, 2002. [20] P. G. Ipeirotis, L. Gravano y M. Sahami. Probe, cuente y clasifique: clasificar bases de datos web ocultas. En Sigmod, 2001. [21] C. Lagoze y H. V. Sompel. La Iniciativa de Archivos Open: Construyendo un marco de interoperabilidad de baja barrera en JCDL, 2001. [22] S. Lawrence y C. L. Giles. Buscando en la red mundial. Science, 280 (5360): 98-100, 1998. [23] V. Z. Liu, J. C. Richard C. Luo y W. W. Chu. DPRO: un enfoque probabilístico para la selección de bases de datos web ocultas utilizando sondeo dinámico. En ICDE, 2004. [24] X. Liu, K. Maly, M. Zubair y M. L. Nelson. DP9-UN SERVICIO DE GATEWAY OAI para rastreadores web. En JCDL, 2002. [25] B. B. Mandelbrot. Geometría fractal de la naturaleza. W. H. Freeman & Co. [26] A. Ntoulas, J. Cho y C. Olston. ¿Qué hay de nuevo en la web?La evolución de la web desde una perspectiva del motor de búsqueda. En www, 2004. [27] A. Ntoulas, P. Zerfos y J. Cho. Descargar contenido web oculto. Informe técnico, UCLA, 2004. [28] S. Olsen. ¿El poder de los motores de búsqueda amenaza la independencia de las redes?http://news.com.com/2009-1023-963618.html.[29] S. Raghavan y H. García-Molina. Arrastrando la web oculta. En VLDB, 2001. [30] G. K. Zipf. Comportamiento humano y el principio de menor esfuerzo. Addison-Wesley, Cambridge, MA, 1949. 109