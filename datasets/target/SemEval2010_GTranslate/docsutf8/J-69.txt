Técnicas de incentivos robustas para redes de pares Michal Feldman1 mfeldman@sims.berkeley.edu kevin lai2 klai@hp.com ion stoica3 isttoica@cs.berkeley.edu John chuang1 chuang@sims.berkeley.edu 1 de la gestión de información y la gestión de información y y la gestión de la información y la gestión y la gestión de la información ySistemas U.C. Berkeley 2 HP Labs 3 División de Ciencias de la Computación U.C. La falta de cooperación (conducción gratuita) de Berkeley es uno de los problemas clave que confronta los sistemas P2P de hoy. Lo que hace que este problema sea particularmente difícil es el conjunto único de desafíos que plantean los sistemas P2P: grandes poblaciones, alta rotación, asimetría de interés, colusión, identidades de costo cero y traidores. Para abordar estos desafíos, modelamos el sistema P2P utilizando el dilema generalizado de los prisioneros (GPD) y proponemos la función de decisión recíproca como la base de una familia de técnicas de incentivos. Estas técnicas están completamente distribuidas e incluyen: discriminar la selección del servidor, la reputación subjetiva basada en Maxflow y las políticas de extraños adaptativos. A través de la simulación, mostramos que estas técnicas pueden llevar un sistema de usuarios estratégicos a niveles de cooperación casi óptimos. Categorías y descriptores de sujetos C.2.4 [Redes de comunicación por computadora]: sistemas distribuidos;J.4 [Ciencias Sociales y del Comportamiento]: Economía del diseño de términos generales, Economía 1. Introducción Muchos sistemas de igual a igual (P2P) dependen de la cooperación entre los usuarios autointerestados. Por ejemplo, en un sistema de intercambio de archivos, la latencia de descarga general de la latencia y la tasa de falla aumentan cuando los usuarios no comparten sus recursos [3]. En una red inalámbrica ad-hoc, la latencia general de la latencia y la tasa de pérdida de los paquetes aumentan cuando los nodos se niegan a reenviar paquetes en nombre de otros [26]. Otros ejemplos son la preservación de archivos [25], los paneles de discusión [17], las subastas en línea [16] y el enrutamiento de superposición [6]. En muchos de estos sistemas, los usuarios tienen desincentivos naturales para cooperar porque la cooperación consume sus propios recursos y puede degradar su propio rendimiento. Como resultado, cada usuarios intenta maximizar su propia utilidad reduce efectivamente el BC Figura 1 general: Ejemplo de asimetría de interés. A quiere servicio de B, B quiere el formulario de servicio C y C quiere el servicio de A. Utility of the System. Evitar esta tragedia de los bienes comunes [18] requiere incentivos para la cooperación. Adoptamos un enfoque teórico del juego para abordar este problema. En particular, utilizamos un modelo de dilema de prisioneros para capturar la tensión esencial entre la utilidad individual y social, las matrices de pago asimétricas para permitir transacciones asimétricas entre pares y un modelo dinámico de población basado en el aprendizaje [14] para especificar el comportamiento de los compañeros individuales que, quese puede cambiar continuamente. Si bien los dilemas sociales se han estudiado ampliamente, las aplicaciones P2P imponen un conjunto único de desafíos, que incluyen: • Grandes poblaciones y alta rotación: un sistema de intercambio de archivos como Gnutella y Kazaa puede superar los 100, 000 usuarios simultáneos, y los nodos pueden tener un promedio de vida de vida.-Tiempo del orden de las actas [33].• Asimetría de interés: las transacciones asimétricas de los sistemas P2P crean la posibilidad de asimetría de interés. En el ejemplo de la Figura 1, A quiere servicio de B, B quiere servicio de C y C quiere servicio de A. • Identidad de costo cero: muchos sistemas P2P permiten a los compañeros cambiar continuamente las identidades (es decir, Whitlwave). Las estrategias que funcionan bien en los juegos de dilema de prisioneros tradicionales como Tit-for-Tat [4] no funcionarán bien en el contexto P2P. Por lo tanto, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en una nueva función de decisión recíproca, para abordar estos desafíos y proporcionar diferentes compensaciones: • Discriminando la selección del servidor: la cooperación requiere familiaridad entre las entidades, ya sea directa o indirectamente. Sin embargo, las grandes poblaciones y la alta rotación de los sistemas P2P hacen que sea menos probable que ocurran interacciones repetidas con una entidad familiar. Mostramos que al hacer que cada compañero mantenga un historial privado de las acciones de otros pares hacia ella, y utilizando la selección discriminatoria del servidor, la función de decisión recíproca puede escalar a grandes poblaciones y niveles moderados de facturación.• Historia compartida: la escala a una mayor facturación y la mitigación de la asimetría de interés requiere un historial compartido. Considere el ejemplo en la Figura 1. Si todos brindan servicio, entonces el sistema funciona de manera óptima. Sin embargo, si todos mantienen solo la historia privada, nadie brindará servicio porque B no sabe que A ha servido C, etc. Mostramos que con la historia compartida, B sabe que A Servido C y, en consecuencia, servirá a A. Esto da como resultado un mayor nivel de cooperación que con la historia privada. El costo de la historia compartida es una infraestructura distribuida (por ejemplo, almacenamiento distribuido basado en la mesa) para almacenar el historial.• Reputación subjetiva basada en Maxflow: la historia compartida crea la posibilidad de colusión. En el ejemplo de la Figura 1, C puede afirmar falsamente que A le sirvió, engañando así a B para que brinde servicio. Mostramos que un algoritmo basado en el máximo flujo que calcula la reputación promueve subjetivamente la cooperación a pesar de la colusión entre 1/3 de la población. La idea básica es que B solo creería C si C ya hubiera brindado servicio a B. El costo del algoritmo MaxFlow es su tiempo de ejecución O (V 3), donde V es el número de nodos en el sistema. Para eliminar este costo, hemos desarrollado una variación de tiempo de ejecución media constante, que intercambia efectividad para la complejidad de la computación. Mostramos que el algoritmo basado en Maxflow escala mejor que la historia privada en presencia de coluders sin la confianza centralizada requerida en trabajos anteriores [9] [20].• Política adaptativa de extraños: las identidades de costo cero permiten a los compañeros no cooperantes escapar de las consecuencias de no cooperar y eventualmente destruir la cooperación en el sistema si no se detienen. Mostramos que si los compañeros recíprotrativos tratan a los extraños (pares sin historia) que usan una política que se adapta al comportamiento de extraños anteriores, los compañeros tienen pocos incentivos para blanquear y blanquear el sistema casi se pueden eliminar del sistema. La política de extraños adaptativos hace esto sin requerir la asignación centralizada de identidades, una tarifa de entrada para los recién llegados o la limitación de tarifas [13] [9] [25].• Historia a corto plazo: la historia también crea la posibilidad de que un compañero de comportamiento previamente bien bien bien con una buena reputación se convierta en traidor y use su buena reputación para explotar a otros pares. El compañero podría estar tomando una decisión estratégica o alguien puede haber secuestrado su identidad (por ejemplo, comprometiendo a su anfitrión). El historial a largo plazo exacerba este problema al permitir que los compañeros con muchas transacciones anteriores exploten esa historia para muchas transacciones nuevas. Mostramos que la historia a corto plazo evita que los traidores alteren la cooperación. El resto del documento está organizado de la siguiente manera. Describimos el modelo en la Sección 2 y la función de decisión reciprocativa en la Sección 3. Luego procedemos a las técnicas de incentivos en la Sección 4. En la Sección 4.1, describimos los desafíos de las grandes poblaciones y la alta rotación y mostramos la efectividad de discriminar la selección del servidor y el historial compartido. En la Sección 4.2, describimos la colusión y demostramos cómo la reputación subjetiva la mitiga. En la Sección 4.3, presentamos el problema de las identidades de costo cero y mostramos cómo una política adaptativa de extraños promueve las identidades persistentes. En la Sección 4.4, mostramos cómo los traidores interrumpen la cooperación y cómo la historia a corto plazo trata con ellos. Discutimos el trabajo relacionado en la Sección 5 y concluimos en la Sección 6. 2. Modelo y suposiciones En esta sección, presentamos nuestros supuestos sobre los sistemas P2P y sus usuarios, e presentamos un modelo que tiene como objetivo capturar el comportamiento de los usuarios en un sistema P2P.2.1 Suposiciones Asumimos un sistema P2P en el que los usuarios son estratégicos, es decir, actúan racionalmente para maximizar su beneficio. Sin embargo, para capturar parte de la imprevisibilidad de la vida real en el comportamiento de los usuarios, permitimos a los usuarios cambiar aleatoriamente su comportamiento con una probabilidad baja (ver Sección 2.4). Para simplificar, asumimos un sistema homogéneo en el que todos los pares emiten y satisfacen solicitudes a la misma tasa. Un par puede satisfacer cualquier solicitud y, a menos que se especifique lo contrario, los pares solicitan un servicio de manera uniforme al azar de la población. Finalmente, suponemos que todas las transacciones incurren en el mismo costo para todos los servidores y proporcionan el mismo beneficio a todos los clientes. Suponemos que los usuarios pueden contaminar el historial compartido con falsas recomendaciones (Sección 4.2), cambiar las identidades en costo cero (Sección 4.3) y falsificar a otros usuarios (Sección 4.4). No asumimos ninguna confianza centralizada o infraestructura centralizada.2.2 Modelo para ayudar al desarrollo y al estudio de los esquemas de incentivos, en esta sección presentamos un modelo de comportamientos de los usuarios. En particular, modelamos los beneficios y los costos de las interacciones P2P (el juego) y la dinámica de la población causada por la mutación, el aprendizaje y la rotación. Nuestro modelo está diseñado para tener las siguientes propiedades que caracterizan un gran conjunto de sistemas P2P: • Dilema social: la cooperación universal debería dar lugar a una utilidad general óptima, pero las personas que explotan la cooperación de los demás mientras no cooperan (es decir, defectos) deberían beneficiarmás que los usuarios que cooperan.• Transacciones asimétricas: un par puede querer el servicio de otro par, sin embargo, actualmente no puede proporcionar el servicio que el segundo compañero desea. Las transacciones deben poder tener pagos asimétricos.• Descubos no rastreables: un par no debe poder determinar la identidad de los compañeros que la han desertado. Esto modela la dificultad o el gasto de determinar que un compañero podría haber brindado un servicio, pero no lo hizo. Por ejemplo, en el sistema de intercambio de archivos Gnutella [21], un par puede simplemente ignorar las consultas a pesar de poseer el archivo deseado, evitando así que el par de consultas identifique el compañero de defectos.• Población dinámica: los compañeros deben poder cambiar su comportamiento e ingresar o abandonar el sistema de forma independiente y continua.1 La excepción se discute en la Sección 4.1.1 103 Defecto cooperado Cooperado Defectclient Server SC RR / SC ST / SC PP / SC TS / Figura 2: Matriz de pago para el dilema generalizado de los prisioneros. T, R, P y S representan la tentación, recompensa, castigo y sucker, respectivamente.2.3 Dilema generalizado de los prisioneros El dilema de los prisioneros, desarrollado por Flood, Dresher y Tucker en 1950 [22] es un juego repetido no cooperativo que satisface el requisito del dilema social. Cada juego consta de dos jugadores que pueden desertar o cooperar. Dependiendo de cómo cada uno actúe, los jugadores reciben una recompensa. Los jugadores usan una estrategia para decidir cómo actuar. Desafortunadamente, el trabajo existente usa una matriz de pago asimétrica específica o solo da la forma general para una simétrica [4]. En cambio, utilizamos el dilema generalizado de los prisioneros (GPD), que especifica la forma general para una matriz de pago asimétrica que preserva el dilema social. En el GPD, un reproductor es el cliente y un jugador es el servidor en cada juego, y es solo la decisión del servidor la que es significativa para determinar la salida de la transacción. Un jugador puede ser un cliente en un juego y un servidor en otro. El cliente y el servidor reciben el pago de una matriz de pago generalizada (Figura 2). RC, SC, TC y PC son los clientes que recompensan y RS, SS, TS y PS son los pagos de los servidores. Una matriz de pago GPD debe tener las siguientes propiedades para crear un dilema social: 1. La cooperación mutua conduce a pagos más altos que la deserción mutua (RS + RC> PS + PC).2. La cooperación mutua conduce a pagos más altos que un jugador que absorbe al otro (RS + RC> SC + TS y RS + RC> SS + TC).3. La deserción domina la cooperación (al menos débil) a nivel individual para la entidad que decide si cooperar o defectos: (TS ≥ RS y PS ≥ SS y (TS> RS o PS> SS)) el último conjunto de desigualdades supone que los clientesNo incurra en un costo independientemente de si cooperan o defectan y, por lo tanto, los clientes siempre cooperan. Estas propiedades corresponden a propiedades similares del dilema clásico de los prisioneros y permiten cualquier forma de transacción asimétrica mientras crea un dilema social. Además, una o más de las cuatro acciones posibles (el cliente coopere y defectos, y el servidor coopere y defectos) puede ser inducible. Si un jugador hace una acción imposible de rastrear, el otro jugador no conoce la identidad del primer jugador. Por ejemplo, para modelar una aplicación P2P como compartir archivos o enrutamiento de superposición, utilizamos los valores de matriz de pago específicos que se muestran en la Figura 3. Esto satisface las desigualdades especificadas anteriormente, donde solo el servidor puede elegir entre cooperar y desertar. Además, para esta matriz de pago en particular, los clientes no pueden rastrear las deserciones del servidor. Esta es la matriz de pago que usamos en nuestros resultados de simulación. Solicitud Servicio No solicite 7 / -1 0 /0 0 /0 0 /0 Proporcionar servicio Ignore la solicitud Servidor de cliente Figura 3: La matriz de pago para una aplicación como el intercambio de archivos P2P o el enrutamiento de superposición.2.4 Dinámica de la población Una característica de los sistemas P2P es que los compañeros cambian su comportamiento e ingresan o dejan el sistema de forma independiente y continua. Varios estudios [4] [28] de los repetidos juegos de dilema de los prisioneros usan un modelo evolutivo [19] [34] de la dinámica de la población. Un modelo evolutivo no es adecuado para los sistemas P2P porque solo especifica el comportamiento global y todos los cambios ocurren en momentos discretos. Por ejemplo, puede especificar que una población del 5 100% coopere jugadores y 5 100% de defectos de los jugadores evoluciona a una población con 3 y 7 jugadores, respectivamente. No especifica qué jugadores específicos cambiaron. Además, toda la conmutación ocurre al final de una generación en lugar de continuamente, como en un sistema P2P real. Como resultado, la dinámica de la población evolutiva no modela con precisión la rotación, los traidores y los extraños. En nuestro modelo, las entidades toman acciones independientes y continuas que cambian la composición de la población. En cada ronda, cada jugador juega un juego como cliente y un juego como servidor. Al final de una ronda, un jugador puede: 1) Mutado 2) Aprender, 3) Volumen de negocios, o 4) Mantente igual. Si un jugador muta, cambia a una estrategia elegida al azar. Si aprende, cambia a una estrategia que cree que producirá una puntuación más alta (descrita con más detalle a continuación). Si mantiene su identidad después de cambiar de estrategias, entonces se le conoce como traidor. Si un jugador sufre la facturación, deja el sistema y es reemplazada por un recién llegado que usa la misma estrategia que el jugador que exitaba. Para aprender, un jugador recopila información local sobre el rendimiento de diferentes estrategias. Esta información consiste en sus observaciones personales del rendimiento de la estrategia y las observaciones de los jugadores con los que interactúa. Esto modela a los usuarios que comunican fuera de banda sobre cómo funcionan las estrategias. Sea S el promedio de ejecución del rendimiento de la estrategia actual de los jugadores por ronda y la edad, la cantidad de rondas que ha estado utilizando la estrategia. Una calificación de Strategys es RunningAverage (S ∗ Age) RunningAverage (edad). Usamos la edad y calculamos el promedio de carrera antes de la relación para evitar que las muestras jóvenes (que tienen más probabilidades de ser atípicas) sesgar la calificación. Al final de una ronda, un jugador cambia a una estrategia mejor calificada con una probabilidad proporcional a la diferencia de puntaje entre su estrategia actual y la estrategia mejor calificada.104 3. Función de decisión reciprocativa En esta sección, presentamos la nueva función de decisión, recíprocatoria, que es la base de nuestras técnicas de incentivos. Una función de decisión se mapea de una historia de un jugador acciones a una decisión de cooperar o defectar a ese jugador. Una estrategia consiste en una función de decisión, un historial privado o compartido, un mecanismo de selección del servidor y una política de extraños. Nuestro enfoque de los incentivos es diseñar estrategias que maximicen el beneficio individual y social. Los usuarios estratégicos elegirán usar tales estrategias y, por lo tanto, conducir el sistema a altos niveles de cooperación. Dos ejemplos de funciones de decisión simples son el 100% cooperado y el 100% de defecto.100% cooperado modela a un usuario ingenuo que aún no se da cuenta de que está siendo explotada.100% Defect Models Un usuario codicioso que tiene la intención de explotar el sistema. En ausencia de técnicas de incentivos, los usuarios de defectos del 100% dominarán rápidamente el 100% cooperará a los usuarios y destruirán la cooperación en el sistema. Nuestros requisitos para una función de decisión son que (1) puede usar el historial compartido y subjetivo, (2) puede lidiar con desacecciones no rastreables y (3) es robusto contra diferentes patrones de deserción. Las funciones de decisión anteriores como Tit-for-Tat [4] e Image [28] (ver Sección 5) no satisfacen estos criterios. Por ejemplo, TIT-for-Tat e Image basan sus decisiones tanto en cooperaciones como en deserciones, por lo tanto, no pueden lidiar con deserciones imposibles de rastrear. En esta sección y las secciones restantes demostramos cómo las estrategias basadas en el recíproco satisfacen todos los requisitos indicados anteriormente. La probabilidad de que un jugador recíproco coopere con un par es una función de su generosidad normalizada. Medidas de generosidad El beneficio que una entidad ha proporcionado en relación con el beneficio que ha consumido. Esto es importante porque las entidades que consumen más servicios de los que brindan, incluso si proporcionan muchos servicios, harán que la cooperación colapse. Para alguna entidad I, deje que Pi y CI sean los servicios que he brindado y consumido, respectivamente. La entidad es la generosidad es simplemente la relación del servicio que brinda al servicio que consume: g (i) = pi/ci.(1) Una posibilidad es cooperar con una probabilidad igual a la generosidad. Aunque esto es efectivo en algunos casos, en otros casos, un jugador recíproco puede consumir más de lo que proporciona (por ejemplo, cuando inicialmente usa la política de defectos de extraños en 4.3). Esto hará que los jugadores recíprocos se deserten unos a otros. Para evitar esta situación, un jugador recíproco utiliza su propia generosidad como un palo de medición para juzgar la generosidad de sus compañeros. La entidad de medidas de generosidad normalizadas es generosidad en relación con la generosidad de la entidad JS. Más concretamente, la entidad se normaliza la generosidad según lo percibe la entidad J es GJ (i) = G (i)/G (J).(2) En el resto de esta sección, describimos nuestro marco de simulación y lo usamos para demostrar los beneficios de la función de decisión reciprocativa basal. Parámetro Valor nominal Sección Tamaño de la población 100 2.4 Tiempo de ejecución 1000 Rondas 2.4 Matriz de pago Compartir el archivo de la matriz 2.3 Ratio utilizando 100% Cooperar 1/3 3 Ratio utilizando el 100% de defecto 1/3 3 Ratio usando la probabilidad de mutación recíproca 1/3 3 0.0 2.4 Probabilidad de aprendizaje 0.052.4 Probabilidad de facturación 0.0001 2.4 Tasa de aciertos 1.0 4.1.1 Tabla 1: Parámetros de simulación predeterminados.3.1 Marco de simulación Nuestro simulador implementa el modelo descrito en la Sección 2. Utilizamos la matriz de pagos de intercambio de archivos asimétricos (Figura 3) con deserciones no rastreables porque modela las transacciones en muchos sistemas P2P como el intercambio de archivos y el reenvío de paquetes en las redes AD-hoc y superpuesto. Nuestro estudio de simulación está compuesto por diferentes escenarios que reflejan los desafíos de varios comportamientos no cooperativos. La Tabla 1 presenta los valores de parámetros nominales utilizados en nuestra simulación. La relación con filas se refiere a la relación inicial de la población total utilizando una estrategia particular. En cada escenario variamos el rango de valor de un parámetro específico para reflejar una situación o ataque particular. Luego variamos las propiedades exactas de la estrategia reciprocativa para defenderse de esa situación o ataque.3.2 Resultados de línea de base 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo de población (a) Total Población: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Cooperator de desertores Recipe. Figura privada 4: La evolución de las poblaciones de estrategia a lo largo del tiempo. Tiempo el número de rondas transcurridas. La población es el número de jugadores que usan una estrategia. En esta sección, presentamos la dinámica del juego para el escenario básico presentado en la Tabla 1 para familiarizar al lector y establecer una línea de base para escenarios más complicados. Las Figuras 4 (a) (60 jugadores) y (b) (120 jugadores) muestran a los jugadores que cambian a estrategias de puntuación más altas con el tiempo en dos carreras separadas del simulador. Cada punto en el gráfico representa el número de jugadores que usan una estrategia particular en un momento en el tiempo. Las figuras 5 (a) y (b) muestran la puntuación general media correspondiente por ronda. Esto mide el grado de cooperación en el sistema: 6 es el máximo posible (logrado cuando todos cooperan) y 0 es el mínimo (logrado cuando todos deserve). Desde la matriz de pago para compartir archivos, una red de 6 significa que todos pueden descargar un archivo y un 0 significa que nadie 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 Mediaverallscore/Tiempo redondo (a) Población total:60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Tiempo (b) Población total: 120 Figura 5: La puntuación media por ronda general con el tiempo.es capaz de hacerlo. Utilizamos esta métrica en todos los resultados posteriores para evaluar nuestras técnicas de incentivos. La Figura 5 (a) muestra que la estrategia recíproca que usa la historia privada hace que un sistema de 60 jugadores converja a un nivel de cooperación de 3.7, pero cae a 0.5 para 120 jugadores. Uno esperaría que el sistema de 60 jugadores alcance el nivel óptimo de cooperación (6) porque todos los desertores se eliminan del sistema. No lo hace por asimetría de interés. Por ejemplo, suponga que el Jugador B está utilizando el recíproco con la historia privada. El jugador A puede solicitar el servicio del Jugador B dos veces en sucesión sin proporcionar servicio al Jugador B mientras tanto. El jugador B no sabe del servicio que el jugador A ha brindado a otros, por lo que el jugador B rechazará el servicio al jugador A, a pesar de que el jugador A es cooperativo. Discutimos soluciones a la asimetría de interés y la falla del recíproco en el sistema de 120 reproductores en la Sección 4.1.4. Técnicas de incentivos basados en el recíproco En esta sección presentamos nuestras técnicas de incentivos y evaluamos su comportamiento por simulación. Para aclarar la exposición, agrupamos nuestras técnicas por los desafíos que abordan: grandes poblaciones y alta rotación (Sección 4.1), colusiones (Sección 4.2), identidades de costo cero (Sección 4.3) y traidores (Sección 4.4).4.1 Grandes poblaciones y una alta rotación Las grandes poblaciones y la alta rotación de los sistemas P2P hacen que sea menos probable que ocurran interacciones repetidas con una entidad familiar. En estas condiciones, basar las decisiones solo en la historia privada (registros sobre interacciones que el par ha estado involucrado directamente) no es efectivo. Además, la historia privada no trata bien con la asimetría de interés. Por ejemplo, si el jugador B ha cooperado con otros, pero no con el jugador A mismo en el pasado, el jugador A no tiene indicios de la generosidad del jugador BS, por lo que puede defectarse indebidamente de él. Proponemos dos mecanismos para aliviar el problema de pocas transacciones repetidas: la selección del servidor y el historial compartido.4.1.1 Selección del servidor Una forma natural de aumentar la probabilidad de interactuar con pares familiares es discriminar la selección del servidor. Sin embargo, la asimetría de las transacciones desafía los mecanismos de selección. A diferencia de la matriz de pago del dilema de los prisioneros, donde los jugadores pueden beneficiarse entre sí dentro de una sola transacción, las transacciones en GPD son asimétricas. Como resultado, un jugador que selecciona a su donante por segunda vez sin contribuir con ella mientras tanto puede enfrentar una deserción. Además, debido a la no identificación de las deserciones, es imposible mantener listas negras para evitar interacciones con desertores conocidos. Para lidiar con transacciones asimétricas, cada jugador contiene (tamaño fijo) listas de donantes anteriores y destinatarios anteriores, y selecciona un servidor de una de estas listas al azar con probabilidades iguales. De esta manera, los usuarios se acercan a sus destinatarios pasados y les dan la oportunidad de corresponder. En escenarios con usuarios selectivos omitimos la suposición de disponibilidad completa para evitar que los jugadores se agrupen en muchos grupos muy pequeños;Por lo tanto, suponemos que cada jugador puede realizar el servicio solicitado con probabilidad P (para los resultados presentados en esta sección, p = .3). Además, para evitar el sesgo a favor de los jugadores selectivos, todos los jugadores (incluidos los no discriminativos) seleccionan servidores para juegos. La Figura 6 demuestra la efectividad del mecanismo de selección propuesto en escenarios con grandes tamaños de población. Fijamos la relación inicial de recíproco en la población (33%) mientras variamos el tamaño de la población (entre 24 y 1000) (observe que, mientras que en las Figuras 4 (a) y (b), los puntos de datos demuestran la evolución del sistema sobreTiempo, cada punto de datos en esta figura es el resultado de una simulación completa para un escenario específico). La figura muestra que la función de decisión reciprocativa que utiliza la historia privada junto con el comportamiento selectivo puede escalar a grandes poblaciones. En la Figura 7 fijamos el tamaño de la población y variamos la tasa de rotación. Demuestra que si bien el comportamiento selectivo es efectivo para las bajas tasas de rotación, a medida que la facturación aumenta, el comportamiento selectivo no escala. Esto ocurre porque la selección solo es efectiva siempre que los jugadores del pasado permanezcan vivos durante el tiempo suficiente para que puedan ser seleccionados para futuros juegos.4.1.2 Historia compartida Para mitigar la asimetría de interés y escala a una tasa de facturación más alta, es necesario en la historia compartida. La historia compartida significa que cada par mantiene registros sobre todas las interacciones que ocurren en el sistema, independientemente de si estaba directamente involucrado en ellas o no. Permite a los jugadores aprovechar las experiencias de otros en casos de pocas transacciones repetidas. Solo requiere que alguien haya interactuado con un jugador en particular para que toda la población lo observe, por lo tanto, escala mejor a grandes poblaciones y altas pérdidas de balón, y también tolera la asimetría de interés. Algunos ejemplos de esquemas de historia compartida son [20] [23] [28]. La Figura 7 muestra la efectividad de la historia compartida bajo altas tasas de rotación. En esta figura, fijamos el tamaño de la población y variamos la tasa de rotación. Si bien los jugadores selectivos con historia privada solo pueden tolerar una facturación moderada, compartió la historia de las pérdidas de balón de hasta aproximadamente 0.1. Esto significa que el 10% de los jugadores abandonan el sistema al final de cada ronda. En la Figura 6 fijamos la rotación y variamos el tamaño de la población. Muestra que el historial compartido hace que el sistema converja con una cooperación y rendimiento óptimos, independientemente del tamaño de la población. Estos resultados muestran que el historial compartido aborda los tres desafíos de las grandes poblaciones, la alta rotación y la asimetría de las transacciones. Sin embargo, la historia compartida tiene dos desventajas. Primero, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MediaverallScore/Round Numplayers Compartido no SEL Private No Sel Private Selective Figura 6: Privado vs. Historia compartida en función del tamaño de la población.0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 MediaverallScore/Round Ratever no Sel No Sel Private No Sel Private Selective Figura 7: Performance del mecanismo de selección bajo facturación. El eje X es la tasa de rotación. El eje Y es el puntaje medio por ronda general.Si bien una implementación descentralizada de la historia privada es directa, la implementación de la historia compartida requiere gastos generales o centralización de comunicación. Se puede implementar un historial compartido descentralizado, por ejemplo, además de un DHT, utilizando un sistema de almacenamiento entre pares [36] o difundiendo información a otras entidades de manera similar a los protocolos de enrutamiento. Segundo, y la historia compartida más fundamental es vulnerable a la colusión. En la siguiente sección proponemos un mecanismo que aborde este problema.4.2 Colusión y otros ataques de historia compartida 4.2.1 Colusión Si bien la historia compartida es escalable, es vulnerable a la colusión. La colusión puede ser positiva (por ejemplo, las entidades de defectos afirman que otras entidades de defectos cooperaron con ellas) o negativas (por ejemplo, las entidades afirman que otras entidades cooperativas desertaron). La colusión subvierte cualquier estrategia en la que todos en el sistema estén de acuerdo con la reputación de un jugador (reputación objetiva). Un ejemplo de reputación objetiva es utilizar la función de decisión recíproca con un historial compartido para contar el número total de cooperaciones que un jugador ha dado y recibido de todas las entidades del sistema;Otro ejemplo es la estrategia de imagen [28]. El efecto de la colusión se amplía en sistemas con identidades de costo cero, donde los usuarios pueden crear identidades falsas que informan declaraciones falsas. En cambio, para lidiar con la colusión, las entidades pueden calcular la reputación subjetiva, donde el jugador A pesa las opiniones del jugador BS en función de cuánto jugador fideos un jugador B. Nuestro algoritmo subjetivo se basa en MaxFlow [24] [32]. Maxflow es un problema teórico de gráfico, que dado un gráfico dirigido con bordes ponderados, pregunta cuál es la mayor tasa a la que el material se puede enviar desde la fuente al objetivo sin violar ninguna restricción de capacidad. Por ejemplo, en la Figura 8, cada borde está etiquetado con la cantidad de tráfico que puede viajar sobre él. El algoritmo MaxFlow calcula la cantidad máxima de tráfico que puede pasar de la (s) fuente (s) al objetivo (t) sin violar las restricciones. En este ejemplo, a pesar de que hay un bucle de bordes de alta capacidad, el flujo máximo entre la fuente y el objetivo es solo 2 (los números en los soportes representan el flujo real en cada borde en la solución).100 (0) 1 (1) 5 (1) S T 10 (1) 100 (1) 1 (1) 100 (1) 20 (0) Figura 8: Cada borde en el gráfico está etiquetado con su capacidad y el flujo realLleva entre paréntesis. El máximo flujo entre la fuente y el destino en el gráfico es 2. C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figura 9: Este gráfico ilustra la robustez del máximo de flujo en presencia de coluders que informan valores falsos de reputación altas. Aplicamos el algoritmo Maxflow construyendo un gráfico cuyos vértices son entidades y los bordes son los servicios que las entidades se han recibido entre sí. Esta información se puede almacenar utilizando los mismos métodos que el historial compartido. Un máximo de flujo es el mayor nivel de reputación que la fuente puede dar al fregadero sin violar las limitaciones de la capacidad de reputación. Como resultado, los nodos que informan deshonestamente valores de reputación altos no podrán subvertir el sistema de reputación. La Figura 9 ilustra un escenario en el que todos los coludentes (etiquetados con C) informan altos valores de reputación entre sí. Cuando el nodo A calcula la reputación subjetiva de B utilizando el algoritmo Maxflow, no se verá afectado por los valores locales de reputación falsa, sino que el máximo flujo en este caso será 0. Esto se debe a que no se ha recibido ningún servicio de ninguno de los Colluders.107 En nuestro algoritmo, el beneficio que ha recibido (indirectamente) de la entidad J es el máximo de Jlow de J a I.Por el contrario, el beneficio de que la entidad que he proporcionado indirectamente a J es el máximo de flujo de i a j. La reputación subjetiva de la entidad j como se percibe por i es: min maxflow (j a i) maxflow (i to j), 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MENTOOverallScore/La población redonda compartió la Figura 10 privada: Historia subjetiva compartida en comparación con la historia compartida objetiva y la historia privada en presencia de coluders. Algoritmo 1 ConstantTimemaxFlow Lound el tiempo medio de ejecución de MaxFlow a una constante.método ctmaxflow (self, src, dst) 1: self.surplus ← self.surplus + self.incement {Use la media de ejecución como predicción.} 2: if random ()> (0.5 ∗ self.surplus/self.mean iterations.) Entonces 3: no devuelve Ninguno {no es suficiente excedente para ejecutar.} 4: Fin si {Obtenga el flujo y el número de iteraciones utilizadas desde el máximo alg.} 5: flujo, iteraciones ← maxflow (self.g, src, dst) 6: self.surplus ← self.surplus - iteraciones {mantenga una media ejecutiva del número de iteraciones utilizadas.} 7: self.mean iterations ← self.α ∗ self.sean iterations + (1 - self.α) ∗ iteraciones 8:Flujo de retorno El costo de MaxFlow es su largo tiempo de ejecución. El algoritmo estándar de preflowpush maxflow tiene el peor de los casos de ejecución de O (v 3). En cambio, usamos el Algoritmo 1 que tiene un tiempo de ejecución medio constante, pero a veces no devuelve ningún flujo a pesar de que existe uno. La idea esencial es limitar el número medio de nodos examinados durante el cálculo del flujo máximo. Esto limita la sobrecarga, pero también limita la efectividad. A pesar de esto, los resultados a continuación muestran que una función de decisión reciprocativa basada en Maxflow escala a poblaciones más altas que una que usa historia privada. La Figura 10 compara la efectividad de la reputación subjetiva con la reputación objetiva en presencia de coluders. En estos escenarios, los desertores coluden al afirmar que otros coludentes que encuentran les dieron 100 cooperaciones para ese encuentro. Además, los parámetros para el algoritmo 1 se establecen de la siguiente manera: incremento = 100, α = 0.9. Como en secciones anteriores, el recíproco con la historia privada da como resultado una cooperación hasta un punto, más allá del cual falla. La diferencia aquí es que la historia compartida objetiva falla para todos los tamaños de población. Esto se debe a que los jugadores recíprocos cooperan con los Colluders debido a su alta reputación. Sin embargo, la historia subjetiva puede alcanzar altos niveles de cooperación independientemente de los coluders. Esto se debe a que no hay rutas de alto peso en el gráfico de cooperación de coluders a cualquier no coludencia, por lo que el máximo flujo de un coludero a cualquier no coludor es 0. Por lo tanto, un jugador recíproco subjetivo concluirá que Colluder no le ha brindado ningún servicio y rechazará el servicio al coludero. Por lo tanto, el algoritmo Maxflow permite al recíproco mantener la escalabilidad de la historia compartida sin ser vulnerable a la colusión o requerir confianza centralizada (por ejemplo, pares de confianza). Dado que vinculamos el tiempo de ejecución del algoritmo Maxflow, la cooperación disminuye a medida que aumenta el tamaño de la población, pero el punto clave es que la función de decisión recíproca subjetiva escala a poblaciones más altas que una que usa historia privada. Esta ventaja solo aumenta con el tiempo a medida que aumenta la potencia de la CPU y se pueden dedicar más ciclos a ejecutar el algoritmo de flujo máximo (al aumentar el parámetro de incremento). A pesar de la robustez del algoritmo Maxflow a la forma simple de colusión descrita anteriormente, todavía tiene vulnerabilidades a ataques más sofisticados. Uno es que una entidad (el topo) brinde servicio y luego se encuentre positivamente sobre otros coluders. Los otros Colluders pueden explotar su reputación para recibir el servicio. Sin embargo, la efectividad de este ataque se basa en la cantidad de servicio que brinda el lunar. Dado que el topo está pagando todo el costo de proporcionar servicio y no recibir nada de beneficio, tiene un fuerte incentivo para dejar de coludir y probar otra estrategia. Esto obliga a los coluderos a usar mecanismos para mantener la cooperación dentro de su grupo, lo que puede impulsar el costo de la colusión para exceder el beneficio.4.2.2 Informes falsos Otro ataque es que un desertor mienta sobre recibir o prestar servicio a otra entidad. Hay cuatro acciones posibles sobre las cuales se pueden mentir: proporcionar servicio, no proporcionar servicio, recibir servicio y no recibir servicio. Afirmar falsamente de recibir el servicio es el simple ataque de colusión descrito anteriormente. Reclamar falsamente no haber prestado servicio no proporciona ningún beneficio para el atacante. Afirmar falsamente de haber prestado servicio o no haberlo recibido le permite a un atacante aumentar su propia reputación y/o reducir la reputación de otra entidad. Una entidad puede querer reducir la reputación de otras entidades para desanimar a otros a seleccionarla y usar exclusivamente su servicio. Estas afirmaciones falsas son claramente identificables en la historia compartida como inconsistencias en las que una entidad afirma que ocurrió una transacción y otra afirma que no lo hizo. Para limitar este ataque, modificamos el algoritmo máximo de flujo para que una entidad siempre crea la entidad que está más cerca de él en el gráfico de flujo. Si ambas entidades son igualmente distantes, entonces el borde en disputa en el flujo no es crítico para la evaluación y se ignora. Esta modificación impide aquellos casos en los que el atacante está haciendo afirmaciones falsas sobre una entidad que está más cerca que ella a la entidad evaluadora, lo que le impide impulsar su propia reputación. Las posibilidades restantes son que el atacante afirme falsamente haber prestado servicio a o no haberlo recibido de una entidad víctima que está más lejos del evaluador que ella. En estos casos, un atacante solo puede reducir la reputación de la víctima. La efectividad de hacer esto está limitada por la cantidad de servicios prestados y recibidos por el atacante, lo que hace que ejecutar este ataque sea costoso.108 4.3 Identidades de costo cero El historial supone que las entidades mantienen identidades persistentes. Sin embargo, en la mayoría de los sistemas P2P, las identidades son de costo cero. Esto es deseable para el crecimiento de la red, ya que alienta a los recién llegados a unirse al sistema. Sin embargo, esto también permite que los usuarios de mal comportamiento escapar de las consecuencias de sus acciones cambiando a nuevas identidades (es decir, blanqueo). Los blanqueadores pueden hacer que el sistema colapse si no son castigados adecuadamente. Desafortunadamente, un jugador no puede decir si un extraño es un blanqueador o un recién llegado legítimo. Siempre cooperar con extraños alienta a los recién llegados a unirse, pero al mismo tiempo alienta el comportamiento de blanqueo. Siempre desertar a los extraños evita que el blanqueo, pero desalienta a los recién llegados a unirse y también puede iniciar ciclos desfavorables de deserción. Esta tensión sugiere que cualquier política más extraña que tenga una probabilidad fija de cooperar con extraños fallará al ser demasiado tacaño cuando la mayoría de los extraños son recién llegados o demasiado generosos cuando la mayoría de los extraños son blanqueadores. Nuestra solución es la política de extraños adaptativos de Stranger. La idea es ser generoso para extraños cuando están siendo generosos y tacaños cuando son tacaños. Deje que PS y CS sean el número de servicios que los extraños han brindado y consumido, respectivamente. La probabilidad de que un jugador que use Stranger Adaptive ayude a un extraño es PS/CS. Sin embargo, no deseamos mantener estos recuentos de forma permanente (por razones descritas en la Sección 4.4). Además, los jugadores pueden no saber cuándo los extraños defectos porque las deserciones no se pueden rastrear (como se describe en la Sección 2). En consecuencia, en lugar de mantener PS y CS, suponemos que K = PS + CS, donde K es una constante y mantenemos la relación de ejecución r = PS/CS. Cuando necesitamos incrementar PS o CS, generamos los valores actuales de PS y CS de K y R: CS = K/(1 + R) PS = CS ∗ R Luego calculamos la nueva R como sigue: R = (PS+ 1)/cs, si el extraño proporcionó el servicio r = ps/(cs + 1), si el servicio consumido por el extraño, este método nos permite mantener una relación de ejecución que refleja la reciente generosidad de extraños sin saber cuándo han desertado extraños.0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MediaverAllscore/Round Ratever Stranger Coopere Defect Stranger Stranger Figura 11: Diferentes políticas más extrañas para recíprocos con historia compartida. El eje X es la tasa de rotación en una escala de registro. El eje Y es el puntaje medio por ronda general. Las Figuras 11 y 12 comparan la efectividad de la estrategia recíproca utilizando diferentes políticas hacia extraños. Figura 11 0 1 2 3 4 5 6 0.0001 0.001 0.01 0.1 1 MediaverAllscore/Round Ratever Stranger Coopere Defecto extraño Stranger Adaptativo Figura 12: Diferentes políticas más extrañas para recíprocos con historia privada. El eje X es la tasa de rotación en una escala de registro. El eje Y es el puntaje medio por ronda general.Compara diferentes políticas de extraños para el recíproco con la historia compartida, mientras que la Figura 12 es con la historia privada. En ambas cifras, los jugadores que usan la estrategia de defectos del 100% cambian su identidad (blanqueo) después de cada transacción y son indistinguibles de los recién llegados legítimos. Los jugadores reciprocativos que usan la política de Coopere Stranger no logran la cooperación. Esta política de extraños permite que los blanqueadores maximicen su recompensa y, en consecuencia, proporciona un alto incentivo para que los usuarios cambien a blanqueo. En contraste, la Figura 11 muestra que la política de defectos de los extraños es efectiva con la historia compartida. Esto se debe a que los blanqueadores siempre parecen ser extraños y, por lo tanto, los jugadores reciprocativos siempre los desertarán. Esto es consistente con el trabajo previo [13] que demuestra que castigar a los extraños se ocupa de los blanqueadores. Sin embargo, la Figura 12 muestra que el defecto más extraño no es efectivo con la historia privada. Esto se debe a que el recíproco requiere cierta cooperación inicial para el arranque. En el caso de la historia compartida, un jugador recíproco puede observar que otro jugador ya ha cooperado con otros. Con la historia privada, el jugador recíproco solo conoce las acciones de los otros jugadores hacia ella. Por lo tanto, la deserción inicial dictada por la Política de defectos de los extraños conducirá a deserciones posteriores, lo que evitará que los jugadores recíprocos cooperen entre sí. En otras simulaciones que no se muestran aquí, la política de Stranger Defect Stranger falla incluso con el historial compartido cuando no hay jugadores iniciales del 100% cooperado. La Figura 11 muestra que con la historia compartida, la política adaptativa de extraños funciona, así como la política de defectos de los extraños hasta que la tasa de rotación es muy alta (el 10% de la población se dio la vuelta después de cada transacción). En estos escenarios, Stranger Adaptive está usando K = 10 y cada jugador mantiene una R privada.Más importante aún, es significativamente mejor que la política de defectos de extraños con la historia privada porque puede arrancar la cooperación. Aunque la política de defectos más extraños es marginalmente más efectiva que la adaptación más extraña a tasas muy altas de rotación, es poco probable que los sistemas P2P funcionen allí porque otros servicios (por ejemplo, enrutamiento) tampoco pueden tolerar una facturación muy alta. Llegamos a la conclusión de que de las políticas más extrañas que hemos explorado, Stranger Adaptive es el más efectivo. Mediante el uso de Stranger Adaptive, los sistemas P2P con identidades de costo cero y una rotación suficientemente baja pueden mantener la cooperación sin una asignación centralizada de identidades.109 4.4 Traidores Traidores son jugadores que adquieren puntuaciones de alta reputación al cooperar por un tiempo, y luego se convierten traidoriamente en desertores antes de abandonar el sistema. Modelan a ambos usuarios que se vuelven deliberadamente para obtener una puntuación más alta y los cooperadores cuyas identidades han sido robadas y explotadas por los desertores. Una estrategia que mantiene la historia a largo plazo sin discriminar entre acciones antiguas y recientes se vuelve altamente vulnerable a la explotación por parte de estos traidores. Los dos gráficos principales en la Figura 13 demuestran el efecto de los traidores en la cooperación en un sistema donde los jugadores mantienen la historia a largo plazo (nunca la historia clara). En estas simulaciones, corremos para 2000 rondas y permitimos a los jugadores cooperativos mantener sus identidades al cambiar a la estrategia del 100% del desertor. Usamos los valores predeterminados para los otros parámetros. Sin traidores, las estrategias cooperativas prosperan. Con los traidores, las estrategias cooperativas prosperan hasta que un cooperador se convierte en traidor después de 600 rondas. A medida que este cooperador explota su reputación para lograr una puntuación alta, otros jugadores cooperativos notan esto y hacen lo mismo a través del aprendizaje. La cooperación finalmente se derrumba. Por otro lado, si mantenemos historia a corto plazo y/o descuentos en la historia antigua frente a la historia reciente, los traidores pueden detectarse rápidamente, y el nivel de cooperación general se mantiene alto, como se muestra en los dos gráficos inferiores en la Figura 130 20 40 60 80 100 1K 2K Holección a largo plazo No Traidor Población 0 20 40 60 80 100 1K 2K Traidores Cooperator Defector Recipe. Compartido 0 20 40 60 80 100 1K 2K Población de tiempo a corto plazo 0 20 40 60 80 100 1K 2K Tiempo Figura 13: Mantener a largo plazo frente a la historia a corto plazo con y sin traidores.5. Trabajo relacionado El trabajo anterior ha examinado el problema de incentivos aplicados a las sociedades en general y más recientemente a aplicaciones de Internet y sistemas de igual a igual en particular. Un fenómeno bien conocido en este contexto es la tragedia de los bienes comunes [18] donde los recursos son inocentes debido a usuarios egoístas que viajan libremente en los recursos de los sistemas, y es especialmente común en grandes redes [29] [3]. El problema se ha estudiado ampliamente adoptando un enfoque teórico del juego. El modelo de dilema de prisioneros proporciona un marco natural para estudiar la efectividad de las diferentes estrategias para establecer la cooperación entre los jugadores. En un entorno de simulación con muchos juegos repetidos, identidades persistentes y sin colusión, Axelrod [4] muestra que la estrategia TIT-for-Tat domina. Nuestro modelo asume que el crecimiento sigue el aprendizaje local en lugar de la dinámica evolutiva [14], y también permite más tipos de ataques. Nowak y Sigmund [28] introducen la estrategia de imagen y demuestran su capacidad para establecer la cooperación entre los jugadores a pesar de las pocas transacciones repetidas por el empleo de la historia compartida. Los jugadores que usan imágenes cooperan con jugadores cuyo recuento global de cooperaciones menos deserciones excede algún umbral. Como resultado, un jugador de imagen es vulnerable a los desertores parciales (si el umbral se establece demasiado bajo) o no coopera con otros reproductores de imagen (si el umbral se establece demasiado). En los últimos años, los investigadores han utilizado la teoría del diseño del mecanismo económico para abordar el problema de cooperación en aplicaciones de Internet. El diseño del mecanismo es el inverso de la teoría del juego. Pregunta cómo diseñar un juego en el que el comportamiento de los jugadores estratégicos resulte en el resultado socialmente deseado. El diseño de mecanismo algorítmico distribuido busca soluciones dentro de este marco que están completamente distribuidos y computacionalmente manejables [12].[10] y [11] son ejemplos de aplicar DAMD al enrutamiento BGP y costos compartidos de multidifusión. Más recientemente, Damd también se ha estudiado en entornos dinámicos [38]. En este contexto, demostrar la superioridad de una estrategia cooperativa (como en el caso de nuestro trabajo) es consistente con el objetivo de incentivar el comportamiento deseado entre los jugadores egoístas. Los desafíos únicos impuestos por los sistemas de igual a igual inspiraron un cuerpo adicional de trabajo [5] [37], principalmente en el contexto del reenvío de paquetes en el enrutamiento inalámbrico ad-hoc [8] [27] [30] [35], yCompartir archivos [15] [31]. Friedman y Resnick [13] consideran el problema de las identidades de costo cero en entornos en línea y descubren que en tales sistemas castigar a todos los recién llegados es inevitable. Usando un modelo teórico, demuestran que dicho sistema puede converger a la cooperación solo para tasas de rotación suficientemente bajas, lo que nuestros resultados confirman.[6] y [9] muestran que el blanqueo y la colusión pueden tener graves consecuencias para los sistemas entre pares y son difíciles de prevenir en un sistema totalmente descentralizado. Algunos clientes comerciales para compartir archivos [1] [2] proporcionan mecanismos de incentivos que se aplican al dificultar que el usuario modifique el código fuente. Estos mecanismos pueden ser evitados por un usuario experto o por una empresa competitiva que libera un cliente compatible sin las restricciones de incentivos. Además, estos mecanismos aún son vulnerables a las identidades y la colusión de costo cero. BitTorrent [7] utiliza TIT-for-Tat como método para la asignación de recursos, donde una tasa de carga de los usuarios dicta su tasa de descarga.6. Conclusiones En este documento adoptamos un enfoque teórico del juego para el problema de la cooperación en las redes entre pares. Al abordar los desafíos impuestos por los sistemas P2P, incluidas las grandes poblaciones, la alta rotación, la asimetría de interés y las identidades de costo cero, proponemos una familia de técnicas de incentivos escalables y robustas, basadas en la función de decisión recíproca, para apoyar el comportamiento cooperativo y mejorar el sistema generalactuación. Encontramos que la adopción de la historia compartida y las técnicas de selección del servidor discriminatorias pueden mitigar el desafío de pocas transacciones repetidas que surgen debido al gran tamaño de la población, una alta rotación y asimetría de interés. Además, la cooperación se puede establecer incluso en presencia de identidades de costo cero mediante el uso de una política adaptativa hacia los extraños. Finalmente, los coluderos y los traidores pueden mantenerse en control a través de la reputación subjetiva y el historial a corto plazo, respectivamente.110 7. Agradecimientos Agradecemos a Mary Baker, T.J.Giuli, Petros Mania, el revisor anónimo, y nuestro pastor, Margo Seltzer, por sus útiles comentarios que ayudaron a mejorar el documento. Este trabajo es apoyado en parte por la National Science Foundation bajo los Premios ITR ANI-0085879 y ANI-0331659, y el premio de carrera ANI-0133811. Las opiniones y conclusiones contenidas en este documento son las de los autores y no deben interpretarse como que representen las políticas oficiales, ya sea expresadas o implícitas, de NSF o el gobierno de los Estados Unidos.8. Referencias [1] Kazaa.http://www.kazaa.com.[2] Limewire.http://www.limewire.com.[3] Adar, E. y Huberman, B. Primer lunes 5, 10 (octubre de 2000).[4] Axelrod, R. La evolución de la cooperación. Basic Books, 1984. [5] Buragohain, C., Agrawal, D. y Suri, S. Un marco teórico de juego para incentivos en sistemas P2P. En la Conferencia Internacional sobre Computación de Peer-To-Peer (septiembre de 2003).[6] Castro, M., Druschel, P., Ganesh, A., Rowstron, A. y Wallach, D. S. Seguridad para redes de superposición de igual a igual estructuras. En Actas de la computación multimedia y las redes 2002 (MMCN 02) (2002).[7] Cohen, B. Los incentivos construyen robustez en Bittorrent. En el primer taller sobre economía de sistemas de pares (2003).[8] Crowcroft, J., Gibbens, R., Kelly, F. y ˘ Ostring, S. Incentivos de modelado para la colaboración en redes ad-hoc móviles. En modelado y optimización en redes móviles, ad-hoc e inalámbrica (2003).[9] Douceur, J. R. El ataque Sybil. En Actas electrónicas del Taller Internacional sobre Sistemas de Peer-To-Peer (2002).[10] Feigenbaum, J., Papadimitriou, C., Sami, R. y Shenker, S. Un mecanismo basado en BGP para el enrutamiento de menor costo. En Actas del Simposio ACM sobre los principios de la computación distribuida (2002).[11] Feigenbaum, J., Papadimitriou, C. y Shenker, S. Compartiendo el costo de las transmisiones de multidifusión. En Journal of Computer and System Sciences (2001), vol.63, pp. 21-41.[12] Feigenbaum, J. y Shenker, S. Diseño de mecanismo algorítmico distribuido: resultados recientes y direcciones futuras. En Actas del Taller Internacional sobre algoritmos y Métodos discretos para la computación y comunicaciones móviles (2002).[13] Friedman, E. y Resnick, P. El costo social de los seudónimos baratos. Journal of Economics and Management Strategy 10, 2 (1998), 173-199.[14] Fudenberg, D. y Levine, D. K. La teoría del aprendizaje en los juegos. The MIT Press, 1999. [15] Golle, P., Leyton-Brown, K., Mironov, I. y Lillibridge, M. Incentivos para compartir en redes entre pares. En Actas de la tercera conferencia de ACM sobre comercio electrónico, octubre de 2001 (2001).[16] Gross, B. y Acquisti, A. Saldos de poder en eBay: ¿compañeros o incidentes? En Taller sobre Economía de Redes de Pesos (2003).[17] Gu, B. y Jarvenpaa, S. ¿Son las contribuciones a los foros técnicos P2P bienes privados o públicos?- Una investigación empírica. En el primer taller sobre economía de sistemas de pares (2003).[18] Hardin, G. La tragedia de los Comunes. Science 162 (1968), 1243-1248.[19] Josef Hofbauer y Karl Sigmund. Juegos evolutivos y dinámica de la población. Cambridge University Press, 1998. [20] Kamvar, S. D., Schlosser, M. T. y Garcia-Molina, H. El algoritmo Eigentrust para la gestión de la reputación en las redes P2P. En Actas de la Duodécima Conferencia Internacional de la World Wide (mayo de 2003).[21] Kan, G. Peer-to-peer: aprovechando el poder de las tecnologías disruptivas, 1ª ed. Oreilly & Associates, Inc., marzo de 2001, cap. Gnutella, pp. 94-122.[22] Kuhn, S. Dilema de prisioneros. En la Enciclopedia de Filosofía de Stanford, Edward N. Zalta, ed., Summer Ed.2003. [23] Lee, S., Sherwood, R. y Bhattacharjee, B. Grupos de pares cooperativos en Niza. En Actas del IEEE Infocom (2003).[24] Levien, R. y Aiken, A. Métricas de confianza resistentes a los ataques para la certificación de clave pública. En Proceedings of the Usenix Security Symposium (1998), pp. 229-242.[25] Mania, P., Roussopoulos, M., Giuli, T. J., Rosenthal, D. S. H., Baker, M. y Muliadi, Y. Preservar las réplicas de pares por votación de muestreo de velocidad limitada. En Simposio ACM sobre principios de sistemas operativos (2003).[26] Marti, S., Giuli, T. J., Lai, K. y Baker, M. Mitigating Ruting Mal comportamiento en redes ad-hoc móviles. En Proceedings of Mobicom (2000), pp. 255-265.[27] Michiardi, P. y Molva, R. Un enfoque teórico del juego para evaluar los mecanismos de aplicación de la cooperación en redes ad-hoc móviles. En modelado y optimización en redes móviles, ad-hoc e inalámbrica (2003).[28] Nowak, M. A. y Sigmund, K. Evolución de la reciprocidad indirecta por puntuación de imágenes. Nature 393 (1998), 573-577.[29] Olson, M. La lógica de la acción colectiva: bienes públicos y la teoría de los grupos. Harvard University Press, 1971. [30] Raghavan, B. y Snoeren, A. Reenvío prioritario en redes ad-hoc con partes autoinetrestidas. En Taller sobre Economía de Sistemas de Pesos (junio de 2003).[31] Ranganathan, K., Ripeanu, M., Sarin, A. y Foster, I. Para compartir o no para compartir: un análisis de incentivos para contribuir en entornos de intercambio de archivos colaborativos. En Taller sobre Economía de Sistemas de Pesos (junio de 2003).[32] Reiter, M. K. y Stubblebine, S. G. Análisis y diseño métrico de autenticación. Transacciones ACM sobre información y seguridad del sistema 2, 2 (1999), 138-158.[33] Saroiu, S., Gummadi, P. K. y Gribble, S. D. Un estudio de medición de sistemas de intercambio de archivos entre pares. En Actas de la computación multimedia y las redes 2002 (MMCN 02) (2002).[34] Smith, J. M. Evolution y la teoría de los juegos. Cambridge University Press, 1982. [35] Urpi, A., Bonuccelli, M. y Giordano, S. Cooperación de modelado en redes ad-hoc móviles: una descripción formal del egoísmo. En modelado y optimización en redes móviles, ad-hoc e inalámbrica (2003).[36] Vishnumurthy, V., Chandrakumar, S. y Sirer, E. G. Karma: un marco económico seguro para el intercambio de recursos P2P. En Taller sobre Economía de Redes de Pesos (2003).[37] Wang, W. y Li, B. Para jugar o controlar: un enfoque teórico de control basado en el juego para la ingeniería de incentivos entre pares. En Taller Internacional sobre Calidad del Servicio (junio de 2003).[38] Woodard, C. J. y Parkes, D. C. Mecanismos a prueba de estrategias para la formación de redes ad-hoc. En Taller sobre Economía de Sistemas de Pesos (junio de 2003).111