Generación rápida de fragmentos de resultados en la búsqueda web Andrew Turpin & Yohannes Tsegay Rmit University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking Csiro ICT Center Canberra, Australia David.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA. hughw@microsoft.com Resumen La presentación de fragmentos de documentos sesgados de consulta como parte de las páginas de resultados presentadas por los motores de búsqueda se ha convertido en una expectativa de los usuarios de los motores de búsqueda. En este artículo exploramos los algoritmos y las estructuras de datos requeridas como parte de un motor de búsqueda para permitir una generación eficiente de fragmentos sesgados de consulta. Comenzamos proponiendo y analizando un método de compresión de documentos que reduce el tiempo de generación de fragmentos en un 58% sobre una línea de base utilizando la biblioteca de compresión ZLIB. Estos experimentos revelan que encontrar documentos sobre el almacenamiento secundario domina el costo total de generar fragmentos y, por lo tanto, los documentos de almacenamiento en caché en RAM son esenciales para un proceso de generación de fragmentos rápidos. Usando la simulación, examinamos el rendimiento de la generación de fragmentos para cachés RAM de diferentes tamaños. Finalmente, proponemos y analizamos la reordenamiento de documentos y la compactación, revelando un esquema que aumenta el número de éxitos de caché de documentos con solo un efecto marginal en la calidad del fragmento. Este esquema duplica efectivamente el número de documentos que pueden caber en un caché de tamaño fijo. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información;H.3.4 [Almacenamiento y recuperación de información]: Sistemas y evaluación de rendimiento de software (eficiencia y efectividad);Algoritmos de términos generales, experimentación, medición, rendimiento 1. Introducción Cada resultado en la lista de resultados de búsqueda entregada por los motores de búsqueda WWW actuales como Search.yahoo.com, Google.com y Search.msn.com generalmente contiene el título y la URL del documento real, enlaces a versiones en vivo y almacenadas en caché del documentoy a veces una indicación de tamaño y tipo de archivo. Además, generalmente se presentan uno o más fragmentos, lo que le da al buscador una vista previa del contenido del documento. Los fragmentos son fragmentos cortos de texto extraídos del contenido del documento (o sus metadatos). Pueden ser estáticos (por ejemplo, siempre muestren las primeras 50 palabras del documento, o el contenido de sus metadatos de descripción, o una descripción tomada de un sitio de directorio como dmoz.org) o sesgo de consulta [20]. Un fragmento con sesgo de consulta se extrae selectivamente sobre la base de su relación con la consulta de los buscadores. La adición de fragmentos informativos a los resultados de búsqueda puede aumentar sustancialmente su valor para los buscadores. Los fragmentos precisos permiten al buscador tomar buenas decisiones sobre qué resultados valen la pena acceder y cuáles pueden ignorarse. En el mejor de los casos, los fragmentos pueden obviar la necesidad de abrir cualquier documento proporcionando directamente la respuesta a los buscadores que necesitan información real, como los datos de contacto de una persona o una organización. Generación de fragmentos con sesgo de consulta mediante la indexación de motores de búsqueda web de la orden de diez mil millones de páginas web y manejar cientos de millones de consultas de búsqueda por día impone una carga computacional muy significativa (recordando que cada búsqueda generalmente genera diez fips). El enfoque de forma simple de mantener una copia de cada documento en un archivo y generar fragmentos abriendo y escaneando archivos, funciona cuando las tasas de consulta son bajas y las colecciones son pequeñas, pero no escala en el grado requerido. La sobrecarga de abrir y leer diez archivos por consulta además de acceder a la estructura de índice para ubicarlos, sería manifiestamente excesivo bajo una carga de consulta pesada. Incluso almacenar diez mil millones de archivos y los cientos de terabytes de datos correspondientes están fuera del alcance de los sistemas de archivos tradicionales. Se han creado sistemas de archivos de propósito especial para abordar estos problemas [6]. Tenga en cuenta que la utilidad de los fragmentos de ninguna manera está restringida a las aplicaciones de búsqueda de todo WEB. La generación eficiente de fragmentos también es importante a escala de servicios de búsqueda de todo el gobierno como www.firstgov.gov (c. 25 millones de páginas) y GovSearch.australia.gov.au (c. 5 millones de páginas) y dentro de grandesempresas como IBM [2] (c. 50 millones de páginas). Los fragmentos pueden ser aún más útiles en las aplicaciones de búsqueda de bases de datos o sistemas de archivos en las que no hay información útil de URL o título. Presentamos un nuevo algoritmo y una estructura compacta de un solo archivo diseñada para una generación rápida de fragmentos de alta calidad y comparamos su rendimiento de espacio/tiempo con una línea de base obvia basada en el compresor ZLIB en varios conjuntos de datos. Reportamos la proporción del tiempo dedicado a las búsqueda de disco, lecturas de disco y procesamiento de CPU;Demostrar que el tiempo para localizar cada documento (buscar tiempo) domina, como se esperaba. Como el tiempo para procesar un documento en RAM es pequeño en comparación con la ubicación y la lectura del documento en la memoria, puede parecer que no se requiere compresión. Sin embargo, esto solo es cierto si no hay caché de documentos en la RAM. El control de la RAM de los sistemas físicos para la experimentación es difícil, por lo tanto, usamos la simulación para mostrar que los documentos de almacenamiento en caché mejoran drásticamente el rendimiento de la generación de fragmentos. A su vez, cuanto más se puedan comprimir los documentos, más se puede ajustar en el caché y, por lo tanto, se puede evitar cuanto más se puedan evitar el disco: la compresión de datos clásica que se explota en estructuras de archivos invertidos y las listas de documentos clasificadas en la computación [24]. Como la presentación de la memoria caché del documento es importante, examinamos la compactación del documento, a diferencia de la compresión, los esquemas imponiendo un pedido a priori de las oraciones dentro de un documento, y luego solo permitiendo oraciones líderes en caché para cada documento. Esto lleva a más ahorros de tiempo, con solo un impacto marginal en la calidad de los fragmentos que se devuelven.2. La generación del fragmento de trabajo relacionado es un tipo especial de resumen de documentos extractivos, en el que las oraciones o fragmentos de oraciones se seleccionan para su inclusión en el resumen sobre el grado en que coinciden con la consulta de búsqueda. Este proceso recibió el nombre de la resumen sesgada por la consulta por Tombros y Sanderson [20] El lector se remite a Mani [13] y a Radev et al.[16] Para descripción general de las muchas aplicaciones diferentes de resumen y para los métodos igualmente diversos para producir resúmenes. Los primeros motores de búsqueda web presentaron fragmentos independientes de la consulta que consisten en los primeros k bytes del documento de resultados. Generar esto es claramente mucho más simple y mucho menos costoso que el procesamiento de documentos para extraer resúmenes sesgados de consulta, ya que no es necesario buscar en el documento los fragmentos de texto que contienen términos de consulta. Hasta donde sabemos, Google fue el primer motor de búsqueda de todo tuweb en proporcionar resúmenes sesgados de consulta, pero Brin y la página [1] solo se encuentran bajo el encabezado del trabajo futuro. La mayor parte del trabajo experimental utilizando resumen sesgado de consultas se ha centrado en comparar su valor con los buscadores en relación con otros tipos de resumen [20, 21], en lugar de una generación eficiente de resúmenes. A pesar de la importancia de la generación de resumen eficiente en la búsqueda web, pocos algoritmos aparecen en la literatura. Silber y McKoy [19] describen un algoritmo de encadenamiento léxico de tiempo lineal para su uso en resúmenes genéricos, pero no ofrecen datos empíricos para el rendimiento de su algoritmo. White et al [21] informan algunos tiempos experimentales de su sistema webDocsum, pero los algoritmos de generación de fragmentos en sí mismos no están aislados, por lo que es difícil inferir el tiempo de generación de fragmentos comparables a los tiempos que informamos en este documento.3. Arquitecturas de motores de búsqueda Un motor de búsqueda debe realizar una variedad de actividades, y se compone de muchos subsistemas, como lo representan las cajas de la Figura 1. Tenga en cuenta que puede haber varios otros subsistemas, como el motor de publicidad o el motor de análisis que se podría agregar fácilmente al diagrama, pero nos hemos concentrado en los subsistemas que son relevantes para la generación de fragmentos. Dependiendo de la cantidad de documentos que los indexan el motor de búsqueda, los datos y los procesos para cada motor de clasificación que se arrastra el motor del motor del motor del motor Léxico Léxico Meta Data Engine Motor Snippet Term & Doc#S SpippetPerdoc Web Consuly Consulta Resultados Página Término#S DOC#S Invertidos Docs PerdocTítulo, URL, ETC Doc#S Documento Meta Data Términos Consulta Término#S Figura 1: Una abstracción de algunos de los subsistemas en un motor de búsqueda. Dependiendo del número de documentos indexados, cada subsistema podría residir en una sola máquina, distribuirse en miles de máquinas, o una combinación de ambas.El subsistema podría distribuirse a través de muchas máquinas, o todas ocupan un solo servidor y sistema de archivos, compitiendo entre sí por los recursos. Del mismo modo, puede ser más eficiente combinar algunos subsistemas en una implementación del diagrama. Por ejemplo, los metadatos, como el título del documento y la URL, requieren un cálculo mínimo además de resaltar las palabras de consulta, pero observamos que es probable que la búsqueda de disco se minimice si el título, la URL y la información de resumen fija se almacenan contiguamente con el texto desde el cual la consulta.Se extraen resúmenes sesgados. Aquí ignoramos el texto fijo y consideramos solo la generación de resúmenes sesgados de consulta: nos concentramos en el motor del fragmento. Además de los datos y programas que operan en esos datos, cada subsistema también tiene su propio esquema de gestión de memoria. El sistema de gestión de memoria puede ser simplemente la jerarquía de memoria proporcionada por el sistema operativo utilizado en máquinas en el subsistema, o puede codificarse explícitamente para optimizar los procesos en el subsistema. Hay muchos documentos sobre el almacenamiento en caché en los motores de búsqueda (ver [3] y las referencias en ellas para un resumen actual), pero parece razonable que haya un caché de consulta en el motor de consulta que almacena páginas de resultados finales precomputados para consultas muy populares. Cuando se emite una de las consultas populares, la página de resultados se obtiene directamente del caché de consulta. Si la consulta emitida no está en la memoria caché de la consulta, entonces el motor de consulta usa los cuatro subsistemas a su vez para ensamblar una página de resultados.1. El motor Léxico mapea los términos de consulta a enteros.2. El motor de clasificación recupera listas invertidas para cada término, usándolas para obtener una lista clasificada de documentos.3. El motor del fragmento utiliza esos números de documentos y números de términos de consulta para generar fragmentos.4. El motor de meta data obtiene otra información sobre cada documento para construir la página de resultados. En un documento dividido en una oración por línea, y una secuencia de términos de consulta.1 para cada línea del texto, l = [w1, w2 ,..., wm] 2 Sea h 1 si l es un encabezado, 0 de lo contrario.3 Sea 2 si L es la primera línea de un documento, 1 si es la segunda línea, 0 de lo contrario.4 Sea C el número de WI que son términos de consulta, contando repeticiones.5 Sea D el número de términos de consulta distintos que coinciden con algunos WI.6 Identifique la ejecución contigua más larga de términos de consulta en L, digamos WJ...WJ+K.7 Use una combinación ponderada de C, D, K, H y para obtener una puntuación s.8 Inserte L en un máximo de montón usando S como clave. Elimine el número de oraciones requeridas del montón para formar el resumen. Figura 2: Ranker de oración simple que opera en texto sin procesar con una oración por línea.4. El motor de fragmento para cada identificador de documento pasado al motor del fragmento, el motor debe generar texto, preferiblemente que contenga términos de consulta, que intentan resumir ese documento. El trabajo previo sobre el resumen identifica la oración como la unidad mínima para la extracción y presentación al usuario [12]. En consecuencia, también asumimos que un proceso de extracción de fragmentos web extraerá oraciones de los documentos. Para construir un fragmento, todas las oraciones en un documento deben clasificarse contra la consulta, y los dos o tres principales regresaron como el fragmento. La puntuación de las oraciones contra las consultas se ha explorado en varios documentos [7, 12, 18, 20, 21], con diferentes características de las oraciones consideradas importantes. Según estas observaciones, la Figura 2 muestra el algoritmo general para calificar oraciones en documentos relevantes, con las oraciones más altas de puntuación que hacen el fragmento para cada documento. El puntaje final de una oración, asignada en el Paso 7, puede derivarse de muchas maneras diferentes. Para evitar el sesgo con cualquier mecanismo de puntuación en particular, comparamos la calidad de las oraciones más adelante en el documento utilizando los componentes individuales de la puntuación, en lugar de una combinación arbitraria de los componentes.4.1 Analización de documentos web A diferencia de las colecciones de texto bien editadas que a menudo son el objetivo de los sistemas de resumen, los datos web a menudo están mal estructurados, mal puntuados y contienen muchos datos que no forman parte de oraciones válidas que serían candidatos para partes de fragmentos. Suponemos que los documentos pasados al motor del fragmento por el motor de indexación tienen todas las etiquetas HTML y JavaScript, y que cada documento se reduce a una serie de tokens de palabras separados por tokens no palabras. Definimos un token de palabras como una secuencia de caracteres alfanuméricos, mientras que una no palabra es una secuencia de caracteres no alfanuméricos como Whitpace y los otros símbolos de puntuación. Ambos se limitan a un máximo de 50 caracteres. Los caracteres adyacentes y repetidos se eliminan de la puntuación. ¿Se incluye en el conjunto de puntuación un marcador de finalización especial de la oración que reemplaza los tres terminadores habituales de oraciones?! ... A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como <br> y <p> terminan las oraciones. Además, una oración debe contener al menos cinco palabras y no más de veinte palabras, con oraciones más largas o más cortas que se rompen y se unen según lo requerido para cumplir con estos criterios [10]. Etiquetas HTML no terminadas: es decir, etiquetas con un aparato ortopédico abierto, pero no hay una núcleo cercana de todo el texto desde la abrazadera abierta a la siguiente abrazadera abierta para descartarse. Un problema importante al resumir las páginas web es la presencia de grandes cantidades de material promocional y de navegación (barras de navegación) visualmente arriba y a la izquierda del contenido de la página real. Por ejemplo, la compañía más maravillosa del mundo. El material de navegación similar, pero a menudo no idéntico, generalmente se presenta en cada página dentro de un sitio. Este material tiende a reducir la calidad de los resúmenes y ralentizar la generación de resumen. En nuestros experimentos, no utilizamos ninguna heurística particular para eliminar la información de navegación, ya que la recopilación de pruebas en uso (WT100G) está previamente con la realización generalizada del estilo actual de publicación web. En WT100G, el tamaño promedio de la página web es más de la mitad del promedio web actual [11]. Anecdóticamente, el aumento se debe a la inclusión de elementos sofisticados de navegación e interfaz y las funciones de JavaScript para apoyarlos. Habiendo definido el formato de documentos que se presentan al motor del fragmento, ahora definimos nuestro esquema de almacenamiento de documentos del sistema de token comprimido (CTS), y el sistema de referencia utilizado para la comparación.4.2 Motor de fragmento de línea de base Un esquema obvio de representación del documento es simplemente comprimir cada documento con un compresor adaptativo bien conocido, y luego descomprimir el documento según sea necesario [1], utilizando un algoritmo de coincidencia de cadenas para efectuar el algoritmo en la Figura 2. En consecuencia, implementamos dicho sistema, utilizando ZLIB [4] con parámetros predeterminados para comprimir cada documento después de haber sido analizado como en la Sección 4.1. Cada documento se almacena en un solo archivo. Si bien es manejable para nuestras pequeñas colecciones de pruebas o pequeñas empresas con millones de documentos, un motor de búsqueda web completo puede requerir múltiples documentos para habitar archivos únicos o un sistema de archivos de propósito especial [6]. Para la generación del fragmento, los documentos requeridos se descomprimen uno a la vez, y se emplea una búsqueda lineal de los términos de consulta proporcionados. La búsqueda está optimizada para nuestra tarea específica, que está restringida a las palabras completas coincidentes y la oración que termina el token, en lugar de la coincidencia de patrones generales.4.3 El motor del fragmento CTS varias optimizaciones sobre la línea de base son posibles. El primero es emplear un método de compresión semiestática en toda la recopilación de documentos, que permitirá una descompresión más rápida con una pérdida de compresión mínima [24]. El uso de un enfoque semistático implica mapear palabras y no palabras producidas por el analizador a tokens enteros individuales, con símbolos frecuentes que reciben enteros pequeños y luego eligen un esquema de codificación que asigna a pequeños números un pequeño número de bits. Las palabras y las no palabras se alternan estrictamente en el archivo comprimido, que siempre comienza con una palabra. En este caso, simplemente asignamos a cada símbolo su número ordinal en una lista de símbolos ordenados por frecuencia. Utilizamos el esquema de codificación VBYTE para codificar la palabra tokens [22]. El conjunto de no palabras se limita a las 64 secuencias de puntuación más comunes en la colección en sí, y están codificadas con un código binario plano de 6 bits. Los 2 bits restantes de cada símbolo de puntuación se utilizan para almacenar información de capitalización. El proceso de calcular el modelo semiestático se complica por el hecho de que el número de palabras y no palabras que aparecen en grandes colecciones web es alto. Si almacenamos todas las palabras y no palabras que aparecen en la colección, y su frecuencia asociada, se requerirían muchos gigabytes de RAM o un árbol B o una estructura similar en el disco [23]. Moffat et al.[14] han examinado esquemas para los modelos de poda durante la compresión utilizando alfabetos grandes, y concluyen que los términos que rara vez ocurren no necesitan residir en el modelo. Más bien, se explican términos raros en el archivo comprimido final, utilizando un token de palabra especial (símbolo de escape), para señalar su ocurrencia. Durante el primer pase de codificación, se guardan dos colas de movimiento hacia adelante;uno para palabras y otro para no palabras. Cada vez que se consume la memoria disponible y se descubre un nuevo símbolo en la colección, se descarta un símbolo existente desde el final de la cola. En nuestra implementación, aplicamos la condición más estricta en el desalojo de que, cuando sea posible, el símbolo desalojado debe tener una frecuencia de uno. Si no hay símbolo con frecuencia uno en la última mitad de la cola, entonces desalojamos los símbolos de frecuencia dos, y así sucesivamente hasta que haya suficiente espacio disponible en el modelo para el nuevo símbolo. El segundo pase de codificación reemplaza cada palabra con su número codificado Vbyte, o el símbolo de escape y una representación ASCII de la palabra si no está en el modelo. Del mismo modo, cada secuencia que no es de palabras se reemplaza con su codeword, o el codeword para un solo carácter espacial si no está en el modelo. Observamos que esta compresión sin pérdidas de no palabras es aceptable cuando los documentos se usan para la generación de fragmentos, pero puede no ser aceptable para una base de datos de documentos. Suponemos que un subsistema separado contendría documentos en caché para otros fines donde la puntuación exacta es importante. Si bien este esquema semiestático debería permitir una descompresión más rápida que la línea de base, también permite la coincidencia directa de los términos de consulta como enteros comprimidos en el archivo comprimido. Es decir, las oraciones se pueden calificar sin tener que descomprimir un documento, y solo las oraciones devueltas como parte de un fragmento deben decodificarse. El sistema CTS almacena todos los documentos contiguamente en un archivo, y una tabla auxiliar de enteros de 64 bits que indica el desplazamiento de inicio de cada documento en el archivo. Además, debe tener acceso a la asignación inversa de números de términos, lo que permite que esas palabras no se explicen en el documento se recuperen y devuelvan al motor de consulta como cadenas. La primera de estas estructuras de datos se puede dividir y distribuir fácilmente si el motor del fragmento ocupa múltiples máquinas;Sin embargo, el segundo no se divide tan fácilmente, ya que cualquier documento en una máquina remota puede requerir acceso a todo el mapeo entero a la cuerda. Esta es la segunda razón para emplear el paso de poda del modelo durante la construcción del código semiestático: limita el tamaño de la tabla de mapeo inverso que debe estar presente en cada máquina que implementa el motor de fragmento.4.4 Evaluación experimental de CTS Todos los experimentos informados en este documento se ejecutaron en un servidor Sun Fire V210 que ejecuta Solaris 10. La máquina consta de procesadores ultrasparc IIII duales de 1.34 GHz y 4 GB de WT10g WT50G WT100G No. Docs.(× 106) 1.7 10.1 18.5 Texto crudo 10, 522 56, 684 102, 833 línea de base (Zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Tabla 1: Espacio de almacenamiento total (MB) para documentos para las tres colecciones de prueba comprimidas y sin comprimir.0 20 40 60 60 0.00.20.40.60.8 Consultas agrupadas en 100s de tiempo (segundos) 0 20 40 60 0.00.20.40.60.8 Consultas agrupadas en 100s tiempo (segundos) 0 20 40 60 0.00.20.40.60.8 Queridas agrupadas en 100s en el tiempo (segundos (segundos (segundos) CTS de línea de base con CTS de almacenamiento en caché sin almacenamiento en caché Figura 3: Tiempo para generar fragmentos para 10 documentos por consulta, promediado sobre cubos de 100 consultas, para las primeras 7000 consultas excitadas en WT10G. Todo el código fuente se compiló usando GCC4.1.1 con -O9 Optimización. Los tiempos se ejecutaron en una máquina desocupada y se promediaron en 10 corridas, con memoria enjuagada entre ejecuciones para eliminar cualquier almacenamiento en caché de archivos de datos. En ausencia de evidencia de lo contrario, suponemos que es importante modelar secuencias de llegada de consultas realistas y la distribución de repeticiones de consulta para nuestros experimentos. En consecuencia, las colecciones de prueba que carecen de registros de consultas reales, como TREC ad-hoc y .gov2 no se consideraron adecuadas. La obtención de registros de consulta extensos y los DOC de resultados asociados para una colección grande correspondiente no es fácil. Hemos utilizado dos colecciones (WT10G y WT100G) de la pista web de TREC [8] junto con consultas de los registros de Excite del mismo período (c. 1997). Además, también utilizamos una colección de tamaño mediano WT50G, obtenida muestran aleatoriamente la mitad de los documentos de WT100G. Las dos primeras filas de la Tabla 1 dan el número de documentos y el tamaño en MB de estas colecciones. Las dos filas finales de la Tabla 1 muestran el tamaño de los conjuntos de documentos resultantes después de la compresión con los esquemas de línea de base y CTS. Como se esperaba, CTS admite una pequeña pérdida de compresión sobre ZLIB, pero ambos reducen sustancialmente el tamaño del texto a aproximadamente el 20% del tamaño original y sin comprimir. Tenga en cuenta que las cifras para CTS no incluyen la asignación inversa de token entero a cadena que se requiere para producir los fragmentos finales, ya que ocupa RAM. Es 1024 MB en estos experimentos. El motor de búsqueda Zettair [25] se utilizó para producir una lista de documentos para resumir para cada consulta. Para la mayoría de los experimentos, se utilizó el esquema de puntuación OKAPI BM25 para determinar las clasificaciones de documentos. Para los experimentos de almacenamiento de almacenamiento estático reportados en la Sección 5, la puntuación de cada documento WT10g WT50G WT100G BASED 75 157 183 CTS 38 70 77 Reducción en el tiempo 49% 56% 58% Tabla 2: Tiempo promedio (MSEC) para las preguntas finales de 7000 en las cuentas finales en elExcite registros utilizando los sistemas de línea de base y CTS en las 3 colecciones de prueba.es un promedio ponderado de 50:50 del puntaje BM25 (normalizado por el documento de puntuación superior para cada consulta) y un puntaje para cada documento independiente de cualquier consulta. Esto es para simular los efectos de los algoritmos de clasificación como PageRank [1] en la distribución de solicitudes de documentos al motor del fragmento. En nuestro caso, utilizamos el recuento de acceso normalizado [5] calculado desde los 20 documentos principales que regresaron a los primeros 1 millón de consultas del registro de excite para determinar el componente de puntaje independiente de la consulta. Los puntos en la Figura 3 indican el tiempo medio de ejecución para generar 10 fragmentos para cada consulta, promediado en grupos de 100 consultas, para las primeras 7000 consultas en el registro de consultas de excita. Solo se muestran los datos para WT10G, pero las otras colecciones mostraron patrones similares. El eje x indica el grupo de 100 consultas;Por ejemplo, 20 indica las consultas 2001 a 2100. Claramente, hay un efecto de almacenamiento en caché, con tiempos que caen sustancialmente después de que se procesan las primeras 1000 consultas más o menos. Todo esto se debe a los bloques de disco de almacenamiento en caché del sistema operativo y quizás prefabando datos antes de solicitudes de lectura específicas. Esto es evidente porque el sistema de referencia no tiene grandes estructuras de datos internos para aprovechar el almacenamiento en caché no basado en el disco, simplemente abre y procesa archivos, y la velocidad es evidente para el sistema de referencia. Parte de esta ganancia se debe a la localidad espacial de las referencias de disco generadas por la transmisión de consulta: las consultas repetidas ya tendrán sus archivos de documentos almacenados en caché en la memoria;y similares consultas diferentes que devuelven los mismos documentos se beneficiarán del almacenamiento en caché de documentos. Pero cuando el registro se procesa después de eliminar todas, excepto la primera solicitud, para cada documento, la aceleración pronunciada sigue siendo evidente a medida que se procesan más consultas (no se muestran en la figura). Esto sugiere que el sistema operativo (o el disco en sí) está leyendo y almacenando una mayor cantidad de datos que la cantidad solicitada y que esto brinda beneficio con suficiente frecuencia para marcar una diferencia apreciable en los tiempos de generación de fragmentos. Esto se confirma por la curva etiquetada con CTS sin almacenamiento en caché, que se generó después de montar el sistema de archivos con una opción de caché (directio en Solaris). Con el almacenamiento en caché del disco apagado, el tiempo promedio para generar fragmentos varía poco. El tiempo para generar diez fragmentos para una consulta, promediado sobre las finales consultas de 7000 en el registro de excite a medida que se han disipado los efectos de almacenamiento en caché, se muestran en la Tabla 2. Una vez que el sistema se ha estabilizado, CTS es más del 50% más rápido que el sistema de referencia. Esto se debe principalmente a los CTS que coinciden con enteros individuales para la mayoría de las palabras de consulta, en lugar de comparar cadenas en el sistema de referencia. La Tabla 3 muestra un desglose del tiempo promedio para generar diez fragmentos en las finales consultas de 7000 del registro de excite en la colección WT50G cuando se procesan los documentos completos, y cuando solo se procesa la primera mitad de cada documento. Como se puede ver, la mayoría del tiempo dedicado a generar un fragmento es localizar el documento en el disco (buscar): 64% para documentos completos y 75% para la mitad de los documentos. Incluso si el monto de procesamiento de un documento debe ser% de DOC procesado para buscar puntaje de lectura y decodificación 100% 45 4 21 50% 45 4 11 Tabla 3: Tiempo para generar 10 fragmentos para una sola consulta (MSEC) para la colección WT50G promediada sobre elConsultas finales de Excite 7000 cuando se procesa todo cada documento (100%) o solo la primera mitad de cada documento (50%).Someterse a la mitad, como en la segunda fila de la tabla, solo hay una reducción del 14% en el tiempo total requerido para generar un fragmento. Como la ubicación de documentos en el almacenamiento secundario ocupa una proporción tan grande del tiempo de generación de fragmentos, parece lógico tratar de reducir su impacto a través del almacenamiento en caché.5. El almacenamiento en caché de documentos en la Sección 3 Observamos que el motor del fragmento tendría su propia RAM en proporción al tamaño de la recopilación de documentos. Por ejemplo, en un motor de búsqueda completo de Web, el motor de fragmento se distribuiría a través de muchas estaciones de trabajo, cada una con al menos 4 GB de RAM. En una pequeña empresa, el motor del fragmento puede estar compartiendo RAM con todos los demás subsistemas en una sola estación de trabajo, por lo tanto, solo tiene 100 MB disponibles. En esta sección usamos simulación para medir el número de golpes de caché en el motor de fragmento a medida que varía el tamaño de la memoria. Comparamos dos políticas de almacenamiento en caché: un caché estático, donde el caché se carga con tantos documentos como puede contener antes de que el sistema comience a responder consultas, y luego nunca cambia;y un caché de uso menos reciente, que comienza como para el caché estático, pero cada vez que se accede a un documento se mueve al frente de una cola, y si se obtiene un documento desde el disco, el último elemento en la cola se desalija. Tenga en cuenta que los documentos se cargan primero en los cachés en orden de disminución de la puntuación independiente de consulta, que se calcula como se describe en la Sección 4.4. Las simulaciones también suponen que existe un caché de consulta para las consultas más frecuentes de Q, y que el motor de fragmento nunca procesa estas consultas. Todas las consultas pasadas a las simulaciones provienen de la segunda mitad del registro de consultas de excita (la primera mitad se usa para calcular puntajes independientes de consulta), y se detienen, se detienen y tienen sus términos ordenados alfabéticamente. Esta alteración final simplemente permite que consultas como Red Dog y Dog Red devuelvan los mismos documentos, como sería el caso en un motor de búsqueda donde los operadores de frases explícitas se requerirían en la consulta para hacer cumplir el orden y la proximidad. La Figura 4 muestra el porcentaje de acceso al documento que alcanzó el caché utilizando los dos esquemas de almacenamiento en caché, con Q 0 o 10,000, en 535,276 consultas de excite en WT100G. El xaxis muestra el porcentaje de documentos que se mantienen en el caché, por lo que el 1.0% corresponde a aproximadamente 185,000 documentos. A partir de esta cifra, está claro que el almacenamiento en caché incluso un pequeño porcentaje de los documentos tiene un gran impacto en reducir el tiempo de búsqueda para la generación de fragmentos. Con el 1% de los documentos almacenados en caché, se evitan alrededor de 222 MB para la colección WT100G, se evitan alrededor del 80% de las buscas de disco. El caché estático funciona sorprendentemente bien (cuadrados en la Figura 4), pero lo supera el caché LRU (círculos). Sin embargo, en una implementación real de LRU, puede haber fragmentación del caché a medida que los documentos se intercambian dentro y fuera. La razón del gran impacto del caché del documento es 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Tamaño de la caché ( % de la colección) % de los accesorioscachehits lru q = 0 lru q = 10,000 estáticos q = 0 estática q = 10,000 Figura 4: porcentaje de laTiempo en el que el motor del fragmento no tiene que ir al disco para generar un fragmento trazado contra el tamaño del caché del documento como porcentaje de todos los documentos en la colección. Los resultados son de una simulación en WT100G con 535,276 consultas excitadas.Eso, para una colección en particular, es mucho más probable que aparezcan algunos documentos en las listas de resultados que en otros. Este efecto ocurre en parte debido a la distribución de frecuencia de consulta aproximadamente Zipfian, y en parte porque la mayoría de los motores de búsqueda web emplean métodos de clasificación que combinan puntajes basados en la consulta con puntajes estáticos (a priori) determinados a partir de factores como medidas de gráficos de enlace, características de URL, puntajes de spam y spam y spam.Entonces en [17]. Es mucho más probable que los documentos con puntajes estáticos altos se recuperen que otros. Además del caché del documento, la RAM del motor del fragmento también debe contener la tabla de decodificación de CTS que mapea los enteros en las cuerdas, que está limitada por un parámetro en el tiempo de compresión (1 GB en nuestros experimentos aquí). Esto es más que compensado por el tamaño reducido de cada documento, lo que permite más documentos en el caché del documento. Por ejemplo, el uso de CTS reduce el tamaño promedio del documento de 5.7 kb a 1.2 kb (como se muestra en la Tabla 1), por lo que una RAM de 2 GB podría tener 368,442 documentos sin comprimir (2% de la colección), o 850,691 documentos más una descompresión de 1 GBTabla (5% de la colección). De hecho, una mayor experimentación con el tamaño del modelo revela que el modelo puede ser muy pequeño y aún CTS ofrece una buena compresión y tiempos de puntuación rápidas. Esto se evidencia en la Figura 5, donde el tamaño comprimido de WT50G se muestra en los símbolos sólidos. Tenga en cuenta que cuando no se usa compresión (el tamaño del modelo es de 0 MB), la colección es de solo 31 GB como marcado HTML, JavaScript, y la puntuación repetida se ha descartado como se describe en la Sección 4.1. Con un modelo de 5 MB, el tamaño de la colección cae en más de la mitad a 14 GB, y aumentar el tamaño del modelo a 750 MB solo provoca una caída de 2 GB en el tamaño de la recolección. La Figura 5 también muestra el tiempo promedio de anotar y decodificar un fragmento (excluyendo el tiempo de búsqueda) con los diferentes tamaños del modelo (símbolos abiertos). Una vez más, hay una gran velocidad cuando se usa un modelo de 5 MB, pero poco 0 200 400 600 15202530 Tamaño del modelo (MB) de colección (GB) o Tiempo (MSEC) Tiempo (GB) (MSEC) Figura 5: Tamaño de recolección de la recolección de la recolección de la recolección de la recolección deLa colección WT50G cuando se comprime con CTS utilizando diferentes límites de memoria en el modelo, y el tiempo promedio para generar un fragmento único, excluyendo el tiempo de búsqueda en consultas de excite 20000 utilizando esos modelos.Mejora con modelos más grandes. Resultados similares se mantienen para la colección WT100G, donde un modelo de aproximadamente 10 MB ofrece un espacio sustancial y ahorros de tiempo en ningún modelo, pero los retornos disminuyen a medida que aumenta el tamaño del modelo. Además de la compresión, hay otro enfoque para reducir el tamaño de cada documento en el caché: no almacene el documento completo en caché. Más bien, almacenan oraciones que probablemente se usen en fragmentos en el caché, y si durante la generación de fragmentos en un documento en caché, las puntuaciones de las oraciones no alcanzan un cierto umbral, luego recupere todo el documento del disco. Esto plantea preguntas sobre cómo elegir oraciones de documentos para poner en caché y qué dejar en el disco, que abordamos en la siguiente sección.6. Las oraciones de reordenamiento de oraciones dentro de cada documento se pueden reordenar para que las oraciones que muy probablemente aparezcan en los fragmentos estén en la parte delantera del documento, por lo tanto, se procesan primero en el momento de la consulta, mientras que las oraciones menos probables se relegan a la parte posterior del documento. Luego, durante el tiempo de consulta, si se encuentran k oraciones con un puntaje que excede algún umbral antes de procesarse todo el documento, se ignora el resto del documento. Además, para mejorar el almacenamiento en caché, solo la cabeza de cada documento puede almacenarse en el caché, con la cola que reside en el disco. Tenga en cuenta que suponemos que el motor de búsqueda debe proporcionar copias en caché de un documento, es decir, el texto exacto del documento tal como estaba indexado, entonces esto sería atendido por otro subsistema en la Figura 1, y no por el alteradoCopiar, almacenamos en el motor del fragmento. Ahora presentamos cuatro enfoques de reordenamiento de oraciones.1. Ordene natural Las primeras oraciones de un documento bien escrita generalmente describen mejor el contenido del documento [12]. Por lo tanto, simplemente procesar un documento para producir un fragmento de calidad. Desafortunadamente, sin embargo, los documentos web a menudo no se escriben bien, con pocas habilidades de escritura editorial o profesional de la creación de una obra de mérito literario. Más importante aún, tal vez, es que estamos produciendo fragmentos con sesgo de consultas, y no hay garantía de que aparezcan términos de consulta en las oraciones hacia el frente de un documento.2. Términos significativos (ST) Luhn introdujeron el concepto de una oración significativa que contiene un grupo de términos significativos [12], un concepto que funciona bien por Tombros y Sanderson [20]. Sea FD, T la frecuencia del término T en el documento D, entonces se determina que el término T es significativo si FD, t ≥ 8 <: 7 - 0.1 × (25 - SD), si SD <25 7, si 25 ≤ SD≤ 40 7 + 0.1 × (SD - 40), de lo contrario, donde SD es el número de oraciones en el documento d.Una sección entre corchetes se define como un grupo de términos en los que los términos más a la izquierda y a la derecha son términos significativos, y no se dividen términos significativos en la sección entre paréntesis por más de cuatro términos no significativos. La puntuación de una sección entre corchetes es el cuadrado del número de palabras significativas que caen en la sección, dividida por el número total de palabras en toda la oración. El puntaje a priori para una oración se calcula como el máximo de todos los puntajes para las secciones entre corchetes de la oración. Luego ordenamos las oraciones por este puntaje.3. Consulta Registro basado (QLT) Muchas consultas web se repiten, y un pequeño número de consultas constituyen un gran volumen de búsquedas totales [9]. Para aprovechar este sesgo, las oraciones que contienen muchos términos de consulta pasados deben promoverse al frente de un documento, mientras que las oraciones que contienen pocos términos de consulta deben ser degradados. En este esquema, las oraciones están ordenadas por el número de términos de oraciones que ocurren en el registro de consulta. Para garantizar que las oraciones largas no dominen las oraciones cualitativas más cortas, el puntaje asignado a cada oración se divide por el número de términos en esa oración, lo que le da a cada oración un puntaje entre 0 y 1. 4. Consulta Registro basado (QLU) Este esquema es como para QLT, pero los términos repetidos en la oración solo se cuentan una vez. Al reordenar las oraciones utilizando esquemas ST, QLT o QLU, nuestro objetivo es finalizar la generación de fragmentos antes de que se use el orden natural, pero aún así producir oraciones con el mismo número de términos de consulta únicos (d en la Figura 2), número total de consultaTérminos (c), la misma puntuación posicional (H+) y el mismo tramo máximo (k). En consecuencia, realizamos experimentos que compararon los métodos, el primer 80% del registro de consulta de excita se usó para reordenar las oraciones cuando sea necesario, y el 20% final para las pruebas. La Figura 6 muestra las diferencias en los componentes de puntuación del fragmento utilizando cada uno de los tres métodos sobre el método de orden natural. Está claro que las oraciones de clasificación que usan el método de términos significativos (ST) conducen al cambio más pequeño en los componentes de puntuación de oraciones. El mayor cambio sobre todos los métodos está en el componente de la posición de oración (H +) de la puntuación, lo que es de esperar ya que no se garantiza que las oraciones de liderazgo y encabezado se procesen después de que se reordenen las oraciones. El segundo componente más afectado es el número de términos de consulta distintos en una oración devuelta, pero si solo el primer 50% del documento se procesa con el método ST, hay una caída de solo el 8% en el número de términos de consulta distintos encontradosen fragmentos. Dependiendo de cómo estos diversos componentes se ponderen para calcular una puntuación de fragmento general, se puede argumentar que hay poco efecto general en las puntuaciones al procesar solo la mitad del documento utilizando el método ST. CONTRO (K) Recuento de término (c) Posición de oración (H + L) Términos distintos (d) 40% 50% 60% 70% ST QLT QLU ST QLT QLU ST QLT QLU ST QLT QLU ST QLT QLU RELATIVEDIFENDIFERENTONATONATURALORDER Tamaño del tamaño del 90% utilizado 90%80% 70% 60% 50% 0% 10% 20% 30% Figura 6: Diferencia relativa en los componentes de la puntuación del fragmento en comparación con los documentos ordenados naturales cuando se reduce la cantidad de documentos procesados, y las oraciones en el documento se reordenan mediante consultaRegistros (QLT, QLU) o términos significativos (ST).7. Discusión En este documento, hemos descrito el esquema de algoritmos y compresión que harían un buen subsistema de motor de fragmento para generar fragmentos de texto del tipo que se muestra en las páginas de resultados de los conocidos motores de búsqueda web. Nuestros experimentos no solo muestran que nuestro esquema es más de un 50% más rápido que la línea de base obvia, sino que también revelan algunos aspectos muy importantes del problema de la generación del fragmento. Principalmente, los documentos de almacenamiento en caché evitan la búsqueda de costos para la memoria secundaria para cada documento que se resumirá y es vital para la generación de fragmentos rápidos. Nuestras simulaciones de almacenamiento en caché muestran que si tan solo el 1% de los documentos se pueden almacenar en caché en RAM como parte del motor de fragmento, posiblemente distribuido en muchas máquinas, entonces se puede evitar alrededor del 75% de las buscas. Nuestro segundo resultado principal es que mantener solo la mitad de cada documento en RAM, duplicando efectivamente el tamaño del caché, tiene poco efecto en la calidad de los fragmentos finales generados a partir de esos medios documentos, siempre que las oraciones que se mantienen en la memoria se eligen utilizandoEl algoritmo de término significativo de Luhn [12]. Tanto nuestro documento de compresión como los esquemas de compactación reducen drásticamente el tiempo necesario para generar fragmentos. Tenga en cuenta que estos resultados se generan utilizando un subconjunto de 100 GB de la web, y el registro de consulta Excite recopilado a partir del mismo período que se creó ese subconjunto. Asumimos, ya que no hay evidencia de lo contrario, que esta colección y registro son representativos de la entrada del motor de búsqueda en otros dominios. En particular, podemos escalar nuestros resultados para examinar qué recursos se requerirían, utilizando nuestro esquema, para proporcionar un motor de fragmento para toda la red mundial. Asumiremos que el motor del fragmento se distribuye en las máquinas M, y que hay N páginas web en la colección para ser indexadas y atendidas por el motor de búsqueda. También asumimos una carga equilibrada para cada máquina, por lo que cada máquina sirve sobre documentos N/M, que se logra fácilmente en la práctica. Cada máquina, por lo tanto, requiere que RAM sostenga lo siguiente.• El modelo CTS, que debe ser 1/1000 del tamaño de la colección sin comprimir (utilizando los resultados en la Figura 5 y Williams et al. [23]). Suponiendo un tamaño promedio de documento sin comprimir de 8 kb [11], esto requeriría N/M × 8.192 bytes de memoria.• Un caché del 1% de todos los documentos N/M. Cada documento requiere 2 kb cuando se comprime con CTS (Tabla 1), y solo se requiere la mitad de cada documento utilizando el reordenamiento de las oraciones ST, lo que requiere un total de bytes N/M × 0.01 × 1024.• La matriz de compensación que proporciona la posición de inicio de cada documento en el archivo comprimido único: 8 bytes por documentos N/M. La cantidad total de RAM requerida por una sola máquina, por lo tanto, sería N/M (8.192 + 10.24 + 8) bytes. Suponiendo que cada máquina tiene 8 GB de RAM, y que hay 20 mil millones de páginas para indexar en la web, se requeriría un total de M = 62 máquinas para el motor de fragmento. Por supuesto, en la práctica, se puede requerir más máquinas para administrar el sistema distribuido, para proporcionar servicios de respaldo para máquinas fallidas y otros servicios de red. Estas máquinas también necesitarían acceso a 37 TB de disco para almacenar las representaciones de documentos comprimidas que no estaban en caché. En este trabajo, hemos evitado deliberadamente comprometerse con un método de puntuación particular para oraciones en documentos. Más bien, hemos informado resultados de precisión en términos de los cuatro componentes que previamente se han demostrado que son importantes para determinar fragmentos útiles [20]. El método CTS puede incorporar cualquier métrica nueva que pueda surgir en el futuro que se calcule en palabras completas. Sin embargo, las técnicas de compactación del documento que utilizan la reordenamiento de oraciones eliminan la relación espacial entre las oraciones, por lo que si una técnica de puntuación se basa en la posición de una oración dentro de un documento, las técnicas agresivas de compactación informadas aquí no se pueden usar. Una variación en el enfoque de compresión semiestática que hemos adoptado en este trabajo se ha utilizado con éxito en el diseño anterior del motor de búsqueda [24], pero hay esquemas de compresión alternativos que permiten la coincidencia directa en el texto comprimido (ver Navarro y M¨akinen [15] para una encuesta reciente). Como buscar el tiempo domina el proceso de generación del fragmento, no nos hemos centrado en esta parte de la generación del fragmento en detalle en este documento. Exploraremos esquemas de compresión alternativos en el trabajo futuro. Agradecimientos Este trabajo fue apoyado en parte por el Proyecto ARC Discovery DP0558916 (AT). Gracias a Nick Lester y Justin Zobel por valiosas discusiones.8. Referencias [1] S. Brin y L. Page. La anatomía de un motor de búsqueda web hipertextual a gran escala. En WWW7, páginas 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J. A. Tomlin y D. P. Williamson. En WWW2003, Budapest, Hungría, mayo de 2003. [3] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Aumentando el rendimiento de los motores de búsqueda web: almacenamiento en caché y captación de los resultados de consultas mediante la explotación de datos de uso histórico. Syst., 24 (1): 51-78, 2006. [4] J-L Gailly y M. Adler. Biblioteca de compresión Zlib.www.zlib.net. Consultado en enero de 2007. [5] S. García, H.E. En V. Estivill-Castro, editor, Proc. Conferencia de Ciencias de la Computación de Australasia, páginas 7-14, Dunedin, Nueva Zelanda, 2004. [6] S. Ghemawat, H. Gobioff y S. Leung. En Sosp 03: Proc.del 19º Simposio ACM sobre principios de sistemas operativos, páginas 29-43, Nueva York, NY, EE. UU., 2003. ACM Press.[7] J. Goldstein, M. Kantrowitz, V. Mittal y J. Carbonell. Resumen de documentos de texto: selección de oraciones y métricas de evaluación. En Sigir99, páginas 121-128, 1999. [8] D. Hawking, Nick C. y Paul Thistlewaite. Descripción general de TREC-7 Pista de colección muy grande. En Proc.de Trec-7, páginas 91-104, noviembre de 1998. [9] B. J. Jansen, A. Spink y J. Pedersen. Una comparación temporal de la búsqueda web de Altavista. Tecnología.(Jasist), 56 (6): 559-570, abril de 2005. [10] J. Kupiec, J. Pedersen y F. Chen. Un resumen de documentos capacitables. En Sigir95, páginas 68-73, 1995. [11] S. Lawrence y C.L. Accesibilidad de la información en la web. Nature, 400: 107-109, julio de 1999. [12] H.P. La creación automática de resúmenes de literatura. IBM Journal, páginas 159-165, abril de 1958. [13] I. Mani. Resumen automático, volumen 3 de procesamiento del lenguaje natural. John Benjamins Publishing Company, Amsterdam/Filadelfia, 2001. [14] A. Moffat, J. Zobel y N. Sharman. Compresión de texto para bases de datos de documentos dinámicos. Conocimiento e ingeniería de datos, 9 (2): 302-313, 1997. [15] G. Navarro y V. M¨akinen. A aparecer.[16] D. R. Radev, E. Hovy y K. McKeown. Introducción al número especial sobre resumen. Lingüista., 28 (4): 399-408, 2002. [17] M. Richardson, A. Prakash y E. Brill. Beyond PageRank: aprendizaje automático para clasificación estática. En WWW06, páginas 707-715, 2006. [18] T. Sakai y K. Sparck-Jones. Resúmenes genéricos para la indexación en la recuperación de la información. En Sigir01, páginas 190-198, 2001. [19] H. G. Silber y K. F. McCoy. Las cadenas léxicas calculadas de manera eficiente como una representación intermedia para el resumen de texto automático. Lingüista., 28 (4): 487-496, 2002. [20] A. Tombros y M. Sanderson. Ventajas de resúmenes sesgados de consulta en la recuperación de información. En Sigir98, páginas 2-10, Melbourne, Aust., Agosto de 1998. [21] R. W. White, I. Ruthven y J. M. José. Encontrar documentos relevantes utilizando oraciones de clasificación superior: una evaluación de dos esquemas alternativos. En Sigir02, páginas 57-64, 2002. [22] H. E. Williams y J. Zobel. Comprimir enteros para el acceso rápido a los archivos. J., 42 (3): 193-201, 1999. [23] H.E. International Journal on Digital Bibliotecas, 5 (2): 99-105, abril de 2005. [24] I. H. Witten, A. Moffat y T. C. Bell. Gestión de gigabytes: compresión e indexación de documentos e imágenes. Morgan Kaufmann Publishing, San Francisco, segunda edición, mayo de 1999. [25] El motor de búsqueda de Zettair.www.seg.rmit.edu.au/zettair.