Predicción de rendimiento de consulta en entornos de búsqueda web Yun Zhou y W. Bruce Croft Departamento de Informática de la Universidad de Massachusetts, Amherst {Yzhou, Crofthth}@cs.umass.edu Técnicas de predicción actuales abstractas, que generalmente están diseñadas para consultas basadas en contenidoTípicamente evaluadas en colecciones de pruebas relativamente homogéneas de tamaños pequeños, enfrentan serios desafíos en entornos de búsqueda web donde las colecciones son significativamente más heterogéneas y existen diferentes tipos de tareas de recuperación. En este artículo, presentamos tres técnicas para abordar estos desafíos. Nos centramos en la predicción de rendimiento para dos tipos de consultas en entornos de búsqueda web: hallazgo basado en contenido y nombrado PAGE. Nuestra evaluación se realiza principalmente en la colección Gov2. Además de evaluar nuestros modelos para los dos tipos de consultas por separado, consideramos una situación más desafiante y realista de que los dos tipos de consultas se mezclan sin información previa sobre los tipos de consultas. Para ayudar a la predicción bajo la situación de la Quera mixta, se adopta un clasificador de consultas novedoso. Los resultados muestran que nuestra predicción del rendimiento de la consulta web es sustancialmente más precisa que las técnicas actuales de predicción de estado de arte. En consecuencia, nuestro documento proporciona un enfoque práctico para la predicción del rendimiento en la configuración web de RealWorld. Categorías y descriptores de temas H.3.3 [Algorización y recuperación de almacenamiento de información]: Búsqueda y recuperación de información -Formulación de la Quera Algoritmos generales, experimentación, teoría 1. La predicción del rendimiento de la consulta de introducción tiene muchas aplicaciones en una variedad de áreas de recuperación de información (IR), como mejorar la consistencia de recuperación, el refinamiento de la consulta y el IR distribuido. La importancia de este problema ha sido reconocida por los investigadores IR y recientemente se han propuesto varios métodos nuevos para su predicción [1, 2, 17]. La mayoría del trabajo en predicción se ha centrado en la tarea tradicional de recuperación ad-hoc donde el rendimiento de la consulta se mide de acuerdo con la relevancia tópica. Estos modelos de predicción se evalúan en colecciones de documentos TREC que generalmente consisten en no más de un millón de artículos relativamente homogéneos de Newswire. Con la popularidad y la influencia de la Web, las técnicas de predicción que funcionarán bien para consultas de estilo Web son muy preferibles. Sin embargo, los entornos de búsqueda web plantean desafíos significativos para los modelos de predicción actuales que están diseñados principalmente para la configuración tradicional de TREC. Aquí describimos algunos de estos desafíos. Primero, las colecciones web, que son mucho más grandes que las colecciones TREC convencionales, incluyen una variedad de documentos que son diferentes en muchos aspectos, como la calidad y el estilo. Las técnicas de predicción actuales pueden ser vulnerables a estas características de las colecciones web. Por ejemplo, la precisión de predicción informada de la técnica de robustez de clasificación y la técnica de claridad en la colección Gov2 (una gran colección web) es significativamente peor en comparación con las otras colecciones TREC [1]. Se informa una precisión de predicción similar en la colección Gov2 utilizando otra técnica en [2], lo que confirma lo difícil de predecir el rendimiento de la consulta en una gran colección web. Además, la búsqueda web va más allá del alcance de la tarea de recuperación ad-hoc basada en la relevancia tópica. Por ejemplo, la tarea de búsqueda de la página nombrada (NP), que es una tarea de navegación, también es popular en la recuperación web. La predicción del rendimiento de la consulta para la tarea NP todavía es necesaria ya que el rendimiento de recuperación de NP está lejos de ser perfecto. De hecho, según el informe sobre la tarea NP de la pista de Terabyte 2005 [3], alrededor del 40% de las consultas de prueba funcionan mal (sin respuesta correcta en los primeros 10 resultados de búsqueda) incluso en la mejor ejecución del grupo superior. Hasta donde sabemos, poca investigación ha abordado explícitamente el problema de la predicción del rendimiento de NP Query. Los modelos de predicción actuales diseñados para consultas basadas en el contenido serán menos efectivos para las consultas NP considerando las diferencias fundamentales entre los dos. En tercer lugar, en los entornos de búsqueda web del mundo real, las consultas de los usuarios suelen ser una mezcla de diferentes tipos y el conocimiento previo sobre el tipo de cada consulta generalmente no está disponible. La situación de la cuarta mixta plantea nuevos problemas para la predicción del rendimiento de la consulta. Por ejemplo, es posible que necesitemos incorporar un clasificador de consulta en los modelos de predicción. A pesar de estos problemas, la capacidad de manejar esta situación es un paso crucial para convertir la predicción del rendimiento de la consulta de un tema de investigación interesante en una herramienta práctica para la recuperación web. En este documento, presentamos tres técnicas para abordar los desafíos anteriores que enfrentan los modelos de predicción actuales en los entornos de búsqueda web. Nuestro trabajo se centra en la predicción del rendimiento de la consulta para la tarea de recuperación basada en contenido (AD-Hoc) y la tarea de búsqueda de nombre de nombre en el contexto de la recuperación web. Nuestra primera técnica, llamada ganancia de información ponderada (WIG), hace uso de las características de proximidad de un solo término y término para estimar la calidad de los documentos recuperados para la predicción. Encontramos que WIG ofrece una precisión de predicción consistente en varias colecciones de pruebas y tipos de consultas. Además, demostramos que se puede lograr una buena precisión de la predicción para la situación de Quera mixta mediante el uso de WIG con la ayuda de un clasificador de tipo de consulta. La retroalimentación de consulta y el cambio de primer rango, que son nuestras técnicas de predicción segunda y tercera, funcionan bien para consultas basadas en contenido y consultas NP, respectivamente. Nuestras principales contribuciones incluyen: (1) una precisión de predicción considerablemente mejorada para consultas basadas en el contenido web sobre varias técnicas de última generación.(2) Nuevas técnicas para predecir con éxito el rendimiento de NP-Query.(3) Una solución práctica y totalmente automática para predecir el rendimiento mixto de la cuarta. Además, una contribución menor es que encontramos que el puntaje de robustez [1], que se propuso originalmente para la predicción del rendimiento, es útil para la clasificación de consultas.2. Trabajo relacionado Como mencionamos en la introducción, se han propuesto una serie de técnicas de predicción recientemente que se centran en consultas basadas en el contenido en la tarea de relevancia tópica (AD-HOC). No sabemos de ningún trabajo publicado que aborde otros tipos de consultas, como consultas NP, y mucho menos una mezcla de tipos de consultas. A continuación, revisamos algunos modelos representativos. La principal dificultad de la predicción del rendimiento proviene del hecho de que muchos factores tienen un impacto en el rendimiento de la recuperación. Cada factor afecta el rendimiento en un grado diferente y el efecto general es difícil de predecir con precisión. Por lo tanto, no es sorprendente notar que las características simples, como la frecuencia de los términos de consulta en la colección [4] y las IDF promedio de los términos de consulta [5], no predicen bien. De hecho, la mayoría de las técnicas exitosas se basan en medir algunas características del documento recuperado establecido para estimar la dificultad del tema. Por ejemplo, la puntuación de claridad [6] mide la coherencia de una lista de documentos por parte de la divergencia KL entre el modelo de consulta y el modelo de recolección. El puntaje de robustez [1] cuantifica otra propiedad de una lista clasificada: la robustez de la clasificación en presencia de incertidumbre. Carmel et al.[2] encontraron que la distancia medida por la divergencia de Jensen-Shannon entre el conjunto de documentos recuperado y la colección se correlaciona significativamente con la precisión promedio. Vinay et al.[7] propuso cuatro medidas para capturar la geometría de los documentos superiores recuperados para la predicción. La medida más efectiva es la sensibilidad a la perturbación del documento, una idea algo similar a la puntuación de robustez. Desafortunadamente, su forma de medir la sensibilidad no funciona igualmente bien para consultas cortas y la precisión de la predicción cae considerablemente cuando se adopta una técnica de recuperación de última generación (como Okapi o un enfoque de modelado de idiomas) para recuperar en lugar de TF-Ponderación de las FDI utilizadas en su papel [16]. Ya se han mencionado las dificultades de aplicar estos modelos en entornos de búsqueda web. En este artículo, adoptamos principalmente el puntaje de claridad y el puntaje de robustez como líneas de base. Mostramos experimentalmente que las líneas de base, incluso después de ser cuidadosamente ajustadas, son inadecuadas para el entorno web. Uno de nuestros modelos de predicción, WIG, está relacionado con el modelo de campo aleatorio de Markov (MRF) para la recuperación de información [8]. El modelo MRF modela directamente la dependencia del término y se encuentra muy efectivo en una variedad de colecciones de pruebas (particularmente colecciones web) y tareas de recuperación. Este modelo se utiliza para estimar la distribución de probabilidad conjunta sobre documentos y consultas, una parte importante de la peluca. La superioridad de la peluca sobre otras técnicas de predicción basadas en características unigram, que se demostrarán más adelante en nuestro artículo, coincide con la de MRF para la recuperación. En otra palabra, es interesante observar que la dependencia del término, cuando se modela adecuadamente, puede ser útil para mejorar y predecir el rendimiento de la recuperación.3. Modelos de predicción 3.1 Ganancia de información ponderada (WIG) Esta sección introduce un enfoque de ganancia de información ponderado que incorpora características de un solo término y proximidad para predecir el rendimiento de las consultas de búsqueda de PAGE basadas en contenido y nombradas (NP). Dado un conjunto de consultas q = {qs} (s = 1,2, .. n) que incluye todas las consultas de usuarios posibles y un conjunto de documentos d = {dt} (t = 1,2 ... m), asumimos queCada par de documentos de consulta (QS, DT) se juzga manualmente y se pondrá en una lista de relevancia si se encuentra que QS es relevante para DT. La probabilidad conjunta P (QS, DT) sobre consultas Q y los documentos d denota la probabilidad de que el par (QS, DT) estará en la lista de relevancia. Tales supuestos son similares a los utilizados en [8]. Suponiendo que el usuario emite consultas Qi ∈Q y la recuperación de los resultados en respuesta a Qi es una lista clasificada L de documentos, calculamos la cantidad de información contenida en P (QS, DT) con respecto a Qi y L por Eq.1es una variante de entropía llamada entropía ponderada [13]. Los pesos en Eq.1 están determinados únicamente por Qi y L.) 1 (), (log), (), (,, ∑− = TS TStStSlq DQPDQWeightDQH I En este documento, elegimos los pesos de la siguiente manera: LindocumentskTopTontainsltTtwherDqweight k kt ts) () 2 (, 0) (,/1), (⎩ ⎨ ⎧ ∈ = = El rango de corte K es un parámetro en nuestro modelo que se discutirá más adelante. En consecuencia, la Eq.1 se puede simplificar de la siguiente manera :) 3 (), (log 1), () (, ∑∈ - = Ltd titslq kt i dqp k dqh Desafortunadamente, entropía ponderada), (, tslq dqh I calculado por eq.3, que representa la cantidad de información sobre la probabilidad de que los documentos clasificados en L sean relevantes para la consulta Qi en promedio, no se puede comparar en diferentes consultas, lo que lo hace inapropiado para predecir directamente el rendimiento de la consulta. Para mitigar este problema, presentamos una distribución de fondo P (QS, C) sobre Q y D imaginando que cada documento en D es reemplazado por el mismo documento especial C que representa el uso promedio del idioma. En este documento, C se crea concatenando cada documento en D. En términos generales, C es la colección (el conjunto de documentos) {DT} sin límites de documento. Del mismo modo, la entropía ponderada), (, CQH SLQI calculado por Eq.3 representa la cantidad de información sobre cómo es probable que un documento promedio (representado por toda la colección) sea relevante para la consulta Qi. Ahora presentamos nuestra peluca predictor de rendimiento, que es la ganancia de información ponderada [13] calculada como la diferencia entre), (, TSLQ DQH I y), (, CQH SLQI. Específicamente, Consulta dada Qi, Colección C y Lista de clasificación L de documentos de documentos, La peluca se calcula de la siguiente manera: 4 (), (), (log 1), (), (log), (), (), () ,, () (, ,, ∑∑ ∈ == - =Ltd i ti ts s ts ts tslqslqi Kt II CQP DQP KCQP DQP DQPeight DQHCQHLCQWIG COMPLETADO BY EQ.4 Mide el cambio de información sobre la calidad de la recuperación (en respuesta a la consulta Qi) de un estado imaginario de que solo un documento promedio está recuperado está recuperadoa un estado posterior que se observan los resultados de búsqueda reales. Presumimos que la peluca se correlaciona positivamente con la efectividad de la recuperación porque la recuperación de alta calidad debería ser mucho más efectiva que simplemente devolver el documento promedio. El corazón de esta técnica es cómo estimar la distribución conjunta P (QS, DT). En el enfoque de modelado de idiomas para IR, se puede aplicar una variedad de modelos fácilmente para estimar esta distribución. Aunque la mayoría de estos modelos se basan en la suposición de palabras BagOf, el trabajo reciente sobre la dependencia del término de modelado bajo el marco de modelado de idiomas ha mostrado mejoras consistentes y significativas en la efectividad de la recuperación sobre los modelos de palabras de las palabras. Inspirados en el éxito de incorporar características de proximidad de término en modelos de lenguaje, decidimos adoptar un modelo de buena dependencia para estimar la probabilidad P (QS, DT). El modelo que elegimos para este documento es el modelo de campo aleatorio de Metzler y Crofts Markov (MRF), que ya ha demostrado superioridad en varias colecciones y diferentes tareas de recuperación [8,9]. Según el modelo MRF, log P (Qi, DT) se puede escribir como) 5 () | (loglog), (log) (1 ∑∈ + - = Iqf tti dpzdqp ξ ξ ξλ donde z1 es una constante que asegura que garanticeP (Qi, DT) suma hasta 1. F (Qi) consiste en un conjunto de características expandidas de la consulta original Qi. Por ejemplo, suponiendo que la consulta Qi sea un programa talentoso de estudiantes, F (Qi) incluye características como el programa y el talentoso estudiante. Consideramos dos tipos de características: características de un solo término T y características de proximidad P. Las características de proximidad incluyen frase exacta (#1) y características de ventana desordenada (#UWN) como se describe en [8]. Tenga en cuenta que F (Qi) es la unión de T (Qi) y P (Qi). Para obtener más detalles sobre F (Qi) como cómo expandir la consulta original Qi a F (Qi), remitimos al lector a [8] y [9]. P (ξ | dt) denota la probabilidad de que la característica ξ ocurra en DT. Más adelante se proporcionarán más detalles sobre P (ξ | DT) más adelante en esta sección. La elección de λξ es algo diferente de la utilizada en [8] ya que λξ juega un papel doble en nuestro modelo. El primer rol, que es el mismo que en [8], es peso entre las características de un solo término y la proximidad. El otro rol, que es específico de nuestra tarea de predicción, es normalizar el tamaño de F (Qi). Encontramos que la siguiente estrategia de peso para λξ satisface los dos roles anteriores y se generaliza bien en una variedad de colecciones y tipos de consultas.) 6 () (, |) (| 1) (, |) (| ⎪ ⎪ ⎩ ⎪ ⎪ ⎨ ⎧ ∈ - ∈ = I T I I T QP QP QT QT ξ λ ξ λ λξ donde | t (qi) || P (Qi) | denota el número de características de un solo término y proximidad en F (Qi) respectivamente. La razón para elegir la función de la raíz cuadrada en el denominador de λξ es penalizar un conjunto de características de gran tamaño adecuadamente, lo que hace que la peluca sea más comparable entre consultas de varias longitudes.λt es un parámetro fijo y establecido en 0.8 según [8] a lo largo de este documento. Del mismo modo, el log p (qi, c) se puede escribir como :) 7 () | (loglog), (log) (2 ∑∈ + - = Iqf i cpzcqp ξ ξ ξλ cuando se caen constantes Z1 y Z2, WIG calculadoEq.4 se puede reescribir de la siguiente manera conectando en Eq.5 y Eq.7:) 8 () | () | (log 1) ,, () () (∑ ∑∈ ∈ ∈ = Ltd qf t i kt i cp dpK lcqwig ξ ξ ξ ξ λ Una de las ventajas de la peluca sobre otras técnicas es que puede manejar bien las consultas basadas en el contenido y NP. Basado en el tipo (o el tipo predicho) de Qi, el cálculo de la peluca en la ecuación.8 difiere en dos aspectos: (1) cómo estimar P (ξ | dt) y p (ξ | c), y (2) cómo elegir K. para consultas basadas en contenido, P (ξ | c) se estima porLa frecuencia relativa de la característica ξ en la colección c en su conjunto. La estimación de p (ξ | dt) es la misma que en [8]. A saber, estimamos P (ξ | dt) por la frecuencia relativa de la característica ξ en DT suavizado linealmente con frecuencia de recolección P (ξ | c). K en Eq.8 se trata como un parámetro libre. Tenga en cuenta que K es el único parámetro libre en el cálculo de WIG para consultas basadas en el contenido porque se supone que todos los parámetros involucrados en P (ξ | DT) se fijan tomando los valores sugeridos en [8]. Con respecto a las consultas NP, utilizamos la estructura de documentos para estimar p (ξ | dt) y p (ξ | c) por la llamada mezcla de modelos de lenguaje propuestos en [10] e incorporados en el modelo MRF para la búsqueda de páginas nombradasRecuperación en [9]. La idea básica es que un documento (colección) se divide en varios campos, como el campo de título, el campo del cuerpo principal y el campo de encabezado. P (ξ | dt) y P (ξ | c) se estiman mediante una combinación lineal de los modelos de lenguaje de cada campo. Debido a las limitaciones de espacio, remitimos al lector a [9] para obtener más detalles. Adoptamos exactamente el mismo conjunto de parámetros que se usan en [9] para la estimación. Con respecto a K en la Eq.8, establecemos K en 1 porque la tarea de búsqueda de páginas nombradas se centra en gran medida en el primer documento clasificado. En consecuencia, no hay parámetros libres en el cálculo de WIG para consultas NP.3.2 Comentarios de consulta En esta sección, presentamos otra técnica llamada retroalimentación de consultas (QF) para la predicción. Supongamos que un usuario emite una consulta Q a un sistema de recuperación y se devuelve una lista clasificada L de documentos. Vemos el sistema de recuperación como un canal ruidoso. Específicamente, suponemos que la salida del canal es L y la entrada es Q. Después de pasar por el canal, Q se corrompe y se transforma en la lista clasificada L. Al pensar en el proceso de recuperación de esta manera, el problema de predecir la efectividad de la recuperación se convierte en la tarea de evaluar la calidad del canal. En otras palabras, la predicción se convierte en una manera de medir el grado de corrupción que surge cuando Q se transforma en L. ya que calcular directamente el grado de corrupción es difícil, abordamos este problema con aproximación. Nuestra idea principal es que medimos en qué medida la información sobre Q se puede recuperar de L en el supuesto de que solo se observa L. Específicamente, diseñamos un decodificador que puede traducir con precisión L en una nueva consulta Q y la similitud entre la consulta original Q y la nueva consulta Q se adopta como un predictor de rendimiento. Este es un boceto de cómo la técnica QF predice el rendimiento de la consulta. Antes de completar más detalles, discutimos brevemente por qué funcionaría este método. Existe una relación entre la similitud definida anteriormente y el rendimiento de recuperación. Por un lado, si la recuperación se ha alejado del sentido original de la consulta Q, la nueva consulta Q extraída de la lista clasificada L en respuesta a Q sería muy diferente de la consulta original Q. Por otro lado, una consulta destilada de una lista clasificada que contiene muchos documentos relevantes es probable que sea similar a la consulta original. Más ejemplos en apoyo de la relación se proporcionarán más adelante. A continuación, detallamos cómo construir el decodificador y cómo medir la similitud S. En esencia, el objetivo del decodificador es comprimir la lista L de la lista L en algunos términos informativos que deberían representar el contenido de los documentos clasificados en L. Nuestro enfoqueA este objetivo es representar la Lista L clasificada por un modelo de idioma (distribución sobre términos). Luego, los términos se clasifican por su contribución a los modelos de idiomas KL (Kullback-Leiber) Divergencia del modelo de recolección de fondo. Se eligirán los términos clasificados para formar la nueva consulta Q. Este enfoque es similar al utilizado en la Sección 4.1 de [11]. Específicamente, tomamos tres pasos para comprimir la Lista L en la consulta Q sin referirnos a la consulta original.1. Adoptamos el modelo de lenguaje de lista clasificado [14], para estimar un modelo de idioma basado en la lista clasificada L. El modelo se puede escribir como :) 9 () | () | () | (∑∈ = ld ldpdwplwp donde w es cualquiera es cualquierTérmino, D es un documento. P (D | L) se estima mediante una función de disminución lineal del rango del documento D. 2. Cada término en p (w | l) se clasifica mediante la siguiente contribución de KL-divergencia :) 10 () | () | (log) | (CWP LWP LWP donde P (W | C) es el modelo de recolección estimado por el parienteFrecuencia del término W en la colección C en su conjunto. 3. Los términos clasificados TOP N por EQ.10 forman una consulta ponderada q = {(wi, ti)} i = 1, n.Donde WI denota el término y peso del I-Th. Ti es la contribución de KL-Divergence de WI en la ecuación.10. Term cruise ship vessel sea passenger KL contribution 0.050 0.040 0.012 0.010 0.009 Table 1: top 5 terms compressed from the ranked list in response to query Cruise ship damage sea life Two representative examples, one for a poorly performing query Cruise ship damage sea life (TRECTema 719; Precisión promedio: 0.08) y el otro para los tratamientos de cáncer de próstata de consultas de alto rendimiento (TEC TEMA 710; Precisión promedio: 0.49), se muestran en las Tabas 1 y 2 respectivamente. Estos ejemplos indican cómo la similitud entre el original y la nueva consulta se correlaciona con el rendimiento de la recuperación. El parámetro n en el paso 3 se establece en 20 empíricamente y elegir un valor mayor de n es innecesario ya que los pesos después de los 20 top generalmente son demasiado pequeños para marcar cualquier diferencia. Term prostate cancer treatment men therapy KL contribution 0.177 0.140 0.028 0.025 0.020 Table 2: top 5 terms compressed from the ranked list in response to query prostate cancer treatments To measure the similarity between original query Q and new query Q, we first use Q to doRecuperación en la misma colección. Se adopta una variante del modelo de probabilidad de consulta [15] para la recuperación. A saber, los documentos se clasifican con :) 11 () | () | (), (∑∈ = Qtw T I II I DWPDQP donde wi es un término en q y ti es el peso asociado. D es un documento. Deje que L denote la nueva lista clasificada devuelta de la recuperación anterior. La similitud se mide mediante la superposición de documentos en L y L. Específicamente, el porcentaje de documentos en los principales documentos K de L que también están presentes en los documentos K superiores en L. El corte K se trata como un parámetro libre. Resumimos aquí cómo la técnica QF predice el rendimiento dada una consulta Q y la lista clasificada asociada L. Primero obtenemos una consulta Q ponderada comprimida de L en los tres pasos anteriores. Luego usamos Q para realizar la recuperación y la nueva lista clasificada es L. La superposición de documentos en L y L se usa para la predicción.3.3 Cambio de primer rango (FRC) En esta sección, proponemos un método llamado Cambio de primer rango (FRC) para la predicción de rendimiento para consultas NP. Este método se deriva de la técnica de robustez de clasificación [1] que está diseñada principalmente para consultas basadas en contenido. Cuando se aplica directamente a las consultas NP, la técnica de robustez será menos efectiva porque tiene en cuenta los documentos mejor clasificados, mientras que las consultas NP generalmente tienen un solo documento relevante. En cambio, nuestra técnica se centra en el documento de primer rango, mientras que la idea principal del método de robustez permanece. Específicamente, el pseudocódigo para calcular FRC se muestra en la Figura 1. Entrada: (1) Lista clasificada l = {di} donde i = 1,100. DI denota el documento clasificado I-Th.(2) Consulta Q 1 Inicializar: (1) Establezca el número de pruebas J = 100000 (2) contador C = 0;2 para i = 1 a j 3 perturban cada documento en L, deje que el resultado sea un conjunto F = {di} donde DI denota la versión perturbada de Di.4 Do Recuperación con consulta Q en el conjunto F 5 C = C+1 si y solo si D1 se clasifica primero en el paso 4 6 FIN DE FOR 7 RETROME LA RELACIÓN C/J Figura 1: Pseudo-código para calcular FRC FRC se aproxima a la probabilidadque el primer documento clasificado en la lista original L permanecerá clasificado primero incluso después de que se perturben los documentos. Cuanto mayor sea la probabilidad, más confianza tenemos en el primer documento clasificado. Por otro lado, en el caso extremo de una clasificación aleatoria, la probabilidad sería tan baja como 0.5. Esperamos que FRC tenga una asociación positiva con el rendimiento de la consulta NP. Adoptamos [1] para implementar el paso de perturbación del documento (Paso 4 en la Fig.1) utilizando las distribuciones de Poisson. Para más detalles, remitimos al lector a [1].4. Evaluación ahora presentamos los resultados de predecir el rendimiento de la consulta por parte de nuestros modelos. Se adoptan tres técnicas de vanguardia como nuestras líneas de base. Evaluamos nuestras técnicas en una variedad de configuraciones de recuperación web. Como se mencionó anteriormente, consideramos dos tipos de consultas, es decir, consultas basadas en contenido (CB) y consultas de búsqueda de PAGE (NP). Primero, supongamos que se conocen los tipos de consultas. Investigamos la correlación entre el rendimiento de recuperación predicho y el rendimiento real para ambos tipos de consultas por separado. Los resultados muestran que nuestros métodos producen mejoras considerables sobre las líneas de base. Luego consideramos un escenario más desafiante en el que no hay información previa sobre los tipos de consultas disponibles. Se consideran dos subcases. En el primero, existe solo un tipo de consulta, pero el tipo real es desconocido. Asumimos una mezcla de los dos tipos de consultas en el segundo caso. Demostramos que nuestros modelos logran una buena precisión en este escenario exigente, haciendo que la predicción sea práctica en un entorno de búsqueda web del mundo real.4.1 Configuración experimental Nuestra evaluación se centra en la colección Gov2 que contiene aproximadamente 25 millones de documentos arrastrados de los sitios web en el dominio .gov durante 2004 [3]. Creamos dos tipos de conjuntos de datos para consultas CB y consultas NP respectivamente. Para el tipo CB, utilizamos los temas ad-hoc de las pistas de terabyte de 2004, 2005 y 2006 y los nombramos TB04-ADHOC, TB05-ADHOC y TB06-ADHOC respectivamente. Además, también utilizamos los temas ad-hoc de la pista robusta de 2004 (RT04) para probar la adaptabilidad de nuestras técnicas a un entorno no WEB. Para las consultas NP, utilizamos los temas de búsqueda de páginas nombradas de las pistas de terabyte de 2005 y 2006 y los nombramos TB05-NP y TB06-NP respectivamente. Todas las consultas utilizadas en nuestros experimentos son títulos de temas de TREC mientras nos centramos en la recuperación web. La Tabla 3 resume los conjuntos de datos anteriores. Nombre de recopilación Tema de tema Tipo de consulta TB04-ADHOC GOV2 701-750 CB TB05-ADHOC GOV2 751-800 CB TB06-ADHOC GOV2 801-850 CB RT04 DISCO 4+5 (MINUS CR) 301-450; 601700 CB TB05-NP GOV2 NP601-NP872 NP TB06-NP GOV2 NP901-NP1081 NP Tabla 3: Resumen de colecciones de pruebas y temas El rendimiento de recuperación de las consultas individuales basadas en contenido y NP se mide por la precisión promedio y el rango recíproco de la primera respuesta correcta respectivamente. Usamos el modelo de campo aleatorio de Markov para la recuperación ad-hoc y nombrada de la página. Adoptamos la misma configuración de parámetros de recuperación utilizados en [8,9]. El motor de búsqueda de Indri [12] se usa para todos nuestros experimentos. Aunque no se informó aquí, también probamos el modelo de probabilidad de consulta para la recuperación ad-hoc y descubrimos que los resultados cambian poco debido a la muy alta correlación entre los rendimientos de consulta obtenidos por los dos modelos de recuperación (0.96 medidos por el coeficiente de Pearsons).4.2 Tipos de consultas conocidos Suponga que se conocen los tipos de consultas. Tratamos cada tipo de consulta por separado y medimos la correlación con precisión promedio (o el rango recíproco en el caso de las consultas NP). Adoptamos la prueba de correlación de los Pearson que refleja el grado de relación lineal entre el rendimiento de recuperación predicho y el real.4.2.1 Consultas basadas en contenido Métodos Claridad robusta JSD WIG QF WIG +QF TB04 +0 5 ADHOC 0.333 0.317 0.362 0.574 0.480 0.637 TB06 ADHOC 0.076 0.294 N/A 0.464 0.422 0.511 Tabla 4: Coeficientes de la correlación de Pearsons para la correlación con la correlación de promedio en el promedio de la correlación de la correlación de promedio en el promedio de la correlación de la correlación de promedio en el promedio de la correlación del promedio en el promedio de la correlación del promedio en el promedio de la correlación de la correlación del promedio.Pistas de terabyte (AD-HOC) para una puntuación de claridad, puntaje de robustez, el método basado en JSD (citamos directamente el puntaje informado en [2]), WIG, retroalimentación de consultas (QF) y una combinación lineal de WIG y QF. Los casos en negrita significan que los resultados son estadísticamente significativos en el nivel de 0.01. La Tabla 4 muestra la correlación con precisión promedio en dos conjuntos de datos: uno es una combinación de TB04-ADHOC y TB05-ADHOC (100 temas en total) y el otro es TB06-ADHOC (50 temas). La razón por la que colocamos TB04-ADHOC y TB05-ADHOC juntos es hacer que nuestros resultados sean comparables a [2]. Nuestras líneas de base son el puntaje de claridad (claridad) [6], el puntaje de robustez (robusto) [1] y el método basado en JSD (JSD) [2]. Para la puntuación de claridad y robustez, hemos probado diferentes configuraciones de parámetros e informamos los coeficientes de correlación más altos que hemos encontrado. Citamos directamente el resultado del método basado en JSD informado en [2]. La tabla también muestra los resultados para el método de ganancia de información ponderada (WIG) y el método de retroalimentación de consultas (QF) para predecir consultas basadas en contenido. Como describimos en la sección anterior, tanto WIG como QF tienen un parámetro libre para establecer, es decir, el rango de corte K. Entrenamos el parámetro en un conjunto de datos y probamos en el otro. Al combinar WIG y QF, se usa una combinación lineal simple y el peso de la combinación se aprende del conjunto de datos de entrenamiento. A partir de estos resultados, podemos ver que nuestros métodos son considerablemente más precisos en comparación con las líneas de base. También observamos que se obtienen más mejoras de la combinación de WIG y QF, lo que sugiere que miden diferentes propiedades del proceso de recuperación que se relacionan con el rendimiento. Descubrimos que nuestros métodos se generalizan bien en TB06-ADHOC, mientras que la correlación para el puntaje de claridad con el rendimiento de recuperación en este conjunto de datos es considerablemente peor. Una investigación adicional muestra que la precisión promedio media de TB06-ADHOC es 0.342 y es aproximadamente un 10% mejor que la del primer conjunto de datos. Mientras que los otros tres métodos generalmente consideran los 100 principales documentos o menos dados una lista clasificada, el método de claridad generalmente necesita los 500 o más documentos principales para medir adecuadamente la coherencia de una lista clasificada. La precisión promedio media más alta hace que las listas clasificadas recuperen por diferentes consultas más similares en términos de coherencia al nivel de los 500 documentos principales. Creemos que esta es la razón principal de la baja precisión del puntaje de claridad en el segundo conjunto de datos. Aunque este documento se centra en un entorno de búsqueda web, es deseable que nuestras técnicas funcionen constantemente bien en otras situaciones. Con este fin, examinamos la efectividad de nuestras técnicas en la sólida pista de 2004. Para nuestros métodos, dividimos uniformemente todas las consultas de prueba en cinco grupos y realizamos una validación cruzada de cinco veces. Cada vez que usamos un grupo para el entrenamiento y los cuatro grupos restantes para las pruebas. Hacemos uso de todas las consultas para nuestras dos líneas de base, es decir, el puntaje de claridad y el puntaje de robustez. Los parámetros para nuestras líneas de base son los mismos que los utilizados en [1]. Los resultados que se muestran en la Tabla 5 demuestran que la precisión de predicción de nuestros métodos está a la par con la de las dos líneas de base fuertes. Claridad WIG ROBUSTA QF 0.464 0.539 0.468 0.464 Tabla 5: Comparación de los coeficientes de correlación de Pearsons en la pista robusta de 2004 para una puntuación de claridad, puntaje de robustez, retroalimentación de pelucas y consultas (QF). Los casos en negrita significan que los resultados son estadísticamente significativos en el nivel de 0.01. Además, examinamos la sensibilidad de predicción de nuestros métodos al rango de corte K. Con respecto a la peluca, es bastante robusto K en las pistas de terabyte (2004-2006) mientras prefiere un pequeño valor de k como 5 en el 2004 robustePista. En otras palabras, un pequeño valor de K es una opción casi óptima para ambos tipos de pistas. Teniendo en cuenta el hecho de que todos los demás parámetros involucrados en WIG son fijos y, en consecuencia, los mismos para los dos casos, esto significa que la peluca puede lograr una precisión de predicción casi óptima en dos situaciones considerablemente diferentes con exactamente la misma configuración de parámetros. Con respecto a QF, prefiere un valor mayor de K, como 100 en las pistas de terabyte y un valor más pequeño de K, como 25 en la pista robusta de 2004.4.2.2 Consultas NP Adoptamos WIG y Cambio de primer rango (FRC) para predecir el rendimiento de NPQuery. También probamos una combinación lineal de los dos como en la sección anterior. El peso de la combinación se obtiene del otro conjunto de datos. Utilizamos la correlación con los rangos recíprocos medidos por la prueba de correlación de Pearsons para evaluar la calidad de la predicción. Los resultados se presentan en la Tabla 6. Nuevamente, nuestras líneas de base son el puntaje de claridad y el puntaje de robustez. Para hacer una comparación justa, sintonizamos el puntaje de claridad de diferentes maneras. Descubrimos que usar el primer documento clasificado para construir el modelo de consulta produce la mejor precisión de predicción. También intentamos utilizar la estructura de documentos utilizando la mezcla de modelos de lenguaje mencionados en la Sección 3.1. Se obtuvo poca mejora. Los coeficientes de correlación para el puntaje de claridad informado en la Tabla 6 son los mejores que hemos encontrado. Como podemos ver, nuestros métodos superan considerablemente la técnica de puntuación de claridad en ambas corridas. Esto confirma nuestra intuición de que el uso de una medida basada en coherencia como la puntuación de claridad es inapropiada para las consultas NP. Métodos claridad robusta. WIG FRC WIG+FRC TB05 -NP 0.150 -0.370 0.458 0.440 0.525 TB06 -NP 0.112 -0.160 0.478 0.386 0.515 Tabla 6: Pearsons Coeficientes de correlación para la correlación con rangos recíprocos en las pistas de Terabyte (NP) para el puntaje de la claridad de la claridad, la correlación, con la correlación con receptrocal rangos en las pistas de Terabyte (NP) para el puntaje de la claridad, el puntaje de claridad, la correlación con la correlación, las rangos recíprocos en las rango de Terabyte (NP) para el puntaje de claridad de PearsonEl primer cambio de rango (FRC) y una combinación lineal de WIG y FRC. Los casos en negrita significan que los resultados son estadísticamente significativos en el nivel de 0.01. Con respecto a la puntuación de robustez, también sintonizamos los parámetros e informamos lo mejor que hemos encontrado. Observamos una correlación negativa interesante y sorprendente con las filas recíprocas. Explicamos este hallazgo brevemente. Un puntaje de alta robustez significa que varios documentos clasificados en la lista de clasificación original todavía están altamente clasificados después de perturbar los documentos. La existencia de dichos documentos es un buen signo de alto rendimiento para las consultas basadas en el contenido, ya que estas consultas generalmente contienen una serie de documentos relevantes [1]. Sin embargo, con respecto a las consultas NP, una diferencia fundamental es que solo hay un documento relevante para cada consulta. La existencia de tales documentos puede confundir la función de clasificación y conducir a un bajo rendimiento de recuperación. Aunque existe la correlación negativa con el rendimiento de la recuperación, la fuerza de la correlación es más débil y menos consistente en comparación con nuestros métodos como se muestra en la Tabla 6. Según el análisis anterior, podemos ver que las técnicas de predicción actuales, como la puntuación de claridad y el puntaje de robustez que están diseñados principalmente para consultas basadas en el contenido, enfrentan desafíos significativos y son inadecuados para tratar consultas NP. Nuestras dos técnicas propuestas para consultas NP demuestran consistentemente una buena precisión de predicción, mostrando el éxito inicial en la resolución del problema de predecir el rendimiento para las consultas NP. Otro punto que queremos enfatizar es que el método WIG funciona bien para ambos tipos de consultas, una propiedad deseable que la mayoría de las técnicas de predicción carecen.4.3 Tipos de consultas desconocidas En esta sección, ejecutamos dos tipos de experimentos sin acceso a etiquetas de tipo de consulta. Primero, suponemos que solo existe un tipo de consulta, pero el tipo es desconocido. En segundo lugar, experimentamos una mezcla de consultas basadas en contenido y NP. Las siguientes dos subsecciones informarán resultados para las dos condiciones respectivamente.4.3.1 Solo existe un tipo, suponemos que todas las consultas son del mismo tipo, es decir, son consultas NP o consultas basadas en contenido. Elegimos WIG para lidiar con este caso porque muestra una buena precisión de predicción para ambos tipos de consultas en la sección anterior. Consideramos dos casos: (1) CB: Las 150 consultas de título de la tarea ad-hoc de las pistas de Terabyte 2004-2006 (2) NP: todas las consultas de 433 NP de la tarea de búsqueda de página con nombre de las pistas de Terabyte 2005 y 2006. Tomamos una estrategia simple etiquetando todas las consultas en cada caso como el mismo tipo (ya sea NP o CB), independientemente de su tipo real. El cálculo de WIG se basará en el tipo de consulta etiquetada en lugar del tipo real. Hay cuatro posibilidades con respecto a la relación entre el tipo real y el tipo etiquetado. La correlación con el rendimiento de recuperación bajo las cuatro posibilidades se presenta en la Tabla 7. Por ejemplo, el valor 0.445 en la intersección entre la segunda fila y la tercera columna muestra el coeficiente de correlación de Pearsons para la correlación con la precisión promedio cuando las consultas basadas en el contenido están etiquetadas incorrectamente como el tipo NP. En base a estos resultados, recomendamos tratar todas las consultas como el tipo NP cuando solo existe un tipo de consulta y no es factible la clasificación de consulta precisa, considerando el riesgo de que ocurra una gran pérdida de precisión si las consultas NP están etiquetadas incorrectamente como consultas basadas en el contenido. Estos resultados también demuestran la fuerte adaptabilidad de la peluca a diferentes tipos de consultas. CB (etiquetado) NP (marcado) CB (real) 0.536 0.445 NP (real) 0.174 0.467 Tabla 7: Comparación de los coeficientes de correlación de Pearson para la correlación con el rendimiento de la recuperación bajo cuatro posibilidades en las pistas de Terabyte (NP). Los casos en negrita significan que los resultados son estadísticamente significativos en el nivel de 0.01.4.3.2 Una mezcla de consultas basadas en contenido y NP Una mezcla de los dos tipos de consultas es una situación más realista que un motor de búsqueda web cumplirá. Evaluamos la precisión de la predicción por la precisión que se pueden identificar consultas de bajo rendimiento por el método de predicción suponiendo que los tipos de consultas reales son desconocidos (pero podemos predecir los tipos de consultas). Esta es una tarea desafiante porque tanto el rendimiento previsto como el real para un tipo de consulta puede ser incomparable a la del otro tipo. A continuación, discutimos cómo implementar nuestra evaluación. Creamos un grupo de consultas que consiste en todas las 150 consultas de título ad-hoc de Terabyte Track 2004-2006 y todas las consultas de 433 NP de Terabyte Track 2005 y 2006. Dividimos las consultas en el grupo en clases: bueno (mejor que el 50% de las consultas del mismo tipo en términos de rendimiento de recuperación) y mal (de lo contrario). De acuerdo con estos estándares, una consulta NP con el rango recíproco por encima de 0.2 o una consulta basada en el contenido con la precisión promedio superior a 0.315 se considerará como buena. Luego, cada vez que seleccionamos aleatoriamente una consulta Q del grupo con probabilidad P de que Q esté basado en contenido. Las consultas restantes se utilizan como datos de entrenamiento. Primero decidimos el tipo de consulta Q según un clasificador de consulta. A saber, el clasificador de consultas nos dice si la consulta Q está basada en NP o basada en el contenido. Basado en el tipo de consulta prevista y la puntuación calculada para la consulta Q mediante una técnica de predicción, se toma una decisión binaria sobre si la consulta Q es buena o mala al compararse con el umbral de puntaje del tipo de consulta prevista obtenida de los datos de capacitación. La precisión de la predicción se mide por la precisión de la decisión binaria. En nuestra implementación, reiteradamente tomamos una consulta de prueba del grupo de consultas y la precisión de la predicción se calcula como el porcentaje de decisiones correctas, es decir, se predice que una buena consulta (mala) es buena (mala). Es obvio que la adivinación aleatoria conducirá a una precisión del 50%. Tomemos el método de peluca, por ejemplo, para ilustrar el proceso. Dos umbrales de pelucas (uno para consultas NP y el otro para consultas basadas en el contenido) están capacitados maximizando la precisión de la predicción en los datos de capacitación. Cuando la consulta de prueba se etiqueta como el tipo NP (CB) por el clasificador de tipo de consulta, se predecirá que será bueno si y solo si la puntuación de la peluca para esta consulta está por encima del umbral de NP (CB). Se tomarán procedimientos similares para otras técnicas de predicción. Ahora presentamos brevemente el clasificador de tipo de consulta automática utilizado en este documento. Encontramos que la puntuación de robustez, aunque originalmente propuesta para la predicción del rendimiento, es un buen indicador de los tipos de consultas. Encontramos que, en promedio, las consultas basadas en el contenido tienen una puntuación de robustez mucho más alta que las consultas NP. Por ejemplo, la Figura 2 muestra las distribuciones de puntajes de robustez para NP y consultas basadas en contenido. Según este hallazgo, el clasificador de puntaje de robustez adjuntará una etiqueta NP (CB) a la consulta si el puntaje de robustez para la consulta está a continuación (arriba) un umbral entrenado a partir de los datos de entrenamiento.0 0.5 1 1.5 2 2.5 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 NP Figura 2: Distribución de puntajes de robustez para consultas NP y CB. Las consultas NP son los temas de 252 NP de la pista de Terabyte 2005. Las consultas basadas en el contenido son el título de 150 ad-hoc de las pistas de Terabyte 2004-2006. Las distribuciones de probabilidad se estiman mediante el método de estimación de densidad del núcleo. Estrategias WIG-1 WIG-2 WIG-3 óptima P = 0.6 0.565 0.624 0.665 0.684 0.701 P = 0.4 0.567 0.633 0.654 0.673 0.696 Tabla 8: Comparación de la precisión de predicción para cinco estrategias en la situación de la Qiebred-Q. Dos formas de probar una consulta del grupo: (1) La consulta muestreada se basa en el contenido con la probabilidad p = 0.6.(Es decir, la consulta es NP con probabilidad 0.4) (2) Establezca la probabilidad p = 0.4. Consideramos cinco estrategias en nuestros experimentos. En la primera estrategia (denotada por robusta), utilizamos el puntaje de robustez para la predicción del rendimiento de la consulta con la ayuda de un clasificador de consulta perfecto que siempre asigna correctamente una consulta en una de las dos categorías (es decir, NP o CB). Esta estrategia representa el nivel de precisión de predicción que las técnicas de predicción actuales pueden lograr en una condición ideal de que se conocen los tipos de consultas. En las siguientes tres estrategias, el método de la peluca se adopta para la predicción del rendimiento. La diferencia entre los tres es que se utilizan tres clasificadores de consulta diferentes para cada estrategia: (1) El clasificador siempre clasifica una consulta en el tipo NP.(2) El clasificador de densidad de probabilidad de puntaje de robustez es el clasificador de puntaje robusto mencionado anteriormente.(3) El clasificador es perfecto. Estas tres estrategias son denotadas por WIG-1, WIG-2 y WIG-3 respectivamente. La razón por la que estamos interesados en WIG-1 se basa en los resultados de la Sección 4.3.1. En la última estrategia (denotada por Optimal) que sirve como un límite superior en qué tan bien podemos hacerlo hasta ahora, utilizamos completamente nuestras técnicas de predicción para cada tipo de consulta, suponiendo que esté disponible un clasificador de consulta perfecto. Específicamente, combinamos linealmente WIG y QF para consultas basadas en contenido y WIG y FRC para consultas NP. Los resultados para las cinco estrategias se muestran en la Tabla 8. Para cada estrategia, intentamos dos formas de probar una consulta del grupo: (1) La consulta muestreada es CB con probabilidad P = 0.6.(La consulta es NP con probabilidad 0.4) (2) Establezca la probabilidad p = 0.4. De la Tabla 8 podemos ver que en términos de precisión de predicción WIG-2 (el método de peluca con el clasificador de consulta automática) no solo es mejor que los dos primeros casos, sino que también está cerca de WIG-3 donde se supone un clasificador perfecto. Se observan algunas mejoras adicionales sobre WIG-3 cuando se combinan con otras técnicas de predicción. El mérito de WIG-2 es que proporciona una solución práctica para identificar automáticamente consultas de bajo rendimiento en un entorno de búsqueda web con tipos de consultas mixtas, que plantea obstáculos considerables para las técnicas de predicción tradicionales.5. Conclusiones y trabajo futuro hasta nuestro conocimiento, nuestro artículo es el primero en explorar a fondo la predicción del rendimiento de la consulta en entornos de búsqueda web. Demostramos que nuestros modelos dieron como resultado una mayor precisión de predicción que las técnicas publicadas anteriormente no especialmente diseñadas para escenarios de búsqueda web. En este documento, nos centramos en dos tipos de consultas en la búsqueda web: consultas de búsqueda basadas en contenido y nombradas (NP), correspondientes a la tarea de recuperación ad-hoc y la tarea de búsqueda de páginas nombradas respectivamente. Para ambos tipos de consultas web, se demostró que nuestros modelos de predicción son sustancialmente más precisos que las técnicas actuales de vanguardia. Además, consideramos un caso más realista de que no hay información previa sobre los tipos de consultas disponibles. Demostramos que el método de la peluca es particularmente adecuado para esta situación. Teniendo en cuenta la adaptabilidad de la peluquería a una variedad de colecciones y tipos de consultas, uno de nuestros planes futuros es aplicar este método para predecir la preferencia del usuario de los resultados de búsqueda en datos realistas recopilados de un motor de búsqueda comercial. Además de la precisión, otro problema importante con el que las técnicas de predicción tienen que tratar en un entorno web es la eficiencia. Afortunadamente, dado que la puntuación de la peluca se calcula sobre los términos y las frases que aparecen en la consulta, este cálculo puede ser muy eficiente con el soporte del índice. Por otro lado, el cálculo de QF y FRC es relativamente menos eficiente ya que QF necesita recuperar toda la colección dos veces y FRC necesita clasificar repetidamente los documentos perturbados. Cómo mejorar la eficiencia de QF y FRC es nuestro trabajo futuro. Además, las técnicas de predicción propuestas en este documento tienen el potencial de mejorar el rendimiento de la recuperación al combinarse con otras técnicas IR. Por ejemplo, nuestras técnicas se pueden incorporar a técnicas de modificación de consultas populares, como la expansión de consultas y la relajación de la consulta. Guiado por la predicción del rendimiento, podemos tomar una mejor decisión sobre cuándo o cómo modificar consultas para mejorar la efectividad de la recuperación. Nos gustaría llevar a cabo investigaciones en esta dirección en el futuro.6. Agradecimientos Este trabajo fue apoyado en parte por el Centro para la Recuperación de Información Inteligente, en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C0023, y en parte por un premio de Google. Cualquier opinión, hallazgos y conclusiones o recomendaciones expresadas en este material son las del autor y no reflejan necesariamente las del patrocinador. Además, agradecemos a Donald Metzler por sus valiosos comentarios sobre este trabajo.7. Referencias [1] Y. Zhou, w.B. Croft, Ranking Robustness: un marco novedoso para predecir el rendimiento de la consulta, en Actas de CIKM 2006. [2] D.Carmel, E.Yom-Tov, A.Darlow, D.Pelleg, ¿Qué hace que una consulta sea difícil?Actas de Sigir 2006. [3] C.L.A. Clarke, F. Scholer, I.Soboroff, la pista de Terabyte TREC 2005, en las actas en línea de 2005 Trec.[4] B. Él y I.ounis. Inferir el rendimiento de la consulta utilizando predictores preretrievales. En Proceedings of the Spire 2004. [5] S. Tomlinson. Recuperación robusta, web y terabyte con Searchserver de colibrí en TREC 2004. En las actas en línea de 2004 Trec.)de Searcher, en Actas de Sigir 2006. [8] D.Metzler, W.B.Croft, Un modelo de Markov aleatorio archivado para dependencias de términos, en los procedimientos de Sigir 2005. [9] D.Metzler, T.STROHMAN, Y.ZHOU, W.B.Croft, Indri en TREC 2005: Terabyte Track, en las actas en línea de 2004 Trec.[10] P. Ogilvie y J. Callan, que combinan representaciones de documentos para la búsqueda de elementos conocidos, en Actas de Sigir 2003. [11] A.Berger, J.Lafferty, Recuperación de información como traducción estadística, en Actas de Sigir 1999. [[12] Indri Search Engine: http://www.lemurproject.org/indri/ [13] I.J. Taneja: sobre medidas de información generalizadas y sus aplicaciones, avances en electrónica y física electrónica, Academic Press (EE. UU.), 76, 1989, 327-413.[14] S.Cronen-Townsend, Y. Zhou y Croft, W. B., Un marco para la expansión de consultas selectivas, en Actas de CIKM 2004. [15] F.Song, W.B.Croft, un modelo de idioma general para la recuperación de información, en procedimientosde Sigir 1999. [16] Contacto personal de correo electrónico con Vishwa Vinay y nuestros propios experimentos [17] E.Yom-Tov, S.Fine, D.Carmel, A.Darlow, aprendiendo a estimar la dificultad de consulta, incluidas las aplicaciones para la detección de contenido faltante yRecuperación de información distribuida, en Actas de Sigir 2005