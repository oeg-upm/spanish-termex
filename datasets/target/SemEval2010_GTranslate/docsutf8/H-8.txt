Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro para la información inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu Resumen Los métodos de bajo costo para adquirir juicios de relevancia pueden ser un boon para los investigadores que necesitan evaluarNuevas tareas o temas de recuperación, pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se pueda confiar en ellos cuando se reutilizan para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que los juicios sean reutilizables: la confianza en una evaluación de nuevos sistemas puede evaluarse con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren un esfuerzo de evaluador adicional. El uso de este método prácticamente garantiza la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y descriptores de sujetos: H.3 Almacenamiento y recuperación de información;H.3.4 Sistemas y software: Evaluación de rendimiento Términos generales: experimentación, medición, confiabilidad 1. Introducción Considere un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha creado un sistema para realizar la tarea y quiere evaluarla. Dado que la tarea es nueva, es poco probable que haya juicios de relevancia existentes. Ella no tiene el tiempo o los recursos para juzgar cada documento, o incluso cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tiene un grado razonable de confianza en sus conclusiones. Pero, ¿qué sucede cuando desarrolla un nuevo sistema y necesita evaluarlo? ¿O otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación de recuperación de información, pero es solo un problema semi-resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento;Simplemente hay demasiados de ellos. La solución utilizada por NIST en TREC (Conferencia de recuperación de texto) es el método de agrupación [19, 20]: todos los sistemas competitivos contribuyen con n documentos a un grupo, y cada documento en ese grupo se juzga. Este método crea grandes conjuntos de juicios que son reutilizables para capacitar o evaluar nuevos sistemas que no contribuyeron al grupo [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación ofrece miles de juicios de relevancia, pero requiere muchas horas de tiempo del anotador (pagado). Como resultado, ha habido una serie de documentos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al.[11], Zobel [21], Sanderson y Joho [17], Carterette et al.[8], y Aslam et al.[4], entre otros. Como veremos, los juicios que producen estos métodos pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. Volviendo a nuestro resasquero hipotético, ¿puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia para evaluar los sistemas invisibles. Si bien podemos decir que fue correcto el 75% del tiempo, o que tenía una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas probablemente estén mal o qué tan seguros debemosestar en cualquiera. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es cuán exactamente podemos evaluar nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no ha recuperado ninguno de los documentos juzgados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones y, lo que es más importante, si podemos confiar en nuestras estimaciones de confianza. Incluso si la confianza no es alta, siempre que podamos confiar en ella, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, no importa cuán pequeño sea, se vuelve reutilizable hasta cierto punto. Las pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir los juicios de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponibles para los investigadores crecería exponencialmente con el tiempo.2. Evaluación robusta anterior damos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y, por ejemplo, el 75% de la confianza de que el Sistema A es mejor que el Sistema B, nos gustaría que no haya más del 25% de nuestra evaluación de la calidad relativa deLos sistemas cambiarán a medida que continuemos juzgando documentos. Nuestra evaluación debe ser robusta para los juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea inferior a cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Se podrían definir otras tareas de evaluación;Estimación de la magnitud de la diferencia o los valores de las medidas en sí mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, vemos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer sobre una estimación de probabilidad es lo que significa. ¿Qué significa tener un 75% de confianza en que el sistema A es mejor que el sistema B? Como se describió anteriormente, queremos que signifique que si continuamos juzgando documentos, solo habrá un 25% de posibilidades de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de la confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos injudicados, específicamente que cada documento injustificado era igualmente relevante o no relevante. Esta suposición es casi seguro que no es realista en la mayoría de las aplicaciones IR. Como resultado, es esta suposición la que determina si las estimaciones de confianza pueden confiar en EB. Antes de elaborar esto, definimos formalmente la confianza.2.1 Estimación de confianza Precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar los documentos relevantes (precisión) como su capacidad para recuperar documentos relevantes (retiro). Por lo general, se escribe como la precisión media en las filas de documentos relevantes: AP = 1 | R |i∈R prec@r (i) donde r es el conjunto de documentos relevantes y r (i) es el rango de documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos se ordenan por rango, podemos expresar precisión como prec@i = 1/i i j = 1 xj. La precisión promedio se convierte en la ecuación cuadrática AP = 1 xi n i = 1 xi/ i i j = 1 xj = 1 xi n i = 1 j≥i aijxixj donde aij = 1/ max {r (i), r (j)}. El uso de AIJ en lugar de 1/I nos permite numerar los documentos arbitrariamente. Para ver por qué esto es cierto, considere un ejemplo de juguete: una lista de 3 documentos con documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 b + 1 2 xbxa + 1 3 xbxc + 1 2 x2 a + 1 3 xaxc + 1 3 x2 c) = 1 2 1 + 2 3 porque xa = 0, xb = 1, xc = 1 1. Aunque el orden B, A, C es diferente del etiquetado A, B, C, no afecta el cálculo. Ahora podemos ver que la precisión promedio en sí es una variable aleatoria con una distribución sobre todas las tareas posibles de relevancia para todos los documentos. Esta variable aleatoria tiene una expectativa, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento I sea relevante: PI = P (xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos los juicios de relevancia, pero creemos PA = 0.4, Pb = 0.8, PC = 0.7. Entonces podemos calcular, p. P (AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P (AP = 1 2) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la expectativa y la varianza: E [AP] ≈ 1 Pi Aiipi + J> I aij pipj v ar [ap] ≈ 1 (pi) 2 n i a2 iipiqi + j> i a2 ijpipj (1 − pipj +) + i = J 2aiiaijpipj (1 - pi) + k> j = i 2aijaikpipjpk (1 - pi) aparguear asintóticamente a una distribución normal con expectativa y varianza como se define anteriormente.1 Para nuestra tarea de evaluación comparativa, estamos interesados en el signode la diferencia en dos precisiones promedio: ΔAP = AP1 - AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos se ordenan arbitrariamente: ΔAP = 1 xi n i = 1 j≥i cij xixj cij = aij - bij donde bijJ se define análoga a AIJ para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP es inferior a cero. Dado que los temas son independientes, podemos extender fácilmente esto como una precisión promedio (MAP). El mapa también se distribuye normalmente;Su expectativa y varianza son: EMAP = 1 t t∈T e [apt] (1) vmap = 1 t2 t∈T v ar [apt] Δmap = MAP1 - MAP2 La confianza se puede estimar calculando la expectativa y la varianza y utilizando la normalidadfunción de densidad para encontrar p (Δmap <0).2.2 Confianza y robustez Al haber definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección con los juicios faltantes.1 Estas son realmente aproximaciones a la verdadera expectativa y varianza, pero el error es una O (N2 - N) insignificante. Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de juicios de relevancia M xm = {x1, x2, ..., xm} (usando X pequeña en lugar de capital X para distinguir entre documentos juzgados y sin juzgar);Estos son los juicios contra los cuales calculamos la confianza. Deje que Zα sea el subconjunto de pares en z para los cuales predecimos que Δmap = −1 con confianza α dada los juicios xm. Para que las estimaciones de confianza sean precisas, necesitamos al menos α · | Zα |de estos pares para tener Δmap = −1 después de haber juzgado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza;Nuestra evaluación será sólida para los juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Los supuestos en los que se basan son las probabilidades de relevancia PI. Necesitamos que estos sean realistas. Argumentamos que la mejor distribución posible de relevancia P (XI) es la que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones injustificadas. Esto se conoce como el principio de la entropía máxima [13]. La entropía de una variable aleatoria x con distribución p (x) se define como h (p) = - i p (x = i) log p (x = i). Esto ha encontrado una amplia gama de usos en informática y recuperación de información. La distribución máxima de entropía es la que maximiza a H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para relevancia al estimar la confianza. Teorema 1. Si p (xn | i, xm) = argmaxph (p), las estimaciones de confianza serán precisas.Cuando XM es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos del que deseamos estimar la relevancia, y yo es información sobre los documentos (no especificados a partir de ahora). Renunciamos a la prueba por el momento, pero es bastante simple. Esto dice que cuanto mejores serán las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una recopilación de pruebas reutilizables se convierte en la tarea de estimar la relevancia de los documentos injudgados. El teorema y su prueba no dicen nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida que nos interesa. Esto significa que las mismas estimaciones de probabilidad pueden decirnos sobre la precisión promedio, así como la precisión, el recuerdo, BPREF, etc. Además, podríamos suponer que la relevancia de los documentos I y J es independiente y logran el mismo resultado, que declaramos como corolario: Corolario 1. Si p (xi | i, xm) = argmaxph (p), las estimaciones de confianza serán precisas. Por lo tanto, la tarea se convierte en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acercamos a la distribución de relevancia máxima de entropía, más cerca llegaremos a la robustez.3. Predecir la relevancia en nuestra declaración del Teorema 1, dejamos la naturaleza de la información que no especificé. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes;Esencialmente, cualquier cosa que pueda modelarse puede usarse como información para predecir la relevancia. Una fuente natural de información son los sistemas de recuperación en sí mismos: cómo clasificaron los documentos juzgados, con qué frecuencia no pudieron clasificar los documentos relevantes, cómo se desempeñan entre los temas, y así sucesivamente. Si tratamos a cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en una agregación de opinión experta. Esto es similar al problema de MetaSearch o Fusion de datos en el que la tarea es tomar K sistemas de entrada y fusionarlos en una sola clasificación. Aslam et al.[3] Identificó previamente una conexión entre evaluación y metasearch. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos conectarnos a la ecuación.1;Los algoritmos de metasearch no tienen tal requisito.2. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y podemos restablecer la relevancia dada cada nuevo juicio. A la luz de (1) arriba, presentamos un modelo probabilístico para la combinación de expertos.3.1 Un modelo para la agregación de opiniones expertas suponga que cada experto J proporciona una probabilidad de relevancia qij = pj (xi = 1). La información sobre la relevancia del documento I será el conjunto de k opiniones de expertos i = qi = (qi1, qi2, · · ·, qik). La distribución de probabilidad que deseamos encontrar es la que maximiza la entropía de Pi = P (xi = 1 | qi). Como resultado, encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la probabilidad [5]. El soplador [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces Pi = P (xi = 1 | qi) = exp k j = 1 λjqij 1 + exp k j = 1 λj qij (2) donde λ1, · · ·, λk son los parámetros de regresión. Incluimos una beta anterior para p (λj) con parámetros α, β. Esto puede verse como un tipo de suavizado para tener en cuenta el hecho de que los datos de capacitación son altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Clemen & Winkler demostró que un modelo de la misma forma era el mejor para agregar probabilidades de expertos [10]. Se ha utilizado un enfoque similar motivado por MaximumentRopy para la agregación de expertos [15]. Aslam y Montague [1] utilizaron un modelo similar para Metasearch, pero asumieron la independencia entre los expertos. ¿De dónde vienen los Qij s? El uso de puntajes crudos y no calibrados como predictores no funcionará porque las distribuciones de puntaje varían demasiado entre los temas. Un marcador de modelado de idiomas, por ejemplo, generalmente dará una puntuación mucho más alta al documento recuperado para una consulta corta que al documento recuperado para una consulta larga. Podríamos capacitar a un modelo de predicción separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no lo suficiente como para capacitar a un modelo a cualquier confianza. Además, parece razonable suponer que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas invisibles. En cambio, calibraremos los puntajes de cada experto individualmente para que los puntajes se puedan comparar tanto dentro del tema como entre el tema. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre los expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas).3.2 Expertos de calibración Cada experto nos da una puntuación y un rango para cada documento. Necesitamos convertirlos en probabilidades. Un método como el utilizado por Manmatha et al.[14] podría usarse para convertir los puntajes en probabilidades de relevancia. El método de preferencia por pares de Carterette y Petkova [9] también podría usarse, intercalando la clasificación de un documento sobre otro como una expresión de preferencia. Deje que Q ∗ sea experto en la probabilidad autoinformada de que el documento I sea relevante. Intuitivamente parece claro que Q ∗ ij debería disminuir con el rango, y debería ser cero si el documento I no está unido (el experto no creía que fuera relevante). El modelo de preferencia por pares puede manejar estos dos requisitos fácilmente, por lo que lo usaremos. Sea θrj (i) el coeficiente de relevancia del documento en rango RJ (i). Queremos encontrar los θs que maximicen la función de probabilidad: ljt (θ) = rj (i) <rj (k) exp (θrj (i) - θrj (k)) 1 + exp (θrj (i) - θrj (k)) Nuevamente incluimos una beta anterior en p (θrj (i)) con parámetros | RT |+ 1 y | nt |+ 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos como parámetros anteriores asegura que las probabilidades resultantes se concentrarán en torno a la relación de documentos relevantes que se han descubierto para el Tema T.Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen documentos más relevantes. Después de encontrar el θ que maximiza la probabilidad, tenemos q ∗ ij = exp (θrj (i)) 1+exp (θrj (i)). Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no lo tenga sea relevante es 0. Dado que Q ∗ IJ se basa en el rango en el que se recupera un documento en lugar de la identidad del documento en sí, las probabilidades son idénticas de experto a experto, p.Si Expert E pone el documento A en el rango 1, y el experto d pone el documento B en el rango 1, tendremos Q ∗ ae = q ∗ bd. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior ofrece probabilidades independientes del tema para cada documento. Pero suponga que un experto que informa que el 90% de probabilidad es solo el 50% del tiempo. Su opinión debe descartarse en función de su rendimiento observado. Específicamente, queremos aprender una función de calibración Qij = CJ (Q ∗ ij) que garantice que las probabilidades predichas se ajusten a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este punto. El método de calibración de Platts SVM [16] se ajusta a una función sigmoide entre Q ∗ ij y los juicios de relevancia para obtener qij = cj (q ∗ ij) = exp (aj +bjq ∗ ij) 1 +exp (aj +bj q ∗ ij). Dado que q ∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se conectan al modelo (2) para encontrar las probabilidades de documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1, E2 han clasificado los documentos A, B, C para el Tema T1 y los Documentos D, E, F para el Tema T2. El primer paso es obtener Q ∗ ij. Lo siguiente es la calibración al rendimiento verdadero para encontrar QIJ. Finalmente obtenemos Pi = P (xi = 1 | qi1, qi2), · · ·.3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren por los datos que toman como entrada y lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. Rangos → Probabilidades (por sistema por tema). Esto nos da q ∗ ij, la probabilidad autoinformada de expertos de la relevancia del documento i. Esto no está supervisado;No requiere datos etiquetados (aunque si tenemos algunos, los usamos para establecer parámetros anteriores).2. Probabilidades → Probabilidades calibradas (por sistema). Esto nos da qij = cj (q ∗ ij), probabilidad de experto js calibrada de la relevancia del documento i. Esto es semisupervisado;Tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos.3. Probabilidades calibradas → Probabilidades de documentos. Esto nos da pi = p (xi = 1 | qi), la probabilidad de relevancia del documento que dio probabilidades de expertos calibradas qij. Esto se supervisa;Aprendemos coeficientes de un conjunto de documentos juzgados y los usamos para estimar la relevancia de los documentos injustificados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Como tal, se puede implementar en un lenguaje de programación estadística como R en algunas líneas de código. El uso de antecedentes beta (conjugados) asegura que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se capacita y se aplica lo suficientemente rápido como para usarse en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/.4. Experimentos Se están considerando tres hipótesis. El primero, y lo más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas que son lo suficientemente robustas como para ser reutilizables;Es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al grupo. Las otras dos hipótesis se relacionan con la mejora que vemos mediante el uso de mejores estimaciones de relevancia que en nuestro trabajo anterior [8]. Estos son que (a) requiere menos rastreo de relevancia no.Temas no.Corre no.juzgado no.rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web04 225 74 88,566 1,763 robusto 05 50 74 37,798 6,561 Terabyte 05 50 58 45,291 10,407 Tabla 1: Número de temas, número de ejecuciones, número de documentos juzgados y el número encontrado relevante para cada uno de nuestros conjuntos de datos.Los juicios para alcanzar la confianza del 95% y (b) la precisión de las predicciones es más alta que si simplemente asumiéramos PI = 0.5 para todos los documentos no juzgados.4.1 Datos obtuvimos ejecuciones ad-hoc completas enviadas a TRECS 3 a 8. Cada ejecución se ubica en la mayoría de los 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista web de TREC 13, la pista robusta2 de TREC 14 y la pista de Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Dejamos de lado el TREC 4 (AD-Hoc 95) establecido para el entrenamiento, TRECS 3 y 5-8 (AD-Hoc 94 y 96-99) para pruebas primarias, y los conjuntos restantes para pruebas adicionales. Usamos los archivos Qrels ensamblados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos listas clasificadas en 100 documentos. No hay razón para que no pudiéramos profundizar, pero la varianza de cálculo es O (N3) y, por lo tanto, es muy de tiempo. Debido a la naturaleza de rango recíproco de AP, no perdemos mucha información truncando en el rango 100. 4.2 Algoritmos compararemos tres algoritmos para adquirir juicios de relevancia. La línea de base es una variación de la agrupación de TREC que llamaremos agrupación incremental (IP). Este algoritmo toma un número K como entrada y presenta los primeros K documentos en orden de rango (sin tener en cuenta el tema) a juzgar. No estimula la relevancia de los documentos innecesarios;Simplemente supone que cualquier documento insuficiente no es relevante. El segundo algoritmo es el presentado en Carterette et al.[8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que son para determinar si existe una diferencia en la precisión promedio media. Para este enfoque PI = 0.5 para todo i;No hay estimación de las probabilidades. Llamaremos a esto MTC para una colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de probabilidades de relevancia. Llamaremos a este RTC para una colección de prueba robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos PI para todos los documentos injustificados que utilizo el modelo de agregación experta de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben establecerse. Entrenamos usando el set Ad-Hoc 95. Limitamos 2 robustos aquí significa una recuperación robusta;Esto es diferente de nuestro objetivo de una evaluación robusta. Algoritmo 1 (MTC) dada dos listas clasificadas y nivel de confianza α, predice el signo de ΔMAP.1: Pi ← 0.5 Para todos los documentos I 2: mientras que P (Δmap <0) <α do 3: Calcule el peso WI para todos los documentos inequívocos I (ver Carterette et al. [8] para detalles) 4: J ← Argmaxiwi 5:XJ ← 1 Si el documento J es relevante, 0 de lo contrario 6: PJ ← XJ 7: Fin mientras la búsqueda de antecedentes uniformes con una variación relativamente alta. Para la agregación de expertos, los parámetros anteriores son α = β = 1. 4.3 Diseño experimental primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios para evaluar un nuevo conjunto desistemas. Para cada ensayo experimental: 1. Elija un subconjunto aleatorio de K corridas.2. De esos k, elija una c <k inicial para evaluar.3. Ejecute RTC a un 95% de confianza en la c.4. Usando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todas las ejecuciones K.5. Calcule EMAP para todas las corridas K, y P (ΔMap <0) para todos los pares de corridas. Hacemos lo mismo para MTC, pero omitir el paso 4. Tenga en cuenta que después de evaluar los primeros sistemas C, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos C = 2: construiremos un conjunto de juicios para evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que ejecutamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis.4.4 Evaluación experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que hace es al menos su confianza estimada. Una forma de evaluar la robustez es a los pares de contenido por su confianza, luego calcular la precisión sobre todos los pares en cada contenedor. Nos gustaría que la precisión sea no menos que el puntaje de confianza más bajo en el contenedor, pero preferiblemente mayor. Dado que las estadísticas resumidas son útiles, ideamos la siguiente métrica. Supongamos que somos un corredor de apuestas que apuesta por si Δmap <0. Usamos RTC o MTC para establecer las probabilidades O = P (ΔMap <0) 1 - P (ΔMap <0). Suponga que un apostador apuesta $ 1 en Δmap ≥ 0. Si resulta que Δmap <0, ganamos el dólar. De lo contrario, pagamos a O. Si nuestras estimaciones de confianza son perfectamente precisas, alcanzamos el punto de equilibrio. Si la confianza es mayor que la precisión, perdemos dinero;Ganamos si la precisión es mayor que la confianza. Contraintuitivamente, el resultado más deseable es llegar a un punto de equilibrio: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o juzgamos más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de los juicios de relevancia adicional, por lo que trataremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación por pares I es: wi = yi - (1 - yi) pi 1 - pi = yi - pi 1 - pi yi = 1 si Δmap <0 y 0 de lo contrario, y pi = p (Δmap <0). La estadística resumida es w, la media de WI. Tenga en cuenta que a medida que PI aumenta, perdemos más por estar equivocado. Esto es como debería ser: la penalización debe ser excelente por perder las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin limitados a medida que se acercan a la certeza de las predicciones, Cap -Wi a los 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar la confianza del 95% en el primer par de sistemas. La mediana es más interesante que la media: la mayoría de los pares requieren unos pocos cientos de juicios, pero algunos pares requieren varios miles. Por lo tanto, la distribución es muy sesgada y la media es fuertemente afectada por esos valores atípicos. Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de los sistemas K por un pequeño conjunto de juicios y la clasificación verdadera utilizando el conjunto completo de juicios. Kendalls τ, un estadístico no paramétrico basado en swaps por pares entre dos listas, es una evaluación estándar para este tipo de estudio. Varía de −1 (perfectamente correlacionado) a 1 (rankings idéntico), con 0, lo que significa que la mitad de los pares se intercambian. Sin embargo, cuando mencionamos en la introducción, una medida de precisión como la correlación de rango no es una buena evaluación de la reutilización. Lo incluimos para completar.4.4.1 Pruebas de hipótesis que ejecutan múltiples ensayos permiten el uso de pruebas de hipótesis estadística para comparar algoritmos. El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas. Como dijimos anteriormente, estamos más interesados en el número medio de juicios que en la media. Una prueba de diferencia en la mediana es la prueba de rango de signo de Wilcoxon. También podemos usar una prueba t emparejada para probar una diferencia en la media. Para la correlación de rango, podemos usar una prueba t emparejada para probar una diferencia en τ.5. Resultados y análisis La comparación entre MTC y RTC se muestra en la Tabla 2. Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos. No podemos reutilizar los juicios de relevancia con mucha confianza. Pero con RTC, los resultados son muy robustos. Hay una ligera caída en la precisión cuando la confianza supere 0.95;No obstante, las predicciones de confianza son confiables. WI medio muestra que RTC está mucho más cerca de 0 que MTC. La distribución de los puntajes de confianza muestra que se logra al menos el 80% de confianza más del 35% del tiempo, lo que indica que ninguno de los algoritmo está siendo demasiado conservador en sus estimaciones de confianza. Las estimaciones de confianza son bastante bajas en general;Esto se debe a que hemos construido una colección de pruebas a partir de solo dos sistemas iniciales. Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza cuando generalizamos a nuevos sistemas. Los resultados más detallados para ambos algoritmos se muestran en la Figura 2. La línea continua es el resultado ideal que daría w = 0. RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcanza aproximadamente 0.97. Después de eso, hay una ligera caída en la precisión que discutimos a continuación. Tenga en cuenta que tanto el% de confianza de MTC RTC en la precisión del contenedor en la precisión del contenedor 0.5 - 0.6 33.7% 61.7% 28.6% 61.9% 0.6 - 0.7 18.1% 73.1% 20.1% 76.3% 0.7 - 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.99.4% 69.0% 12.1% 84.9% 0.9 - 0.95 7.3% 78.0% 6.6% 93.1% 0.95 - 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% w −5.34 −0.39 medios de 251 235 τ τ τ τ τ τ τ τ τ τ τ τ τ τ τ τ τ media τ °0.393 0.555 Tabla 2: Confía de que P (ΔMap <0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC. Cada contenedor contiene más de 1,000 ensayos de los conjuntos ADHOC 3, 5-8. RTC es mucho más robusto que MTC. W se define en la Sección 4.4;Más cerca de 0 es mejor. La mediana de juicio es el número de juicios para alcanzar la confianza del 95% en los dos primeros sistemas. La media τ es la correlación de rango promedio para los 10 sistemas.0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 Precisión Confianza Breakevenven RTC MTC Figura 2: Confianza versus precisión de MTC y RTC. La línea continua es el resultado perfecto que daría w = 0;El rendimiento debe estar en o por encima de esta línea. Cada punto representa al menos 500 comparaciones por pares.Los algoritmos están muy por encima de la línea hasta la confianza 0.7. Esto se debe a que el rendimiento de línea de base en estos conjuntos de datos es alto;Es bastante fácil lograr una precisión del 75% haciendo muy poco trabajo [7]. Número de juicios: el número medio de juicios requeridos por MTC para alcanzar el 95% de confianza en los dos primeros sistemas es 251, un promedio de 5 por tema. La mediana requerida por RTC es 235, aproximadamente 4.7 por tema. Aunque los números están cerca, la mediana de RTCS es significativamente menor por una prueba de wilcoxon emparejada (P <0,0001). A modo de comparación, un grupo de profundidad 100 daría como resultado un mínimo de 5,000 juicios para cada par. La diferencia en las medias es mucho mayor. MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema.(Recuerde que eso significa que están fuertemente sesgados por unos pocos pares que toman miles de juicios). Esta diferencia es significativa por una prueba t emparejada (P <0,0001). El diez por ciento de los conjuntos dieron como resultado 100 o menos juicios (menos de dos por tema). El rendimiento en estos es muy alto: W = 0.41 y una precisión del 99.7% cuando la confianza es al menos 0.9. Esto muestra que incluso pequeñas colecciones pueden ser reutilizables. Para el 50% de los sets con más de 235 juicios, la precisión es del 93% cuando la confianza es al menos 0.9. Correlación de rango: MTC y RTC clasifican los 10 sistemas por EMAP (ecuación (1)) calculados utilizando sus respectivas estimaciones de probabilidad. La correlación media de rango τ entre el mapa verdadero y el EMAP es 0.393 para MTC y 0.555 para RTC. Esta diferencia es significativa por una prueba t emparejada (P <0,0001). Tenga en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia. Es más importante que estimemos la confianza en cada comparación por pares correctamente. Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por mapa utilizando solo aquellos juicios (todos los documentos injustificados asumidos no relevantes). Calculamos la correlación τ con la clasificación verdadera. La correlación media τ es 0.398, que no es significativamente diferente de MTC, pero es significativamente menor que RTC. El uso de estimaciones uniformes de la probabilidad es indistinguible de la línea de base, mientras que la estimación de la relevancia por parte de la agregación experta aumenta el rendimiento mucho: casi el 40% sobre MTC e IP. Overecita: es posible sobrepacharse: si demasiados juicios provienen de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco confiables. Vimos esto en la Tabla 2 y la Figura 2, donde RTC exhibe una caída en la precisión cuando la confianza es de alrededor del 97%. De hecho, el número de juicios realizados antes de una predicción incorrecta es más del 50% mayor que el número realizado antes de una predicción correcta. El sobreajuste es difícil de cuantificar exactamente, porque hacer juicios de más relevancia no siempre lo causa: a niveles de confianza más altos, se realizan más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confidencias. Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión;La diferencia parece ser cuando un sistema tiene mucho más juicios que el otro. Comparaciones por pares: nuestras comparaciones por pares se dividen en uno de los tres grupos: 1. Las dos corridas originales a partir de las cuales se adquieren juicios de relevancia;2. Una de las carreras originales frente a una de las nuevas carreras;3. Dos nuevas carreras. La Tabla 3 muestra la confianza frente a los resultados de precisión para cada uno de estos tres grupos. Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales. Esto probablemente se deba a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP. Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos. El peor de los casos: el caso intuitivamente es más probable que produzca un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común. Si queremos que los juicios sean reutilizables, debemos poder generalizar incluso a corridas que son muy diferentes de las que se usan para adquirir los juicios de relevancia. Una simple medida de similitud de dos corridas es el porcentaje promedio de documentos que recuperaron en común para cada tema [2]. Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud. Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 - 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza versus precisión de RTC cuando se compara las dos carreras originales, una carrera original y una nueva carrera,y dos nuevas carreras. RTC es robusto en los tres casos.precisión cuando una confianza similar 0 - 0.1 0.1 - 0.2 0.2 - 0.3 0.5 - 0.6 68.4% 63.1% 61.4% 0.6 - 0.7 84.2% 78.6% 76.6% 0.7 - 0.8 82.0% 79.8% 78.9% 0.8 - 0.9 93.6% 83.3% 82.111-0.95 99.3% 92.7% 92.4% 0.95-0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza versus precisión de RTC cuando un par de sistemas retrateado 0-30% documentosen común (desencadenado en 0%-10%, 10%-20%y 20%30%). RTC es robusto en los tres casos. Tabla 4. El rendimiento es de hecho muy robusto incluso cuando la similitud es baja. Cuando las dos ejecuciones comparten muy pocos documentos en común, W es realmente positivo. MTC e IP tuvieron un desempeño bastante mal en estos casos. Cuando la similitud fue entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC. Por conjunto de datos: todos los resultados anteriores solo han estado en las colecciones ad-hoc. Hicimos los mismos experimentos en nuestros conjuntos de datos adicionales y rompimos los resultados mediante el conjunto de datos para ver cómo varía el rendimiento. Los resultados en la Tabla 5 muestran todo sobre cada conjunto, incluida la precisión agrupada, W, la media τ y el número medio de juicios para alcanzar la confianza del 95% en los dos primeros sistemas. Los resultados son altamente consistentes de la recopilación a la recopilación, lo que sugiere que nuestro método no es excesivo a ningún conjunto de datos en particular.6. Conclusiones y trabajo futuro En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una colección de pruebas y presentó un modelo que puede lograr una reutilización con conjuntos de juicios de relevancia muy pequeños. La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos precisa que RTC, que puede eliminar el sesgo para dar una evaluación robusta. Las estimaciones de confianza de RTC, además de ser precisas, proporcionan una guía para obtener juicios adicionales: concéntrese en juzgar documentos a partir de las comparaciones de menor confianza. A la larga, vemos pequeños conjuntos de relevancia Confianza de juicio ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robusto 05 terabyte 05 0.5-0.6 64.1% 61.8% 62.2% 62.0% 62.0% 62.0% 62.0%59.4% 64.3% 61.5% 61.6% 0.6 - 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 - 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 - 0.9.2.2% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 - 0.9.% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 - 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 - 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.93.93.93.93.% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.0.07 -0.41 -0.67 Median juzgó 235 276 243 179 448 310 3200.573 0.556 0.579 0.532 0.596 0.565 0.574 Tabla 5: Precisión, W, media τ y número medio de juicios para los 8 conjuntos de pruebas. Los resultados son muy consistentes entre los conjuntos de datos.Los investigadores comparten los investigadores, cada grupo contribuye con algunos juicios más para ganar más confianza sobre sus sistemas particulares. A medida que pasa el tiempo, el número de juicios crece hasta que haya un 100% de confianza en cada evaluación, y hay una recopilación de prueba completa para la tarea. Vemos más uso para este método en escenarios como la recuperación web en la que el corpus está cambiando con frecuencia. Podría aplicarse a la evaluación en una colección de pruebas dinámicas según lo definido por Soboroff [18]. El modelo que presentamos en la Sección 3 de ninguna manera es la única posibilidad para crear una colección de prueba robusta. Un modelo de agregación de expertos más simple puede funcionar tan bien o mejor (aunque todos nuestros esfuerzos para simplificar fracasaron). Además de la agregación de expertos, podríamos estimar las probabilidades observando similitudes entre documentos. Esta es un área obvia para la exploración futura. Además, valdrá la pena investigar el problema del sobreajuste: las circunstancias en las que ocurre y lo que se puede hacer para prevenirlo. Mientras tanto, limitar las estimaciones de confianza al 95% es un truco que resuelve el problema. Tenemos muchos más resultados experimentales para los que desafortunadamente no teníamos espacio, pero que refuerzan la noción de que RTC es muy robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación por pares de sistemas. Agradecimientos Este trabajo fue apoyado en parte por el Centro para la Recuperación de Información Inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023. Cualquier opinión, hallazgos y conclusiones o recomendaciones expresadas en este material son las del autor y no reflejan necesariamente las del patrocinador.7. Referencias [1] J. Aslam y M. Montague. Modelos para MetaSearch. En Actas de Sigir, páginas 275-285, 2001. [2] J. Aslam y R. Savell. Sobre la efectividad de la evaluación de los sistemas de recuperación en ausencia de juicios de relevancia. En Actas de Sigir, páginas 361-362, 2003. [3] J. A. Aslam, V. Pavlu y R. Savell. Un modelo unificado para metasearch, agrupación y evaluación del sistema. En Actas de CIKM, páginas 484-491, 2003. [4] J. A. Aslam, V. Pavlu y E. Yilmaz. Un método estadístico para la evaluación del sistema utilizando juicios incompletos. En Actas de Sigir, páginas 541-548, 2006. [5] A. L. Berger, S. D. Pietra y V. J. D. Pietra. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Computational Linguistics, 22 (1): 39-71, 1996. [6] D. J. Blower. Una fácil derivación de la regresión logística desde la perspectiva bayesiana y máxima de entropía. En Actas del 23º trabajo internacional sobre inferencia bayesiana y métodos de entropía máxima en ciencia e ingeniería, páginas 30-43, 2004. [7] B. Carterette y J. Allan. Metodología de investigación en estudios de esfuerzo del asesor para la evaluación de la recuperación. En Actas de Riao, 2007. [8] B. Carterette, J. Allan y R. K. Sitaraman. Colecciones de pruebas mínimas para la evaluación de recuperación. En Actas de Sigir, páginas 268-275, 2006. [9] B. Carterette y D. I. Petkova. Aprender una clasificación de preferencias por pares. En Actas de Sigir, 2006. [10] R. T. Clemen y R. L. Winkler. Unanimidad y compromiso entre los pronosticadores de probabilidad. Management Science, 36 (7): 767-779, julio de 1990. [11] G. V. Cormack, C. R. Palmer y C. L. Clarke. Construcción eficiente de grandes colecciones de pruebas. En Actas de Sigir, páginas 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern y D. B. Rubin. Análisis de datos bayesianos. Chapman y Hall/CRC, 2004. [13] E. T. Jaynes. Teoría de la probabilidad: la lógica de la ciencia. Cambridge University Press, 2003. [14] R. Manmatha y H. Sever. Un enfoque formal para la normalización de la puntuación para MetaSearch. En Actas de HLT, páginas 88-93, 2002. [15] I. J. Myung, S. Ramamoorti y J. Andrew D. Baily. Agregación máxima de entropía de predicciones expertas. Management Science, 42 (10): 1420-1436, octubre de 1996. [16] J. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparación con métodos de probabilidad regularizados.Páginas 61-74, 2000. [17] M. Sanderson y H. Joho. Formando colecciones de pruebas sin agrupación de sistemas. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 33-40, 2004. [18] I. Soboroff. Colecciones de pruebas dinámicas: medir la efectividad de la búsqueda en la web en vivo. En Actas de Sigir, páginas 276-283, 2006. [19] K. Sparck Jones y C. J. Van Rijsbergen. Colecciones de pruebas de recuperación de información. Journal of Documation, 32 (1): 59-75, 1976. [20] E. M. Voorhees y D. K. Harman, editores. TREC: Experimento y evaluación en la recuperación de la información. MIT Press, 2005. [21] J. Zobel. ¿Qué tan confiables son los resultados de los experimentos de recuperación de información a gran escala? En Actas de Sigir, páginas 307-314, 1998.