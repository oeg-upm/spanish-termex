Recuperación de información sensible al contexto Uso de retroalimentación implícita Xuehua Departamento de Informática Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign Chengxiang Zhai Departamento de Ciencias de la Computación Universidad de Illinois en Urbana Champaign Resumen A MajorLa limitación de la mayoría de los modelos y sistemas de recuperación existentes es que la decisión de recuperación se toma basada únicamente en la consulta y la recopilación de documentos;La información sobre el usuario real y el contexto de búsqueda se ignora en gran medida. En este documento, estudiamos cómo explotar la información de retroalimentación implícita, incluidas consultas anteriores e información de clics, para mejorar la precisión de la recuperación en un entorno de recuperación de información interactiva. Proponemos varios algoritmos de recuperación sensibles al contexto basados en modelos de lenguaje estadístico para combinar las consultas anteriores y hacer clic en resúmenes de documentos con la consulta actual para una mejor clasificación de documentos. Utilizamos los datos de TREC AP para crear una recopilación de pruebas con información de contexto de búsqueda y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de la retroalimentación implícita, especialmente los resúmenes de documentos haciendo clic, puede mejorar sustancialmente el rendimiento de la recuperación. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: Modelos de recuperación Términos generales Algoritmos 1. Introducción En la mayoría de los modelos de recuperación de información existentes, el problema de recuperación se trata como que involucra una sola consulta y un conjunto de documentos. Sin embargo, de una sola consulta, el sistema de recuperación solo puede tener una pista muy limitada sobre la necesidad de la información de los usuarios. Por lo tanto, un sistema de recuperación óptimo debe tratar de explotar la mayor cantidad de información de contexto adicional posible para mejorar la precisión de la recuperación, siempre que esté disponible. De hecho, la recuperación sensible al contexto se ha identificado como un desafío importante en la investigación de recuperación de información [2]. Hay muchos tipos de contexto que podemos explotar. La retroalimentación de relevancia [14] puede considerarse como una forma para que un usuario proporcione más contexto de búsqueda y se sabe que es efectivo para mejorar la precisión de la recuperación. Sin embargo, la retroalimentación de relevancia requiere que un usuario proporcione explícitamente información de retroalimentación, como especificar la categoría de la necesidad de información o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a involucrar actividades adicionales, mientras que los beneficios no siempre son obvios para el usuario, un usuario a menudo es reacio a proporcionar dicha información de retroalimentación. Por lo tanto, la efectividad de la retroalimentación de relevancia puede ser limitada en aplicaciones reales. Por esta razón, la retroalimentación implícita ha atraído mucha atención recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperación utilizando la consulta inicial de los usuarios pueden no ser satisfactorios;A menudo, el usuario necesitaría revisar la consulta para mejorar la precisión de recuperación/clasificación [8]. Para una necesidad de información compleja o difícil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de información esté completamente satisfecha. En un escenario de recuperación tan interactivo, la información naturalmente disponible para el sistema de recuperación es más que solo la consulta de usuario actual y la recopilación de documentos; en general, todo el historial de interacción puede estar disponible para el sistema de recuperación, incluidas las consultas anteriores, información sobre la cualDocumentos El usuario ha elegido ver, e incluso cómo un usuario ha leído un documento (por ejemplo, que parte de un documento que el usuario pasa mucho tiempo en lectura). Definimos la retroalimentación implícita en términos generales como explotando todo el historial de interacción naturalmente disponible para mejorar los resultados de la recuperación. Una ventaja importante de la retroalimentación implícita es que podemos mejorar la precisión de la recuperación sin requerir ningún esfuerzo del usuario. Por ejemplo, si la consulta actual es Java, sin conocer ninguna información adicional, sería imposible saber si está destinado a significar el lenguaje de programación de Java o la isla Java en Indonesia. Como resultado, los documentos recuperados probablemente tendrán ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programación y otros pueden estar sobre la isla. Sin embargo, cualquier usuario en particular es poco probable que busque ambos tipos de documentos. Tal ambigüedad se puede resolver explotando la información del historial. Por ejemplo, si sabemos que la consulta anterior del usuario es la programación CGI, sugeriría fuertemente que es el lenguaje de programación que el usuario está buscando. La retroalimentación implícita se estudió en varios trabajos anteriores. En [11], Joachims exploró cómo capturar y explotar la información de clics y demostró que dicha información de retroalimentación implícita puede mejorar la precisión de búsqueda para un grupo de personas. En [18], se realizó un estudio de simulación de la efectividad de diferentes algoritmos de retroalimentación implícitos, y se propusieron y evaluaron varios modelos de recuperación diseñados para explotar la información de clics. En [17], algunos algoritmos de recuperación existentes se adaptan para mejorar los resultados de búsqueda en función del historial de navegación de un usuario. Otro trabajo relacionado sobre el uso del contexto incluye búsqueda personalizada [1, 3, 4, 7, 10], análisis de registro de consultas [5], factores de contexto [12] y consultas implícitas [6]. Si bien el trabajo anterior se ha centrado principalmente en usar información de clics, en este documento, utilizamos información de clics y consultas anteriores, y nos centramos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperación. Específicamente, desarrollamos modelos para usar información de retroalimentación implícita, como consulta e historial de clics de la sesión de búsqueda actual para mejorar la precisión de la recuperación. Utilizamos el modelo de recuperación de divergencia KL [19] como base y proponemos tratar la recuperación sensible al contexto como estimación de un modelo de lenguaje de consulta basado en la consulta actual y cualquier información de contexto de búsqueda. Proponemos varios modelos de lenguaje estadístico para incorporar la consulta y el historial de clics en el modelo de divergencia KL. Un desafío en el estudio de modelos de retroalimentación implícitos es que no existe una recopilación de pruebas adecuada para la evaluación. Por lo tanto, utilizamos los datos de TREC AP para crear una recopilación de pruebas con información de retroalimentación implícita, que puede usarse para evaluar cuantitativamente modelos de retroalimentación implícitos. Hasta donde sabemos, esta es la primera prueba establecida para la retroalimentación implícita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de información de retroalimentación implícita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de la recuperación sin requerir un esfuerzo adicional del usuario. Las secciones restantes se organizan de la siguiente manera. En la Sección 2, intentamos definir el problema de la retroalimentación implícita e introducir algunos términos que usaremos más adelante. En la Sección 3, proponemos varios modelos de retroalimentación implícitos basados en modelos de lenguaje estadístico. En la Sección 4, describimos cómo creamos el conjunto de datos para experimentos de retroalimentación implícitos. En la Sección 5, evaluamos diferentes modelos de retroalimentación implícitos en el conjunto de datos creado. La Sección 6 es nuestras conclusiones y trabajo futuro.2. Definición del problema Hay dos tipos de información de contexto que podemos usar para la retroalimentación implícita. Uno es el contexto a corto plazo, que es la información circundante inmediata que arroja luz sobre la necesidad actual de la información actual de los usuarios en una sola sesión. Una sesión puede considerarse como un período que consiste en todas las interacciones para la misma necesidad de información. La categoría de información de los usuarios necesita (por ejemplo, niños o deportes), consultas anteriores y documentos recientemente vistos son ejemplos de contexto a corto plazo. Dicha información se relaciona más directamente con la necesidad de información actual del usuario y, por lo tanto, se puede esperar que sea más útil para mejorar la búsqueda actual. En general, el contexto a corto plazo es más útil para mejorar la búsqueda en la sesión actual, pero puede no ser tan útil para las actividades de búsqueda en una sesión diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a información como un nivel educativo de usuarios e interés general, el historial de consultas de usuarios acumulados e información de clics de los usuarios anteriores;Dicha información es generalmente estable durante mucho tiempo y a menudo se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisión de búsqueda para una sesión en particular. En este artículo, nos centramos en el contexto a corto plazo, aunque algunos de nuestros métodos también se pueden usar para incorporar naturalmente algún contexto a largo plazo. En una sola sesión de búsqueda, un usuario puede interactuar con el sistema de búsqueda varias veces. Durante las interacciones, el usuario modificaría continuamente la consulta. Por lo tanto, para la consulta actual QK (a excepción de la primera consulta de una sesión de búsqueda), hay un historial de consulta, HQ = (Q1, ..., QK - 1) asociado con ella, que consiste en las consultas anteriores dadas por elmismo usuario en la sesión actual. Tenga en cuenta que suponemos que los límites de la sesión se conocen en este documento. En la práctica, necesitamos técnicas para descubrir automáticamente los límites de las sesiones, que se han estudiado en [9, 16]. Tradicionalmente, el sistema de recuperación solo utiliza la consulta actual QK para hacer la recuperación. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas útiles sobre la necesidad actual de la información actual de los usuarios como se ve en el ejemplo de Java dado en la sección anterior. De hecho, nuestro trabajo anterior [15] ha demostrado que el historial de consultas a corto plazo es útil para mejorar la precisión de la recuperación. Además del historial de consultas, puede haber otra información de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente frecuentemente hace clic en algunos documentos para ver. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el título, el resumen y quizás también el contenido y la ubicación (por ejemplo, la URL) del documento hecho hecho. Aunque no está claro si un documento visto es realmente relevante para la necesidad de la información del usuario, podemos asumir de manera segura que la información de resumen/título mostrada sobre el documento es atractiva para el usuario, por lo que transmite información sobre la necesidad de la información de los usuarios. Supongamos que concatenamos toda la información de texto que se muestra sobre un documento (generalmente título y resumen) juntos, también tendremos un resumen CI en cada ronda de recuperación. En general, podemos tener un historial de resúmenes c1, ..., CK - 1. También explotaremos dicho historial de clics HC = (C1, ..., CK - 1) para mejorar nuestra precisión de búsqueda para la consulta actual QK. El trabajo anterior también ha mostrado resultados positivos utilizando información de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son información de retroalimentación implícita, que naturalmente existe en la recuperación de información interactiva, por lo tanto, no se necesita ningún esfuerzo adicional para el usuario para recopilarlos. En este documento, estudiamos cómo explotar dicha información (HQ y HC), desarrollamos modelos para incorporar el historial de consultas y el historial de clics en una función de clasificación de recuperación y evaluar cuantitativamente estos modelos.3. Los modelos de lenguaje para contextenciales de información sensible a la prueba intuitiva de la consulta HQ y el historial de clics HC son útiles para mejorar la precisión de búsqueda para la consulta actual QK. Una pregunta de investigación importante es cómo podemos explotar dicha información de manera efectiva. Proponemos utilizar modelos de lenguaje estadístico para modelar una necesidad de información de los usuarios y desarrollar cuatro modelos de lenguaje sensibles al contexto específicos para incorporar información de contexto en un modelo de recuperación básica.3.1 Modelo de recuperación básica Usamos el método de divergencia Kullback-Leibbler (KL) [19] como nuestro método de recuperación básica. Según este modelo, la tarea de recuperación implica calcular un modelo de lenguaje de consulta θq para una consulta dada y un modelo de lenguaje de documento θd para un documento y luego calcular su divergencia KL d (θq || θd), que sirve como la puntuación del documento. Una ventaja de este enfoque es que podemos incorporar naturalmente el contexto de búsqueda como evidencia adicional para mejorar nuestra estimación del modelo de lenguaje de consulta. Formalmente, deje que HQ = (Q1, ..., Qk - 1) sea el historial de consulta y la consulta actual sea QK. Deje que HC = (C1, ..., CK - 1) sea el historial de clics. Tenga en cuenta que CI es la concatenación de todos los resúmenes de documentos en la ronda de recuperación, ya que podemos tratar razonablemente todos estos resúmenes por igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos mediante P (W | θk), basado en la consulta actual QK, así como en el Histor de la consulta HQ e History History HC. Ahora describimos varios modelos de idiomas diferentes para explotar HQ y HC para estimar P (W | θk). Usaremos C (w, x) para denotar el recuento de la palabra w en el texto x, que podría ser una consulta o un resumen de documentos haciendo clic o cualquier otro texto. Usaremos | x |para denotar la longitud del texto x o el número total de palabras en X. 3.2 Interpolación de coeficiente fijo (fijación) Nuestra primera idea es resumir el HQ del historial de consultas con un modelo de lenguaje unigram P (W | HQ) y la historia de clic HC conOtro modelo de lenguaje unigram P (W | HC). Luego interpolamos linealmente estos dos modelos de historia para obtener el modelo de historia P (W | H). Finalmente, interpolamos el modelo de historia P (W | H) con el modelo de consulta actual P (W | QK). Estos modelos se definen de la siguiente manera.p (w | qi) = c (w, qi) | qi |p (w | hq) = 1 k - 1 i = k - 1 i = 1 p (w | qi) p (w | ci) = c (w, ci) | ci |p (w | hc) = 1 k - 1 i = k - 1 i = 1 p (w | ci) p (w | h) = βp (w | hc) + (1 - β) p (w | hq)p (w | θk) = αp (w | qk) + (1 - α) p (w | h) donde β ∈ [0, 1] es un parámetro para controlar el peso en cada modelo de historia, y donde α ∈ [0, 1] es un parámetro para controlar el peso en la consulta actual y la información del historial. Si combinamos estas ecuaciones, vemos que p (w | θk) = αp (w | qk) + (1 - α) [βp (w | hc) + (1 - β) p (w | hq)] es decir, el modelo de consulta de contexto estimado es solo una interpolación de coeficiente fijo de tres modelos P (W | QK), P (W | HQ) y P (W | HC).3.3 Interpolación bayesiana (BayesInt) Un posible problema con el enfoque de fijación es que los coeficientes, especialmente α, se fijan en todas las consultas. Pero intuitivamente, si nuestra consulta actual QK es muy larga, deberíamos confiar más en la consulta actual, mientras que si QK solo tiene una palabra, puede ser beneficioso poner más peso en la historia. Para capturar esta intuición, tratamos P (W | HQ) y P (W | HC) como Dirichlet Priors y QK como datos observados para estimar un modelo de consulta de contexto utilizando estimador bayesiano. El modelo estimado viene dado por p (w | θk) = c (w, qk) + µp (w | hq) + νp (w | hc) | qk |+ µ + ν = | qk || QK |+ µ + ν p (w | qk) + µ + ν | qk |+ µ + ν [µ µ + ν p (w | hq) + ν µ + ν p (w | hc)] donde µ es el tamaño de muestra anterior para p (w | hq) y ν es el tamaño de muestra anterior para p(W | HC). Vemos que la única diferencia entre Bayesint y Fixint es que los coeficientes de interpolación ahora se adaptan a la longitud de la consulta. De hecho, al ver Bayesint como fijación, vemos que α = | qk || QK |+µ+ν, β = ν ν+µ, por lo tanto con µ y ν fijo, tendremos un α dependiente de la consulta. Más tarde mostraremos que un α tan adaptativo funciona empíricamente mejor que un α fijo.3.4 Actualización bayesiana en línea (en línea) Tanto FixInt como BayesInt resuman la información del historial promediando los modelos de idiomas unigram estimados en función de consultas anteriores o resúmenes. Esto significa que todas las consultas anteriores se tratan por igual y, por lo tanto, se hacen clic en los resúmenes. Sin embargo, a medida que el usuario interactúa con el sistema y adquiere más conocimiento sobre la información en la recopilación, presumiblemente, las consultas reformuladas mejorarán cada vez más. Por lo tanto, asignar pesos en descomposición a las consultas anteriores para confiar en una consulta reciente más que una consulta anterior parece ser razonable. Curiosamente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de la información de los usuarios después de ver cada consulta, naturalmente podríamos obtener pesos en descomposición en las consultas anteriores. Dado que una estrategia de actualización en línea incremental puede usarse para explotar cualquier evidencia en un sistema de recuperación interactivo, la presentamos de una manera más general. En un sistema de recuperación típico, el sistema de recuperación responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar los documentos, el sistema debe tener algún modelo para la necesidad de información de los usuarios. En el modelo de recuperación de divergencia KL, esto significa que el sistema debe calcular un modelo de consulta siempre que un usuario ingrese una consulta (nueva). Una forma principalmente de actualizar el modelo de consulta es utilizar la estimación bayesiana, que discutimos a continuación.3.4.1 Actualización bayesiana Primero discutimos cómo aplicamos la estimación bayesiana para actualizar un modelo de consulta en general. Sea P (W | φ) nuestro modelo de consulta actual y t sea una nueva evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen en el que se hace clic). Para actualizar el modelo de consulta basado en t, usamos φ para definir un Dirichlet previamente parametrizado como Dir (µt p (w1 | φ), ..., µt p (wn | φ)) donde µt es el tamaño de muestra equivalente delprevio. Usamos Dirichlet Prior porque es un conjugado anterior para distribuciones multinomiales. Con tal conjugado anterior, la distribución predictiva de φ (o de manera equivalente, la media de la distribución posterior de φ viene dada por p (w | φ) = c (w, t) + µt p (w | φ) | t |+ µt (1) donde c (w, t) es el recuento de w en t y | t | es la longitud de T. parámetro µt indica nuestra confianza en el anterior expresado en términos de una muestra de texto equivalente comparable con T. para T.Ejemplo, µt = 1 indica que la influencia del anterior es equivalente a agregar una palabra adicional a T. 3.4.2 Actualización del modelo de consulta secuencial Ahora discutimos cómo podemos actualizar nuestro modelo de consulta a lo largo del tiempo durante un proceso de recuperación interactivo utilizando la estimación bayesiana. En general, suponemos que el sistema de recuperación mantiene un modelo de consulta actual φi en cualquier momento. Tan pronto como obtengamos alguna evidencia de retroalimentación implícita en forma de una pieza de texto TI, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos información sobre el usuario. Por ejemplo, podemos tener información sobre qué documentos ha visto el usuario en el pasado. Utilizamos dicha información para definir un anterior en el modelo de consulta, que se denota por φ0. Después de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados Q1. El modelo de consulta actualizado φ1 se puede usar para clasificar documentos en respuesta a Q1. A medida que el usuario ve algunos documentos, el texto de resumen mostrado para dichos documentos C1 (es decir, resúmenes en los que se hace clic) puede servir como algunos datos nuevos para que actualicemos aún más el modelo de consulta para obtener φ1. A medida que obtenemos la segunda consulta Q2 del usuario, podemos actualizar φ1 para obtener un nuevo modelo φ2. En general, podemos repetir este proceso de actualización para actualizar iterativamente el modelo de consulta. Claramente, vemos dos tipos de actualización: (1) actualización basada en una nueva consulta Qi;(2) Actualización basada en un nuevo resumen de CI. En ambos casos, podemos tratar el modelo actual como un modelo previo del modelo de consulta de contexto y tratar la nueva consulta observada o hacer clic en el resumen como datos observados. Por lo tanto, tenemos las siguientes ecuaciones de actualización: P (W | φi) = C (W, Qi) + µIP (W | φi - 1) | Qi |+ µi p (w | φi) = c (w, ci) + νip (w | φi) | Ci |+ νi donde µI es el tamaño de muestra equivalente para el anterior cuando se actualiza el modelo basado en una consulta, mientras que νi es el tamaño de muestra equivalente para el anterior cuando se actualiza el modelo basado en un resumen haciendo clic. Si establecemos µi = 0 (o νi = 0) esencialmente ignoramos el modelo anterior, por lo tanto, iniciaría un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen de resumen hecho). Por otro lado, si establecemos µi = +∞ (o νi = +∞) esencialmente ignoramos la consulta observada (o el resumen haciendo clic) y no actualizamos nuestro modelo. Por lo tanto, el modelo sigue siendo el mismo que si no observamos ninguna evidencia de texto nueva. En general, los parámetros µI y νi pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, podemos tener un historial de consultas muy escaso, por lo que podríamos usar un µI más pequeño, pero más tarde como el historial de consultas es más rico, podemos considerar usar un µI más grande. Pero en nuestros experimentos, a menos que se indiquen lo contrario, los colocamos en las mismas constantes, es decir, ∀i, j, µi = µJ, νi = νJ. Tenga en cuenta que podemos tomar P (W | φi) o P (W | φi) como nuestro modelo de consulta de contexto para documentos de clasificación. Esto sugiere que no tenemos que esperar hasta que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperación;En su lugar, tan pronto como recolectamos un resumen de CI, podemos actualizar el modelo de consulta y usar P (w | φi) para volver a relacionar inmediatamente cualquier documento que un usuario aún no haya visto. Para calificar documentos después de ver la consulta QK, usamos P (W | φk), es decir, P (W | θk) = P (W | φk) 3.5 Actualización bayesiana por lotes (BatchUp) Si establecemos los parámetros de tamaño de muestra equivalentes en constante fijo., el algoritmo en línea introduciría un factor en descomposición: la interpolación repetida haría que los datos tempranos tengan un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario se vuelve cada vez mejor en la formulación de la consulta a medida que pasa el tiempo, pero no es necesariamente apropiado para la información de clics, especialmente porque usamos el resumen mostrado, en lugar deEl contenido real de un documento haciendo clic. Una forma de evitar aplicar una interpolación en descomposición a los datos de clics es hacer en línea solo para el historial de consulta Q = (Q1, ..., Qi - 1), pero no para los datos de clics C. Primero amortamos todos los datos de clics.Juntos y usen toda la parte de los datos de clic para actualizar el modelo generado mediante la ejecución de OnlineUP en consultas anteriores. Las ecuaciones de actualización son las siguientes.p (w | φi) = c (w, qi) + µip (w | φi - 1) | qi |+ µi p (w | ψi) = i - 1 j = 1 c (w, cj) + νip (w | φi) i - 1 j = 1 | cj |+ νi donde µI tiene la misma interpretación que en onlineup, pero ahora indica en qué medida queremos confiar en los resúmenes haciendo clic. Como en OnlineUP, establecemos todos los µis y νis al mismo valor. Y para clasificar los documentos después de ver la consulta actual QK, usamos P (W | θk) = P (W | ψk) 4. Recopilación de datos Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino también el historial de consulta y el historial de clics para cada tema. Dado que no hay dicho conjunto de datos disponible para nosotros, tenemos que crear uno. Hay dos opciones. Una es extraer temas y cualquier historial de consultas asociado e historial de clic para cada tema del registro de un sistema de recuperación (por ejemplo, motor de búsqueda). Pero el problema es que no tenemos juicios de relevancia sobre dichos datos. La otra opción es usar un conjunto de datos TREC, que tiene una base de datos de texto, descripción del tema y archivo de juicio de relevancia. Desafortunadamente, no hay historial de consultas ni datos de historial de clics. Decidimos aumentar un conjunto de datos TREC recopilando el historial de consultas y los datos del historial de clics. Seleccionamos los datos TREC AP88, AP89 y AP90 como nuestra base de datos de texto, porque los datos AP se han utilizado en varias tareas TREC y tiene juicios relativamente completos. Hay artículos de noticias en total 242918 y la longitud promedio del documento es de 416 palabras. La mayoría de los artículos tienen títulos. Si no, seleccionamos la primera oración del texto como título. Para el preprocesamiento, solo hacemos plegamiento de casos y no hacemos la eliminación de la palabra de parada o las derivaciones. Seleccionamos 30 temas relativamente difíciles de TREC TEMICS 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisión entre los temas de TREC 1-150 de acuerdo con algunos experimentos de línea de base utilizando el modelo de divergencia KL con suavizado bayesiano [20]. La razón por la que seleccionamos temas difíciles es que el usuario tendría que tener varias interacciones con el sistema de recuperación para obtener resultados satisfactorios para que podamos esperar recopilar un historial de consultas relativamente más rico y los datos del historial de clics del usuario. En aplicaciones reales, también podemos esperar que nuestros modelos sean más útiles para temas tan difíciles, por lo que nuestra estrategia de recopilación de datos refleja bien las aplicaciones del mundo real. Indexemos el conjunto de datos TREC AP y configuramos un motor de búsqueda y una interfaz web para artículos de noticias TREC AP. Utilizamos 3 sujetos para hacer experimentos para recopilar el historial de consultas y los datos del historial de clics. A cada sujeto se le asigna 10 temas y dadas las descripciones del tema proporcionadas por TREC. Para cada tema, la primera consulta es el título del tema dado en la descripción original del tema TREC. Después de que el sujeto presente la consulta, el motor de búsqueda hará recuperación y devolverá una lista clasificada de resultados de búsqueda al sujeto. El sujeto navegará por los resultados y tal vez haga clic en uno o más resultados para explorar el texto completo de los artículos. El sujeto también puede modificar la consulta para hacer otra búsqueda. Para cada tema, el tema compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el número de tema de un menú de selección antes de enviar la consulta al motor de búsqueda para que podamos detectar fácilmente el límite de la sesión, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones del usuario, incluidas las consultas enviadas y los documentos haciendo clic. Para cada consulta, almacenamos los términos de consulta y las páginas de resultados asociadas. Y para cada documento haciendo clic, almacenamos el resumen como se muestra en la página de resultados de búsqueda. El resumen del artículo depende de la consulta y se calcula en línea utilizando la recuperación de pasaje de longitud fija (modelo de divergencia KL con suavizado bayesiano). Entre las 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de la consulta es de 3.71 palabras. En total, hay 91 documentos que se hacen clic para ver. Entonces, en promedio, hay alrededor de 3 clics por tema. La longitud promedio de la consulta de resumen de resumen en línea en línea de la consulta de lotes (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) Map@20docs mapa pr@20docs mapa pr@20 docs map pr@20docs Q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + hc 0.0311117 0.031110 0.03110 0.031110 0.031110 0.031141110 0.031110 0.031110 0.0311411141110 0.03114111111 215 0.0733 0.0342 0.1100 Mejorar.3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Improve 72.4% 32.6% 93.8%39.4% 67.7% 20.2% 92.4% 39.4% Q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 Q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250. 9% 47.8% 6.9% 77.2% 16.4% Tabla1: Efecto del uso del historial de consultas y los datos de clic para la clasificación de documentos.son 34.4 palabras. Entre 91 documentos haciendo clic, 29 documentos se juzgan relevantes de acuerdo con el archivo de juicio de TREC. Este conjunto de datos está disponible públicamente 1.5. Experimentos 5.1 Diseño del experimento Nuestra hipótesis principal es que el uso del contexto de búsqueda (es decir, el historial de consultas e información de clics) puede ayudar a mejorar la precisión de la búsqueda. En particular, el contexto de búsqueda puede proporcionar información adicional para ayudarnos a estimar un mejor modelo de consulta que usar solo la consulta actual. Por lo tanto, la mayoría de nuestros experimentos implican comparar el rendimiento de la recuperación utilizando solo la consulta actual (ignorando así cualquier contexto) con eso utilizando la consulta actual y el contexto de búsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, hacemos tales comparaciones para cada versión de consultas. Utilizamos dos medidas de rendimiento: (1) Precisión promedio media (MAP): esta es la precisión promedio no interpolada estándar y sirve como una buena medida de la precisión general de clasificación.(2) Precisión a 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es más significativa que el mapa y refleja la utilidad para los usuarios que solo leen los 20 documentos principales. En todos los casos, la cifra informada es el promedio sobre los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de búsqueda (es decir, FixInt, BayesInt, Onlineup y Batchup). Cada modelo tiene precisamente dos parámetros (α y β para fijación; µ y ν para otros). Tenga en cuenta que µ y ν pueden necesitar ser interpretados de manera diferente para diferentes métodos. Varimos estos parámetros e identificamos el rendimiento óptimo para cada método. También variamos los parámetros para estudiar la sensibilidad de nuestros algoritmos a la configuración de los parámetros.5.2 Análisis de resultados 5.2.1 Efecto general del contexto de búsqueda Comparamos el rendimiento óptimo de cuatro modelos con aquellos que usan la consulta actual solo en la Tabla 1. Una fila etiquetada con Qi es el rendimiento de línea de base y una fila etiquetada con Qi + HQ + HC es el rendimiento de usar el contexto de búsqueda. Podemos hacer varias observaciones de esta tabla: 1. La comparación de las actuaciones de referencia indica que en promedio las consultas reformuladas son mejores que las consultas anteriores con el desempeño de que Q4 es el mejor. Los usuarios generalmente formulan mejores y mejores consultas.2. El uso del contexto de búsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede ver por el hecho de que la 1 http://sifaka.cs.uiuc.edu/ir/ucair/qchistory.zip, la mejora para Q4 y Q3 es generalmente más sustancial en comparación con Q2. En realidad, en muchos casos con Q2, usar el contexto puede dañar el rendimiento, probablemente porque la historia en ese punto es escasa. Cuando el contexto de búsqueda es rico, la mejora del rendimiento puede ser bastante sustancial. Por ejemplo, Batchup logra una mejora del 92.4% en la precisión promedio media sobre el Q3 y el 77.2% de mejora con respecto a Q4.(Sin embargo, las precisiones generalmente bajas también hacen que la mejora relativa sea engañosamente alta, sin embargo.) 3. Entre los cuatro modelos que usan el contexto de búsqueda, las actuaciones de FixTint y Onlineup son claramente peores que las de BayesInt y Batchup. Dado que BayesInt funciona mejor que la fijación y la principal diferencia entre Bayesint y Fixint es que el primero usa un coeficiente adaptativo para la interpolación, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y una interpolación de estilo bayesiano tiene sentido. La principal diferencia entre Onlineup y Batchup es que OnlineUp utiliza coeficientes de descomposición para combinar los múltiples resúmenes haciendo clic, mientras que Batchup simplemente concatena todos los resúmenes haciendo clic. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que en línea indica que los pesos para combinar los resúmenes haciendo clic en realidad no deberían estar en descomposición. Si bien OnlineUp es teóricamente atractivo, su rendimiento es inferior a Bayesint y Batchup, probablemente debido al coeficiente en descomposición. En general, Batchup parece ser el mejor método cuando variamos la configuración de los parámetros. Tenemos dos tipos diferentes de contexto de búsqueda: el historial de consultas y los datos de clics. Ahora analizamos la contribución de cada tipo de contexto.5.2.2 Utilizando el historial de consultas solo en cada uno de los cuatro modelos, podemos desactivar los datos del historial de clics configurando los parámetros de manera adecuada. Esto nos permite evaluar el efecto de usar el historial de consultas solo. Utilizamos la misma configuración de parámetros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. Aquí vemos que, en general, el beneficio de usar el historial de consultas es muy limitado con resultados mixtos. Esto es diferente de lo que se informa en un estudio anterior [15], donde el uso del historial de consultas es consistentemente útil. Otra observación es que las ejecuciones de contexto funcionan mal en Q2, pero generalmente funcionan (ligeramente) mejor que las líneas de base para Q3 y Q4. Esto es nuevamente probable porque al principio la consulta inicial, que es el título en la descripción del tema original de TREC, puede no ser una buena consulta;De hecho, en promedio, las actuaciones de estas consultas de primera generación son claramente más pobres que las de todas las demás consultas formuladas por el usuario en las generaciones posteriores. Otra observación más es que cuando se usa el historial de consultas solamente, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, en línea y la consulta de Batchint Batchint Bayesint en línea (α = 0.1, β = 0) (µ = 0.2, ν = 0) (µ = 5.0, ν = +∞) (µ = 2.0, ν = =+ ∞) Map PR@20Docs Map PR@20Docs Map PR@20Docs Map PR@20Docs Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 Q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 mejora.-68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Improve -38.2% -38.2%7.1% 2.3% 5.5% -10.1% 8.1% -2.2% Q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 Q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 mejoran % 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto del uso del historial de consultas solo para la clasificación de documentos.µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.05520.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: Precisión promedio de Batchup usando el historial de consultas Solo son esencialmente el mismo algoritmo. Los resultados mostrados reflejan así la variación causada por el parámetro µ. Una configuración más pequeña de 2.0 se ve mejor que un valor mayor de 5.0. Se puede ver una imagen más completa de la influencia de la configuración de µ en la Tabla 3, donde mostramos las cifras de rendimiento para un rango más amplio de valores de µ. El valor de µ se puede interpretar como cuántas palabras consideramos el historial de consultas. Un valor mayor, por lo tanto, pone más peso en la historia y se considera que duele más el rendimiento cuando la información del historial no es rica. Por lo tanto, mientras que para Q4 el mejor rendimiento tiende a lograrse para µ ∈ [2, 5], solo cuando µ = 0.5 vemos un pequeño beneficio para Q2. Como es de esperar, un µ excesivamente grande dañaría el rendimiento en general, pero el Q2 se duele más y Q4 apenas se duele, lo que indica que a medida que acumulamos más y más información sobre el historial de consultas, podemos poner más y más peso sobre la información del historial. Esto también sugiere que una mejor estrategia probablemente debería ajustar dinámicamente los parámetros de acuerdo con la cantidad de información del historial que tenemos. Los resultados del historial de consultas mixtas sugieren que el efecto positivo del uso de la información de retroalimentación implícita puede provenir en gran medida del uso del historial de clics, lo cual es cierto como discutimos en la próxima subsección.5.2.3 Usando el historial de clics solo ahora apagamos el historial de consultas y solo usamos los resúmenes haciendo clic más la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de usar información de clics es mucho más significativo que el de usar el historial de consultas. Vemos un efecto positivo general, a menudo con una mejora significativa sobre la línea de base. También está claro que cuanto más ricos sean los datos del contexto, más mejora utilizando resúmenes haciendo clic. Aparte de una degradación ocasional de precisión en 20 documentos, la mejora es bastante consistente y, a menudo, bastante sustancial. Estos resultados muestran que el texto de resumen hecho hecho es en general bastante útil para inferir una necesidad de la información del usuario. Intuitivamente, utilizando el texto resumido, en lugar del contenido real del documento, tiene más sentido, ya que es muy posible que el documento detrás de un resumen aparentemente relevante sea realmente no relevante.29 de los 91 documentos haciendo clic son relevantes. La actualización del modelo de consulta basado en tales resúmenes elevaría los rangos de estos documentos relevantes, causando una mejora del rendimiento. Sin embargo, dicha mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuánta mejora hemos logrado para mejorar los rangos de los documentos relevantes invisibles, excluyimos estos 29 documentos relevantes de nuestro archivo de juicio y recomputamos el desempeño de BayesInt y la línea de base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Tenga en cuenta que el rendimiento del método de referencia es menor debido a la eliminación de los 29 documentos relevantes, que generalmente se habrían clasificado en los resultados. De la Tabla 5, vemos claramente que el uso de resúmenes haciendo clic también ayuda a mejorar significativamente las filas de documentos relevantes invisibles. Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0263 0.100 Q2 + HC 0.0314 0.100 Mejorar.19.4% 0% Q3 0.0331 0.125 Q3 + HC 0.0661 0.178 Mejorar 99.7% 42.4% Q4 0.0442 0.165 Q4 + HC 0.0739 0.188 Mejorar 67.2% 13.9% Tabla 5: BayesInt evaluada en documentos relevantes no considerados La pregunta que no tiene siete hace clic si el hecho es útil que los datos siguen siendo útilesSi ninguno de los documentos haciendo clic es relevante. Para responder a esta pregunta, sacamos los 29 resúmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto más pequeño de resúmenes HC y reevaluamos el rendimiento del método BayesInt utilizando HC con la misma configuración de parámetros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisión promedio se mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan en solo 62 clics no relevantes. En realidad, un usuario es más probable que haga clic en algunos resúmenes relevantes, lo que ayudaría a traer documentos más relevantes como hemos visto en la Tabla 4 y la Tabla 5. FixInt Bayesint OnlineUp Batchup Consulter (α = 0.1, β = 1) (µ = 0, ν = 5.0) (µk = 5.0, ν = 15, ∀i <k, µi = +∞) (µ = 0, ν = 15) MAP PR@20DOCS MAP PR@20DOCS MAP PR@20DOCS MAP PR@20DOCS Q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 Q2 + HC 0.0324 0.1117 0.0338 0.1133 0.0358 0.1300 0.0344 0.1167 Mejora.3.8% -2.9% 8.3% -1.5% 14.7% 13.0% 10.3% 1.5% Q3 0.0421 0.1483 0.0421 0.1483 0.04210 0.1483 0.0420 0.1483 Q3 + HC 0.0726 0.1967 0.0766 0.2033 0.0622 0.1767 0.05513. 9% 37.1% 47.7% 19.2% 21.9% 11.3% Q4 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 0.0536 0.1930 Q4 + HC 0.0891 0.2233 0.0925 0.2283 0.0772 0.2217 0.0623 0.2050 mejorar 66.2% 15.5% 72.6%) : Efecto de usar datos de clicsSolo para la clasificación de documentos. Consulta bayesint (µ = 0, ν = 5.0) Mapa PR@20docs Q2 0.0312 0.1150 Q2 + HC 0.0313 0.0950 Mejora.0.3% -17.4% Q3 0.0421 0.1483 Q3 + HC 0.0521 0.1820 Mejorar 23.8% 23.0% Q4 0.0536 0.1930 Q4 + HC 0.0620 0.1850 Mejora 15.7% -4.1% Tabla 6: Efecto de uso solo de datos de Clickhrough no relevantes 5.2.2Información de contexto comparando los resultados en la Tabla 1, la Tabla 2 y la Tabla 4, podemos ver que el beneficio de la información del historial de consultas y el de la información de clics son en su mayoría aditivos, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno solo, peroLa mayor parte de la mejora claramente proviene de la información de clics. En la Tabla 7, mostramos este efecto para el método Batchup.5.2.5 Sensibilidad de parámetros Los cuatro modelos tienen dos parámetros para controlar los pesos relativos de HQ, HC y QK, aunque la parametrización es diferente de un modelo a otro. En esta subsección, estudiamos la sensibilidad de los parámetros para Batchup, que parece funcionar relativamente mejor que otros. Batchup tiene dos parámetros µ y ν. Primero miramos µ. Cuando µ se establece en 0, el historial de consultas no se usa en absoluto, y esencialmente usamos los datos de clic combinados con la consulta actual. Si aumentamos µ, incorporaremos gradualmente más información de las consultas anteriores. En la Tabla 8, mostramos cómo cambia la precisión promedio de Batchup a medida que variamos µ con ν fijado a 15.0, donde se logra el mejor rendimiento de Batchup. Vemos que el rendimiento es mayormente insensible al cambio de µ para Q3 y Q4, pero está disminuyendo a medida que µ aumenta para Q2. El patrón también es similar cuando establecemos ν en otros valores. Además del hecho de que Q1 es generalmente peor que Q2, Q3 y Q4, otra posible razón por la cual la sensibilidad es menor para Q3 y Q4 puede ser que generalmente tenemos más datos de clic disponibles para Q3 y Q4 que para Q2 y elLa influencia dominante de los datos de clics ha hecho las pequeñas diferencias causadas por µ menos visibles para Q3 y Q4. El mejor rendimiento generalmente se logra cuando µ es alrededor de 2.0, lo que significa que la información de consulta pasada es tan útil como aproximadamente 2 palabras en la consulta actual. Excepto por Q2, claramente existe cierta compensación entre la consulta actual y el mapa de consultas de consultas anteriores PR@20DOCS Q2 0.0312 0.1150 Q2 + HQ 0.0287 0.0967 Mejora.-8.0% -15.9% Q2 + HC 0.0344 0.1167 Mejora.10.3% 1.5% Q2 + HQ + HC 0.0342 0.1100 Mejorar.9.6% -4.3% Q3 0.0421 0.1483 Q3 + HQ 0.0455 0.1450 Mejorar 8.1% -2.2% Q3 + HC 0.0513 0.1650 Mejorar 21.9% 11.3% Q3 + HQ + HC 0.0810 0.2067 Mejorar 92.4% 39.4% Q4 0.0536 0.1930 Q4 + HQ + HQ + HQ + HQ3.0% -0.8% Q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% Q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: beneficio aditivo de la información de contexto y el uso de una combinación equilibrada de ellos logran un mejor rendimiento que usar cada uno de ellossolo. Ahora pasamos al otro parámetro ν. Cuando ν está configurado en 0, solo usamos los datos de clics;Cuando ν se establece en +∞, solo usamos el historial de consulta y la consulta actual. Con µ establecido en 2.0, donde se logra el mejor rendimiento de Batchup, variamos ν y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando ν ≤ 30, con el mejor rendimiento a menudo logrado en ν = 15. Esto significa que la información combinada del historial de consultas y la consulta actual es tan útil como unas 15 palabras en los datos de clics, lo que indica que la información de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que Batchup no solo funciona mejor que otros métodos, sino que también es bastante robusto.6. Conclusiones y trabajo futuro En este documento, hemos explorado cómo explotar la información de retroalimentación implícita, incluido el historial de consultas y el historial de clics dentro de la misma sesión de búsqueda, para mejorar el rendimiento de la recuperación de la información. Utilizando el modelo de recuperación de divergencia KL como base, propusimos y estudiamos cuatro modelos de lenguaje estadístico para la recuperación de información sensible al contexto, es decir, FixInt, BayesInt, Onlineup y Batchup. Usamos datos de TREC AP para crear un conjunto de prueba µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 Q2 + HQ + HC PR@20 0.1333 0.1233 0.1100 0.1033 0.0217 0.09033333333333333333333333333333333333333333333333333333333333333333333333333333333NA ¿0.10333333333NA. 70.0783 0.0767 0.0750 Mapa 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 Q3 + HQ + HC PR@20 0.210 0.2150 0.2067 0.20567 0.205 0.2067 0.2067 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 Q4 + HQ +HC PR@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de µ en Batchup ν 0 1 2 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0296 0.0290 Q2 + HQ + HCPR@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 mapa 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 Q3 + hq + hq + hc@20 201917 2067 0.2017 0.1783 0.1600 0.1550 Mapa 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.09190.0761 0.0664 0.0625 Q4 + HQ + HC PR@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de ν en lote para evaluar modelos de retroalimentación implícitos. Los resultados del experimento muestran que el uso de la retroalimentación implícita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de la recuperación sin requerir ningún esfuerzo adicional del usuario. El trabajo actual se puede extender de varias maneras: primero, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar información de retroalimentación implícita. Sería interesante desarrollar modelos más sofisticados para explotar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar un resumen hecho de manera diferente dependiendo de si la consulta actual es una generalización o refinamiento de la consulta anterior. En segundo lugar, los modelos propuestos se pueden implementar en cualquier sistema práctico. Actualmente estamos desarrollando un agente de búsqueda personalizado del lado del cliente, que incorporará algunos de los algoritmos propuestos. También haremos un estudio de usuario para evaluar la efectividad de estos modelos en la búsqueda web real. Finalmente, debemos estudiar más a fondo un marco de recuperación general para la toma de decisiones secuenciales en la recuperación de información interactiva y estudiar cómo optimizar algunos de los parámetros en los modelos de recuperación sensibles al contexto.7. Agradecimientos Este material se basa en parte en el trabajo respaldado por la National Science Foundation bajo los números de premios IIS-0347933 e IIS-0428472. Agradecemos a los revisores anónimos por sus útiles comentarios.8. Referencias [1] E. Adar y D. Karger. Haystack: entornos de información por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. Desafíos en la recuperación de la información y el modelado de idiomas. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. SearchPad: Captura explícita del contexto de búsqueda para admitir la búsqueda web. En procedimiento de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. Comentarios y personalización de relevancia: una perspectiva de modelado de idiomas. En el taller de Segundo Delos: Sistemas de personalización y recomendación en bibliotecas digitales, 2001. [5] H. Cui, J.-R.Wen, J.-Y. Nie y W.-Y. Mamá. Expansión de consultas probabilísticas utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Currell, R. Sarin y E. Horvitz. Consultas implícitas (IQ) para la búsqueda contextualizada (descripción de demostración). En Actas de Sigir 2004, página 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocar la búsqueda en contexto: el concepto revisitado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien e Y. Oyang. Sugerencia de término basado en la sesión de consulta para la búsqueda web interactiva. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. Identificación de sesión de registro web dinámico con modelos de lenguaje estadístico. Journal of the American Society for Information Science and Technology, 55 (14): 1290-1303, 2004. [10] G. Jeh y J. Widom. Escala de búsqueda web personalizada. En procedimiento de WWW 2003, 2003. [11] T. Joachims. Optimización de los motores de búsqueda utilizando datos de clics. En Actas de Sigkdd 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar tiempo como retroalimentación implícita: comprensión de los efectos de la tarea. En Actas de Sigir 2004, 2004. [13] D. Kelly y J. Teevan. Comentarios implícitos para inferir la preferencia del usuario. Sigir Forum, 32 (2), 2003. [14] J. Rocchio. Relevancia de retroalimentación de retroalimentación. En los experimentos del sistema de recuperación inteligente en el procesamiento automático de documentos, páginas 313-323, Kansas City, MO, 1971. Prentice Hall.[15] X. Shen y C. Zhai. Explotación del historial de consultas para la clasificación de documentos en la recuperación de información interactiva (póster). En Actas de Sigir 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de búsqueda basado en sesión (póster). En Actas de Sigir 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil de usuario construido sin ningún esfuerzo de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. Van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentación implícitos. En Actas de ECIR 2004, páginas 311-326, 2004. [19] C. Zhai y J. Lafferty. Comentarios basados en modelos en el modelo de recuperación de divergencia KL. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. En Actas de Sigir 2001, 2001.