Marco de maximización de utilidad unificado para la selección de recursos Luo Si Language Technology Inst. School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 lsi@cs.cmu.edu Jamie Callan Language Technology Inst. School of Compute Science Carnegie Mellon University Pittsburgh, PA 15213 callan@cs.cmu.edu Resumen Este documento presenta un marco de utilidad unificado para la selección de recursos de la recuperación de información de texto distribuido. Este nuevo marco muestra una forma eficiente y efectiva de inferir las probabilidades de relevancia de todos los documentos en las bases de datos de texto. Con la información de relevancia estimada, la selección de recursos se puede hacer optimizando explícitamente los objetivos de diferentes aplicaciones. Específicamente, cuando se usa para la recomendación de la base de datos, la selección está optimizada para el objetivo de HighRecall (incluya tantos documentos relevantes como sea posible en las bases de datos seleccionadas);Cuando se usa para la recuperación de documentos distribuidos, la selección se dirige al objetivo de alta precisión (alta precisión en la lista final de documentos fusionados). Este nuevo modelo proporciona un marco más sólido para la recuperación de información distribuida. Los estudios empíricos muestran que es al menos tan efectivo como otros algoritmos de última generación. Categorías y descriptores de asignaturas H.3.3 [Búsqueda y recuperación de información]: Algoritmos de términos generales 1. Introducción Los motores de búsqueda convencionales como Google o Altavista utilizan una solución de recuperación de información ad-hoc suponiendo que todos los documentos de búsqueda se puedan copiar en una única base de datos centralizada con el propósito de indexación. La recuperación de información distribuida, también conocida como búsqueda federada [1,4,7,11,14,22] es diferente de la recuperación de información ad-hoc, ya que aborda los casos en que los documentos no se pueden adquirir y almacenar en una sola base de datos. Por ejemplo, los contenidos web ocultos (también llamados contenidos web invisibles o profundos) son información en la web a la que los motores de búsqueda convencionales no pueden acceder. Se ha estimado que los contenidos web ocultos son 2-50 [19] veces más grandes que los contenidos que pueden buscar en los motores de búsqueda convencionales. Por lo tanto, es muy importante buscar en este tipo de información valiosa. La arquitectura de la solución de búsqueda distribuida está altamente influenciada por diferentes características ambientales. En una pequeña red de área local, como entornos de pequeñas empresas, los proveedores de información pueden cooperar para proporcionar estadísticas de corpus o usar el mismo tipo de motores de búsqueda. La investigación de recuperación de información distribuida temprana se centró en este tipo de entornos cooperativos [1,8]. Por otro lado, en una red de área amplia, como entornos corporativos muy grandes o en la web, hay muchos tipos de motores de búsqueda y es difícil suponer que todos los proveedores de información pueden cooperar según sea necesario. Incluso si están dispuestos a cooperar en estos entornos, puede ser difícil hacer cumplir una solución única para todos los proveedores de información o para detectar si las fuentes de información proporcionan la información correcta según sea necesario. Muchas aplicaciones entran en el último tipo de entornos no cooperativos, como el Proyecto Mental [16] que integra bibliotecas digitales no cooperantes o el sistema Qprober [9] que admite la navegación y la búsqueda de bases de datos web ocultas no cooperativas. En este artículo, nos centramos principalmente en entornos no cooperativos que contienen múltiples tipos de motores de búsqueda independientes. Hay tres subproblemas importantes en la recuperación de información distribuida. Primero, se debe adquirir información sobre el contenido de cada base de datos individual (representación de recursos) [1,8,21]. En segundo lugar, dada una consulta, se debe seleccionar un conjunto de recursos para hacer la búsqueda (selección de recursos) [5,7,21]. En tercer lugar, los resultados recuperados de todos los recursos seleccionados deben fusionarse en una sola lista final antes de que pueda presentarse al usuario final (recuperación y fusión de resultados) [1,5,20,22]. Existen muchos tipos de soluciones para la recuperación de información distribuida. Invisible-web.net1 proporciona navegación guiada de bases de datos web ocultas mediante la recopilación de las descripciones de recursos de estas bases de datos y construyendo jerarquías de clases que las agrupan mediante temas similares. Un sistema de recomendación de la base de datos va un paso más allá de un sistema de navegación como invisible-web.net recomendando la mayoría de las fuentes de información relevantes para las consultas de los usuarios. Se compone de la descripción de recursos y los componentes de selección de recursos. Esta solución es útil cuando los usuarios desean explorar las bases de datos seleccionadas por sí mismos en lugar de pedirle al sistema que recupere los documentos relevantes automáticamente. La recuperación de documentos distribuidos es una tarea más sofisticada. Selecciona fuentes de información relevantes para consultas de usuarios como lo hace el sistema de recomendación de la base de datos. Además, las consultas de los usuarios se reenvían a las bases de datos seleccionadas correspondientes y las listas clasificadas individuales devueltas se fusionan en una lista única para presentar a los usuarios. El objetivo de un sistema de recomendación de base de datos es seleccionar un pequeño conjunto de recursos que contengan tantos documentos relevantes como sea posible, que llamamos un objetivo de alta recuperación. Por otro lado, la efectividad de la recuperación de documentos distribuidos a menudo se mide por la precisión de la lista final de resultados de documentos fusionados, que llamamos un objetivo de alta precisión. Investigaciones previas indicaron que estos dos objetivos están relacionados pero no idénticos [4,21]. Sin embargo, la mayoría de las soluciones anteriores simplemente utilizan un algoritmo efectivo de selección de recursos del sistema de recomendación de la base de datos para el sistema de recuperación de documentos distribuido o resuelven la inconsistencia con métodos heurísticos [1,4,21]. Este documento presenta un marco de maximización de utilidad unificado para integrar el problema de selección de recursos de la recomendación de la base de datos y la recuperación de documentos distribuidos juntos mediante el tratamiento de ellos como objetivos de optimización diferentes. Primero, una base de datos de muestra centralizada se crea muestras aleatorias de una pequeña cantidad de documentos de cada base de datos con muestreo basado en consultas [1];Las estadísticas de tamaño de la base de datos también se estiman [21]. Se aprende un modelo de transformación logística fuera de línea con una pequeña cantidad de consultas de entrenamiento para asignar los puntajes de documentos centralizados en la base de datos de muestra centralizada a las probabilidades de relevancia correspondientes. En segundo lugar, después de enviar una nueva consulta, la consulta se puede utilizar para buscar la base de datos de muestra centralizada que produce una puntuación para cada documento muestreado. La probabilidad de relevancia para cada documento en la base de datos de muestra centralizada se puede estimar aplicando el modelo logístico a cada puntaje de documentos. Luego, las probabilidades de relevancia de todos los documentos (en su mayoría invisibles) entre las bases de datos disponibles se pueden estimar utilizando las probabilidades de relevancia de los documentos en la base de datos de muestra centralizada y las estimaciones del tamaño de la base de datos. Para la tarea de selección de recursos para un sistema de recomendación de base de datos, las bases de datos pueden ser clasificadas por el número esperado de documentos relevantes para cumplir con el objetivo de alta recuperación. Para la selección de recursos para un sistema de recuperación de documentos distribuido, las bases de datos que contienen un pequeño número de documentos con grandes probabilidades de relevancia se favorecen sobre las bases de datos que contienen muchos documentos con pequeñas probabilidades de relevancia. Este criterio de selección cumple con el objetivo de alta precisión de la aplicación distribuida de recuperación de documentos. Además, el algoritmo de aprendizaje semi-supervisado (SSL) [20,22] se aplica para fusionar los documentos devueltos en una lista final clasificada. El marco de utilidad unificado hace muy pocos supuestos y funciona en entornos no cooperativos. Dos características clave lo convierten en un modelo más sólido para la recuperación de información distribuida: i) Formaliza los problemas de selección de recursos de diferentes aplicaciones como diversas funciones de utilidad, y optimiza las funciones de utilidad para lograr los resultados óptimos en consecuencia;y ii) muestra una forma efectiva y eficiente de estimar las probabilidades de relevancia de todos los documentos en todas las bases de datos. Específicamente, el marco construye modelos logísticos en la base de datos de muestra centralizada para transformar los puntajes de recuperación centralizados en las probabilidades de relevancia correspondientes y utiliza la base de datos de muestra centralizada como el puente entre las bases de datos individuales y el modelo logístico. El esfuerzo humano (juicio de relevancia) requerido para capacitar al modelo logístico centralizado único no se escala con el número de bases de datos. Esta es una gran ventaja sobre investigaciones anteriores, que requirió la cantidad de esfuerzo humano para ser lineal con el número de bases de datos [7,15]. El marco de utilidad unificado no solo es más teóricamente sólido sino también muy efectivo. Los estudios empíricos muestran que el nuevo modelo es al menos tan preciso como los algoritmos de última generación en una variedad de configuraciones. La siguiente sección discute el trabajo relacionado. La Sección 3 describe el nuevo modelo de maximización de utilidad unificado. La Sección 4 explica nuestra metodología experimental. Las secciones 5 y 6 presentan nuestros resultados experimentales para la selección de recursos y la recuperación de documentos. La sección 7 concluye.2. Investigación previa ha habido una investigación considerable sobre todos los subproblemas de la recuperación de información distribuida. Encuestamos el trabajo más relacionado en esta sección. El primer problema de recuperación de información distribuida es la representación de recursos. El protocolo de inicio es una solución para adquirir descripciones de recursos en entornos cooperativos [8]. Sin embargo, en entornos no cooperativos, incluso las bases de datos están dispuestas a compartir su información, no es fácil juzgar si la información que proporcionan es precisa o no. Además, no es fácil coordinar las bases de datos para proporcionar representaciones de recursos que sean compatibles entre sí. Por lo tanto, en entornos no cooperativos, una opción común es el muestreo basado en la consulta, que genera y envía consultas a los motores de búsqueda individuales y recupera algunos documentos para construir las descripciones. Como los documentos muestreados son seleccionados por consultas aleatorias, el muestreo basado en consultas no es fácilmente engañado por ningún spammer adversario que esté interesado en atraer más tráfico. Los experimentos han demostrado que se pueden construir descripciones de recursos bastante precisas enviando alrededor de 80 consultas y descargando unos 300 documentos [1]. Se han propuesto muchos algoritmos de selección de recursos como GGloss/VGloss [8] y Cori [1] en la última década. El algoritmo CORI representa cada base de datos por sus términos, las frecuencias del documento y un pequeño número de estadísticas de Corpus (detalles en [1]). Como la investigación previa en diferentes conjuntos de datos ha demostrado que el algoritmo CORI es el más estable y efectivo de los tres algoritmos [1,17,18], lo usamos como un algoritmo de referencia en este trabajo. El algoritmo de selección de recursos relevante de estimación de distribución de documentos (REDDE [21]) es un algoritmo reciente que intenta estimar la distribución de documentos relevantes en las bases de datos disponibles y clasifica las bases de datos en consecuencia. Aunque se ha demostrado que el algoritmo Redde es efectivo, se basa en constantes heurísticas que se establecen empíricamente [21]. El último paso del subproblema de recuperación de documentos es la fusión de resultados, que es el proceso de transformar las puntuaciones de documentos de 33 bases específicas de la base de datos en puntajes comparables de documentos independientes de la base de datos. El algoritmo de fusión de resultados semi supervisado (SSL) [20,22] utiliza los documentos adquiridos por muestreo basado en consultas como datos de entrenamiento y regresión lineal para aprender los modelos de fusión específicos de la base de datos específicos de la consulta. Estos modelos lineales se utilizan para convertir los puntajes de documentos específicos de la base de datos en los puntajes de documentos centralizados aproximados. Se ha demostrado que el algoritmo SSL es efectivo [22]. Sirve como un componente importante de nuestro marco de maximización de utilidad unificado (Sección 3). Para lograr resultados precisos de recuperación de documentos, muchos métodos anteriores simplemente utilizan algoritmos de selección de recursos que son efectivos del sistema de recomendación de la base de datos. Pero como se señaló anteriormente, un buen algoritmo de selección de recursos optimizado para alta recuperación puede no funcionar bien para la recuperación de documentos, lo que se dirige al objetivo de alta precisión. Este tipo de inconsistencia se ha observado en investigaciones anteriores [4,21]. La investigación en [21] trató de resolver el problema con un método heurístico. La investigación más similar a lo que proponemos aquí es el marco teórico de decisión (DTF) [7,15]. Este marco calcula una selección que minimiza los costos generales (por ejemplo, calidad de recuperación, tiempo) del sistema de recuperación de documentos y varios métodos [15] para estimar la calidad de la recuperación. Sin embargo, dos puntos distinguen nuestra investigación del modelo DTF. Primero, el DTF es un marco diseñado específicamente para la recuperación de documentos, pero nuestro nuevo modelo integra dos aplicaciones distintas con diferentes requisitos (recomendación de la base de datos y recuperación de documentos distribuidos) en el mismo marco unificado. En segundo lugar, el DTF crea un modelo para cada base de datos para calcular las probabilidades de relevancia. Esto requiere juicios de relevancia humana para los resultados recuperados de cada base de datos. En contraste, nuestro enfoque solo construye un modelo logístico para la base de datos de muestra centralizada. La base de datos de muestra centralizada puede servir como un puente para conectar las bases de datos individuales con el modelo logístico centralizado, por lo tanto, se pueden estimar las probabilidades de relevancia de los documentos en diferentes bases de datos. Esta estrategia puede ahorrar una gran cantidad de esfuerzo de juicio humano y es una gran ventaja del marco de maximización de la utilidad unificada sobre el DTF, especialmente cuando hay una gran cantidad de bases de datos.3. Marco de maximización de la utilidad unificada El marco de maximización de la utilidad unificada (UUM) se basa en estimar las probabilidades de relevancia de los documentos (en su mayoría invisibles) disponibles en el entorno de búsqueda distribuido. En esta sección describimos cómo se estiman las probabilidades de relevancia y cómo el modelo de maximización de utilidad unificado las usa. También describimos cómo el modelo se puede optimizar para el objetivo de alta recuperación de un sistema de recomendación de la base de datos y el objetivo de alta precisión de un sistema de recuperación de documentos distribuido.3.1 Estimación de probabilidades de relevancia Como se señaló anteriormente, el propósito de la selección de recursos es alto y el propósito de la recuperación de documentos es de alta precisión. Para cumplir con estos objetivos diversos, el tema clave es estimar las probabilidades de relevancia de los documentos en varias bases de datos. Este es un problema difícil porque solo podemos observar una muestra del contenido de cada base de datos utilizando el muestreo basado en consultas. Nuestra estrategia es hacer uso completo de toda la información disponible para calcular las estimaciones de probabilidad.3.1.1 Probabilidades de aprendizaje de relevancia En el paso de descripción de recursos, la base de datos de muestra centralizada se crea mediante un muestreo basado en consultas y los tamaños de base de datos se estiman utilizando el método de resumen de muestra [21]. Al mismo tiempo, se aplica un algoritmo de recuperación efectivo (investigación [2]) en la base de datos de muestra centralizada con un pequeño número (por ejemplo, 50) de consultas de capacitación. Para cada consulta de capacitación, el algoritmo de selección de recursos CORI [1] se aplica para seleccionar algún número (por ejemplo, 10) de bases de datos y recuperar 50 ID de documento de cada base de datos. El algoritmo de fusión de resultados SSL [20,22] se utiliza para fusionar los resultados. Luego, podemos descargar los 50 documentos principales en la lista final fusionada y calcular sus puntajes centralizados correspondientes utilizando la investigación y las estadísticas del corpus de la base de datos de muestra centralizada. Los puntajes centralizados se normalizan aún más (divididos por la puntuación centralizada máxima para cada consulta), ya que se ha sugerido este método para mejorar la precisión de la estimación en investigaciones anteriores [15]. El juicio humano se adquiere para esos documentos y se crea un modelo logístico para transformar los puntajes de documentos centralizados normalizados a probabilidades de relevancia de la siguiente manera: ()) (exp (1)) (exp (|) (_ _ DSBA DSBA DRELPDR CCC CCC++ + == (1) donde) (_ DSC es la puntuación de documento centralizada normalizada y AC y BC son los dos parámetros del modelo logístico. Estos dos parámetros se estiman maximizando las probabilidades de relevancia de las consultas de capacitación. El modelo logístico nos proporciona la herramienta para calcular las probabilidades de relevancia a partir de puntajes de documentos centralizados.3.1.2 Estimación de puntajes de documentos centralizados Cuando el usuario envía una nueva consulta, se calculan los puntajes de documentos centralizados de los documentos en la base de datos de muestra centralizada. Sin embargo, para calcular las probabilidades de relevancia, necesitamos estimar los puntajes de documentos centralizados para todos los documentos en las bases de datos en lugar de solo los documentos muestreados. Este objetivo se logra utilizando: las puntuaciones centralizadas de los documentos en la base de datos de muestra centralizada y las estadísticas de tamaño de la base de datos. Definimos el factor de escala de la base de datos para la base de datos IPH como la relación del tamaño estimado de la base de datos y el número de documentos muestreados de esta base de datos de la siguiente manera: ^ _ i i i db db db samp n sf n = (2) donde ^ idbn es el estimado estimadoEl tamaño de la base de datos y el SAMPN _IDB es el número de documentos de la base de datos IPH en la base de datos de muestra centralizada. La intuición detrás del factor de escala de la base de datos es que, para una base de datos cuyo factor de escala es 50, si un documento de esta base de datos en la base de datos de muestra centralizada tiene una puntuación de documento centralizada de 0.5, podemos suponer que hay alrededor de 50 documentos en esa base de datos.que tienen puntajes de aproximadamente 0.5. En realidad, podemos aplicar un método de interpolación lineal no paramétrico más fino para estimar la curva de puntaje de documento centralizado para cada base de datos. Formalmente, clasificamos todos los documentos muestreados de la base de datos IPH por sus puntajes centralizados del Documento 34 para obtener la lista de puntaje de documentos centralizados muestreados {SC (DSI1), SC (DSI2), SC (DSI3), ... ..};Suponemos que si pudiéramos calcular los puntajes de documentos centralizados para todos los documentos en esta base de datos y obtenga la lista completa de puntajes de documentos centralizados, el documento superior en la lista muestreada tendría un rango SFDBI/2, el segundo documento en la lista de muestras rangoSFDBI3/2, y así sucesivamente. Por lo tanto, los puntos de datos de los documentos muestreados en la lista completa son: {(sfdbi/2, sc (dsi1)), (sfdbi3/2, sc (dsi2)), (sfdbi5/2, sc (dsi3)), ....}. La interpolación lineal por partes se aplica para estimar la curva de puntaje de documento centralizado, como se ilustra en la Figura 1. La lista completa de puntaje de documentos centralizados se puede estimar calculando los valores de diferentes rangos en la curva de documento centralizado como:], 1 [,) (S ^^ c idbij njd ∈. Se puede ver en la Figura 1 que más puntos de datos de muestra producen estimaciones más precisas de las curvas de puntaje de documentos centralizados. Sin embargo, para bases de datos con grandes relaciones de escala de bases de datos, este tipo de interpolación lineal puede ser bastante inexacta, especialmente para los documentos más clasificados (por ejemplo, [1, SFDBI/2]). Por lo tanto, se propone una solución alternativa para estimar las puntuaciones de documentos centralizadas de los documentos clasificados para bases de datos con relaciones a gran escala (por ejemplo, mayores de 100). Específicamente, se crea un modelo logístico para cada una de estas bases de datos. El modelo logístico se utiliza para estimar la puntuación de documento centralizado del documento Top 1 en la base de datos correspondiente utilizando los dos documentos muestreados de esa base de datos con puntajes centralizados más altos.)) () (exp (1)) () (Exp () (22110 22110 ^ 1 Iciicii Iciicii IC DSSDSSS DSSDSS DS ααα αααα +++ ++ = (3) 0iα, 1iα y 2iα son los parámetros del modelo logístico del modelo. Para cada consulta de capacitación, se descarga el documento recuperado de cada base de datos y se calcula el puntaje de documento centralizado correspondiente. Junto con los puntajes de los dos principales documentos muestreados, estos parámetros se pueden estimar. Después de estimar la puntuación centralizada del documento superior, se ajusta una función exponencial para la parte superior ([1, SFDBI/2]) de la curva de puntaje de documento centralizado como:] 2/, 1 [)*exp () (10^ idbiiijc sfjjds ∈+= ββ (4) ^ 0 1 1Log (()) I C I es Dβ β = - (5)) 12/()) (log () ((log ( ^ 11 1 - - = IDB ICIC ISF DSDSS β (6) Los dos parámetros 0iβ y 1iβ están ajustados para asegurarse de que la función exponencial pase a través de los dos puntos (1, ^ 1) (IC DS) y (SFDBI/2, SC (DSI1)). La función exponencial solo se usa para ajustar la parte superior de la curva de puntuación de documento centralizada y la parte inferior de la curva todavía está equipada con el método de interpolación lineal descrito anteriormente. El ajuste al ajustar la función exponencial de los documentos mejor clasificados se ha demostrado empíricamente para producir resultados más precisos. Desde las curvas de puntaje de documentos centralizados, podemos estimar las listas completas de puntaje de documentos centralizados en consecuencia para todas las bases de datos disponibles. Después de que los puntajes de documentos centralizados estimados se normalizan, las listas completas de probabilidades de relevancia pueden construirse a partir de las listas de puntaje de documentos centralizados completos mediante la Ecuación 1. Formalmente para la base de datos IPH, la lista completa de probabilidades de relevancia es:], 1 [,) (r ^^ idbij njd ∈. 3.2 El modelo de maximización de utilidad unificado En esta sección, definimos formalmente el nuevo modelo de maximización de utilidad unificado queOptimiza los problemas de selección de recursos para dos objetivos de alta recuperación (recomendación de la base de datos) y de alta precisión (recuperación de documentos distribuidos) en el mismo marco. En la tarea de recomendación de la base de datos, el sistema debe decidir cómo clasificar las bases de datos. En la tarea de recuperación de documentos, el sistema no solo necesita seleccionar las bases de datos, sino que también necesita decidir cuántos documentos recuperar de cada base de datos seleccionada. Generalizamos el proceso de selección de recomendación de la base de datos, que recomienda implícitamente todos los documentos en cada base de datos seleccionada, como un caso especial de la decisión de selección para la tarea de recuperación de documentos. Formalmente, denotamos DI como el número de documentos que nos gustaría recuperar de la base de datos IPH y, .....}, {21 ddd = como una acción de selección para todas las bases de datos. La decisión de selección de la base de datos se toma en función de las listas completas de probabilidades de relevancia para todas las bases de datos. Las listas completas de probabilidades de relevancia se infieren de toda la información disponible específicamente SR, que representa las descripciones de recursos adquiridas por muestreo basado en la consulta y las estimaciones de tamaño de la base de datos adquiridas por resolución de muestras;CS significa los puntajes de documentos centralizados de los documentos en la base de datos de muestra centralizada. Si el método para estimar los puntajes de documentos centralizados y las probabilidades de relevancia en la Sección 3.1 es aceptable, entonces las listas completas más probables de probabilidades de relevancia pueden derivarse y las denotamos como 1 ^ ^ * 1 {(R (), [1,]), dbjd j nθ = ∈ 2 ^ ^ 2 (r (), [1,]), .......} dbjd j n∈. El vector aleatorio denota un conjunto arbitrario de listas completas de probabilidades de relevancia y), | (CS SRP θ como la probabilidad de generar este conjunto de listas. Finalmente, a cada acción de selección D y un conjunto de listas completas de la Figura 1. Construcción de interpolación lineal de la lista completa de puntajes de documentos centralizados (el factor de escala de base de datos es 50).35 Probabilidades de relevancia θ, asociamos una función de utilidad) (du θ que indica el beneficio de hacer la selección D cuando las verdaderas listas completas de probabilidades de relevancia son θ. Por lo tanto, la decisión de selección definida por el marco bayesiano es: θθθ θ dsrpdud cs d). | (), (Maxarg * = (7) un enfoque común para simplificar el cálculo en el marco bayesiano es calcular solo la función de utilidad en elLos valores de parámetros más probables en lugar de calcular toda la expectativa. En otras palabras, solo necesitamos calcular), ( * du θ y la ecuación 7 se simplifica de la siguiente manera :), (maxarg * * θdud d = (8) Esta ecuación sirve como el modelo básico tanto para el sistema de recomendación de la base de datos como para el sistema de recomendación de la base de datos como paraSistema de recuperación de documentos. 3.3 La selección de recursos para alta recuperación de alta recuperación es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada, como la recomendación de la base de datos. El objetivo es seleccionar un pequeño conjunto de recursos (por ejemplo, menos de las bases de datos NSDB) que contienen tantos documentos relevantes como sea posible, que se puede definir formalmente como: = = i n j iji idb ddidu ^ 1 ^ *) (r) (r)(), (θ (9) I (Di) es la función indicadora, que es 1 cuando la base de datos IPH se selecciona y 0 de lo contrario. Conecte esta ecuación en el modelo básico en la Ecuación 8 y asocie la restricción del número de base de datos seleccionada para obtener lo siguiente: SDB I I I N J IJI D NDITOSUBEXT DDID IDB = = =) (:) (R) (Maxarg ^ 1 ^* (10)La solución de este problema de optimización es muy simple. Podemos calcular el número esperado de documentos relevantes para cada base de datos de la siguiente manera: = = Idb I n J ijrd dn ^ 1 ^^) (r (11) Las bases de datos NSDB con el mayor número esperado de documentos relevantes pueden seleccionarse para cumplir con elMeta de alta recuperación. Llamamos a esto el algoritmo UUM/HR (maximización de utilidad unificada para alta recuperación).3.4 La selección de recursos para la alta precisión de alta precisión es el objetivo del algoritmo de selección de recursos en tareas de búsqueda federada, como la recuperación de documentos distribuidos. Se mide por la precisión en la parte superior de la lista final de documentos fusionados. Este criterio de alta precisión se realiza mediante la siguiente función de utilidad, que mide la precisión de los documentos recuperados de las bases de datos seleccionadas.= = i d j iji i ddidu 1 ^ *) (r) (), (θ (12) Tenga en cuenta que la diferencia clave entre la ecuación 12 y la ecuación 9 es que la ecuación 9 suma las probabilidades de relevancia de todos los documentos en una base de datos,mientras que la ecuación 12 solo considera una parte mucho más pequeña de la clasificación. Específicamente, podemos calcular la decisión de selección óptima de: = = I d J iji d i ddid 1 ^*) (r) (maxarg (13) diferentes tipos de restricciones causadas por diferentes características de las tareas de recuperación de documentos pueden asociarse con el problema de optimización anterior. El más común es seleccionar un número fijo (NSDB) de bases de datos y recuperar un número fijo (NRDOC) de documentos de cada base de datos seleccionada, definido formalmente como: 0,) (:) (R) (Maxarg 1 ^* ≠ = == = = Irdoci Sdb I I I D J IJI D Difnd nditosubject Ddid I (14) Este problema de optimización se puede resolver fácilmente calculando el número de documentos relevantes esperados en la parte superior de cada lista de datos completa de probabilidades de relevancia: = = RDOC I n n n NJ ijrdtop dn 1 ^^ _) (r (15) Entonces las bases de datos pueden clasificarse por estos valores y seleccionados. Llamamos a esto el algoritmo UUM/HP-FL (maximización de utilidad unificada para una alta precisión con clasificaciones de documentos de longitud fija de cada base de datos seleccionada). Una situación más compleja es variar el número de documentos recuperados de cada base de datos seleccionada. Más específicamente, permitimos que diferentes bases de datos seleccionadas devuelvan diferentes números de documentos. Para la simplificación, las longitudes de la lista de resultados deben ser múltiplos de un número de línea de base 10. (Este valor también puede variar, pero para la simplificación se establece en 10 en este documento). Esta restricción está configurada para simular el comportamiento de los motores de búsqueda comerciales en la web.(Los motores de búsqueda como Google y AltaVista devuelven solo 10 o 20 ID de documento para cada página de resultados). Este procedimiento ahorra el tiempo de cálculo de calcular la selección óptima de la base de datos al permitir que el paso de programación dinámica sea 10 en lugar de 1 (más detalles se discute más tarde). Para una simplificación adicional, restringimos a seleccionar en la mayoría de los 100 documentos de cada base de datos (di <= 100), entonces, el problema de optimización de selección se formaliza de la siguiente manera:] 10 .. ,, 2,1,0 [,*10) (::) (R) (maxarg _ 1 ^* ∈ = = = = = kkd nd nditosubject ddid i rdoctotal i i sdb i i i d j iji d i (16) ntotal_rdoc es el número total de documentos que se recuperarán. Desafortunadamente, no hay una solución simple para este problema de optimización, ya que hay para las ecuaciones 10 y 14. Sin embargo, se puede aplicar un algoritmo de programación dinámico 36 para calcular la solución óptima. Los pasos básicos de este método de programación dinámica se describen en la Figura 2. Como este algoritmo permite recuperar listas de resultados de longitudes variables de cada base de datos seleccionada, se llama algoritmo UUM/HP-VL. Después de tomar las decisiones de selección, se buscan las bases de datos seleccionadas y las ID de documento correspondientes se recuperan de cada base de datos. El último paso de la recuperación de documentos es fusionar los resultados devueltos en una lista de clasificación única con el algoritmo de aprendizaje semisuuficiente. Se señaló antes que el algoritmo SSL asigna los puntajes específicos de la base de datos en las puntuaciones de documentos centralizados y construye la lista final clasificada en consecuencia, lo que es consistente con todos nuestros procedimientos de selección donde documentos con mayores probabilidades de relevancia (por lo tanto, puntajes de documentos centralizados más altos)se seleccionan.4. Metodología experimental 4.1 Test Bacets Es deseable evaluar los algoritmos de recuperación de información distribuida con tallas de prueba que simulan estrechamente las aplicaciones del mundo real. Las colecciones web TREC WT2G o WT10G [4,13] proporcionan una forma de dividir documentos de diferentes servidores web. De esta manera, se podría crear un gran número (O (1000)) de bases de datos con contenidos bastante diversos, lo que puede hacer de este Testbed un buen candidato para simular los entornos operativos, como la web oculta de dominio abierto. Sin embargo, dos debilidad de esta prueba de prueba son: i) Cada base de datos contiene solo una pequeña cantidad de documento (259 documentos por promedio para WT2G) [4];y ii) el contenido de WT2G o WT10G se arrastran arbitrariamente de la web. No es probable que una base de datos web oculta proporcione páginas de inicio personales o páginas web que indican que las páginas están en construcción y no hay información útil. Estos tipos de páginas web están contenidas en los conjuntos de datos WT2G/WT10G. Por lo tanto, los datos web ruidosos no son similares a los de los contenidos de la base de datos web ocultas de alta calidad, que generalmente están organizados por expertos en dominios. Otra opción son las noticias del TREC/datos del gobierno [1,15,17, 18,21]. TREC News/Gobierno Datos se concentran en temas relativamente estrechos. En comparación con los datos web de TREC: i) Las noticias/documentos gubernamentales son mucho más similares a los contenidos proporcionados por una base de datos orientada al tema que una página web arbitraria, ii) una base de datos en este banco de pruebas es más grande que la de los datos web de TREC. Por promedio, una base de datos contiene miles de documentos, que es más realista que una base de datos de datos web TREC con aproximadamente 250 documentos. Como el contenido y los tamaños de las bases de datos en TREC News/Gobierno Testbed son más similares a los de una base de datos orientada al tema, es un buen candidato para simular los entornos de recuperación de información distribuidos de grandes organizaciones (empresas) o sitios web ocultos específicos de dominios, como West que proporciona acceso a bases de datos de texto legales, financieras y de noticias [3]. Como la mayoría de los sistemas de recuperación de información distribuida actuales se desarrollan para los entornos de grandes organizaciones (empresas) o la red oculta específica de dominios que no se elaboran en este trabajo, no se eligió en este trabajo. TREC123-100COL-BYSOURCE Testbed es uno de los TREC News/Gobierno Testbed más utilizado [1,15,17,21]. Fue elegido en este trabajo. También se utilizaron tres tallas en [21] con distribuciones de tamaño de base de datos sesgadas y diferentes tipos de distribuciones de documentos relevantes para dar una simulación más exhaustiva para entornos reales. TREC123-100COL-BYSOURCE: se crearon 100 bases de datos a partir de TREC CDS 1, 2 y 3. Fueron organizados por fuente y fecha de publicación [1]. Los tamaños de las bases de datos no están sesgados. Los detalles están en la Tabla 1. Tres tallas construidas en [21] se basaron en el TREC123-100ColbySource Testbed. Cada testbecho contiene muchas bases de datos pequeñas y dos grandes bases de datos creadas mediante la fusión de aproximadamente 10-20 bases de datos pequeñas juntas. Entrada: listas completas de probabilidades de relevancia para todos los | db |bases de datos. Salida: Solución de selección óptima para la ecuación 16. i) Cree la matriz tridimensional: SEL (1 .. | DB |, 1..ntotal_rdoc/10, 1..nsdb) Cada sel (x, y, z) está asociadoCon una decisión de selección XYZD, que representa la mejor decisión de selección en la condición: solo las bases de datos del número 1 al número X se consideran para la selección;Totalmente y*10 documentos serán recuperados;Solo las bases de datos z se seleccionan fuera de los candidatos de base de datos X. Y SEL (X, Y, Z) es el valor de utilidad correspondiente al elegir la mejor selección.ii) Inicializar SEL (1, 1..ntotal_rdoc/10, 1..nsdb) con solo la información de relevancia estimada de la primera base de datos.iii) iterar el candidato de la base de datos actual I de 2 a | db |Para cada entrada sel (i, y, z): encuentre k tal que :) 10, min (1:)) () 1 ,, 1 ((maxarg * 10 ^ * yktosubject drzkyiselk kj ij k ≤≤ +−−−= ≤) ,, 1 ()) () 1 ,, 1 (( * * 10 ^ * zyiseldrzkyiselif kj ij −>+−−− ≤ Esto significa que debemos recuperar * 10 k ∗ documentos de la base de datos, de lo contrario nosotrosNo debe seleccionar esta base de datos y se debe mantener la mejor solución anterior SEL (I-1, Y, Z). Luego establezca el valor de IYZD y SEL (i, y, z) en consecuencia.iv) La mejor solución de selección viene dada por _ /10 ||Toral RDOC SDBDB N ND y el valor de utilidad correspondiente es SEL (| DB |, NTOTAL_RDOC/10, NSDB). Figura 2. El procedimiento de optimización de programación dinámica para la ecuación 16. Tabla 1: Estadísticas de Testbed. Número de documentos Tamaño (MB) Tamaño de la prueba (GB) Min AVG MAX MIN AVG MAX TREC123 3.2 752 10782 39713 28 32 42 Tabla2: Estadísticas de conjunto de consultas. Nombre TREC TEMA SET TREC TEMA CAMPO Longitud promedio (palabras) TREC123 51-150 Título 3.1 37 TREC123-2LDB-60COL (Representante): Las bases de datos en el TREC123-100COL-BYSOURCE se clasificaron con orden alfabético. Se crearon dos grandes bases de datos fusionando 20 pequeñas bases de datos con el método de redondeo. Por lo tanto, las dos grandes bases de datos tienen documentos más relevantes debido a sus grandes tamaños, a pesar de que las densidades de los documentos relevantes son aproximadamente las mismas que las bases de datos pequeñas. TREC123-AP-WSJ-60COL (relevante): las 24 colecciones de Associated Press y las 16 colecciones de Wall Street Journal en la prueba TREC123-100COL-BySource se colapsaron en dos grandes bases de datos Apall y Wsjall. Las otras 60 colecciones quedaron sin cambios. Las bases de datos APALL y WSJALL tienen densidades más altas de documentos relevantes para las consultas TREC que las bases de datos pequeñas. Por lo tanto, las dos grandes bases de datos tienen muchos documentos más relevantes que las bases de datos pequeñas. TREC123-FR-DOE-81COL (no relevante): las 13 colecciones del Registro Federal y las 6 colecciones del Departamento de Energía en el TREC123-100COL-BySource Testbed se colapsaron en dos grandes bases de datos Fall y Doeall. Las otras 80 colecciones quedaron sin cambios. Las bases de datos Fall y Doeall tienen densidades más bajas de documentos relevantes para las consultas TREC que las bases de datos pequeñas, a pesar de que son mucho más grandes.Se crearon 100 consultas a partir de los campos de título de los temas de TREC 51-150. Las consultas 101-150 se usaron como consultas de entrenamiento y las consultas 51-100 se usaron como consultas de prueba (detalles en la Tabla 2).4.2 Motores de búsqueda En los entornos de recuperación de información distribuida no cooperativa de grandes organizaciones (empresas) o la web oculta específica de dominios, diferentes bases de datos pueden usar diferentes tipos de motores de búsqueda. Para simular el entorno de motor múltiple, se utilizaron tres tipos diferentes de motores de búsqueda en los experimentos: investigación [2], un modelo de lenguaje estadístico unigram con suavizado lineal [12,20] y un algoritmo de recuperación de TFIDF con peso LTC [12,20]. Todos estos algoritmos se implementaron con el kit de herramientas Lemur [12]. Estos tres tipos de motores de búsqueda se asignaron a las bases de datos entre las cuatro bolas de prueba de manera redonda.5. Resultados: la selección de recursos de la recomendación de la base de datos Las cuatro bolas de prueba descritas en la Sección 4 se utilizaron en los experimentos para evaluar la efectividad de la selección de recursos del sistema de recomendación de la base de datos. Las descripciones de recursos se crearon utilizando muestreo basado en consultas. Se enviaron alrededor de 80 consultas a cada base de datos para descargar 300 documentos únicos. Las estadísticas de tamaño de la base de datos se estimaron mediante el método de resumen de muestra [21]. Se utilizaron cincuenta consultas (101-150) como consultas de capacitación para construir el modelo logístico relevante y para adaptarse a las funciones exponenciales de las curvas de puntaje de documentos centralizados para bases de datos de relaciones grandes (detalles en la Sección 3.1). Otras 50 consultas (51-100) se usaron como datos de prueba. Los algoritmos de selección de recursos de los sistemas de recomendación de la base de datos se comparan típicamente utilizando el NR métrico de recuperación [1,17,18,21]. Sea B denotar una clasificación de línea de base, que a menudo es el RBR (clasificación basada en la relevancia), y E como una clasificación proporcionada por un algoritmo de selección de recursos. Y deje que BI y EI denoten el número de documentos relevantes en la base de datos clasificada de B o E. Entonces RN se define de la siguiente manera: = = = K I I K I I K B E R 1 1 (17) Por lo general, el objetivo es buscar solo unas pocas bases de datos, por lo queNuestras cifras solo muestran resultados para seleccionar hasta 20 bases de datos. Los experimentos resumidos en la Figura 3 compararon la efectividad de los tres algoritmos de selección de recursos, a saber, los Cori, Redde y UUM/HR. El algoritmo UUM/HR se describe en la Sección 3.3. Se puede ver en la Figura 3 que los algoritmos Redde y UUM/HR son más efectivos (en los tallas de prueba Representantes, relevantes y no relevantes) o tan buenos como (en el TREC123-100Col Testbed) el algoritmo de selección de recursos CORI. El algoritmo UUM/HR es más efectivo que el algoritmo REDDE en el representante y los bacos de prueba relevantes y es casi el mismo que el algoritmo Redde en el TREC123100col y las tallas de pruebas no relevantes. Esto sugiere que el algoritmo UUM/HR es más robusto que el algoritmo Redde. Cabe señalar que al seleccionar solo unas pocas bases de datos en el TREC123-100col o las bases de pruebas no relacionadas, el algoritmo redee tiene una pequeña ventaja sobre el algoritmo UUM/HR. Atribuimos esto a dos causas: i) el algoritmo Redde se sintonizó en el TREC123-100col Testbed;y ii) aunque la diferencia es pequeña, esto puede sugerir que nuestro modelo logístico de estimación de probabilidades de relevancia no es lo suficientemente precisa. Más datos de entrenamiento o un modelo más sofisticado pueden ayudar a resolver este rompecabezas menor. Colecciones seleccionadas. Colecciones seleccionadas. TREC123-100COL Testbed. Test Bed de Representante. Colección seleccionada. Colección seleccionada. Testbed de prueba relevante. Bed de prueba no relevante. Figura 3. Experimentos de selección de recursos en las cuatro bolas de prueba.38 6. Resultados: la efectividad de la recuperación de documentos para la recuperación del documento, se buscan las bases de datos seleccionadas y los resultados devueltos se fusionan en una sola lista final. En todos los experimentos discutidos en esta sección, los resultados recuperados de las bases de datos individuales fueron combinados por el algoritmo de fusión de resultados semisuuperados de aprendizaje. Esta versión del algoritmo SSL [22] puede descargar un pequeño número de textos de documentos devueltos sobre la mosca para crear datos de capacitación adicionales en el proceso de aprendizaje de los modelos lineales que mapean las puntuaciones de documentos específicas de la base de datos en puntajes de documentos centralizados estimados. Se ha demostrado que es muy efectivo en entornos donde solo se recuperan las listas de resultados cortas de cada base de datos seleccionada [22]. Este es un escenario común en entornos operativos y fue el caso de nuestros experimentos. La efectividad de la recuperación de documentos se midió por precisión en la parte superior de la lista final de documentos. Los experimentos en esta sección se realizaron para estudiar la efectividad de la recuperación del documento de cinco algoritmos de selección, a saber, los algoritmos Cori, Redde, Uum/HR, UUM/HP-FL y UUM/HP-VL. Los últimos tres algoritmos se propusieron en la Sección 3. Los primeros cuatro algoritmos seleccionaron 3 o 5 bases de datos, y se recuperaron 50 documentos de cada base de datos seleccionada. El algoritmo UUM/HP-FL también seleccionó 3 o 5 bases de datos, pero se permitió ajustar el número de documentos para recuperar de cada base de datos seleccionada;El número recuperado se limitó a ser de 10 a 100, y un múltiplo de 10. Los TREC123-100col y las bolas de prueba representativas se seleccionaron para la recuperación de documentos, ya que representan dos casos extremos de efectividad de selección de recursos;En un caso, el algoritmo CORI es tan bueno como los otros algoritmos y en el otro caso es bastante la Tabla 5. Precisión en el testamento representativo cuando se seleccionaron 3 bases de datos.(La primera línea de base es Cori; la segunda línea de base para los métodos UUM/HP es UUM/HR.) Precisión en Doc Rank Cori Redde UUM/HR UUM/HP-FL UUM/HP-VL 5 DOCS 0.3720 0.4080 (+9.7%) 0.4640 (+24.7%) 0.4600 (+23.7%) (-0.9%) 0.5000 (+34.4%) (+7.8%) 10 Docs 0.3400 0.4060 (+19.4%) 0.4600 (+35.3%) 0.4540 (+33.5%) (-1.3%) 0.4640 (+36.5%) (+0.9%) 15 Docs 0.3120 0.3880 (+24.4%) 0.4320 (+38.5%) 0.4240 (+35.9%) (-1.9%) 0.4413 (+41.4%) (+2.2) 20 Docs 0.3000 0.3750 (+25.0%) 0.4080 (+36.0%) 0.4040 (+34.7%)(-1.0%) 0.4240 (+41.3%) (+4.0%) 30 Docs 0.2533 0.3440 (+35.8%) 0.3847 (+51.9%) 0.3747 (+47.9%) (-2.6%) 0.3887 (+53.5%) (+1.0%) Tabla 6. Precisión en la prueba de prueba representativa cuando se seleccionaron 5 bases de datos.(La primera línea de base es Cori; la segunda línea de base para los métodos UUM/HP es UUM/HR.) Precisión en Doc Rank Cori Redde UUM/HR UUM/HP-FL UUM/HP-VL 5 DOCS 0.3960 0.4080 (+3.0%) 0.4560 (+15.2%) 0.4280 (+8.1%) (-6.1%) 0.4520 (+14.1%) (-0.9%) 10 Docs 0.3880 0.4060 (+4.6%) 0.4280 (+10.3%) 0.4460 (+15.0%) (+4.2%) 0.4560 (+17.5%) (+6.5%) 15 Docs 0.3533 0.3987 (+12.999%) 0.4227 (+19.6%) 0.4440 (+25.7%) (+5.0%) 0.4453 (+26.0%) (+5.4%) 20 Docs 0.3330 0.3960 (+18.9%) 0.4140 (+24.3%) 0.4290 (+28.8%) (+3.6%) 0.4350 (+30.6%) (+5.1%) 30 Docs 0.2967 0.3740 (+26.1%) 0.4013 (+35.3%) 0.3987 (+34.4%) (-0.7%) 0.4060 (+36.8%) (+1.2%) Tabla 3. Precisión en el TREC123-100COL-BySource Testbed cuando se seleccionaron 3 bases de datos.(La primera línea de base es Cori; la segunda línea de base para los métodos UUM/HP es UUM/HR.) Precisión en Doc Rank Cori Redde UUM/HR UUM/HP-FL UUM/HP-VL 5 DOCS 0.3640 0.3480 (-4.4%) 0.3960 (+8.8%) 0.4680 (+28.6%) (+18.1%) 0.4640 (+27.5%) (+17.2%) 10 Docs 0.3360 0.3200 (-4.8%) 0.3520 (+4.8%) 0.4240 (+26.2%) (+20.5%) 0.4220 (+25.6%) (+19.9%) 15 Docs 0.3253 0.3187 (-2.00%) 0.3347 (+2.9%) 0.3973 (+22.2%) (+15.7%) 0.3920 (+20.5%) (+17.1%) 20 Docs 0.3140 0.2980 (-5.1%) 0.3270 (+4.1%) 0.3720 (+18.5%)) (+13.8%) 0.3700 (+17.8%) (+13.2%) 30 Docs 0.2780 0.2660 (-4.3%) 0.2973 (+6.9%) 0.3413 (+22.8%) (+14.8%) 0.3400 (+22.3%) (+14.4%) Tabla 4. Precisión en el TREC123-100COL-BySource Testbed cuando se seleccionaron 5 bases de datos.(La primera línea de base es Cori; la segunda línea de base para los métodos UUM/HP es UUM/HR.) Precisión en DOC Rank Cori Redde UUM/HR UUM/HP-FL UUM/HP-VL 5 DOCS 0.4000 0.3920 (-2.0%) 0.4280 (+7.0%) 0.4680 (+17.0%) (+9.4%) 0.4600 (+15.0%) (+7.5%) 10 Docs 0.3800 0.3760 (-1.1%) 0.3800 (+0.0%) 0.4180 (+10.0%) (+10.0%) 0.4320 (+13.7%) (+13.7%) 15 Docs 0.3560 0.3560 (+0.0.0.0.0.0.0%) 0.3720 (+4.5%) 0.3920 (+10.1%) (+5.4%) 0.4080 (+14.6%) (+9.7%) 20 Docs 0.3430 0.3390 (-1.2%) 0.3550 (+3.5%) 0.3710 (+8.2%) (+4.5%) 0.3830 (+11.7%) (+7.9%) 30 Docs 0.3240 0.3140 (-3.1%) 0.3313 (+2.3%) 0.3500 (+8.0%) (+5.6%) 0.3487 (+7.6%) (+5.3%) 39 mucho peor que los otros algoritmos. Las tablas 3 y 4 muestran los resultados en el TREC123-100col Testbed, y las tablas 5 y 6 muestran los resultados en el Bed de la prueba representativa. En el TREC123-100col Testbed, la efectividad de la recuperación del documento del algoritmo de selección de Cori es aproximadamente el mismo o un poco mejor que el algoritmo Redde, pero ambos son peores que los otros tres algoritmos (tablas 3 y 4). El algoritmo UUM/HR tiene una pequeña ventaja sobre los algoritmos Cori y Redde. Antes se señaló una diferencia principal entre el algoritmo UUM/HR y el algoritmo Redde: el UUM/HR utiliza datos de entrenamiento e interpolación lineal para estimar las curvas de puntaje de documento centralizado, mientras que el algoritmo Redde [21] usa un método heurístico, supone que elLas curvas de puntaje de documentos centralizadas son funciones de paso y no se distinguen entre la parte superior de las curvas. Esta diferencia hace que UUM/HR sea mejor que el algoritmo Redde al distinguir documentos con altas probabilidades de relevancia de las bajas probabilidades de relevancia. Por lo tanto, el UUM/HR refleja el objetivo de recuperación de alta precisión mejor que el algoritmo Redde y, por lo tanto, es más efectivo para la recuperación de documentos. El algoritmo UUM/HR no optimiza explícitamente la decisión de selección con respecto al objetivo de alta precisión, ya que los algoritmos UUM/HP-FL y UUM/HP-VL están diseñados para hacer. Se puede ver que en este tallbed, los algoritmos UUM/HP-FL y UUM/HP-VL son mucho más efectivos que todos los demás algoritmos. Esto indica que su poder proviene de optimizar explícitamente el objetivo de alta precisión de la recuperación de documentos en las ecuaciones 14 y 16. En el Bed de prueba representativo, Cori es mucho menos efectivo que otros algoritmos para la recuperación de documentos distribuidos (Tablas 5 y 6). Los resultados de recuperación del documento del algoritmo Redde son mejores que los del algoritmo Cori, pero aún peores que los resultados del algoritmo UUM/HR. En esta prueba, los tres algoritmos UUM son igualmente efectivos. El análisis detallado muestra que la superposición de las bases de datos seleccionadas entre los algoritmos UUM/HR, UUM/HP-FL y UUM/HP-VL es mucho más grande que los experimentos en el Bed de prueba TREC123-100COL, ya que todos ellos tienden a seleccionar los dosgrandes bases de datos. Esto explica por qué son igualmente efectivos para la recuperación de documentos. En entornos operativos reales, las bases de datos pueden no devolver puntajes de documentos e informar solo listas de resultados clasificadas. Como el modelo de maximización de utilidad unificado solo utiliza puntajes de recuperación de documentos muestreados con un algoritmo de recuperación centralizado para calcular las probabilidades de relevancia, toma decisiones de selección de bases de datos sin referirse a los puntajes de documentos de bases de datos individuales y puede generalizarse fácilmente a este caso de listas de rango de rangosin puntajes de documentos. El único ajuste es que el algoritmo SSL fusiona las listas clasificadas sin puntajes de documentos al asignar los documentos con puntajes de pseudo-documento normalizados para sus rangos (en una lista clasificada de 50 documentos, el primero tiene un puntaje de 1, el segundo tiene una puntuación.de 0.98 etc.), que se ha estudiado en [22]. Los resultados del experimento en TREC123-100COL-BySource Testbed con 3 bases de datos seleccionadas se muestran en la Tabla 7. La configuración del experimento fue la misma que antes, excepto que los puntajes del documento se eliminaron intencionalmente y las bases de datos seleccionadas solo devuelven listas clasificadas de ID de documento. Se puede ver en los resultados que el UUM/HP-FL y UUM/HP-VL funcionan bien con bases de datos que no devuelven los puntajes de documentos y aún son más efectivos que otras alternativas. No se informan otros experimentos con bases de datos que no devuelven los puntajes de documentos, pero muestran resultados similares para demostrar la efectividad de los algoritmos UUM/HP-FL y UUM/HPVL. Los experimentos anteriores sugieren que es muy importante optimizar el objetivo de alta precisión explícitamente en la recuperación de documentos. Los nuevos algoritmos basados en este principio logran mejores o al menos buenos resultados como los algoritmos de última generación en varios entornos.7. Conclusión La recuperación de información distribuida resuelve el problema de encontrar información dispersa entre muchas bases de datos de texto en redes de área local e internets. La mayoría de las investigaciones previas utilizan algoritmo efectivo de selección de recursos del sistema de recomendación de la base de datos para la aplicación de recuperación de documentos distribuidos. Argumentamos que el objetivo de selección de recursos de alta recuperación de la recomendación de la base de datos y el objetivo de alta precisión de la recuperación de documentos están relacionados pero no idénticos. Este tipo de inconsistencia también se ha observado en trabajos anteriores, pero las soluciones anteriores utilizaron métodos heurísticos o cooperación asumida por bases de datos individuales (por ejemplo, todas las bases de datos utilizaron el mismo tipo de motores de búsqueda), lo que con frecuencia no es cierto en el entorno no cooperativo. En este trabajo, proponemos un modelo de maximización de utilidad unificado para integrar la selección de recursos de la recomendación de la base de datos y las tareas de recuperación de documentos en un solo marco unificado. En este marco, las decisiones de selección se obtienen optimizando diferentes funciones objetivas. Hasta donde sabemos, este es el primer trabajo que intenta ver y modelar teóricamente la tarea de recuperación de información distribuida de manera integrada. El nuevo marco continúa una tendencia de investigación reciente que estudia el uso de muestreo basado en consultas y una base de datos de muestra centralizada. Un solo modelo logístico fue entrenado en la tabla centralizada 7. Precisión en el TREC123-100COL-BySource Testbed cuando se seleccionaron 3 bases de datos (la primera línea de base es CORI; la segunda línea de base para los métodos UUM/HP es UUM/HR.) (Los motores de búsqueda no devuelven puntajes de documentos) en Doc Rank Cori ReddeUUM/HR UUM/HP-FL UUM/HP-VL 5 DOCS 0.3520 0.3240 (-8.0%) 0.3680 (+4.6%) 0.4520 (+28.4%) (+22.8%) 0.4520 (+28.4%) (+22.8) 10Docs 0.3320 0.3140 (-5.4%) 0.3340 (+0.6%) 0.4120 (+24.1%) (+23.4%) 0.4020 (+21.1%) (+20.4%) 15 Docs 0.3227 0.2987 (-7.4%) 0.3280 (+1.6%0.3920 (+21.5%) (+19.5%) 0.3733 (+15.7%) (+13.8%) 20 Docs 0.3030 0.2860 (-5.6%) 0.3130 (+3.3%) 0.3670 (+21.2%) (+17.3%) 0.35900(+18.5%) (+14.7%) 30 Docs 0.2727 0.2640 (-3.2%) 0.2900 (+6.3%) 0.3273 (+20.0%) (+12.9%) 0.3273 (+20.0%) (+12.9%) 40 Datos de muestra de muestraPara estimar las probabilidades de relevancia de los documentos por sus puntajes de recuperación centralizados, mientras que la base de datos de muestra centralizada sirve como un puente para conectar las bases de datos individuales con el modelo logístico centralizado. Por lo tanto, las probabilidades de relevancia para todos los documentos en las bases de datos se pueden estimar con una cantidad muy pequeña de juicio de relevancia humana, que es mucho más eficiente que los métodos anteriores que crean un modelo separado para cada base de datos. Este marco no solo es más teóricamente sólido sino también muy efectivo. Un algoritmo para la selección de recursos (UUM/HR) y dos algoritmos para la recuperación de documentos (UUM/HP-FL y UUM/HP-VL) se derivan de este marco. Se han realizado estudios empíricos en tallas para simular las soluciones de búsqueda distribuidas de grandes organizaciones (empresas) o la web oculta específica del dominio. Además, los algoritmos de selección de recursos UUM/HP-FL y UUM/HP-VL se extienden con una variante de resultados de fusión de resultados SSL para abordar la tarea de recuperación de documentos distribuidos cuando las bases de datos seleccionadas no devuelven los puntajes de los documentos. Los experimentos han demostrado que estos algoritmos logran resultados que son al menos tan buenos como el estado de arte anterior y, a veces, considerablemente mejor. El análisis detallado indica que la ventaja de estos algoritmos proviene de optimizar explícitamente los objetivos de las tareas específicas. El marco de maximización de utilidad unificado está abierto para diferentes extensiones. Cuando el costo se asocia con la búsqueda de las bases de datos en línea, el marco de servicios públicos se puede ajustar para estimar automáticamente el mejor número de bases de datos para buscar para que se pueda recuperar una gran cantidad de documentos relevantes con costos relativamente pequeños. Otra extensión del marco es considerar la efectividad de la recuperación de las bases de datos en línea, que es un tema importante en los entornos operativos. Todas estas son las instrucciones de la investigación futura. Reconocimiento Esta investigación fue apoyada por NSF Subvenciones EIA-9983253 e IIS-0118767. Cualquier opinión, hallazgos, conclusiones o recomendaciones expresadas en este documento son los autores, y no reflejan necesariamente las del patrocinador. Referencias [1] J. Callan.(2000). Recuperación de información distribuida. En W.B. Croft, editor, avances en recuperación de información. Kluwer Publishers Academic.(pp. 127-150).[2] J. Callan, W.B. Croft y J. Broglio.(1995). Experimentos de trec y tipster con investigación. Procesamiento y gestión de la información, 31 (3).(pp. 327-343).[3] J. G. Conrad, X. S. Guo, P. Jackson y M. Meziou.(2002). Selección de la base de datos utilizando recursos de recolección lógicos físicos y adquiridos reales en un entorno operativo masivo de dominios específicos. Búsqueda distribuida sobre la web oculta: muestreo y selección de bases de datos jerárquicas. En Actas de la 28ª Conferencia Internacional sobre bases de datos muy grandes (VLDB).[4] N. Craswell.(2000). Métodos para la recuperación de información distribuida. Ph. D. Tesis, la Universidad de la Nación Australiana.[5] N. Craswell, D. Hawking y P. Thistlewaite.(1999). Fusionar los resultados de los motores de búsqueda aislados. En Actas de la décima conferencia de la base de datos de Australasia.[6] D. Dsouza, J. Thom y J. Zobel.(2000). Una comparación de técnicas para seleccionar colecciones de texto. En Actas de la 11ª Conferencia de la Base de Datos de Australia.[7] N. Fuhr.(1999). Un enfoque teórico de decisión para la selección de bases de datos en IR en red. Transacciones ACM en sistemas de información, 17 (3).(pp. 229-249).[8] L. Gravano, C. Chang, H. García-Molina y A. Paepcke.(1997). Comienza: Propuesta de Stanford para metasearches de Internet. En Actas de la 20ª Conferencia Internacional de ACM-Sigmod sobre gestión de datos.[9] L. Gravano, P. ipeirotis y M. Sahami.(2003). QPROBER: un sistema para la clasificación automática de bases de datos Hidden-Web. Transacciones ACM en sistemas de información, 21 (1).[10] P. ipeirotis y L. Gravano.(2002). Búsqueda distribuida sobre la web oculta: muestreo y selección de bases de datos jerárquicas. En Actas de la 28ª Conferencia Internacional sobre bases de datos muy grandes (VLDB).[11] Invisibleweb.com.http://www.invisibleweb.com [12] The Lemur Toolkit.http://www.cs.cmu.edu/~lemur [13] J. Lu y J. Callan.(2003). Recuperación de información basada en el contenido en redes entre pares. En Actas de la 12ª Conferencia Internacional sobre Gestión de Información y Conocimiento.[14] W. Meng, C.T. Yu y K.L. Liu.(2002) Construyendo motores de metasearch eficientes y efectivos. ACM Comput. Sobrev.34 (1).[15] H. Nottelmann y N. Fuhr.(2003). Evaluar diferentes métodos para estimar la calidad de la recuperación para la selección de recursos. En Actas de la 25ª Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información.[16] H., Nottelmann y N., Fuhr.(2003). La arquitectura mental para las bibliotecas digitales federadas multimedia heterogéneas. Taller ACM Sigir 2003 sobre recuperación de información distribuida.[17] A.L. Powell, J.C. French, J. Callan, M. Connell y C.L. Viles.(2000). El impacto de la selección de la base de datos en la búsqueda distribuida. En Actas de la 23a Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información.[18] A.L. Powell y J.C. French.(2003). Comparando el rendimiento de los algoritmos de selección de la base de datos. Transacciones ACM en sistemas de información, 21 (4).(pp. 412-456).[19] C. Sherman (2001). Busque la web invisible. Guardian ilimitado.[20] L. Si y J. Callan.(2002). Uso de datos y regresión muestreados para fusionar los resultados del motor de búsqueda. En Actas de la 25ª Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información.[21] L. Si y J. Callan.(2003). Método de estimación de distribución de documentos relevante para la selección de recursos. En Actas de la 26ª Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información.[22] L. Si y J. Callan.(2003). Un método de aprendizaje semi-supervisado para fusionar los resultados de los motores de búsqueda. Transacciones ACM en sistemas de información, 21 (4).(pp. 457-491).41