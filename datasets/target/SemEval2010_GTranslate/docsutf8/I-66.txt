Permitir una araña en una red de POMDPS: Generación de políticas garantizadas de calidad Pradeep Varakantham, Janusz Marecki, Yuichi Yabu ∗, Milind Tambe, Makoto Yokoo ∗ Universidad del Sur de California, Los Ángeles, CA 90089, {Varakant, Marecki, Tambe} @usc.edu ∗ Dept. de Sistemas Inteligentes, Universidad de Kyushu, Fukuoka, 812-8581 Japón, yokoo@is.kyushu-u.ac.jp Resumen distribuido los problemas de decisión de Markov parcialmente observables (POMDP distribuidos) son un enfoque popular para modelar sistemas de múltiples agentes que actúan en dominios inciertos en dominios inciertos.. Dada la significativa complejidad de resolver POMDP distribuidos, particularmente a medida que ampliamos el número de agentes, un enfoque popular se ha centrado en soluciones aproximadas. Aunque este enfoque es eficiente, los algoritmos dentro de este enfoque no proporcionan ninguna garantía sobre la calidad de la solución. Un segundo enfoque menos popular se centra en la optimización global, pero los resultados típicos están disponibles solo para dos agentes, y también a un costo computacional considerable. Este documento supera las limitaciones de ambos enfoques al proporcionar araña, una combinación novedosa de tres características clave para la generación de políticas en POMDP distribuidos: (i) Explota la estructura de interacción del agente dada una red de agentes (es decir, permitiendo una escala más fácil a un número mayor a un número más grande.de agentes);(ii) Utiliza una combinación de heurísticas para la búsqueda de políticas aceleradas;y (iii) permite aproximaciones garantizadas de calidad, lo que permite una compensación sistemática de calidad de solución para el tiempo. Los resultados experimentales muestran órdenes de mejora de magnitud en el rendimiento en comparación con los algoritmos óptimos globales anteriores. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial]: Sistemas de inteligencia artificiales distribuidos Algoritmos generales de sistemas generales, Teoría 1. Introducción Los problemas de decisión de Markov parcialmente observables (POMDP distribuidos) están surgiendo como un enfoque popular para modelar la toma de decisiones secuenciales en equipos que operan bajo incertidumbre [9, 4, 1, 2, 13]. La incertidumbre surge a causa del no determinismo en los resultados de las acciones y porque el estado mundial solo puede ser parcialmente (o incorrectamente) observable. Desafortunadamente, como lo muestran Bernstein et al.[3], el problema de encontrar la política conjunta óptima para los POMDP distribuidos generales es NEXP Complete. Los investigadores han intentado dos tipos diferentes de enfoques para resolver estos modelos. La primera categoría consiste en técnicas aproximadas altamente eficientes, que pueden no alcanzar soluciones óptimas globalmente [2, 9, 11]. El problema clave con estas técnicas ha sido su incapacidad para proporcionar garantías sobre la calidad de la solución. En contraste, la segunda categoría de enfoques menos popular se ha centrado en un resultado óptimo global [13, 5, 10]. Aunque estos enfoques obtienen soluciones óptimas, generalmente consideran solo dos agentes. Además, no explotan la estructura en las interacciones de los agentes y, por lo tanto, están severamente obstaculizados con respecto a la escalabilidad al considerar más de dos agentes. Para abordar estos problemas con los enfoques existentes, proponemos técnicas aproximadas que proporcionan garantías sobre la calidad de la solución al tiempo que se centran en una red de más de dos agentes. Primero proponemos el algoritmo básico de araña (búsqueda de políticas en entornos distribuidos). Hay dos características novedosas clave en Spider: (i) Es una rama y una técnica de búsqueda heurística vinculada que utiliza una función heurística basada en MDP para buscar una política conjunta óptima;(ii) Explota la estructura de red de los agentes organizando a los agentes en un pseudo árbol de búsqueda de profundidad (DFS) y aprovecha la independencia en las diferentes ramas del árbol DFS. Luego proporcionamos tres mejoras para mejorar la eficiencia del algoritmo de araña básica al tiempo que proporcionamos garantías sobre la calidad de la solución. La primera mejora utiliza abstracciones para acelerar, pero no sacrifica la calidad de la solución. En particular, inicialmente realiza la búsqueda de rama y encuadernado en políticas abstractas y luego se extiende a las políticas completas. La segunda mejora obtiene aceleración al sacrificar la calidad de la solución, pero dentro de un parámetro de entrada que proporciona la diferencia de valor esperada tolerable de la solución óptima. La tercera mejora se basa nuevamente en limitar la búsqueda de eficiencia, sin embargo, con un parámetro de tolerancia que se proporciona como un porcentaje de óptimo. Experimentamos con el dominio de la red de sensores presentado en Nair et al.[10], un representante de dominio de una clase importante de problemas con redes de agentes que trabajan en entornos inciertos. En nuestros experimentos, ilustramos que Spider domina un enfoque óptimo global existente llamado Goa [10], el único algoritmo óptimo global conocido con resultados experimentales demostrados para más de dos agentes. Además, demostramos que la abstracción mejora el rendimiento de Spider significativamente (al tiempo que proporciona soluciones óptimas). Finalmente demostramos una característica clave de Spider: al utilizar las mejoras de aproximación, permite compensaciones de principios en tiempo de ejecución frente a la calidad de la solución.822 978-81-904262-7-5 (RPS) C 2007 Ifaamas 2. Dominio: las redes de sensores distribuidos de sensores distribuidos son una clase importante e importante de dominios que motivan nuestro trabajo. Este documento se centra en un conjunto de problemas de seguimiento de objetivos que surgen en ciertos tipos de redes de sensores [6] introducidas por primera vez en [10]. La Figura 1 muestra una instancia de problema específica dentro de este tipo que consta de tres sensores. Aquí, cada nodo del sensor puede escanear en una de cuatro direcciones: norte, sur, este o oeste (ver Figura 1). Para rastrear un objetivo y obtener una recompensa asociada, dos sensores con áreas de escaneo superpuestas deben coordinarse escaneando la misma área simultáneamente. En la Figura 1, para rastrear un objetivo en LOC11, Sensor1 necesita escanear este y Sensor2 necesita escanear hacia el oeste simultáneamente. Por lo tanto, los sensores tienen que actuar de manera coordinada. Suponemos que hay dos objetivos independientes y que cada movimiento de objetivos es incierto y no afectado por los agentes del sensor. Según el área de escaneo, cada sensor recibe observaciones que pueden tener falsos positivos y falsos negativos. Las observaciones y las transiciones de los sensores son independientes de las acciones de los demás, por ejemplo, las observaciones que el sensor1 recibe son independientes de las acciones del sensor2. Cada agente incurre en un costo para escanear si el objetivo está presente o no, pero no hay costo si se apaga. Dada la incertidumbre de observación de los sensores, los objetivos de las transiciones inciertas y la naturaleza distribuida de los nodos del sensor, estas redes de sensores proporcionan un dominio útil para aplicar modelos POMDP distribuidos. Figura 1: una configuración del sensor de 3 cadenas 3. Modelo de fondo 3.1: POMDP distribuido en red El modelo ND-POMDP se introdujo en [10], motivado por dominios como las redes de sensores introducidas en la Sección 2. Se define como la tupla S, A, P, Ω, O, R, B, donde S = × 1≤i≤nsi × Su es el conjunto de estados del mundo. SI se refiere al conjunto de estados locales del Agente I y Su es el conjunto de estados inútiles. El estado inafectable se refiere a esa parte del estado mundial que no puede verse afectado por las acciones de los agentes, p.Factores ambientales como las ubicaciones objetivo que ningún agente puede controlar. A = × 1≤i≤nai es el conjunto de acciones conjuntas, donde AI es el conjunto de acción para el agente i.ND-POMDP asume la independencia de la transición, donde la función de transición se define como p (s, a, s) = pu (su, su) · 1≤i≤n pi (Si, Su, Ai, Si), donde a = a1,..., an es la acción conjunta realizada en el estado S = S1 ,..., Sn, Su y S = S1 ,..., Sn, Su es el estado resultante. Ω = × 1≤i≤nΩi es el conjunto de observaciones articulares donde ωi es el conjunto de observaciones para los agentes i.La independencia observacional se supone en ND-POMDP, es decir, la función de observación conjunta se define como O (S, A, ω) = 1≤i≤n oi (Si, Su, Ai, ωi), donde S = S1 ,..., Sn, Su es el estado mundial que resulta de los agentes que realizan A = A1 ,..., an en el estado anterior, y ω = ω1 ,..., ωn ∈ ω es la observación recibida en el estado s.Esto implica que la observación de cada agente depende solo del estado inafectable, su acción local y de su estado local resultante. La función de recompensa, r, se define como r (s, a) = l rl (sl1, ..., slr, su, al1, ..., alr), donde cada l podría referirse a cualquier subgrupo de agentesy r = | l |. Según la función de recompensa, se construye un hipergrafo de interacción. Un hiper-enlace, L, existe entre un subconjunto de agentes para todos los RL que comprenden R. El hipergrafo de interacción se define como g = (ag, e), donde los agentes, ag, son los vértices y e = {l | l⊆ AG ∧ RL es un componente de R} son los bordes. El estado de creencia inicial (distribución sobre el estado inicial), b, se define como b (s) = bu (su) · 1≤i≤n bi (Si), donde BU y BI se refieren a la distribución sobre el estado inicial inafectable yEl agente es el estado de creencia inicial, respectivamente. El objetivo en ND-POMDP es calcular la política conjunta π = π1 ,..., πn que maximiza la recompensa esperada de los equipos sobre un horizonte finito t a partir del estado de creencia b. Un ND-POMDP es similar a un problema de optimización de restricciones distribuidas N-ARY (DCOP) [8, 12] donde la variable en cada nodo representa la política seleccionada por un agente individual, πi con el dominio de la variable siendo el conjunto de todosPolíticas locales, πi. El componente de recompensa rl donde | l |= 1 puede considerarse como una restricción local, mientras que el componente de recompensa RL donde L> 1 corresponde a una restricción no local en el gráfico de restricción.3.2 Algoritmo: Algoritmo óptimo global (Goa) En trabajos anteriores, Goa se ha definido como un algoritmo óptimo global para ND-POMDPS [10]. Usaremos Goa en nuestras comparaciones experimentales, ya que Goa es un algoritmo óptimo global de última generación, y de hecho el único con resultados experimentales disponibles para redes de más de dos agentes. Goa toma prestado de un algoritmo DCOP óptimo global llamado DPOP [12]. El paso del mensaje Goas sigue el de DPOP. La primera fase es la propagación de Util, donde los mensajes de utilidad, en este caso los valores de las políticas, se transmiten de las hojas a la raíz. El valor para una política en un agente se define como la suma de los mejores valores de respuesta de sus hijos y la recompensa de la política conjunta asociada con la política de los padres. Por lo tanto, dada una política para un nodo principal, Goa requiere que un agente itere a través de todas sus políticas, encuentre la mejor política de respuesta y devuelva el valor al padre, mientras que en el nodo principal, para encontrar la mejor política, un agente requiere suniños para devolver sus mejores respuestas a cada una de sus políticas. Este proceso de propagación de Util se repite en cada nivel en el árbol, hasta que la raíz agota todas sus políticas. En la segunda fase de propagación de valor, donde las políticas óptimas se transmiten desde la raíz hasta las hojas. GOA aprovecha las interacciones locales en el gráfico de interacción, podando las evaluaciones de políticas conjuntas innecesarias (asociadas con nodos no conectados directamente en el árbol). Dado que el gráfico de interacción captura todas las interacciones de recompensa entre los agentes y, como este algoritmo itera a través de todas las evaluaciones de políticas conjuntas relevantes, este algoritmo produce una solución globalmente óptima.4. Spider como se menciona en la Sección 3.1, un ND-POMDP puede tratarse como un DCOP, donde el objetivo es calcular una política conjunta que maximice la recompensa articular general. La técnica de fuerza bruta para calcular una política óptima sería examinar los valores esperados para todas las políticas conjuntas posibles. La idea clave en Spider es evitar el cálculo de los valores esperados para todo el espacio de las políticas conjuntas, utilizando los límites superiores en los valores esperados de las políticas y la estructura de interacción de los agentes. Similar a algunos de los algoritmos para DCOP [8, 12], Spider tiene un paso de preprocesamiento que construye un árbol DFS correspondiente a la estructura de interacción dada. Tenga en cuenta que estos árboles DFS son pseudo árboles [12] que permiten vínculos entre antepasados y niños. Empleamos la heurística máxima de nodo restringido (MCN) utilizado en el algoritmo DCOP, adoptamos [8], sin embargo, también se pueden emplear otras heurísticas (como la heurística MLSP de [7]). La heurística de MCN intenta colocar a los agentes con más cantidad de limitaciones en la parte superior del árbol. Este árbol rige cómo la búsqueda de la articulación óptima del sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 823 Icy procede en araña. Los algoritmos presentados en este documento se pueden extender fácilmente a los hiper-árboles, sin embargo, para fines expositivos, asumimos árboles binarios. Spider es un algoritmo para la planificación centralizada y la ejecución distribuida en POMDP distribuidos. En este artículo, empleamos la siguiente notación para denotar políticas y valores esperados: antepasados (i) ⇒ Agentes de I a la raíz (sin incluir I). Árbol (i) ⇒ Agentes en el sub-árbol (sin incluir i) para el cual yo es la raíz.πroot+ ⇒ Política conjunta de todos los agentes.πi+ ⇒ Política conjunta de todos los agentes en el árbol (i) ∪ i.πi− ⇒ Política conjunta de agentes que están en antepasados (i).πi ⇒ Política del agente ésimo.ˆV [πi, πi−] ⇒ Bound superior en el valor esperado para πi+ administrado πi y políticas de agentes antepasados, es decir, πi−.ˆVj [πi, πi−] ⇒ Bound superior en el valor esperado para πi+ del niño JTH.V [πi, πi−] ⇒ Valor esperado para πi Políticas dadas de agentes antepasados, πi−.V [πi+, πi−] ⇒ Valor esperado para πi+ políticas dadas de agentes antepasados, πi−.VJ [πi+, πi−] ⇒ Valor esperado para πi+ del niño Jth. Figura 2: Ejecución de Spider, un esquema de ejemplo 4.1 de Spider Spider se basa en la idea de la búsqueda de rama y la búsqueda unida, donde los nodos en el árbol de búsqueda representan políticas conjuntas parciales/completas. La Figura 2 muestra un ejemplo de árbol de búsqueda para el algoritmo de araña, utilizando un ejemplo de la cadena de tres agentes. Antes de que comience Spider, su búsqueda, creamos un árbol DFS (es decir, pseudo árbol) de la cadena de tres agentes, con el agente medio como la raíz de este árbol. La araña explota la estructura de este árbol DFS mientras participa en su búsqueda. Tenga en cuenta que en nuestra figura de ejemplo, a cada agente se le asigna una política con t = 2. Por lo tanto, cada rectángulo redondeado (nodo del árbol de búsqueda) indica una política de unión parcial/completa, un rectángulo indica un agente y los óvalos internos a un agente muestran su política. El valor heurístico o esperado real para una política conjunta se indica en la esquina superior derecha del rectángulo redondeado. Si el número está en cursiva y subrayado, implica que se proporciona el valor esperado real de la política conjunta. La araña comienza sin una política asignada a ninguno de los agentes (que se muestran en el nivel 1 del árbol de búsqueda). El nivel 2 del árbol de búsqueda indica que las políticas conjuntas se clasifican en función de los límites superiores calculados para las políticas de los agentes de la raíz. El nivel 3 muestra un nodo de búsqueda de araña con una política conjunta completa (una política asignada a cada uno de los agentes). El valor esperado para esta política conjunta se usa para podar los nodos en el nivel 2 (los que tienen límites superiores <234) al crear políticas para cada agente no hojas I, la araña potencialmente realiza dos pasos: 1. Obteniendo límites superiores y clasificación: en este paso, el agente I calcula los límites superiores en los valores esperados, ˆV [πi, πi−] de las políticas conjuntas πi+ correspondientes a cada una de sus políticas πi y políticas de ancestros fijos. Una heurística basada en MDP se usa para calcular estos límites superiores en los valores esperados. La descripción detallada sobre esta heurística MDP se proporciona en la Sección 4.2. Todas las políticas del agente I, πi se clasifican en función de estos límites superiores (también conocidos como valores heurísticos en adelante) en orden descendente. La exploración de estas políticas (en el paso 2 a continuación) se realizan en este orden descendente. Como se indica en el nivel 2 del árbol de búsqueda (de la Figura 2), todas las políticas conjuntas se clasifican en función de los valores heurísticos, indicados en la esquina superior derecha de cada política articular. La intuición detrás de la clasificación y luego la exploración de políticas en orden descendente de los límites superiores, es que las políticas con límites superiores más altos podrían producir políticas articulares con valores esperados más altos.2. Exploración y poda: la exploración implica calcular la mejor política de respuesta de respuesta πi+, ∗ correspondiente a las políticas de antepasados fijos del agente I, πi−. Esto se realiza iterando a través de todas las políticas del agente I, es decir, Πi y sumando dos cantidades para cada política: (i) la mejor respuesta para todos los niños (obtenidos realizando los pasos 1 y 2 en cada uno de los nodos infantiles);(ii) El valor esperado obtenido por I para políticas fijas de antepasados. Por lo tanto, la exploración de una política πi produce un valor esperado real de una política conjunta, πi+ representada como V [πi+, πi−]. La política con el valor esperado más alto es la mejor política de respuesta. La poda se refiere a evitar explorar todas las políticas (o calcular los valores esperados) en el Agente I utilizando el mejor valor esperado actual, Vmax [πi+, πi−]. En adelante, este Vmax [πi+, πi−] se denominará umbral. Una política, no es necesario explorar si el límite superior para esa política, ˆv [πi, πi−] es menor que el umbral. Esto se debe a que el valor esperado para la mejor política conjunta alcanzable para esa política será menor que el umbral. Por otro lado, al considerar un agente de hoja, Spider calcula la mejor política de respuesta (y en consecuencia su valor esperado) correspondiente a las políticas fijas de sus antepasados, πi−. Esto se logra calculando los valores esperados para cada una de las políticas (correspondientes a políticas fijas de antepasados) y seleccionando la política de valor esperado más alto. En la Figura 2, Spider asigna las mejores políticas de respuesta a los agentes de la hoja en el nivel 3. La política para el agente de la hoja izquierda es realizar la acción este en cada paso de tiempo en la política, mientras que la política para el agente de hoja adecuado debe funcionar en cada paso de tiempo. Estas mejores políticas de respuesta de los agentes de la hoja producen un valor esperado real de 234 para la política conjunta completa. El algoritmo 1 proporciona el código pseudo para la araña. Este algoritmo genera la mejor política conjunta, πi+, ∗ (con un valor esperado mayor que el umbral) para los agentes en el árbol (i). Las líneas 3-8 calculan la mejor política de respuesta de un agente de hoja I, mientras que las líneas 9-23 calculan la mejor política de respuesta de respuesta para los agentes en el árbol (i). Este mejor cálculo de respuesta para un agente no hojado incluye: (a) clasificación de políticas (en orden descendente) basada en valores heurísticos en la línea 11;(b) calcular las mejores políticas de respuesta en cada uno de los niños para políticas fijas del agente I en las líneas 16-20;y (c) mantener 824 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (Aamas 07) Algoritmo 1 Spider (I, πi−, umbral) 1: πi+, ∗ ← NULL 2: πi ← Get-All-Polyies (Horizon, Ai, ωi) 3: If IS es IS-Leaf (i) luego 4: para todos πi ∈ πi do 5: V [πi, πi−] ← Conjunta-Reward (πi, πi−) 6: si v [πi, πi−]> umbral luego 7: πi+,∗ ← πi 8: umbral ← v [πi, πi−] 9: else 10 10: niños ← niños (i) 11: ˆπi ← Sorteo superior (i, πi, πi−) 12: para todos πi ∈ ˆπi do13: ˜πi+ ← πi 14: if ˆv [πi, πi−] <umbral luego 15: vaya a la línea 12 16: para todos los niños j ∈ HODO 17: Jthres ← umbral - v [πi, πi -] - − σk∈Children, k = j ˆvk [πi, πi−] 18: πj+, ∗ ← Spider (J, πi πi−, jthres) 19: ˜πi+ ← ˜πi+ πj+, ∗ 20: ˆvj [πi, πi−] ← V [πj++, ∗, πi πi−] 21: if v [˜πi+, πi−]> umbral luego 22: umbral ← v [˜πi+, πi−] 23: πi+, ∗ ← ˜πi+24: return πi+, ∗ algorithm 2 superior-Dbound-sort (i, πi, πi−) 1: niños ← niños (i) 2: ˆπi ← nulo / * almacena la lista ordenada * / 3: para todos πi ∈ πi do 4: ˆv [πi, πi−]]← Conjunte-Reward (πi, πi−) 5: para todos los niños J ∈ Do 6: ˆvj [πi, πi−] ← Unido superior (i, j, πi πi−) 7: ˆv [πi, πi−] +← ˆVj [πi, πi−] 8: ˆπi ← Insertar en orden (πi, ˆπi) 9: return ˆπi mejor valor esperado, política conjunta en las líneas 21-23. El algoritmo 2 proporciona el código pseudo para clasificar las políticas basadas en los límites superiores en los valores esperados de las políticas conjuntas. El valor esperado para un agente I consta de dos partes: valor obtenido de antepasados y valor obtenido de sus hijos. La línea 4 calcula el valor esperado obtenido de los antepasados del agente (utilizando la función conjunta-recompensa), mientras que las líneas 5-7 calculan el valor heurístico de los niños. La suma de estas dos partes produce un límite superior en el valor esperado para el Agente I, y la línea 8 del algoritmo clasifica las políticas basadas en estos límites superiores.4.2 Función heurística basada en MDP La función heurística rápidamente proporciona un límite superior en el valor esperado que se puede obtener de los agentes en el árbol (i). El subárbol de los agentes es un POMDP distribuido en sí mismo y la idea aquí es construir un MDP centralizado correspondiente al POMDP distribuido (sub-árbol) y obtener el valor esperado de la política óptima para este MDP centralizado. Para reiterar esto en términos de los agentes en la estructura de interacción del árbol DFS, asumimos la observabilidad total para los agentes en el árbol (i) y para las políticas fijas de los agentes en {antepasados (i) ∪ i}, calculamos el valor articular ˆV [πi+, πi−]. Utilizamos la siguiente notación para presentar las ecuaciones para calcular los límites superiores/valores heurísticos (para los agentes I y K): deje que Ei- denote el conjunto de enlaces entre agentes en {antepasados (i) ∪ I} y árbol (i), EI+denota el conjunto de enlaces entre los agentes en el árbol (i). Además, si l ∈ Ei−, entonces L1 es el agente en {antepasados (i) ∪ i} y L2 es el agente en el árbol (i), que L se conecta juntos. Primero compacimos la notación estándar: OT K = OK (ST+1 K, ST+1 U, πk (ωt K), ωt+1 k) (1) Pt K = Pk (St K, St U, πk (ωtk), st+1 k) · ot k pt u = p (st u, st+1 u) st l = st l1, st l2, st u;ωt L = ωt L1, ωt L2 rt l = rl (st l, πl1 (ωt L1), πl2 (ωt l2)) vt l = v t πl (st l, st u, ωt l1, ωt l2) dependiendo de laUbicación del agente k En el árbol del agente tenemos los siguientes casos: si k ∈ {antepasados (i) ∪ i}, ˆpt k = pt k, (2) si k ∈ Tree (i), ˆpt k = pk (st k, st u, πk (ωt k), st+ 1 k) si l ∈ Ei−, ˆrt l = max {al2} rl (st l, πl1 (ωt l1), al2) si l ∈ Ei+, ˆrt l = max{al1, al2} rl (st l, al1, al2) La función de valor para un agente i ejecutando la política conjunta πi+ en el tiempo η - 1 es proporcionada por la ecuación: V η - 1 πi+ (Sη - 1, Ωη - 1) = l∈Ei− vη - 1 l + lleei + vη - 1 l (3) donde vη - 1 l = rη - 1 l + Ω η l, sη pη - 1 l1 pη - 1 l2 pη - 1 u vηL algoritmo 3 límite superior (i, j, πj−) 1: val ← 0 2: para todos l ∈ Ej− ∪ eJ+ do 3: si l ∈ Ej− entonces πl1 ← φ 4: para todos s0 l do 5:Val + ← Startbel [S0 L] · Tiempo superior (I, S0 L, J, πl1,) 6: Algoritmo de Val de retorno 4 Tiempo superior (I, St L, J, πl1, ωt L1) 1: maxv al ← −∞ 2: para todos Al1, al2 do 3: si l ∈ Ei− y l ∈ Ej− entonces AL1 ← πl1 (ωt L1) 4: Val ← Get-Reward (St L, Al1, Al2) 5: Si t <πi.horizon-1, entonces 6: para todos st+1 l, ωt+1 l1 do 7: Futv al ← Pt u ˆpt l1 ˆpt l2 8: futv al ∗ ← en el tiempo superior (ST+1L, J, πl1, ωt L1 ωt + 1 L1) 9: Val + ← Futv al 10: Si Val> maxv al, entonces maxv al ← Val 11: return máximo de límite superior en el valor esperado para un enlace se calcula mediante la modificaciónLa ecuación 3 para reflejar la suposición de observabilidad completa. Esto implica eliminar el término de probabilidad de observación para los agentes en el árbol (i) y maximizar el valor futuro ˆvη l sobre las acciones de esos agentes (en el árbol (i)). Por lo tanto, la ecuación para el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (aamas 07) 825 Cálculo del límite superior en un enlace l, es el siguiente: si l ∈ Ei−, ˆvη-1 l = ˆrη-1 l + max al2 ω η l1, sη l ˆPη - 1 l1 ˆPη - 1 l2 pη - 1 u ˆvη l if l ∈ Ei +, ˆVη - 1 l = ˆrη - 1 l + max al1, al2 s η l ˆpη - 1 l1 ˆpη - 1 l2 pη - 1 uˆVη l Algoritmo 3 y el algoritmo 4 proporcionan el algoritmo para calcular el límite superior para el niño J del Agente I, utilizando las ecuaciones descriptadas anteriormente. Mientras que el algoritmo 4 calcula el límite superior en un enlace dado el estado inicial, el algoritmo 3 suma los valores de límite superior calculados sobre cada uno de los enlaces en Ei− ∪ Ei+.4.3 Algoritmo de abstracción 5 Spider-ABS (I, πi−, umbral) 1: πi+, ∗ ← NULL 2: πi ← Get-Polyies (<>, 1) 3: if is-lef (i) luego 4: para todos πi∈ πi do 5: absheuristic ← get-abs heuristic (πi, πi−) 6: absheuristic ∗ ← (tiempo de tiempo-πi.horizon) 7: if πi.horizon = timehorizon y πi.absnodes = 0 luego 8: v [πi, πi−] ← Conjunta-Reward (πi, πi−) 9: si v [πi, πi−]> umbral luego 10: πi+, ∗ ← πi;Umbral ← V [πi, πi−] 11: de lo contrario si v [πi, πi−] + absheuristic> umbral luego 12: ˆπi ← extend-policy (πi, πi.absnodes + 1) 13: πi + ← insortado- de inserción--Políticas (ˆπi) 14: Eliminar (πi) 15: else 16: Niños ← Niños (i) 17: πi ← Sorteo superior (i, πi, πi−) 18: para todos πi ∈ πi do 19: ˜πi+← πi 20: absheuristic ← get-abs heuristic (πi, πi−) 21: absheuristic ∗ ← (tiempohorizon-πi.horizon) 22: if πi.horizon = timeHorizon y πi.absnodes = 0 luego 23: if ˆv [πi, πi−] <umbral y πi.absnodes = 0 luego 24: vaya a la línea 19 25: Para todos los niños j ∈ Do 26: jthres ← umbral - v [πi, πi -] - σk∈Children, k = j ˆvk [πi, πi−] 27: πj+, ∗ ← Spider (j, πi πi−, jthres) 28: ˜πi+ ← ˜πi+ πj+, ∗;ˆVj [πi, πi−] ← V [πj+, ∗, πi πi−] 29: si v [πi+, πi−]> umbral luego 30: umbral ← v [πi+, πi−];πi +, ∗ ← ˜πi + 31: de lo contrario si ˆv [πi +, πi−] + absheuristic> umbral luego 32: ˆπi ← extend-policy (πi, πi.absnodes + 1) 33: πi + ← insertados con policias (ˆπi) 34: eliminar (πi) 35: return πi+, ∗ en araña, la fase de exploración/poda solo puede comenzar después de que el cálculo heurístico (o límite superior) y la clasificación de las políticas hayan terminado. Proporcionamos un enfoque para posiblemente eludir la exploración de un grupo de políticas basadas en el cálculo heurístico para una política abstracta, lo que lleva a una mejora en el rendimiento del tiempo de ejecución (sin pérdidas en calidad de solución). Los pasos importantes en esta técnica son definir la política abstracta y cómo se calculan los valores heurísticos para las políticas abstractas. En este artículo, proponemos dos tipos de abstracción: 1. Abstracción basada en horizonte (HBA): aquí, la política abstracta se define como una política de horizonte más corta. Representa un grupo de políticas de horizonte más largas que tienen las mismas acciones que la política abstracta para tiempos menores o iguales al horizonte de la política abstracta. En la Figura 3 (a), una política abstracta T = 1 que realiza la acción del este, representa un grupo de políticas T = 2, que funcionan este en el primer paso. Para HBA, hay dos partes para el cálculo heurístico: (a) calcular el límite superior para el horizonte de la política abstracta. Esto es lo mismo que el cálculo heurístico definido por el algoritmo getheuristic () para la araña, sin embargo, con un horizonte de tiempo más corto (horizonte de la política abstracta).(b) Calcular la recompensa máxima posible que se puede acumular en un paso de tiempo (usando get-abs-heuristic ()) y multiplicándola por el número de pasos de tiempo al horizonte temporal. Esta recompensa máxima posible (por un paso de tiempo) se obtiene iterando a través de todas las acciones de todos los agentes en el árbol (i) y calculando la recompensa articular máxima para cualquier acción articular. La suma de (a) y (b) es el valor heurístico para una política abstracta de HBA.2. Abstracción basada en nodos (NBA): aquí se obtiene una política abstracta al no asociar acciones a ciertos nodos del árbol de políticas. A diferencia de HBA, esto implica múltiples niveles de abstracción. Esto se ilustra en la Figura 3 (b), donde hay t = 2 políticas que no tienen una acción para la observación TP. Estas políticas t = 2 incompletas son abstracciones para t = 2 políticas completas. El aumento de los niveles de abstracción conduce a un cálculo más rápido de una política conjunta completa, πroot+ y también a un cálculo y exploración heurística más cortos, fases de poda. Para la NBA, el cálculo heurístico es similar al de una política normal, excepto en los casos en que no hay acción asociada con los nodos de política. En tales casos, la recompensa inmediata se toma como RMAX (recompensa máxima para cualquier acción). Combinamos ambas técnicas de abstracción mencionadas anteriormente en una técnica, Spider-ABS. El algoritmo 5 proporciona el algoritmo para esta técnica de abstracción. Para calcular una política conjunta óptima con Spider-ABS, un agente no hojado que inicialmente examina todas las políticas abstractas T = 1 (línea 2) y las clasifica en función de los cálculos heurísticos de políticas abstractas (línea 17). El horizonte de abstracción aumenta gradualmente y estas políticas abstractas se exploran en el orden descendente de los valores heurísticos y los que tienen valores heurísticos menores que el umbral se podan (líneas 23-24). La exploración en Spider-ABS tiene la misma definición que en Spider si la política que se explora tiene un horizonte de cálculo de política que es igual al horizonte de tiempo real y si todos los nodos de la política tienen una acción asociada con ellos (líneas 25-30). Sin embargo, si no se cumplen esas condiciones, entonces es sustituido por un grupo de políticas que representa (usando la función extend-poliy ()) (líneas 31-32). La función Extend-Policy () también es responsable de inicializar el horizonte y los absnodes de una política.Absnodes representa el número de nodos en el último nivel en el árbol de políticas, que no tienen una acción asignada a ellos. Si πi.absnodes = | ωi | πi.horizon - 1 (es decir, el número total de nodos de política posibles en πi.horizon), entonces πi.absnodes se establece en cero y πi.horizon aumenta en 1. De lo contrario, πi.absnodes aumenta en 1. Por lo tanto, esta función combina HBA y NBA mediante el uso de las variables de política, horizonte y absnodes. Antes de sustituir la política abstracta con un grupo de políticas, esas políticas se clasifican en función de los valores heurísticos (línea 33). Se adopta un tipo similar de cálculo de mejor respuesta basado en abstracción en los agentes de la hoja (líneas 3-14).4.4 Aproximación de valor (VAX) En esta sección, presentamos una mejora aproximada a la araña llamada VAX. La entrada a esta técnica es un parámetro de aproximación, que determina la diferencia con la calidad de solución óptima. Este parámetro de aproximación se usa en cada agente para podar las políticas articulares. El mecanismo de poda en Spider and Spider-ABS dicta que una política conjunta se poda solo si el umbral es exactamente mayor que el valor heurístico. Sin embargo, el 826 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 3: El ejemplo de abstracción para (a) HBA (abstracción basada en horizonte) y (b) idea de NBA (abstracción basada en nodos) en esta técnica es podar una política si una política siSe satisface la siguiente condición: umbral +> ˆv [πi, πi−]. Además de la condición de poda, Vax es lo mismo que Spider/Spider-ABS. En el ejemplo de la Figura 2, si el valor heurístico para la segunda política conjunta (o el segundo nodo del árbol de búsqueda) en el nivel 2 fue 238 en lugar de 232, entonces esa política no se podía podarse utilizando araña o araña-ABS. Sin embargo, en VAX con un parámetro de aproximación de 5, la política conjunta en consideración también se poda. Esto se debe a que el umbral (234) en esa coyuntura más el parámetro de aproximación (5), es decir, 239 habría sido mayor que el valor heurístico para esa política conjunta (238). Se puede observar en el ejemplo (solo discutido) que este tipo de poda puede conducir a menos exploraciones y, por lo tanto, conducir a una mejora en el rendimiento general del tiempo de ejecución. Sin embargo, esto puede implicar un sacrificio en la calidad de la solución porque esta técnica puede podar una solución óptima candidata. Un límite en el error introducido por este algoritmo aproximado en función de, se proporciona por la Proposición 3. 4.5 Accionamiento porcentual (PAX) En esta sección, presentamos la segunda mejora de la aproximación sobre la araña llamada PAX. La entrada a esta técnica es un parámetro, δ que representa el porcentaje mínimo de la calidad de solución óptima que se desea. La salida de esta técnica es una política con un valor esperado que es al menos δ% de la calidad de solución óptima. Se poda una política si se cumple la siguiente condición: umbral> Δ 100 ˆv [πi, πi−]. Al igual que en Vax, la única diferencia entre Pax y Spider/Spider-ABS es esta condición de poda. Nuevamente en la Figura 2, si el valor heurístico para el segundo nodo del árbol de búsqueda en el nivel 2 fuera 238 en lugar de 232, entonces pax con un parámetro de entrada del 98% podría podar ese nodo de árbol de búsqueda (desde 98 100 ∗ 238 <234). Este tipo de poda conduce a menos exploraciones y, por lo tanto, una mejora en el rendimiento del tiempo de ejecución, mientras que potencialmente conduce a una pérdida en la calidad de la solución. La Proposición 4 proporciona el límite de pérdida de calidad.4.6 Resultados teóricos Proposición 1. La heurística proporcionada por el uso de la heurística MDP centralizada es admisible. Prueba. Para que el valor proporcionado por la heurística sea admisible, debe ser una estimación excesiva del valor esperado para una política conjunta. Por lo tanto, debemos mostrar que: para l ∈ Ei+ ∪ ei−: ˆvt l ≥ vt l (consulte la notación en la sección 4.2) Usamos la inducción matemática en t para probar esto. Caso base: t = t - 1. Independientemente de si l ∈ Ei− o l ∈ Ei+, ˆrt L se calcula maximizando sobre todas las acciones de los agentes en el árbol (i), mientras que RT L se calcula para políticas fijas de los mismos agentes. Por lo tanto, ˆrt l ≥ rt l y también ˆvt l ≥ vt l. Asunción: la proposición es válida para t = η, donde 1 ≤ η <t - 1. Ahora tenemos que demostrar que la proposición es válida para t = η - 1. Mostramos la prueba de l ∈ Ei− y se puede adoptar un razonamiento similar para probar para l ∈ Ei+. La función de valor heurístico para l ∈ Ei− es proporcionada por la siguiente ecuación: ˆvη - 1 l = ˆrη - 1 l + max al2 Ω η l1, s η l ˆpη - 1 l1 ˆpη - 1 l2 pη - 1 u ˆvη l reescribirel rhs y usando eqn 2 (en la sección 4.2) = ˆrη - 1 l + max al2 Ω η l1, s η l pη - 1 u pη - 1 l1 ˆpη - 1 l2 ˆvη l = ˆrη - 1 l + Ω η l1,s η l pη - 1 u pη - 1 l1 max al2 ˆpη - 1 l2 ˆvη l ya que maxal2 ˆpη - 1 l2 ˆvη l ≥ ωl2 oη - 1 l2 ˆpη - 1 l2 ˆvη l y pη - 1 l2 = oη - 1 l2 ˆpηpη−1 l2 ≥ˆrη - 1 l + Ω η l1, s η l pη - 1 u pη - 1 l1 ωl2 pη - 1 l2 ˆvη l ya que ˆvη l ≥ vη l (desde el supuesto) ≥ˆrη - 1 l + Ω η η ηL1, S η l pη - 1 u pη - 1 l1 ωl2 pη - 1 l2 vη l ya que ˆrη - 1 l ≥ rη - 1 l (por definición) ≥rη - 1 l + Ω η l1, s η l pη - 1u pη - 1 l1 ωl2 pη - 1 l2 vη l = rη - 1 l + (Ω η l, s η l) pη - 1 u pη - 1 l1 pη - 1 l2 vη l = vη - 1 l así probado. Proposición 2. Spider proporciona una solución óptima. Prueba. Spider examina todas las políticas articulares posibles dada la estructura de interacción de los agentes. La única excepción es cuando una política conjunta se poda en función del valor heurístico. Por lo tanto, mientras no se poda una política óptima candidata, Spider devolverá una política óptima. Como se demuestra en la Propuesta 1, el valor esperado para una política conjunta es siempre un límite superior. Por lo tanto, cuando se poda una política conjunta, no puede ser una solución óptima. Proposición 3. Error ligado a la calidad de la solución para VAX (implementado sobre Spider-ABS) con un parámetro de aproximación de IS ρ, donde ρ es el número de nodos de hoja en el árbol DFS. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 827 Prueba. Probamos esta proposición utilizando la inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, πk. Una política, πk se poda si ˆv [πk, πk−] <umbral +. Por lo tanto, la mejor política de respuesta calculada por VAX sería como máximo lejos de la mejor respuesta óptima. Por lo tanto, la proposición es válida para el caso base. Asunción: la proposición se cumple para D, donde 1 ≤ profundidad ≤ d.Ahora tenemos que demostrar que la propuesta es dada para D + 1. Sin pérdida de generalidad, supongamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños es de profundidad ≤ d, y por lo tanto, desde la suposición, el error introducido en el niño kth es ρk, donde ρk es el número de nodos de hoja en el niño de la raíz. Por lo tanto, ρ = k ρk, donde ρ es el número de nodos de hoja en el árbol. En Spider-ABS, umbral en el agente de la raíz, ThressPider = K V [πk+, πk−]. Sin embargo, con Vax, el umbral en el agente de la raíz será (en el peor de los casos), ThreshVax = K V [πk+, πk−] - k ρk. Por lo tanto, con Vax, una política conjunta se poda en el agente raíz si ˆV [πroot, πroot−] <Threshvax + ⇒ ˆV [πroot, πroot−] <Threshspider - ((k ρk) - 1) ≤ ThreshSpider - (K ρk)≤ Threshspider - ρ. Por lo tanto probado. Proposición 4. Para Pax (implementado sobre Spider-ABS) con un parámetro de entrada de δ, la calidad de la solución es al menos Δ 100 V [πroot+, ∗], donde V [πroot+, ∗] denota la calidad de solución óptima. Prueba. Probamos esta proposición utilizando la inducción matemática en la profundidad del árbol DFS. Caso base: profundidad = 1 (es decir, un nodo). La mejor respuesta se calcula iterando a través de todas las políticas, πk. Una política, πk se poda si Δ 100 ˆV [πk, πk−] <umbral. Por lo tanto, la mejor política de respuesta calculada por PAX sería al menos Δ 100 veces la mejor respuesta óptima. Por lo tanto, la proposición es válida para el caso base. Asunción: la proposición se cumple para D, donde 1 ≤ profundidad ≤ d.Ahora tenemos que demostrar que la propuesta es dada para D + 1. Sin pérdida de generalidad, supongamos que el nodo raíz de este árbol tiene k hijos. Cada uno de estos niños es de profundidad ≤ d, y por lo tanto, por la suposición, la calidad de la solución en el niño kth es al menos Δ 100 V [πk+, ∗, πk−] para pax. Con Spider-ABS, una política conjunta se poda en el agente raíz si ˆV [πroot, πroot−] <k v [πk+, ∗, πk−]. Sin embargo, con Pax, se poda una política conjunta si Δ 100 ˆV [πroot, πroot−] <k Δ 100 V [πk+, ∗, πk−] ⇒ ˆv [πroot, πroot−] <k v [πk+, ∗, πk−]]. Dado que la condición de poda en el agente de la raíz en PAX es la misma que la de Spider-ABS, no se introduce ningún error en el agente de la raíz y todo el error se introduce en los niños. Por lo tanto, la calidad general de la solución es al menos δ 100 de la solución óptima. Por lo tanto probado.5. Resultados experimentales Todos nuestros experimentos se realizaron en el dominio de la red de sensores de la Sección 2. Las cinco configuraciones de red empleadas se muestran en la Figura 4. Los algoritmos con los que experimentamos son Goa, Spider, Spider-ABS, Pax y Vax. Comparamos contra Goa porque es el único algoritmo óptimo global que considera a más de dos agentes. Realizamos dos conjuntos de experimentos: (i) En primer lugar, comparamos el rendimiento del tiempo de ejecución de los algoritmos anteriores y (ii) en segundo lugar, experimentamos con Pax y Vax para estudiar la compensación entre el tiempo de ejecución y la calidad de la solución. Los experimentos se terminaron después de 10000 segundos1. La Figura 5 (a) proporciona comparaciones en tiempo de ejecución entre los algoritmos óptimos Goa, la araña, la araña ABS y los algoritmos aproximados, Pax (de 30) y Vax (δ de 80). El eje X denota las especificaciones de la máquina 1 para todos los experimentos: procesador Intel Xeon 3.6 GHz, configuración de red del sensor de 2 GB de RAM utilizada, mientras que el eje Y indica el tiempo de ejecución (en una escala de registro). El horizonte temporal del cálculo de políticas fue 3. Para cada configuración (3 cadena, 4 cadena, 4 estrellas y 5 estrellas), hay cinco barras que indican el tiempo que lleva Goa, Spider, Spiderabs, Pax y Vax. Goa no terminó dentro del límite de tiempo para configuraciones de 4 estrellas y 5 estrellas. Spider-ABS dominaron el Spider y Goa para todas las configuraciones. Por ejemplo, en la configuración de 3 cadenas, Spider-ABS proporciona aceleración de 230 veces sobre Goa y 2 veces acelera sobre Spider y para la configuración de 4 cadenas proporciona aceleración de 58 veces sobre GOA y aceleración de 2 veces sobre Spider. Los dos enfoques de aproximación, VAX y PAX proporcionaron una mejora adicional en el rendimiento sobre las arañas ABS. Por ejemplo, en la configuración de 5 estrellas, VAX proporciona una aceleración de 15 veces y PAX proporciona una aceleración de 8 veces sobre Spider-ABS. Las Figuras 5 (b) proporcionan una comparación de la calidad de la solución obtenida utilizando los diferentes algoritmos para los problemas probados en la Figura 5 (a). El eje X denota la configuración de la red del sensor, mientras que el eje Y indica la calidad de la solución. Dado que Goa, Spider y Spider-ABS son algoritmos óptimos globales, la calidad de la solución es la misma para todos esos algoritmos. Para la configuración 5-P, los algoritmos óptimos globales no terminaron dentro del límite de 10000 segundos, por lo que la barra para una calidad óptima indica un límite superior en la calidad de solución óptima. Con ambas aproximaciones, obtuvimos una calidad de solución que estaba cerca de la calidad de solución óptima. En las configuraciones de 3 y 4 estrellas, es notable que tanto Pax como Vax obtuvieron casi la misma calidad real que los algoritmos óptimos globales, a pesar del parámetro de aproximación y δ. Para otras configuraciones también, la pérdida de calidad fue inferior al 20% de la calidad de solución óptima. La Figura 5 (c) proporciona el tiempo de solución con PAX (para diferentes epsilones). El eje X denota el parámetro de aproximación, δ (porcentaje a óptimo) utilizado, mientras que el eje Y denota el tiempo necesario para calcular la solución (en una escala logarítmica). El horizonte del tiempo para todas las configuraciones fue 4. A medida que δ disminuyó de 70 a 30, el tiempo de la solución disminuyó drásticamente. Por ejemplo, en el caso de 3 cadenas hubo una aceleración total de 170 veces cuando el δ se cambió de 70 a 30. Curiosamente, incluso con un bajo δ del 30%, la calidad de la solución real se mantuvo igual a la obtenida al 70%. La Figura 5 (d) proporciona el tiempo de solución para todas las configuraciones con VAX (para diferentes epsilons). El eje X denota el parámetro de aproximación, utilizado, mientras que el eje Y denota el tiempo necesario para calcular la solución (en una escala logarítmica). El horizonte del tiempo para todas las configuraciones fue 4. A medida que aumentó, el tiempo de solución disminuyó drásticamente. Por ejemplo, en el caso de 4 estrellas hubo una aceleración total de 73 veces cuando se cambió de 60 a 140. Una vez más, la calidad de la solución real no cambió con la variable Epsilon. Figura 4: Configuraciones de red de sensores 828 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 5: Comparación de Goa, Spider, Spider-ABS y Vax para T = 3 en (a) tiempo de ejecución y (b) calidad de solución;(c) Tiempo de solución para PAX con un porcentaje variable a óptimo para T = 4 (d) Tiempo hasta solución para VAX con Epsilon variable para t = 4 6. Resumen y trabajo relacionado Este documento presenta cuatro algoritmos Spider, Spider-ABS, Pax y Vax que proporcionan una nueva combinación de características para la búsqueda de políticas en POMDP distribuidos: (i) Estructura de interacción de agentes explotando dada una red de agentes (es decir, una escala más fácil de escalaa mayor número de agentes);(ii) usar la búsqueda de rama y unión con una función heurística basada en MDP;(iii) utilizar la abstracción para mejorar el rendimiento del tiempo de ejecución sin sacrificar la calidad de la solución;(iv) proporcionar límites de porcentaje a priori en la calidad de las soluciones con PAX;y (v) proporcionar límites de valor esperados en la calidad de las soluciones utilizando VAX. Estas características permiten la compensación sistemática de la calidad de la solución para el tiempo de ejecución en redes de agentes que operan bajo incertidumbre. Los resultados experimentales muestran órdenes de mejora de magnitud en el rendimiento sobre los algoritmos óptimos globales anteriores. Los investigadores generalmente han empleado dos tipos de técnicas para resolver POMDP distribuidos. El primer conjunto de técnicas calcula soluciones óptimas globales. Hansen et al.[5] Presente un algoritmo basado en la programación dinámica y la eliminación iterada de las políticas dominantes, que proporciona soluciones óptimas para POMDP distribuidos. Szer et al.[13] Proporcionar un método de búsqueda heurística óptimo para resolver POMDP descentralizados. Este algoritmo se basa en la combinación de un algoritmo de búsqueda heurística clásica, una teoría de control ∗ y descentralizada. Las diferencias clave entre Spider y MAA* son: (a) Las mejoras de Spider (VAX y PAX) proporcionan aproximaciones garantizadas de calidad, mientras que MaA* es un algoritmo óptimo global y, por lo tanto, implica una complejidad computacional significativa;(b) Debido a la incapacidad de Maa*para explotar la estructura de interacción, se ilustra solo con dos agentes. Sin embargo, Spider ha sido ilustrado para redes de agentes;y (c) Spider explora la política conjunta Un agente a la vez, mientras que Maa* lo expande un paso de vez en cuando (simultáneamente para todos los agentes). El segundo conjunto de técnicas busca políticas aproximadas. Emerymontemerlo et al.[4] Aproximadamente los POSG como una serie de juegos bayesianos de un solo paso que usan heurísticas para aproximar el valor futuro, intercambiando un aspecto limitado para la eficiencia computacional, lo que resulta en políticas localmente óptimas (con respecto a la heurística seleccionada). Nair et al.[9] El algoritmo S JESP utiliza una programación dinámica para alcanzar una solución óptima local para POMDPS descentralizados de horizonte finito. Peshkin et al.[11] y Bernstein et al.[2] son ejemplos de técnicas de búsqueda de políticas que buscan políticas localmente óptimas. Aunque todas las técnicas anteriores mejoran considerablemente la eficiencia del cálculo de políticas, no pueden proporcionar límites de error en la calidad de la solución. Este aspecto de los límites de calidad diferencia la araña de todas las técnicas anteriores. Agradecimientos. Este material se basa en un trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición bajo el No. NBCHD030010. Las opiniones y conclusiones contenidas en este documento son las de los autores, y no deben interpretarse como que representen las políticas oficiales, ya sea expresadas o implícitas, de la Agencia de Proyectos de Investigación Avanzada de Defensa o el Gobierno de los Estados Unidos.7. Referencias [1] R. Becker, S. Zilberstein, V. Lesser y C.V.Goldman. Resolución de procesos de decisión de Markov descentralizados independientes de transición. Jair, 22: 423-455, 2004. [2] D. S. Bernstein, E.A. Hansen y S. Zilberstein. Iteración de política limitada para POMDP descentralizados. En Ijcai, 2005. [3] D. S. Bernstein, S. Zilberstein y N. Inmerman. La complejidad del control descentralizado de los MDP. En UAI, 2000. [4] R. Emery-Montemerlo, G. Gordon, J. Schneider y S. Thrun. Soluciones aproximadas para juegos estocásticos parcialmente observables con pagos comunes. En Aamas, 2004. [5] E. Hansen, D. Bernstein y S. Zilberstein. Programación dinámica para juegos estocásticos parcialmente observables. En AAAI, 2004. [6] V. Lesser, C. Ortiz y M. Tambe. Redes del sensor distribuido: una perspectiva multiagente. Kluwer, 2003. [7] R. Maheswaran, M. Tambe, E. Bowring, J. Pearce y P. Varakantham. Tomando DCOP al mundo real: soluciones completas eficientes para la programación de eventos distribuidos. En Aamas, 2004. [8] P. J. Modi, W. Shen, M. Tambe y M. Yokoo. Un método completo asincrónico para la optimización de restricciones distribuidas. En Aamas, 2003. [9] R. Nair, D. Pynadath, M. Yokoo, M. Tambe y S. Marsella. Taming POMDP descentralizado: hacia un cálculo de políticas eficiente para configuraciones multiagentes. En Ijcai, 2003. [10] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDP distribuidos en red: una síntesis de optimización de restricciones distribuidas y POMDPS. En AAAI, 2005. [11] L. Peshkin, N. Meuleau, K.-E.Kim y L. Kaelbling. Aprender a cooperar a través de la búsqueda de políticas. En UAI, 2000. [12] A. Petcu y B. Faltings. Un método escalable para la optimización de restricciones multiagente. En Ijcai, 2005. [13] D. Szer, F. Charpillet y S. Zilberstein. MAA*: Un algoritmo de búsqueda heurística para resolver POMDPS descentralizados. En Ijcai, 2005. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 829