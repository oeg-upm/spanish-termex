Comentarios de término para la recuperación de información con modelos de idiomas bin Tan †, Atulya Velivelli ‡, Hui Fang †, Chengxiang Zhai † Departamento. de informática †, Departamento de Ingeniería Eléctrica e Informática ‡ Universidad de Illinois en Urbana-Champaign bintan@cs.uiuc.edu, velivell@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.Resumen EDU En este documento estudiamos comentarios basados en términos para la recuperación de información en el enfoque de modelado de idiomas. Con la retroalimentación de término, un usuario juzga directamente la relevancia de los términos individuales sin interacción con los documentos de retroalimentación, tomando el control total del proceso de expansión de la consulta. Proponemos un método basado en clúster para seleccionar los términos que se presentarán al usuario para el juicio, así como los algoritmos efectivos para construir modelos de lenguaje de consulta refinada a partir de la retroalimentación del término del usuario. Se muestra que nuestros algoritmos aportan una mejora significativa en la precisión de la recuperación sobre una línea de base que no es de retroalimentación y logran un rendimiento comparable a la retroalimentación de relevancia. Son útiles incluso cuando no hay documentos relevantes en la parte superior. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: Modelos de recuperación Términos generales Algoritmos 1. Introducción En el enfoque de modelado de idiomas para la recuperación de la información, la retroalimentación a menudo se modela como estimación de un modelo de consulta mejorado o modelo de relevancia basado en un conjunto de documentos de retroalimentación [25, 13]. Esto está en línea con la forma tradicional de hacer comentarios de relevancia: presentar a un usuario documentos/pasajes para el juicio de relevancia y luego extraer términos de los documentos o pasajes juzgados para expandir la consulta inicial. Es una forma indirecta de buscar asistencia de usuarios para la construcción del modelo de consulta, en el sentido de que el modelo de consulta refinada (basado en términos) se aprende a través de documentos/pasajes de retroalimentación, que son estructuras de términos de alto nivel. Tiene la desventaja de que los términos irrelevantes, que ocurren junto con los relevantes en el contenido juzgado, pueden usarse erróneamente para la expansión de la consulta, causando efectos no deseados. Por ejemplo, para los logros del telescopio Hubble de la consulta TREC, cuando un documento relevante habla más sobre la reparación de los telescopios que sus descubrimientos, se pueden agregar términos irrelevantes como la caminata espacial a la consulta modificada. Podemos considerar una forma más directa de involucrar a un usuario en la mejora del modelo de consulta, sin un paso intermediario de comentarios de documentos que pueda introducir ruido. La idea es presentar un número (razonable) de términos individuales al usuario y pedirle que juzgue la relevancia de cada término o especifique directamente sus probabilidades en el modelo de consulta. Esta estrategia ha sido discutida en [15], pero que sepamos, no se ha estudiado seriamente en la literatura de modelado de idiomas existente. En comparación con la retroalimentación de relevancia tradicional, este enfoque basado en términos para el refinamiento del modelo de consulta interactiva tiene varias ventajas. Primero, el usuario tiene un mejor control del modelo de consulta final a través de la manipulación directa de los términos: él/ella puede dictar qué términos son relevantes, irrelevantes y posiblemente en qué medida. Esto evita el riesgo de traer términos no deseados al modelo de consulta, aunque a veces el usuario introduce términos de baja calidad. Segundo, porque un término tarda menos tiempo en juzgar que un texto completo o resumen de un documento, y tan pocos, ya que alrededor de 20 términos presentados pueden generar una mejora significativa en el rendimiento de la recuperación (como mostraremos más adelante), los comentarios del término hacen que sea más rápido recopilar comentarios de los usuarios. Esto es especialmente útil para la búsqueda interactiva de ADHOC. En tercer lugar, a veces no hay documentos relevantes en la N superior de los resultados inicialmente recuperados si el tema es difícil. Esto a menudo es cierto cuando N está limitado a ser pequeño, lo que surge del hecho de que el usuario no está dispuesto a juzgar demasiados documentos. En este caso, la retroalimentación de relevancia es inútil, ya que no se puede aprovechar ningún documento relevante, pero la retroalimentación a menudo es a menudo útil, al permitir que los términos relevantes se elijan de documentos irrelevantes. Durante nuestra participación en la pista dura TREC 2005 y el estudio continuo después, exploramos cómo explotar los comentarios de términos del usuario para construir modelos de consulta mejorados para la recuperación de información en el enfoque de modelado de idiomas. Identificamos dos subtareas clave de la retroalimentación basada en términos, es decir, la selección de términos de presentación previa a la retroalimentación y la construcción del modelo de consulta posterior a la retroalimentación, con algoritmos efectivos desarrollados para ambos. Impusimos una estructura de clúster secundario en los términos y descubrimos que una vista de clúster arroja información adicional sobre la necesidad de información de los usuarios, y proporciona una buena forma de utilizar la retroalimentación de términos. A través de experimentos, encontramos que la retroalimentación del término mejora significativamente sobre la línea de base que no es de retroalimentación, a pesar de que el usuario a menudo comete errores en el juicio de relevancia. Entre nuestros algoritmos, el que tiene el mejor rendimiento de recuperación es TCFB, la combinación de TFB, el algoritmo de retroalimentación de término directo y CFB, el algoritmo de retroalimentación basado en clúster. También variamos el número de términos de retroalimentación y observamos una mejora razonable incluso a números bajos. Finalmente, al comparar la retroalimentación a término con la retroalimentación a nivel de documento, encontramos que es una alternativa viable a esta última con un rendimiento de recuperación competitivo. El resto del documento está organizado de la siguiente manera. La Sección 2 discute algún trabajo relacionado. La Sección 4 describe nuestro enfoque general para la retroalimentación del término. Presentamos nuestro método para la selección del término de presentación en la Sección 3 y los algoritmos para la construcción del modelo de consulta en la Sección 5. Los resultados del experimento se dan en la Sección 6. La Sección 7 concluye este documento.2. La retroalimentación de relevancia laboral relacionada [17, 19] se ha reconocido durante mucho tiempo como un método efectivo para mejorar el rendimiento de la recuperación. Normalmente, los principales documentos N recuperados utilizando la consulta original se presentan al usuario para el juicio, después de lo cual se extraen los términos de los documentos relevantes juzgados, ponderados por su potencial de atraer documentos más relevantes y agregados al modelo de consulta. La consulta ampliada generalmente representa la información de los usuarios que necesita mejor que la original, que a menudo es solo una consulta de palabras clave corta. Una segunda iteración de recuperación utilizando esta consulta modificada generalmente produce un aumento significativo en la precisión de la recuperación. En los casos en que el verdadero juicio de relevancia no está disponible y se supone que todos los documentos principales de N son relevantes, se llama retroalimentación ciega o pseudo [5, 16] y generalmente aún trae una mejora del rendimiento. Debido a que el documento es una unidad de texto grande, cuando se usa para retroalimentación de relevancia, se pueden introducir muchos términos irrelevantes en el proceso de retroalimentación. Para superar esto, se propone la retroalimentación del pasaje y se muestra para mejorar el rendimiento de la retroalimentación [1, 23]. Una solución más directa es pedirle al usuario su juicio de relevancia de los términos de retroalimentación. Por ejemplo, en algunos sistemas de retroalimentación de relevancia, como [12], hay un paso de interacción que permite al usuario agregar o eliminar los términos de expansión después de extraer automáticamente de los documentos relevantes. Esto se clasifica como expansión de la consulta interactiva, donde la consulta original se aumenta con términos proporcionados por el usuario, que pueden provenir de la entrada directa del usuario (texto de forma libre o palabras clave) [22, 7, 10] o la selección del usuario de los términos sugeridos al sistema(Usando tesauros [6, 22] o extraído de documentos de retroalimentación [6, 22, 12, 4, 7]). En muchos casos, se ha encontrado la retroalimentación de relevancia a término para mejorar efectivamente el rendimiento de la recuperación [6, 22, 12, 4, 10]. Por ejemplo, el estudio en [12] muestra que el usuario prefiere tener un conocimiento explícito y un control directo de qué términos se utilizan para la expansión de la consulta, y la interfaz penetrable que proporciona esta libertad se muestra que funciona mejor que otras interfaces. Sin embargo, en algunos otros casos no hay un beneficio significativo [3, 14], incluso si al usuario le gusta interactuar con términos de expansión. En un estudio simulado realizado en [18], el autor compara el rendimiento de recuperación de la expansión de consultas interactivas y la expansión automática de consultas con un estudio simulado, y sugiere que los beneficios potenciales de los primeros pueden ser difíciles de lograr. Se encuentra que el usuario no es bueno para identificar términos útiles para la expansión de la consulta, cuando una interfaz de presentación de término simple no puede proporcionar suficiente contexto semántico de los términos de retroalimentación. Nuestro trabajo difiere de los anteriores en dos aspectos importantes. Primero, cuando elegimos los términos para presentar al usuario para el juicio de relevancia, no solo consideramos el valor de un solo término (por ejemplo, la frecuencia relativa de un término en los documentos superiores, que se pueden medir mediante métricas como el valor de selección de Robertson y simplificadaDistancia de Kullback-Leíler como se enumera en [24]), pero también examina la estructura del clúster de los términos, a fin de producir una cobertura equilibrada de los diferentes aspectos del tema. En segundo lugar, con el marco de modelado de idiomas, permitimos una construcción elaborada del modelo de consulta actualizado, estableciendo diferentes probabilidades para diferentes términos en función de si se trata de un término de consulta, su importancia en los documentos principales y su membresía en el clúster. Aunque existen técnicas para ajustar los pesos de los términos de consulta para los modelos de espacio vectorial [17] y los modelos de relevancia probable [9], la mayoría de los trabajos mencionados no los usan, eligiendo solo agregar términos de retroalimentación a la consulta original (utilizando así pesos iguales para ellos), que puede conducir a un rendimiento de recuperación más pobre. La combinación de los dos aspectos permite que nuestro método funcione mucho mejor que la línea de base. La forma habitual de presentación del término de retroalimentación es solo para mostrar los términos en una lista. Ha habido algunos trabajos en interfaces de usuario alternativas.[8] organiza términos en una jerarquía, y [11] compara tres interfaces diferentes, incluidos los términos + casillas de verificación, términos + contexto (oraciones) + casillas de verificación, oraciones + cuadro de texto de entrada. En ambos estudios, sin embargo, no hay una diferencia de rendimiento significativa. En nuestro trabajo adoptamos el enfoque más simple de los términos + casillas de verificación. Nos centramos en la presentación de términos y la construcción del modelo de consulta a partir de términos de retroalimentación, y creemos que usar contextos para mejorar la calidad de los términos de retroalimentación debería ser ortogonal a nuestro método.3. Enfoque general seguimos el enfoque de modelado de idiomas y basamos nuestro método en el modelo de recuperación de divergencia KL propuesto en [25]. Con este modelo, la tarea de recuperación implica estimar un modelo de lenguaje de consulta θq de una consulta dada, un modelo de lenguaje de documento θd de cada documento, y calcular su divergencia KL d (θq || θd), que luego se usa para calificar los documentos.[25] trata la retroalimentación de relevancia como un problema de reestimación del modelo de consulta, es decir, calcular un modelo de consulta actualizado θq dado el texto de consulta original y la evidencia adicional llevada por los documentos relevantes juzgados. Adoptamos esta opinión y presentamos nuestra tarea como actualización del modelo de consulta desde los comentarios de los términos del usuario. Aquí hay dos subtareas clave: primero, cómo elegir los mejores términos para presentar al usuario para juzgar, para recopilar evidencia máxima sobre la necesidad de la información del usuario. En segundo lugar, cómo calcular un modelo de consulta actualizado basado en este término evidencia de retroalimentación, de modo que captura la necesidad de información de los usuarios y se traduce en un buen rendimiento de recuperación.4. Selección de término de presentación La selección adecuada de los términos que se presentará al usuario para el juicio es crucial para el éxito de la retroalimentación del término. Si los términos están mal elegidos y hay pocos relevantes, el usuario tendrá dificultades para buscar términos útiles para ayudar a aclarar su necesidad de información. Si los términos relevantes son abundantes, pero todos se concentran en un solo aspecto del tema de la consulta, entonces solo podremos obtener comentarios sobre ese aspecto y faltar a otros, lo que resulta en una pérdida de amplitud en los resultados recuperados. Por lo tanto, es importante seleccionar cuidadosamente los términos de presentación para maximizar la ganancia esperada de los comentarios de los usuarios, es decir, aquellos que potencialmente pueden revelar la mayoría de las pruebas de la información de los usuarios. Esto es similar a la retroalimentación activa [21], lo que sugiere que un sistema de recuperación debe investigar activamente la necesidad de la información del usuario, y en el caso de la retroalimentación de relevancia, los documentos de retroalimentación deben elegirse para maximizar los beneficios de aprendizaje (por ejemplo, diversamente para aumentar la cobertura). En nuestro enfoque, los principales documentos de N de una recuperación inicial utilizando la consulta original forman la fuente de los términos de retroalimentación: todos los términos que aparecen en ellos se consideran candidatos para presentar al usuario. Estos documentos sirven como pseudo-retroalimentación, ya que proporcionan un contexto mucho más rico que la consulta original (generalmente muy corta), mientras que no se le pide al usuario que juzgue su relevancia. Debido a la última razón, es posible hacer que N sea bastante grande (por ejemplo, en nuestros experimentos establecemos N = 60) para aumentar su cobertura de diferentes aspectos en el tema. La forma más simple de seleccionar los términos de retroalimentación es elegir los términos M más frecuentes de los documentos n. Este método, sin embargo, tiene dos inconvenientes. Primero, se seleccionarán muchos términos ruidosos comunes debido a sus altas frecuencias en la recopilación de documentos, a menos que se use una lista de palabras de parada para el filtrado. En segundo lugar, la lista de presentación tenderá a ser llenada por términos desde aspectos principales del tema;Es probable que los de un aspecto menor se pierdan debido a sus frecuencias relativamente bajas. Resolvemos los problemas anteriores en dos medidas correspondientes. Primero, presentamos un modelo de fondo θB que se estima a partir de estadísticas de recolección y explica los términos comunes, por lo que es mucho menos probable que aparezcan en la lista de presentación. En segundo lugar, los términos se seleccionan de múltiples grupos en los documentos de pseudo-retretería, para garantizar una representación suficiente de diferentes aspectos del tema. Confiamos en el modelo multinomial de mezcla, que se utiliza para el descubrimiento de temas en [26]. Específicamente, suponemos que los documentos n contienen k clústeres {ci |i = 1, 2, · · · k}, cada una caracterizada por una distribución de palabras multinomial (también conocida como modelo de lenguaje unigram) θi y correspondiente a un aspecto del tema. Los documentos se consideran muestreados a partir de una mezcla de componentes K + 1, incluidos los clústeres K y el modelo de fondo: p (w | d) = λbp (w | θb) + (1 - λb) k i = 1 πd, ip, ip(w | θi) donde w es una palabra, λb es el peso de la mezcla para el modelo de fondo θb, y πd, i es el peso de la mezcla específica del documento para el modelo de clúster I-Th. Luego estimamos los modelos de clúster maximizando la probabilidad de que los documentos de pseudo-retroalimentación se generen a partir del modelo de mezcla multinomial: log p (d | λ) = d∈D w∈V c (w; d) log p (w | d) donde d = {di |i = 1, 2, · · · n} es el conjunto de n documentos, v es el vocabulario, c (w; d) es la frecuencia de ws en d y λ = {θi |i = 1, 2, · · · k} ∪ {πdij |i = 1, 2, · · · n, j = 1, 2, · · · k} es el conjunto de parámetros del modelo a estimar. Los modelos de clúster pueden estimarse de manera eficiente utilizando el algoritmo de maximización de expectativas (EM). Para sus detalles, remitimos al lector a [26]. La Tabla 1 muestra los modelos de clúster para los desastres del túnel de transporte de consultas TREC (k = 3). Tenga en cuenta que solo el clúster medio es relevante. Tabla 1: Modelos de clúster para el tema 363 Desastres de túnel de transporte Clúster 1 Clúster 2 Clúster 3 Túnel 0.0768 Túnel 0.0935 Túnel 0.0454 Transporte 0.0364 Fuego 0.0295 Transporte 0.0406 Tráfico 0.0206 Camión 0.0236 Toll 0.0166 Railwai 0.0186 French 0.0220 Amtrak 0.0153 Harbor 0146 Toll. 0140 coche0.0154 aeropuerto 0.0122 Bridg 0.0139 italiano 0.0152 Turnpik 0.0105 kilómetos 0.0136 FireFight 0.0144 Lui 0.0095 Camión 0.0133 Blaze 0.0127 Jersei 0.0093 Construcción 0.0131 BLANC. Elegimos el l = m/K Términos con probabilidades más altas para formar un total de M Términos de presentación. Si un término está en la parte superior L en múltiples grupos, lo asignamos al clúster donde tiene la mayor probabilidad y dejamos que los otros grupos tomen un término más como compensación. También filtramos términos en el texto de la consulta original porque tienden a ser siempre relevantes cuando la consulta es corta. Los Términos seleccionados se presentan al usuario para su juicio. En la Figura 1 se muestra un formulario de retroalimentación de muestra (completado). En este estudio solo tratamos con el juicio binario: un término presentado no está marcado, y un usuario puede verificarlo para indicar relevancia. Tampoco explicamos explícitamente la retroalimentación negativa (es decir, penalizando los términos irrelevantes), porque con la retroalimentación binaria, un término sin control no es necesariamente irrelevante (tal vez el usuario no está seguro de su relevancia). Podríamos pedirle al usuario un juicio más fino (por ejemplo, elegir entre altamente relevantes, algo relevantes, no saben, algo irrelevantes y altamente irrelevantes), pero la retroalimentación binaria es más compacta, tomando menos espacio para mostrar y menos esfuerzo del usuario para hacer un juicio.5. Estimando modelos de consulta a partir de la retroalimentación a término En esta sección, presentamos varios algoritmos para explotar la retroalimentación del término. Los algoritmos toman como entrada la consulta original q, los clústeres {θi} generado por el algoritmo de descubrimiento de temas, el conjunto de términos de retroalimentación t y su juicio de relevancia r, y emite un modelo de lenguaje de consulta actualizado θq haceEvidencia para capturar la necesidad de información de los usuarios. Primero describimos nuestras anotaciones: • θq: el modelo de consulta original, derivado de los términos de consulta solamente: p (w | θq) = c (w; q) | q |donde c (w; q) es el recuento de w en q, y | q |= w∈Q c (w; q) es la longitud de la consulta.• θq: el modelo de consulta actualizado que necesitamos estimar a partir de la retroalimentación del término.• θi (i = 1, 2, ... K): El modelo de lenguaje unigram de clúster CI, según lo estimado utilizando el algoritmo de descubrimiento de temas.• t = {ti, j} (i = 1 ... K, J = 1... L): El conjunto de términos presentados al usuario para el juicio.Ti, J es el término J-these elegido de Cluster CI.• R = {ΔW | W ∈ T}: ΔW es una variable indicadora que es 1 si W se juzga relevante o 0 de lo contrario.5.1 TFB (retroalimentación directa) Esta es una forma directa de retroalimentación de términos que no involucra ninguna estructura secundaria. Damos un peso de 1 a los términos juzgados relevantes por el usuario, un peso de μ a los términos de consulta, peso cero a otros términos, y luego aplicamos la normalización: p (w | θq) = ΔW + μ c (w; q) w∈T ΔW + μ | Q |donde w ∈T ΔW es el número total de términos que se juzgan relevantes. Llamamos a este método TFB (retroalimentación directa del término). Si dejamos μ = 1, este enfoque es equivalente a agregar los términos relevantes después de la consulta original, que es lo que hace la expansión de la consulta estándar (sin reescribir a término). Si establecemos μ> 1, estamos poniendo más énfasis en los términos de la consulta que los verificados. Tenga en cuenta que el modelo de resultados estará más sesgado hacia θq si la consulta original es larga o la retroalimentación del usuario es débil, lo que tiene sentido, ya que podemos confiar más en la consulta original en cualquier caso. Figura 1: Formulario de aclaración lleno para el tema 363 363 Desastres de túnel de transporte Seleccione todos los términos que sean relevantes para el tema.Traffic Railway Harbor Rail Bridge Kilómetro Construcción Cross Swiss Cross Link Kong Hong River Project Meder Shanghai Camión de bomberos Francés Fumo Fiencias italianas Blaze Blaze Blanc Mont Victim Franc Franc El conductor de rescate Chamonix Emerge Toll Amtrak Train Airport Turnpike Lui Jersey Pass Rome Z Center Electron Road Boston Velocidad BU SENTO 5.2CFB (retroalimentación del clúster) Aquí explotamos la estructura del clúster que jugó un papel importante cuando seleccionamos los términos de presentación. Los grupos representan diferentes aspectos del tema de la consulta, cada uno de los cuales puede o no ser relevante. Si podemos identificar los grupos relevantes, podemos combinarlos para generar un modelo de consulta que sea bueno para descubrir documentos pertenecientes a estos grupos (en lugar de los irrelevantes). Podríamos pedirle al usuario que juzgue directamente la relevancia de un clúster después de ver los términos representativos en ese clúster, pero esta a veces sería una tarea difícil para el usuario, que tiene que adivinar la semántica de un clúster a través de su conjunto de términos, que puedenno estar bien conectados entre sí debido a la falta de contexto. Por lo tanto, proponemos aprender la retroalimentación del clúster indirectamente, inferiendo la relevancia de un clúster a través de la relevancia de sus términos de retroalimentación. Debido a que cada clúster tiene un número igual de términos presentados al usuario, la medida más simple de una relevancia de los grupos es el número de términos que se juzgan relevantes en él. Intuitivamente, cuanto más se marcan los términos relevantes en un clúster, más cerca estará el clúster para el tema de la consulta y cuanto más debe participar el clúster en la modificación de la consulta. Si combinamos los modelos de clúster usando pesos determinados de esta manera e interpolamos con el modelo de consulta original, obtenemos la siguiente fórmula para la actualización de la consulta, que llamamos CFB (retroalimentación del clúster): P (W | θq) = λp (W | θq) + (1 - λ) k i = 1 l j = 1 Δti, j k k = 1 l j = 1 Δtk, j p (w | θi) donde l j = 1 Δti, j es el número de términos relevantes en el clústerCI, y K K = 1 L J = 1 ΔTK, J es el número total de términos relevantes. Observamos que cuando solo hay un grupo (k = 1), la fórmula anterior degenera a p (w | θq) = λp (w | θq) + (1- λ) p (w | θ1) que es simplemente pseudo-Comentarios de la forma propuesta en [25].5.3 TCFB (retroalimentación de clúster de términos) TFB y CFB tienen sus inconvenientes. TFB asigna probabilidades no cero a los términos presentados que están marcados relevantes, pero ignora por completo (mucho más) otras, que pueden dejarse sin control debido a la ignorancia de los usuarios, o simplemente no incluidos en la lista de presentación, pero deberíamos poder poderpara inferir su relevancia de los controlados. Por ejemplo, en la Figura 1, dado que se verifican hasta 5 términos en el grupo medio (la tercera y cuarta columna), debemos tener una gran confianza en la relevancia de otros términos en ese clúster. El problema de los TFB de los remedios de CFB tratando los términos en un grupo colectivamente, de modo que los términos no controlados/no presentados reciben pesos cuando los términos presentados en sus grupos se juzgan como relevantes, pero no distingue qué términos en un grupo se presentan o juzgan. Intuitivamente, los términos relevantes juzgados deberían recibir pesos más grandes porque el usuario indica explícitamente como relevante. Por lo tanto, tratamos de combinar los dos métodos, con la esperanza de obtener lo mejor de ambos. Hacemos esto interpolando el modelo TFB con el modelo CFB, y lo llamamos TCFB: P (W | θq) = αP (W | θqt F B) + (1 - α) P (W | θqcf B) 6. Experimentos En esta sección, describimos los resultados de nuestros experimentos. Primero describimos la configuración de nuestro experimento y presentamos una descripción general del rendimiento de varios métodos. Luego discutimos los efectos de variar la configuración de parámetros en los algoritmos, así como el número de términos de presentación. A continuación, analizamos el comportamiento de retroalimentación del término del usuario y su relación con el rendimiento de la recuperación. Finalmente, comparamos la retroalimentación del término con la retroalimentación de relevancia y mostramos que tiene su ventaja particular.6.1 Configuración del experimento y resultados básicos Aprovechamos la oportunidad de Trec 2005 Hard Track [2] para la evaluación de nuestros algoritmos. Las pistas utilizaron la colección Aquaint, un cuerpo de 3GB de texto en inglés de Newswire. Los temas incluyeron 50 que anteriormente se sabían que eran difíciles, es decir, con bajo rendimiento de recuperación. Es para estos temas difíciles que los comentarios de los usuarios son más útiles, ya que puede proporcionar información para desambiguar las consultas;Con temas fáciles, el usuario puede no estar dispuesto a gastar esfuerzos para recibir comentarios si los resultados de recuperación automática son lo suficientemente buenos. Los participantes de la pista pudieron enviar formularios de aclaración diseñados a medida (CF) para solicitar comentarios de los evaluadores humanos proporcionados por la Tabla 2: rendimiento de recuperación para diferentes métodos y tipos de FQ. La última fila es el porcentaje de mejora del mapa sobre la línea de base. La configuración de parámetros μ = 4, λ = 0.1, α = 0.3 son casi óptimas. Línea de base TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 PR@30 0.393 0.467 0.475 0.457 1 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906% 0% 31.5%31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Tabla 3: Variación del mapa con el número de términos presentados.# Términos TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 0.275 0.274 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 360.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 nist. Diseñamos tres conjuntos de formularios de aclaración para la retroalimentación de términos, que difieren en la elección de K, el número de grupos y L, el número de términos presentados de cada grupo. Son: 1 × 48, un gran grupo con 48 términos, 3 × 16, 3 grupos con 16 términos cada uno y 6 × 8, 6 grupos con 8 términos cada uno. El número total de términos presentados (M) se fija en 48, por lo que al comparar el rendimiento de los diferentes tipos de formularios de aclaración podemos conocer los efectos del diferente grado de agrupación. Para cada tema, un asesor completaría los formularios ordenados por 6 × 8, 1 × 48 y 3 × 16, pasando hasta tres minutos en cada formulario. El formulario de aclaración de la muestra que se muestra en la Figura 1 es del tipo 3 × 16. Es una interfaz simple y compacta en la que el usuario puede verificar los términos relevantes. La forma se explica por sí misma;No hay necesidad de capacitación adicional en el usuario sobre cómo usarla. Nuestras consultas initinales se construyen solo utilizando las descripciones del título del tema, que tienen un promedio de 2.7 palabras de longitud. Como nuestra línea de base, utilizamos el método de recuperación de divergencia KL implementado en el Lemur Toolkit1 con 5 documentos de retroalimentación de pseudo. Detenemos los términos, elegimos el suavizado de Dirichlet con un anterior de 2000, y los modelos de lenguaje de consulta truncados a 50 términos (estas configuraciones se utilizan a lo largo de los experimentos). Para todos los demás parámetros utilizamos la configuración predeterminada de los lémures. La línea de base resulta funcionar por encima del promedio entre los participantes de la pista. Después de una ejecución inicial utilizando este método de recuperación de línea de base, tomamos los 60 documentos principales para cada tema y aplicamos el algoritmo de descubrimiento del tema para generar los clústeres (1, 3 o 6 de ellos), en función de los cuales generamos formularios de aclaración. Después de recibir comentarios de los usuarios, ejecutamos los algoritmos de retroalimentación del término (TFB, CFB o TCFB) para estimar los modelos de consulta actualizados, que luego se utilizan para una segunda iteración de recuperación. Evaluamos el rendimiento de los diferentes métodos de recuperación en sus clasificaciones de los 1000 principales documentos. Las métricas de evaluación que adoptamos incluyen precisión media promedio (no interpolada) (MAP), precisión en el top 30 (PR@30) y el total de recuperación relevante (RR). La Tabla 2 muestra el rendimiento de varios métodos y configuraciones de K × L. Los sufijos (1C, 3C, 6C) después de TFB, CFB, TCFB representan el número de clústeres (k). Por ejemplo, TCFB3C significa el método TCFB en los formularios de aclaración 3 × 16. De la Tabla 2 podemos hacer las siguientes observaciones: 1 http://www.lemurproject.com 1. Todos los métodos funcionan considerablemente mejor que la línea de base de Pseudofeedback, con TCFB3C logrando una mejora más alta del 41.1% en MAP, lo que indica una contribución significativa de la retroalimentación a término para la aclaración de la necesidad de la información de los usuarios. En otras palabras, la retroalimentación del término es realmente útil para mejorar la precisión de la recuperación.2. Para TFB, el rendimiento es casi igual en los formularios de aclaración 1 × 48 y 3 × 16 en términos de mapa (aunque este último es ligeramente mejor en Pr@30 y RR), y un poco peor en los 6 × 8.3. Tanto CFB3C como CFB6C funcionan mejor que sus contrapartes TFB en las tres métricas, lo que sugiere que la retroalimentación sobre una estructura de clúster secundaria es realmente beneficiosa. CFB1C es realmente peor porque no puede ajustar el peso de su clúster (único) a partir de la retroalimentación del término y es simplemente pseudofreedback.4. Aunque TCFB es solo una mezcla simple de TFB y CFB por interpolación, es capaz de superar a ambos. Esto respalda nuestra especulación de que TCFB supera los inconvenientes de TFB (prestando atención solo a los términos verificados) y CFB (no distinguiendo los términos verificados y desanimados en un clúster). Excepto por TCFB6C V.S. CFB6C, la ventaja de rendimiento de TCFB sobre TFB/CFB es significativa en P <0.05 utilizando la prueba de rango firmada de Wilcoxon. Esto no es cierto en el caso de TFB V.S. CFB, cada uno de los cuales es mejor que el otro en casi la mitad de los temas.6.2 Reducción de los términos de presentación En algunas situaciones, es posible que tengan que reducir el número de términos de presentación debido a los límites en el espacio de visualización o los esfuerzos de retroalimentación de los usuarios. Es interesante saber si nuestro rendimiento de los algoritmos se deteriora cuando el usuario se presenta con menos términos. Debido a que los términos de presentación dentro de cada clúster se generan en orden decreciente de sus frecuencias, la lista de presentación forma un subconjunto del original si su tamaño se reduce2. Por lo tanto, podemos simular fácilmente lo que sucede cuando el número de términos de presentación disminuye 2 Hay complejidades que surgen de los términos que aparecen en los mejores grupos, pero estas son excepciones de M a M: mantendremos todos los juicios de la mejor l = M/K términos en cada clúster y descartar los de otros. La Tabla 3 muestra el rendimiento de varios algoritmos ya que el número de términos de presentación varía de 6 a 48. Encontramos que el rendimiento de TFB es más susceptible a la reducción del término de presentación que el de CFB o TCFB. Por ejemplo, en 12 términos, el mapa de TFB3C es del 90.6% de eso en 48 términos, mientras que los números para CFB3C y TCFB3C son 98.0% y 96.1% respectivamente. Conjeturamos la razón para ser que, si bien el rendimiento de TFBS depende en gran medida de cuántos buenos términos se eligen para la expansión de la consulta, CFB solo necesita una estimación aproximada de los pesos del clúster para funcionar. Además, los formularios de aclaración 3 × 16 parecen ser más robustos que los 6 × 8: a los 12 términos, el mapa de TFB6C es del 87.1% de eso en 48 términos, inferiores a 90.6% para TFB3C. Del mismo modo, para CFB es 95.0% contra 98.0%. Esto es natual, ya que para un gran número de clúster de 6, es más fácil ingresar a la situación en la que cada clúster obtiene muy pocos términos de presentación para que la diversificación de temas sea útil. En general, nos sorprende ver que los algoritmos aún pueden funcionar razonablemente bien cuando el número de términos de presentación es pequeño. Por ejemplo, a solo 12 términos CFB3C (el formulario de aclaración es de tamaño 3 × 4) aún puede mejorar el 36.5% sobre la línea de base, cayendo ligeramente de 39.3% a 48 términos.6.3 Análisis de comentarios de los usuarios En esta parte, estudiamos varios aspectos del comportamiento de retroalimentación de los usuarios y si están conectados al rendimiento de la recuperación. Figura 2: Distribuciones de tiempo de finalización del formulario de aclaración 0-30 30-60 60−90 90-120 120-150 150−180 0 5 10 15 20 25 30 35 Tiempo de finalización (segundos) #Tópicos 1 × 48 3 × 16 6 × 8La Figura 2 muestra la distribución del tiempo necesario para completar un formulario de aclaración3. Vemos que el usuario generalmente puede finalizar la retroalimentación de términos dentro de un período de tiempo razonablemente corto: para más de la mitad de los temas, el formulario de aclaración se completa en solo 1 minuto, y solo una pequeña fracción de temas (menos del 10% para1 × 48 y 3 × 16) Tome más de 2 minutos. Esto sugiere que la retroalimentación del término es adecuada para la recuperación ad-hoc interactiva, donde un usuario generalmente no quiere pasar demasiado tiempo en proporcionar comentarios. Encontramos que un usuario a menudo comete errores al juzgar la relevancia del término. A veces, se puede dejar un término relevante porque su conexión con el tema de la consulta no es obvia para el usuario. Otras veces se puede incluir un término dudoso, pero resulta ser irrelevante. Tome el tema en la Figura 1 por ejemplo. Hubo un desastre de incendio en Mont 3, el tiempo máximo es de 180 segundos, ya que el asesor NIST se vería obligado a presentar el formulario en ese momento. Tabla 4: Estadísticas de selección de términos (promedio de tema) CF Tipo 1 × 48 3 × 16 6 × 8 # Términos verificados 14.8 13.3 11.2 # Rel.Términos 15.0 12.6 11.2 # Rel.Términos verificados 7.9 6.9 5.9 Precisión 0.534 0.519 0.527 Recuerde 0.526 0.548 0.527 Blanc Tunnel entre Francia e Italia en 1999, pero el usuario no pudo seleccionar palabras clave como Mont, Blanc, francés e italiano debido a su ignorancia del evento. De hecho, sin el contexto adecuado, sería difícil hacer un juicio perfecto. ¿Cuál es entonces, la medida en que el usuario es bueno en la retroalimentación de términos? ¿Tiene un gran impacto en el rendimiento de la recuperación? Para responder a estas preguntas, necesitamos una medida de los términos individuales de la verdadera relevancia. Adoptamos la métrica de divergencia KL simplificada utilizada en [24] para decidir los términos de expansión de la consulta como nuestra medida de relevancia de término: σkld (w) = p (w | r) log p (w | r) p (w | ¬r) donde p(w | r) es la probabilidad de que un documento relevante contenga el término w, y p (w | ¬r) es la probabilidad de que un documento irrelevante contenga W, los cuales se pueden calcular fácilmente mediante una estimación de máxima probabilidad dada la relevancia a nivel de documentojuicio. Si σkld (w)> 0, W es más probable que aparezca en documentos relevantes que los irrelevantes. Consideramos un término relevante si su valor de divergencia KL simplificado es mayor que un cierto umbral σ0. Luego podemos definir la precisión y el recuerdo del juicio a término del usuario en consecuencia: la precisión es la fracción de los términos verificados por el usuario que son relevantes;El recuerdo es la fracción de los términos relevantes presentados que el usuario verifica. La Tabla 4 muestra el número de términos verificados, términos relevantes y términos verificados relevantes cuando σ0 se establece en 1.0, así como la precisión/retirada del juicio a término del usuario. Tenga en cuenta que cuando los formularios de aclaración contienen más grupos, se verifican menos términos: 14.8 para 1 × 48, 13.3 para 3 × 16 y 11.2 para 6 × 8. El patrón similar se mantiene en términos relevantes y términos verificados relevantes. Parece haber una compensación entre la creciente diversidad de temas al agrupar y perder términos extra relevantes: cuando hay más grupos, cada uno de ellos obtiene menos términos presentes, lo que puede dañar un grupo relevante importante que contiene muchos términos relevantes. Por lo tanto, no siempre es útil tener más grupos, por ejemplo, TFB6C es realmente peor que TFB1C. El hallazgo principal que podemos hacer con la Tabla 4 es que el usuario no es particularmente bueno para identificar términos relevantes, lo que se hace eco del descubrimiento en [18]. En el caso de los formularios de aclaración 3 × 16, el número promedio de términos verificados como relevantes por el usuario es 13.3 por tema, y el número promedio de términos relevantes cuyo valor σkld excede 1.0 es 12.6. El usuario puede reconocer solo 6.9 de estos términos en promedio. De hecho, la precisión y el retiro de los términos de retroalimentación del usuario (como se definió anteriormente) están lejos de ser perfectas. Por otro lado, si el usuario ha verificado correctamente todos esos términos relevantes, el rendimiento de nuestros algoritmos habría aumentado mucho, como se muestra en la Tabla 5. Vemos que TFB obtiene una gran mejora cuando hay un oráculo que verifica todos los términos relevantes, mientras que CFB cumple con un cuello de botella alrededor del mapa de 0.325, ya que todo lo que hace es ajustar los pesos del clúster, y cuando los pesos aprendidos están cerca de ser precisos, no puedebeneficiarse más de la retroalimentación a término. También tenga en cuenta que TCFB no supera a TFB, probablemente porque TFB es lo suficientemente preciso. Tabla 5: Cambio de mapa cuando use todos (y solo) términos relevantes (σkld> 1.0) para retroalimentación.Término original Term retroalimentación Término relevante TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparación con la retroalimentación de relevancia Ahora contamos con la retroalimentación de la relevancia del documento con el documento.con los principales documentos N de una recuperación inicial y pidió juzgar su relevancia. El proceso de retroalimentación se simula utilizando el juicio de relevancia del documento de NIST. Utilizamos el método de retroalimentación basado en el modelo de mezcla propuesto en [25], con el ruido de la mezcla establecido en 0.95 y el coeficiente de retroalimentación establecido en 0.9. La evaluación comparativa de la retroalimentación de relevancia contra otros métodos se complica por el hecho de que algunos documentos ya se han visto durante la retroalimentación, por lo que no tiene sentido incluirlos en los resultados de recuperación de la segunda ejecución. Sin embargo, esto no es válido para la retroalimentación a término. Por lo tanto, para hacerlo justo W.R.T.ganancia de información de los usuarios, si los documentos de comentarios son relevantes, deben mantenerse en la parte superior de la clasificación;Si son irrelevantes, deben quedarse fuera. Por lo tanto, utilizamos comentarios de relevancia para producir una clasificación de los 1000 principales documentos recuperados, pero con cada documento de retroalimentación excluidos, y luego preparamos los documentos de retroalimentación relevantes en el frente. La Tabla 6 muestra el rendimiento de la retroalimentación de relevancia para diferentes valores de N y lo compara con TCFB3C. Tabla 6: Realización de retroalimentación de relevancia para el diferente número de documentos de retroalimentación (n). N Map PR@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 Vemos que el rendimiento de TCFB3C es comparable al de la retroalimentación de relevancia utilizando 5 documentos. Aunque es más pobre que cuando hay 10 documentos de retroalimentación en términos de MAP y PR@30, recupera más documentos (4947) cuando baja la lista de clasificación. Tratamos de comparar la calidad de los términos insertados automáticamente en retroalimentación relevante con la de los términos seleccionados manualmente en la retroalimentación del término. Esto se hace truncando el modelo de consulta modificado de retroalimentación de relevancia a un tamaño igual al número de términos verificados para el mismo tema. Luego podemos comparar los términos en el modelo truncado con los términos verificados. La Figura 3 muestra la distribución de los términos puntajes σkld. Encontramos que la retroalimentación del término tiende a producir términos de expansión de mayor calidad (aquellos con σkld> 1) en comparación con la retroalimentación de relevancia (con 10 documentos de retroalimentación). Esto no contradice el hecho de que este último produce un mayor rendimiento de recuperación. En realidad, cuando usamos el modelo de consulta truncada en lugar del intacto refinado de la retroalimentación de relevancia, el mapa es solo 0.304. La Verdad Figura 3: Comparación de la calidad del término de expansión entre la retroalimentación de relevancia (con 10 documentos de retroalimentación) y la retroalimentación de término (con 3 × 16 CFS) −1-0 0-1 1−2 2-3 3−4 4−5 5−6 0 50 100 100 150 200 250 300 350 σkld #terms Relevancia La retroalimentación El tiempo de retroalimentación es, aunque hay muchos términos no deseados en el modelo de consulta ampliada de los documentos de retroalimentación, también hay términos más relevantes que los que el usuario puede seleccionar de la lista de la lista deLos términos de presentación generados con documentos de pseudo-retroalimentación, y los efectos positivos a menudo superan a los negativos. Estamos interesados en saber en qué circunstancias el término comentarios tiene ventaja sobre la retroalimentación relevante. Una de esas situaciones es cuando ninguno de los principales documentos de retroalimentación es relevante, lo que hace que la retroalimentación de relevancia sea inútil. Esto no es poco frecuente, como uno podría haber pensado: de los 50 temas, hay 13 casos de este tipo cuando n = 5, 10 cuando n = 10 y aún 3 cuando n = 20. Cuando esto sucede, uno solo puede retroceder al método de recuperación original;Se pierde el poder de la retroalimentación relevante. Sorprendentemente, en 11 de los 13 casos en los que la retroalimentación de relevancia parece imposible, el usuario puede verificar al menos 2 términos relevantes del formulario de aclaración 3 × 16 (consideramos que el término t es relevante si σkld (t)> 1.0). Además, en 10 de ellos tcfb3c supera a la línea de base de pseudo-retroalimentación, aumentando el mapa de 0.076 a 0.146 en promedio (estos son temas particularmente difíciles). Creemos que hay dos posibles explicaciones para este fenómeno de la retroalimentación de términos activos, incluso cuando la retroalimentación de relevancia no funciona: primero, incluso si ninguno de los documentos de N (supongamos que es un número pequeño) son relevantes, aún podemos encontrar relevantes relevantesDocumentos en el top 60, que es más inclusivo pero generalmente inalcanzable cuando las personas están haciendo comentarios de relevancia en la búsqueda interactiva ad-hoc, de la cual podemos extraer términos de retroalimentación. Esto es cierto para la piratería del Tema 367, donde los 10 documentos de comentarios principales se tratan de piratería de software, sin embargo, hay documentos entre 10-60 que se trata de piratería en los mares (que se trata de la necesidad real de la información), contribuyendo términos como pirata, envío para la selección en el formulario de aclaración. En segundo lugar, para algunos temas, un documento debe cumplir con una condición especial para ser relevante. Los principales documentos N pueden estar relacionados con el tema, pero no obstante irrelevante. En este caso, aún podemos extraer términos útiles de estos documentos, incluso si no califican como relevantes. Por ejemplo, en las compras en línea del consumidor del Tema 639, un documento debe mencionar lo que contribuye al crecimiento de las compras para igualar la necesidad de información especificada, por lo tanto, ninguno de los 10 principales documentos de comentarios se considera relevantes. Sin embargo, los términos de retroalimentación como el comercio minorista, el comercio son buenos para la expansión de la consulta.7. Conclusiones En este documento estudiamos el uso de comentarios de términos para la recuperación de información interactiva en el enfoque de modelado de idiomas. Propusimos un método basado en clúster para seleccionar términos de presentación, así como algoritmos para estimar los modelos de consulta refinadas de los comentarios de los términos del usuario. Vimos una mejora significativa en la precisión de la recuperación traída por la retroalimentación del término, a pesar del hecho de que un usuario a menudo comete errores en un juicio relevante que perjudica su rendimiento. Encontramos que el algoritmo de mejor rendimiento es TCFB, que se beneficia de la combinación de evidencia de término directamente observada con TFB y la relevancia del clúster con CFB indirectamente aprendida con CFB. Cuando reducimos el número de términos de presentación, la retroalimentación del término aún puede mantener gran parte de su ganancia de rendimiento sobre la línea de base. Finalmente, comparamos la retroalimentación de términos con la retroalimentación de relevancia a nivel de documento, y descubrimos que el rendimiento de TCFB3CS está a la par con este último con 5 documentos de retroalimentación. Consideramos la retroalimentación del término como una alternativa viable a la retroalimentación de relevancia tradicional, especialmente cuando no hay documentos relevantes en la parte superior. Proponemos extender nuestro trabajo de varias maneras. Primero, queremos estudiar si el uso de varios contextos puede ayudar al usuario a identificar mejor la relevancia del término, sin sacrificar la simplicidad y la compacidad de la retroalimentación de términos. En segundo lugar, actualmente todos los términos se presentan al usuario en un solo lote. En su lugar, podríamos considerar la retroalimentación iterativa, presentando primero un pequeño número de términos, y mostrar más términos después de recibir comentarios de los usuarios o detenerse cuando la consulta refinada es lo suficientemente buena. Los términos presentados deben seleccionarse dinámicamente para maximizar los beneficios de aprendizaje en cualquier momento. En tercer lugar, tenemos planes de incorporar comentarios de términos en nuestra barra de herramientas UCAIR [20], un complemento de Internet Explorer, para que funcione para la búsqueda web. También estamos interesados en estudiar cómo combinar la retroalimentación de términos con comentarios de relevancia o comentarios implícitos. Podríamos, por ejemplo, permitir al usuario modificar dinámicamente los términos en un modelo de idioma aprendido de los documentos de retroalimentación.8. Reconocimiento Este trabajo es apoyado en parte por la National Science Foundation otorga IIS-0347933 e IIS-0428472.9. Referencias [1] J. Allan. Comentarios de relevancia con demasiados datos. En Actas de la 18ª Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 337-343, 1995. [2] J. Allan. Descripción general de la pista dura en TREC 2005 - Recuperación de alta precisión de documentos. En la Decimocuarta Conferencia de Recuperación de Textos, 2005. [3] P. Anick. Uso de comentarios terminológicos para el refinamiento de búsqueda web: un estudio basado en registros. En Actas de la 26ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 88-95, 2003. [4] P. G. Anick y S. Tipirneni. El Asistente de búsqueda de paráfrasis: Comentarios terminológicos para la búsqueda de información iterativa. En Actas de la 22a Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 153-159, 1999. [5] C. Buckley, G. Salton, J. Allan y A. Singhal. Expansión de consulta automática usando Smart. En Actas de la tercera conferencia de recuperación de texto, 1994. [6] D. Harman. Hacia la expansión de consultas interactivas. En Actas de la 11ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 321-331, 1988. [7] N. A. Jaleel, A. Corrada-Emmanuel, Q. Li, X. Liu, C. Wade yJ. Allan. UMass en Trec 2003: Hard y Qa. En Trec, páginas 715-725, 2003. [8] H. Joho, C. Coverson, M. Sanderson y M. Beaulieu. Presentación jerárquica de términos de expansión. En Actas del Simposio ACM de 2002 sobre computación aplicada, páginas 645-649, 2002. [9] K. S. Jones, S. Walker y S. E. Robertson. Un modelo probabilístico de recuperación de información: desarrollo y estado. Informe técnico 446, Laboratorio de Computación, Universidad de Cambridge, 1998. [10] D. Kelly, V. D. Dollu y X. Fu. El usuario locuaz: una fuente de términos independiente del documento para la expansión de la consulta. En Actas de la 28ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 457-464, 2005. [11] D. Kelly y X. Fu. Pedición de retroalimentación de relevancia del término: una investigación de la fuente del término y el contexto. En Actas de la 29a Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, 2006. [12] J. Koenemann y N. Belkin. Un caso para la interacción: un estudio del comportamiento y efectividad de recuperación de información interactiva. En Actas de la Conferencia Sigchi sobre Factores Humanos en Sistemas de Computación, páginas 205-212, 1996. [13] V. Lavrenko y W. B. Croft. Modelos de idiomas basados en relevancia. En investigación y desarrollo en recuperación de información, páginas 120-127, 2001. [14] Y. Nemeth, B. Shapira y M. Taeib-Maimon. Evaluación del valor real y percibido de la expansión automática e interactiva de la consulta. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 526-527, 2004. [15] J. Ponte. Un enfoque de modelado de idiomas para la recuperación de información. Tesis doctoral, Universidad de Massachusetts en Amherst, 1998. [16] S. E. Robertson, S. Walker, S. Jones, M. Beaulieu y M. Gatford. Okapi en TREC-3. En Actas de la tercera conferencia de recuperación de texto, 1994. [17] J. Rocchio. Comentarios de relevancia en la recuperación de información. En el sistema de recuperación inteligente, las páginas 313-323.1971. [18] I. Ruthven. Reexaminando la efectividad potencial de la expansión de consulta interactiva. En Actas de la 26ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 213-220, 2003. [19] G. Salton y C. Buckley. Mejora del rendimiento de la recuperación por retroalimentación relevante. Journal of the American Society for Information Science, 41: 288-297, 1990. [20] X. Shen, B. Tan y C. Zhai. Modelado de usuario implícito para búsqueda personalizada. En Actas de la 14ª Conferencia Internacional de ACM sobre Gestión de Información y Conocimiento, páginas 824-831, 2005. [21] X. Shen y C. Zhai. Comentarios activos en la recuperación de información ad-hoc. En Actas de la 28ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 59-66, 2005. [22] A. Spink. Comentarios de relevancia y expansión de consultas: relación con el diseño. En Actas de la 17ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 81-90, 1994. [23] J. Xu y W. B. Croft. Expansión de consulta utilizando análisis de documentos locales y globales. En Actas de la 19ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 4-11, 1996. [24] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC-13: pistas web y duras. En Actas de la 13ª Conferencia de Recuperación de Textos, 2004. [25] C. Zhai y J. Lafferty. Comentarios basados en modelos en el enfoque de modelado de idiomas para la recuperación de información. En Actas de la Décima Conferencia Internacional sobre Gestión de Información y Conocimiento, páginas 403-410, 2001. [26] C. Zhai, A. Velivelli y B. Yu. Un modelo de mezcla de colección cruzada para minería de texto comparativo. En Actas de la Décima Conferencia Internacional de ACM Sigkdd sobre Discovery y Minería de datos, páginas 743-748, 2004.