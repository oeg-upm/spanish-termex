Representación de características para la detección efectiva de acción-ítem Paul N. Bennett Departamento de Ciencias de la Computación Carnegie Mellon University Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Language Technologies Institu. Universidad de Carnegie Mellon Pittsburgh, PA 15213 jgc+@cs.cmu.edu Los usuarios de correo electrónico abstracto enfrentan un desafío cada vez mayor en la administración de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, las solicitudes de acción y otras funciones más alládiseminacion de informacion. Mientras que las técnicas de recuperación de información y aprendizaje automático están obteniendo aceptación inicial en el filtrado de spam y la asignación de carpetas automatizadas, este documento informa sobre una nueva tarea: detección automatizada de elementos de acción, para marcar correos electrónicos que requieren respuestas, y para resaltar el pasaje específico (s) indicando la (s) solicitud (s) de acción. A diferencia de la clasificación de texto estándar basada en el tema, la detección de mensajes de acción requiere inferir la intención de los remitentes, y como tal, responde menos bien a la clasificación pura de la bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidos, como N-grams (hasta n = 4) con selección de características chi cuadrado, y las señales contextuales para la ubicación de acción de acción mejoran el rendimiento hasta un 10% sobre unigramas, utilizando en ambos casos de estado deLos clasificadores de ART, como SVM con selección automatizada de modelos a través de validación cruzada integrada. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información;I.2.6 [Inteligencia artificial]: aprendizaje;I.5.4 [Reconocimiento de patrones]: Aplicaciones Términos generales Experimentación 1. Introducción Los usuarios de correo electrónico enfrentan una tarea cada vez más difícil de administrar sus bandejas de entrada frente a los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar los correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir el correo electrónico de basura, y administrar rápidamente las solicitudes que demandan de: Henry Hutchins <hhutchins@innovative.comPany.com> a: Sara Smith;Joe Johnson;William Woolings Asunto: Reunión con posibles clientes enviados: Vie 10/10/2005 8:08 AM Hola a todos, me gustaría recordarles a todos que el grupo de Grty nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en cafetería + 10:30 a.m. Descripción general de la compañía + 11:00 a.m. Reuniones individuales (continuar durante el almuerzo) + 2:00 p.m.Tour de instalaciones + 3:00 p.m. Argumento de venta Para que esto se apague sin problemas, me gustaría practicar la presentación con mucha anticipación. Como resultado, necesitaré cada una de sus piezas para el miércoles. ¡Sigan con el buen trabajo!-Henry Figura 1: Un correo electrónico con ítem de acción enfatizada, una solicitud explícita que requiere la atención o acción de los destinatarios.la atención o acción de los receptores. La detección automatizada de información de acción se dirige al tercio de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intentando resaltar la oración (u otra longitud de pasaje) que indica directamente la acciónpedido. Tal sistema de detección se puede usar como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Vemos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y clasificación de prioridad, entre otras funciones. La utilidad de dicho detector puede manifestarse como un método para priorizar los correos electrónicos de acuerdo con los criterios orientados a las tareas distintos de los estándares del tema y el remitente o como un medio para garantizar que el usuario del correo electrónico no haya dejado caer la pelota proverbial al olvidar abordar abordaruna solicitud de acción. La detección de ítems de acción difiere de la clasificación de texto estándar de dos maneras importantes. Primero, el usuario está interesado tanto en detectar si un correo electrónico contiene elementos de acción y en localizar exactamente dónde están contenidas estas solicitudes de elementos de acción en el cuerpo de correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, si esa etiqueta corresponde a una carpeta de correo electrónico o un vocabulario de indexación controlado [12, 15, 22]. En segundo lugar, la detección de acción de acción intenta recuperar la intención de los remitentes de correo electrónico, ya sea que se refiera a obtener respuesta o acción por parte del receptor;Tenga en cuenta que para esta tarea, los clasificadores que usan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En su lugar, encontramos que necesitamos más funciones cargadas de información, como N-Grams de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien usando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que puede requerir más que un enfoque de la bolsa de palabras, también funciona bastante bien utilizando solo características unigram [14]. Detección y seguimiento de temas (TDT), también funciona bien con conjuntos de características unigram [1, 20]. Creemos que la detección de acción de acción es una de las primeras instancias claras de una tarea relacionada con IR en la que debemos ir más allá de las palabras de las palabras para lograr un alto rendimiento, aunque no muy lejos, ya que parecesatisfacer. Primero revisamos el trabajo relacionado para problemas de clasificación de texto similares, como la clasificación de prioridad de correo electrónico y la identificación de la Ley del habla. Luego, definimos más formalmente el problema de detección de elementos de acción, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y resaltan los desafíos en la construcción de sistemas que pueden funcionar bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre la representación de características y las técnicas de selección apropiadas para este problema y cómo los enfoques de clasificación de texto estándar pueden adaptarse para pasar sin problemas del problema de detección de nivel de oración al problema de clasificación de nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como a establecer líneas de base para una serie de algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de estos documentos y consideramos direcciones interesantes para el trabajo futuro.2. Trabajo relacionado Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al.[5] Describa una ontología de los actos del habla, como proponer una reunión, e intentar predecir cuándo un correo electrónico contiene una de estas actos de habla. Consideramos que los elementos de acción son un tipo de habla específico importante que cae dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos en el nivel de documento. Por el contrario, consideramos si la precisión se puede aumentar mediante el uso de juicios humanos de grano más fino que marcan las oraciones y frases específicas de interés. Corston-Oliver et al.[6] Considere detectar elementos en el correo electrónico para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que no consideran que las preguntas objetivas simples pertenezcan a esta categoría. Incluimos preguntas, pero tenga en cuenta que no todas las preguntas son elementos de acción: algunas son una convención retórica o simplemente social, ¿cómo estás? Desde una perspectiva de aprendizaje, mientras utilizan juicios en el nivel de oración, no comparan explícitamente qué si algún beneficio ofrece los juicios de grano más finos. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican una SVM estándar en el nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede reformularse lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos los enfoques a nivel de documento y a nivel de oración y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al.[7] presentaron métodos para aprender redes sociales del correo electrónico. En este trabajo, no nos centramos en las relaciones entre pares;Sin embargo, tales métodos podrían complementar los aquí, ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción.3. Definición y enfoque del problema En contraste con el trabajo previo, nos centramos explícitamente en los beneficios que los juicios humanos a nivel de oración más costosos ofrecen los juicios de nivel de documentos de grano grueso. Además, consideramos múltiples enfoques de clasificación de texto estándar y analizamos las diferencias cuantitativas y cualitativas que surgen al tomar un nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos centramos en la representación necesaria para lograr el rendimiento más competitivo.3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen los elementos de acción. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasifique un documento sobre si contiene o no un Item de acción.2. Ranking de documentos: clasifique los documentos de tal manera que todos los documentos que contienen esmeriles de acción ocurren lo más alto posible en la clasificación.3. Detección de oraciones: clasifique cada oración en un documento sobre si es o no un elemento de acción. Como en la mayoría de las tareas de recuperación de información, el peso que la métrica de evaluación debe dar a la precisión y el recuerdo depende de la naturaleza de la aplicación. En situaciones en las que un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en el retiro de 1) puede ser lo más importante ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre los usuarios. Por el contrario, la detección de alta precisión en un recuerdo bajo será de creciente importancia cuando el usuario esté bajo presión de tiempo severa y, por lo tanto, probablemente no lea todo el correo. Este puede ser el caso de los gerentes de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel en las situaciones de tiempo de tiempo y simplemente para aliviar los usuarios requirió el tiempo para inyectar el mensaje.3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden venir en una de dos formas: un marcado de documentos proporciona una etiqueta sí/no para cada documento sobre si contiene un elemento de acción;Un marcado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Aficionamos a los juicios humanos un marcado de frases ya que la vista de los usuarios del ítem de acción puede no corresponder con los límites de oraciones reales o los límites de oración predichos. Obviamente, es sencillo generar un marcado de documentos consistente con un marcado de frases etiquetando un documento sí si y solo si contiene al menos una frase etiquetada por sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados con los problemas básicos que hemos enumerado como la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Luego, el documento se puede convertir a un vector de valor de características y el aprendizaje progresa como de costumbre. La aplicación de un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, uno debe hacer pasos adicionales. Por ejemplo, si el clasificador predice un documento contiene un elemento de acción, entonces las áreas del documento que contienen una alta concentración de palabras que el modelo pesa en favor de los elementos de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recolección del conjunto de capacitación son más bajos ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las oraciones específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que el marcado de frases proporcionado por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar una oración parcialmente superpuesta al convertirla a una instancia de aprendizaje. Una vez capacitado, la aplicación de los clasificadores resultantes a la detección de oraciones ahora es sencilla, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben agregarse para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de las etiquetas Morespecíficas que permiten al alumno centrar la atención en las oraciones clave en lugar de tener que aprender en base a los datos que la mayoría de las palabras en el correo electrónico proporcionan no o poca información sobre la membresía de la clase.3.2.1 Características Considere algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, avíseme, lo antes posible, lo tenga. Cada una de estas frases consiste en palabras comunes que ocurren en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de un elemento de acción. Además, el orden puede ser importante: considere tenerlo frente a usted. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema que típico de problemas como la clasificación de temas. Por lo tanto, consideramos todos los N-gramos hasta el tamaño 4. Cuando usamos N-Grams, si encontramos un N-gramo de tamaño 4 en un segmento de texto, podemos representar el texto como solo una aparición del ngram o como una ocurrencia del N-gramo y una ocurrencia de cada N más pequeño N-gram contenido por él. Elegimos la segunda de estas alternativas, ya que esto permitirá que el algoritmo mismo retroceda sin problemas en términos de retiro. Métodos como Na¨ıve Bayes pueden verse afectados por tal representación debido a la doble conteo. Dado que la puntuación que termina las oraciones puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de comienzo y fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo la puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo el correo electrónico carece de puntuación adecuada. Además, para los clasificadores a nivel de oración que usan NGRAM, además codificamos para cada oración una codificación binaria de la posición de la oración en relación con el documento. Esta codificación tiene ocho características asociadas que representan qué octual (el primer octavo, segundo octavo, etc.) contiene la oración.3.2.2 Detalles de implementación Para comparar el nivel de documento con el enfoque a nivel de oración, comparamos las predicciones en el nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites de nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de acción de acción marcado como un elemento de acción. Al evaluar la detección de sentencia para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad terrestre. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga a ninguno de los métodos. Si se estuvieran evaluando múltiples sistemas de segmentación, uno necesitaría usar una métrica que coincida con oraciones positivas predichas con frases etiquetadas como positivas. La métrica necesitaría castigar a las predicciones verdaderas demasiado largas, así como predicciones demasiado cortas. Nuestros criterios para convertir en instancias etiquetadas incluyen implícitamente ambos criterios. Dado que la segmentación es fija, una predicción demasiado larga predeciría que sí para muchos casos no, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales, todas las cuales no contienen el 30% del elemento de acción. Del mismo modo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en el ítem de acción pero no constituir toda la acción-ítem. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración anterior anterior/siguiente que es un elemento de acción donde predijimos incorrectamente no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer una predicción a nivel de documento y una puntuación a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positivas. Para producir una puntuación de documento para la clasificación, la confianza de que el documento contiene un elemento de acción es: ψ (d) = 1 n (d) s∈D | π (s) = 1 ψ (s) si es para cualquier S∈ D, π (s) = 1 1 n (d) maxs∈D ψ (s) O.W.donde s es una oración en el documento d, π es la predicción de clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza en que π (s) = 1 y n (d) es el mayor de 1 y el número de(Unigram) Tokens en el documento. En otras palabras, cuando cualquier oración se predice positiva, la puntuación del documento es la suma de longitud normalizada de los puntajes de la oración por encima del umbral. Cuando no se predice positivo, la puntuación del documento es el puntaje de oración máximo normalizado por longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras o oraciones. Por lo tanto, incluimos un factor de normalización de longitud.4. Análisis experimental 4.1 Los datos que nuestro corpus consiste en correos electrónicos obtenidos de voluntarios en una institución educativa y cubren temas como: organizar un taller de investigación, organizar entrevistas con candidatos laborales, procedimientos de publicación y anuncios de conversaciones. Los mensajes se anonimizaron reemplazando los nombres de cada individuo e institución con una seudonía. Después del anonimato de identidad, los corpus tienen tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor a menudo deja en un mensaje de correo electrónico al responder al correo electrónico. El material citado puede actuar como ruido cuando se aprende, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones de los corpus. La forma sin procesar contiene los mensajes básicos. La versión de salto automático contiene los mensajes después de que el material citado se haya eliminado automáticamente. La versión salpicada a mano contiene los mensajes después de que un humano haya eliminado el material citado. Además, la versión salpicada a mano ha tenido cualquier contenido XML y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios informados aquí se realizan con la versión salpicada a mano. Esto nos permite equilibrar la carga cognitiva en términos de número de tokens que deben leerse en los estudios de usuario que informamos; incluido el material citado, complicaría los estudios de usuarios ya que algunos usuarios podrían omitir el material mientras que otros lo leen. Además, asegurando que se elimine todo el material citado 1, tenemos una versión aún más anonimizada del corpus que puede estar disponible para alguna experimentación externa. Comuníquese con los autores para obtener más información sobre cómo obtener estos datos.evita que la validación cruzada, ya que de lo contrario, un elemento de prueba podría ocurrir como material citado en un documento de capacitación.4.1.1 Etiquetado de datos Dos anotadores humanos etiquetaron cada mensaje en cuanto a si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía un ítem de acción. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida y se les dijo que resalte las frases o oraciones que componen la solicitud. Anotador 1 No YES Annotator 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de anotadores humanos a nivel de documento Anotador Uno etiquetado 324 Mensajes como elementos de acción. Anotador dos etiquetados 327 mensajes como elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo en el nivel del documento cuando ambos marcaron el mismo documento que no contenían elementos de acción o ambos marcados al menos un ítem de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores acordaron el 93% del tiempo. La estadística de Kappa [3, 5] a menudo se usa para evaluar el acuerdo entre anotador: κ = a-r 1-r a es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada la clase empírica Priors. Un valor cercano a −1 implica que los anotadores están de acuerdo con mucha menos frecuencia de lo esperado al azar, mientras que un valor cercano a 1 significa que están de acuerdo con más frecuencia de lo que se esperaba al azar. En el nivel del documento, la estadística de Kappa para el acuerdo entre annotador es 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados para tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada sentencia para crear una oración-Corpus con etiquetas como se describe en la Sección 3.2.2, luego considere el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sobre ningún juicio. Realizamos esta comparación sobre el corpus salpicado a mano, ya que eso elimina los juicios de NO espurios que provienen de incluir material citado, etc. Ambos anotadores eran libres de etiquetar al sujeto como un elemento de acción, pero como tampoco, también omitimos la línea de asunto del mensaje. Esto solo reduce el número de acuerdos no. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores acordaron el 98% del tiempo, y la estadística de Kappa para el Acuerdo entre Annotador es 0.82. Para producir un solo conjunto de juicios, los anotadores humanos pasaron por cada anotación donde hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero anecdóticamente informaron que la mayoría de los desacuerdos eran casos de supervisión clara del anotador o diferentes interpretaciones de las declaraciones condicionales. Por ejemplo, si desea mantener su trabajo, llegar a la reunión de mañana implica una acción requerida en la que si desea unirse al anotador 1 no sí anotador 2 no 5810 65 sí 74 352 Tabla 2: Acuerdo de anotadores humanos a nivel de oración elLa piscina de apuestas de fútbol, Ven a la reunión de mañana no. El primero sería un ítem de acción en la mayoría de los contextos, mientras que el segundo no. Por supuesto, muchas declaraciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin ítems de acción y 328 correos electrónicos que contienen ActionItems. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de acción de acción;55 mensajes tienen dos segmentos de acción de acción;11 mensajes tienen tres segmentos de acción de acción. Dos mensajes tienen cuatro segmentos de acción de acción, y un mensaje tiene seis segmentos de acción de acción. Calcular el acuerdo a nivel de oración utilizando los juicios estándar de oro reconciliados con cada uno de los juicios individuales de los anotadores ofrece un kappa de 0.89 para el anotador uno y un kappa de 0.92 para el anotador dos. En términos de características del mensaje, había en promedio 132 tokens de contenido en el cuerpo después de desnudar. Para los mensajes de acción de acción, hubo 115. Sin embargo, al examinar la Figura 2, vemos que las distribuciones de longitud son casi idénticas. Como se esperaría para el correo electrónico, es una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens).4.2 Clasificadores para este experimento, hemos seleccionado una variedad de algoritmos de clasificación de texto estándar. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que difieren en líneas como discriminativos versus generativos y perezosos versus ansiosos. Hemos hecho esto para proporcionar un muestreo competitivo y completo de los métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de Strawman mediante la introducción de una nueva representación. Al probar a fondo las opciones de clasificadores alternativos, demostramos que las mejoras de representación sobre la bolsa de palabras no se deben al uso de la información en la bolsa de palabras mal.4.2.1 KNN Empleamos una variante estándar del algoritmo vecino K-Nearest utilizado en la clasificación de texto, KNN con el umbral de puntaje de corte S [19]. Utilizamos un aumento de los términos de los términos con un voto de la distancia conmovidos de los vecinos para calcular el puntaje antes de umbrarlo. Para elegir el valor de S para el umbral, realizamos una validación cruzada de baja uno sobre el conjunto de entrenamiento. El valor de K se establece en 2 (log2 n + 1) donde n es el número de puntos de entrenamiento. Esta regla para elegir K está teóricamente motivada por resultados que muestran que tal regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que con frecuencia conduce a resultados comparables con optimización numérica K a través de un procedimiento de validación cruzada.4.2.2 Na¨ıve Bayes Utilizamos un clasificador multinomial Na¨ıve Bayes estándar [16]. Al usar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la palabra anterior) y una condima m de Laplace, respectivamente.0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número deMessages Número de tokens Todos los mensajes Mensajes de acción de acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de mensajes Número de tokens de tokensTodos los mensajes Mensajes de acción de acción Figura 2: El histograma (izquierda) y la distribución (derecha) de la longitud del mensaje. Se usó un tamaño de contenedor de 20 palabras. Solo se contaron las fichas en el cuerpo después de la mano. Después de desnudar, la mayoría de las palabras que quedan suelen ser contenido de mensaje real. Clasificadores Documento Documento unigram Ngram oración Unigram oración ngram f1 knn 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve bayes 0.6572 ± 0.0749 0.6484 ± 0.05513 0.7715 ± SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682± 0.0451 Perceptron votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión KNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.80922 ± 0.0752. 0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.03090.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptron votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: rendimiento de documento promedio de la detección de documentos durante la cross-validación para la cross-validación de la cross de la cross de la cross de la cross.. El mejor rendimiento para cada clasificador se muestra en negrita.4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características TFIDF y la norma L2 como se implementa en el paquete SVMLight V6.01 [11]. Se utilizaron todas las configuraciones predeterminadas.4.2.4 Perceptron votado como el SVM, el Perceptron votado es un método de aprendizaje basado en el núcleo. Utilizamos la misma representación de características y kernel que lo hemos hecho para el SVM, un núcleo lineal con peso TFIDF y una norma L2. El Perceptron votado es un método de aprendizaje en línea que mantiene un historial de perceptrones pasados utilizados, así como un peso que significa con qué frecuencia ese perceptrón era correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el Perceptron actual y una clasificación incorrecta actualiza el Perceptron. La salida del clasificador utiliza los pesos en el Perceptra para hacer una clasificación votada final. Cuando se usa en una maniña fuera de línea, se pueden realizar múltiples pases a través de los datos de entrenamiento. Tanto el perceptrón votado como el SVM dan una solución del mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el perceptrón votado aumenta el margen de la solución después de cada pase a través de los datos de entrenamiento [10]. Desde Cohen et al.[5] Obtenga peores resultados utilizando un SVM que un perceptrón votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no estar en un área con un gran margen. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no vaya a pasar por alto un clasificador alternativo competitivo para el SVM para la representación básica de la bolsa de palabras.4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. La medida F1 [18, 21] es la media armónica de precisión y recuerdo donde la precisión = positivos correctos predijeron positivos y recuerdo = positivos correctos positivos reales.4.4 Metodología experimental Realizamos una validación cruzada de 10 veces estándar en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de capacitación o completamente en el conjunto de pruebas para cada pliegue. Para las pruebas de significación, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada pliegue de validación cruzada con un valor P de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Cada uno de los siguientes números de características se probó: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características que se pueden usar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el F1 de nivel de documento óptimo para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de N-Gram, las características de N-grams y la posición también están sujetas a la eliminación del método de selección de características.4.5 Resultados Los resultados para la clasificación a nivel de documento se dan en la Tabla 3. La hipótesis principal que nos preocupa es que los N-Grams son críticos para esta tarea;Si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que usan n-gram (documento denotado ngram) y aquellos que usan solo características unigram (documento unigram). Examinando la Tabla 3, observamos que este es el caso de cada clasificador, excepto Na¨ıve Bayes. Esta diferencia en el rendimiento producida por la representación de N-Gram es estadísticamente significativa para cada clasificador, excepto para Na¨ıve Bayes y la métrica de precisión para KNN (ver Tabla 4). Na¨ıve Bayes El bajo rendimiento con la representación de N-Gram no es sorprendente ya que la bolsa de N-Grams causa doble cuenta de duplicación como se menciona en la Sección 3.2.1;Sin embargo, Na¨ıve Bayes no se duele a nivel de oración porque los ejemplos escasos proporcionan pocas posibilidades de efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado del lenguaje, modelar los N-Grams directamente sería preferible a Na¨ıve Bayes. Más importante aún, para la hipótesis de N-Gram, los N-Grams conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación N-Gram a nivel de oración y la representación unigram es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigram, cuando se hace en el nivel de oración, se retira implícitamente en el poder de los N-Grams. Una mejora adicional significaría que el orden de las palabras es importante incluso cuando solo se considera una pequeña ventana del tamaño de la oración. Por lo tanto, los juicios a nivel de oración de grano más fino permiten que una representación unigram tenga éxito, pero solo cuando se realiza en una pequeña ventana, que se comporta como una representación de N-gram para todos los fines prácticos. Ganador del documento Ganador de la oración Knn Ngram ngram na¨ıve bayes unigram ngram svm ngram † ngram votado perceptron ngram † ngram Tabla 4: resultados de significancia para n-gram versus unigrams para la detección de documentos utilizando clasificadores de nivel de documento y niveles de nivel de oración. Cuando el resultado F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con A †. F1 Ganador de precisión Ganador de la oración KNN Sentencia Na¨ıve Bayes Sentencia Sentencia SVM Sentencia Sentencia Votado Documento de oración de percepción Tabla 5: Resultados de significación para clasificadores a nivel de oración frente a clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Al destacar aún más la mejora de juicios de grano más fino y N-Grams, la Figura 3 representa gráficamente el borde que el clasificador de nivel de oración SVM tiene sobre el enfoque estándar de la bolsa de palabras con una curva de recolección de precisión. En el área de alta precisión del gráfico, el borde consistente del clasificador a nivel de oración es bastante impresionante, continuando en la precisión 1 a 0.1 de recuerdo. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada ordenada de acción. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, de acuerdo con la gran mejora de 0.6904 a 0.7682 en la puntuación F1. Teniendo en cuenta la naturaleza relativa inexplorada de la clasificación a nivel de oración, esto da una gran esperanza de mayores aumentos en el rendimiento. Precisión F1 unigram ngram unigram ngram knn 0.9519 0.9536 0.6540 0.6686 na¨ıve bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.95559 0.9579 0.6271 0.6672 CLASEMIENTO DEL SENTENCIA DEL SENTENCIÓN 0.8895 0.9247 0.3744 4. Iers en detección de oraciones aunque Cohen et al.[5] observó que el perceptrón votado con una sola iteración de entrenamiento superó a SVM en un conjunto de tareas similares, no vemos tal comportamiento aquí. Esto fortalece aún más la evidencia de que un clasificador alternativo con la representación de la bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador de Perceptron votado mejora cuando aumentan el número de iteraciones de capacitación, pero aún es más bajo que el clasificador SVM. Los resultados de la detección de oraciones se presentan en la Tabla 6. Con respecto al problema de detección de oraciones, observamos que la medida F1 da una mejor idea del espacio restante de mejora en este problema difícil. Es decir, a diferencia de la detección de documentos, donde los documentos de ActionItem son bastante comunes, las oraciones de acción de acción son muy raras. Por lo tanto, como en otros problemas de texto, los números de precisión son engañosamente altos debido a la precisión predeterminada alcanzable al predecir siempre no. Aunque los resultados aquí están significativamente superiores al aleación, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y la clasificación. Figura 4: Los usuarios encuentran elementos de acción más rápido cuando están ayudados por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso creado para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventosas de trío. Había tres conjuntos distintos de correo electrónico en los que los usuarios tuvieron que encontrar elementos de acción. Estos conjuntos se presentaron en un orden aleatorio (desordenado), ordenados por el clasificador (ordenado) o ordenados por el clasificador y con el 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Acción de recuperación de precisión-Tuento de detección de ítem SVM (selección posterior al modelo) Documento de oración unigram NGRAM Figura 3: Tanto N-Grams como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar.oración central en la ventana de confianza más alta resaltada (orden+ayuda). Para realizar comparaciones justas entre las condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual;Es decir, la carga de lectura cognitiva debe ser aproximadamente la misma antes de la reordenamiento de los clasificadores. Además, los usuarios generalmente muestran efectos de práctica mejorando en la tarea general y, por lo tanto, se desempeñan mejor en conjuntos de mensajes posteriores. Esto generalmente se maneja variando el orden de los conjuntos entre los usuarios para que las medias sean comparables. Al omitir más detalles, observamos que los conjuntos estaban equilibrados para el número total de tokens y se usó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que a intervalos de 5, 10 y 15 minutos, los usuarios encontraron constantemente significativamente más elementos de acción cuando el clasificador lo asistió, pero se ayudaron de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda constantemente a los usuarios, no obtuvimos un impacto adicional en el usuario final al destacar. Como se mencionó anteriormente, esto podría ser el resultado de la gran sala de mejora que todavía existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser el resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, destacar la oración incorrecta cerca de un Item de acción real perjudica a los usuarios, pero si un indicador vago (por ejemplo, una flecha) apunta al área aproximada que el usuario no es consciente de la falta de falta. Dado que los estudios de usuarios utilizaron una ventana de tres oraciones, creemos que esto jugó un papel, así como una precisión de detección de oraciones.4.6 Discusión En contraste con los problemas en los que los N-Grams han producido poca diferencia, creemos que su poder aquí proviene del hecho de que muchos de los N-Grams significativos para los elementos de acción consisten en palabras comunes, por ejemplo, hágamelo saber. Por lo tanto, el enfoque unigram a nivel de documento no puede obtener mucha influencia, incluso al modelar su probabilidad conjunta correctamente, ya que estas palabras a menudo coinciden en el documento, pero no necesariamente en una frase. Además, la detección de acción de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden confiar en agregar evidencia de una gran cantidad de indicadores débiles en todo el documento. A pesar de que descartamos la información del encabezado, examinar las características mejor clasificadas en el nivel de documento revela que muchas de las características son nombres o partes de las direcciones de correo electrónico que ocurrieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden acontienen muchos o ningún elemento de acción. Algunos ejemplos son términos como Org, Bob y Gov. Observamos que estas características serán sensibles a la distribución particular (remitentes/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieren menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá de la bolsa de palabras puede ser la metodología, e investigar propiedades tales como curvas de aprendizaje y qué tan bien las transferencias de modelo pueden resaltar las diferencias en los modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones.Fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin reentrenamiento.5. El trabajo futuro Al aplicar clasificadores de texto a nivel de documento está bastante bien entendido, existe el potencial para aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Dichos métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas de TFIDF, que pueden no ser apropiadas ya que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis por frase. Además, el etiquetado de entidad con nombre, las expresiones de tiempo, etc., parecen probables candidatos para características que pueden mejorar aún más esta tarea. Actualmente estamos cursando algunas de estas vías para ver qué ganancias adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la simple representación de la bolsa de las palabras en el nivel de documento conduce a un modelo aprendido que se comporta como un contexto específico de la previa dependiente del remitente/receptor y el tema general, una primera opción sería tratarlo como tal al combinarEstimaciones de probabilidad con el clasificador de nivel de oración. Tal modelo podría servir como un ejemplo general para otros problemas en los que la bolsa de palabras puede establecer un modelo de referencia, pero se necesitan enfoques más ricos para lograr el rendimiento más allá de esa línea de base.6. Resumen y conclusiones La efectividad de la detección de nivel de oración argumenta que el etiquetado a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo interactúa esto con la cantidad de datos de capacitación disponibles. La detección de oraciones que luego se aglomera para la detección a nivel de documento funciona sorprendentemente mejor dado un retiro bajo de lo esperado con los elementos a nivel de oración. Esto, a su vez, indica que los métodos de segmentación de oraciones mejorados podrían producir mejoras adicionales en la clasificación. En este trabajo, examinamos cómo los elementos de acción pueden detectarse de manera efectiva en los correos electrónicos. Nuestro análisis empírico ha demostrado que los N-Grams son de importancia clave para aprovechar al máximo los juicios de nivel de documentos. Cuando los juicios de grano más fino están disponibles, entonces un enfoque estándar de la bolsa de palabras utilizando un pequeño tamaño de la ventana (oración) y las técnicas de segmentación automática pueden producir resultados casi tan buenos como los enfoques basados en N-Gram. Agradecimientos Este material se basa en el trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el No. NBCHD030010. Cualquier opinión, hallazgos y conclusiones o recomendaciones expresadas en este material son las del autor (s) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), o el Centro de Negocios Interiornornational (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recopilación de datos fueron esenciales para construir el corpus, y Jill y Aaron Steinfeld por su dirección de los experimentos HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de Corpus y el soporte de Curtis Huttenhowers del paquete de preprocesamiento de texto. Finalmente, agradecemos a Scott Fahlman por su aliento y discusiones útiles sobre este tema.7. Referencias [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron e Y. Yang. Estudio piloto de detección y seguimiento de temas: informe final. En Actas del Taller de transcripción y comprensión de Noticias de Broadcast DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automático de reglas de decisión para la categorización de texto. Transacciones ACM en Sistemas de Información, 12 (3): 233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo sobre tareas de clasificación: la estadística de Kappa. Computacional Lingüística, 22 (2): 249-254, 1996. [4] J. Carroll. Extracción de alta precisión de relaciones gramaticales. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (Coling), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprender a clasificar el correo electrónico en actos de habla. En EMNLP-2004 (Conferencia sobre métodos empíricos en procesamiento del lenguaje natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen de correo electrónico centrado en la tarea. En el resumen de texto se ramifican: Actas del taller de ACL-04, páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extraer redes sociales e información de contacto del correo electrónico y la web. En CEAS-2004 (Conferencia sobre correo electrónico y anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia ACM sobre Gestión de Información y Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Gran clasificación de margen utilizando el algoritmo Perceptron. Aprendizaje automático, 37 (3): 277-296, 1999. [11] T. Joachims. Hacer práctico el aprendizaje SVM a gran escala. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en los métodos del núcleo - Aprendizaje de vectores de soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia ACM sobre Bibliotecas Digitales, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En Sigir 92, Actas de la 15ª Conferencia Anual de ACM internacional sobre investigación y desarrollo en recuperación de información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para la clasificación de género precisa. En Actas de la Conferencia Europea sobre Aprendizaje Autor (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio de comparación de núcleos para la clasificación de texto multiclabel utilizando la asociación de categorías. En la vigésima primera conferencia internacional sobre aprendizaje automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto de Naive Bayes. En Notas de trabajo de AAAI 98 (la 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre aprendizaje para la categorización de texto, páginas 41-48, 1998. TR WS-98-05.[17] F. Sebastiani. Aprendizaje automático en categorización de texto automatizado. ACM Computing Surveys, 34 (1): 1-47, marzo de 2002. [18] C. J. Van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de los enfoques estadísticos para la categorización de texto. Recuperación de información, 1 (1/2): 67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y el seguimiento de los temas. IEEE Expert, número especial sobre aplicaciones de recuperación de información inteligente, 1999. [21] Y. Yang y X. Liu. Un reexamen de los métodos de categorización de texto. En Sigir 99, Actas de la 22a Conferencia Internacional de ACM anual sobre investigación y desarrollo en recuperación de información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedad condicionada por el tema. En Actas de la Conferencia Internacional de ACM SIGKDD sobre Discovery y Minería de datos de conocimiento, julio de 2002.