Seguimiento de los predecesores inmediatos en los cálculos distribuidos Emmanuelle Anceaume Jean-Michel H´Eltary Michel Raynal Irisa, Campus Beaulieu 35042 Rennes Cedex, France FirstName.lastname@irisa.fr Aver Ractation A Guhal se modela como un conjunto parcialmente ordenado de eventos relevantes (el relevanteLos eventos son un subconjunto de los eventos primitivos producidos por el cálculo). Un importante problema de computación distribuida relacionada con la causalidad, que llamamos al problema de seguimiento de predecesores inmediatos (IPT), consiste en asociar con cada evento relevante, sobre la marcha y sin usar mensajes de control adicionales, el conjunto de eventos relevantes que son sus predecesores inmediatos enel orden parcial. Entonces, IPT es el cálculo sobre la marcha de la reducción transitiva (es decir, diagrama de Hasse) de la relación de causalidad definida por un cálculo distribuido. Este documento aborda el problema IPT: presenta una familia de protocolos que proporciona a cada evento relevante una marca de tiempo que identifica exactamente a sus predecesores inmediatos. La familia se define mediante una condición general que permite que los mensajes de aplicación sean información de control de pildo cuyo tamaño puede ser menor que n (el número de procesos). En ese sentido, esta familia define los protocolos IPT del tamaño del tamaño del mensaje. Según la forma en que se implementa la condición general, se pueden obtener diferentes protocolos IPT. Dos de ellos se exhiben. Categorías y descriptores de sujetos C.2.4 [Sistemas distribuidos]: Términos generales de cálculos distribuidos asíncronos 1. Introducción Un cálculo distribuido consiste en un conjunto de procesos que cooperan para lograr un objetivo común. Una característica principal de estos cálculos radica en el hecho de que los procesos no comparten una memoria global común y se comunican solo intercambiando mensajes a través de una red de comunicación. Además, los retrasos en la transferencia de mensajes son finitos pero impredecibles. Este modelo de cálculo define lo que se conoce como el modelo de sistema distribuido asíncrono. Es particularmente importante ya que incluye sistemas que abarcan grandes áreas geográficas y sistemas que están sujetos a cargas impredecibles. En consecuencia, los conceptos, herramientas y mecanismos desarrollados para sistemas distribuidos asincrónicos revelan que son importantes y generales. La causalidad es un concepto clave para comprender y dominar el comportamiento de los sistemas distribuidos asincrónicos [18]. Más precisamente, dados dos eventos E y F de un cálculo distribuido, un problema crucial que debe resolverse en muchas aplicaciones distribuidas es saber si están causalmente relacionados, es decir, si la ocurrencia de una de ellas es una consecuencia dela aparición del otro. El pasado causal de un evento E es el conjunto de eventos de los que E depende causalmente. Se dice que los eventos que no son causalmente dependientes son concurrentes. Se han introducido relojes vectoriales [5, 16] para permitir que los procesos rastreen la causalidad (y la concurrencia) entre los eventos que producen. La marca de tiempo de un evento producido por un proceso es el valor actual del reloj vectorial del proceso correspondiente. De esa manera, al asociar las marcas de tiempo del vector con eventos, es posible decidir de forma segura si dos eventos están causalmente relacionados o no. Por lo general, de acuerdo con el problema en el que se enfoca, un diseñador solo está interesado en un subconjunto de los eventos producidos por una ejecución distribuida (por ejemplo, solo los eventos de punto de control son significativos cuando uno está interesado en determinar los puntos de control globales consistentes [12]). Se deduce que la detección de dependencias causales (o concurrencia) de todos los eventos del cálculo distribuido no es deseable en todas las aplicaciones [7, 15]. En otras palabras, entre todos los eventos que pueden ocurrir en un cálculo distribuido, solo un subconjunto de ellos es relevante. En este artículo, estamos interesados en la restricción de la relación de causalidad con el subconjunto de eventos definidos como los eventos relevantes del cálculo. Al ser un orden parcial estricto, la relación de causalidad es transitiva. Como consecuencia, entre todos los eventos relevantes que preceden causalmente a un evento relevante dado, solo un subconjunto son sus predecesores inmediatos: esos son los eventos F de modo que no hay un evento relevante en ningún camino causal de F a E.Desafortunadamente, dada solo la marca de tiempo del vector asociado con un evento, no es posible determinar qué eventos de su pasado causal son sus predecesores inmediatos. Esto proviene del hecho de que la marca de tiempo del vector asociada con E determina, para cada proceso, el último evento relevante pertenece al pasado causal de E, pero dicho evento no es necesariamente un predecesor inmediato de e.Sin embargo, algunas aplicaciones [4, 6] requieren asociarse con cada evento relevante solo el conjunto de sus predecesores inmediatos. Esas aplicaciones están relacionadas principalmente con el análisis de cálculos distribuidos. Algunos de esos análisis requieren la construcción de la red de cortes consistentes producidos por el cálculo [15, 16]. Se muestra en [4] que el seguimiento de predecesores inmediatos permite un eficiente en la construcción de la mosca de esta red. En términos más generales, estas aplicaciones están interesadas en la estructura misma del pasado causal. En este contexto, la determinación de los predecesores inmediatos se convierte en un problema importante [6]. Además, en algunas circunstancias, esta determinación tiene que satisfacer las limitaciones de comportamiento. Si el patrón de comunicación del cálculo distribuido no puede modificarse, la determinación debe hacerse sin agregar mensajes de control. Cuando los predecesores inmediatos se utilizan para monitorear el cálculo, debe hacerse en la marcha. Llamamos al seguimiento del predecesor inmediato (IPT) el problema que consiste en determinar sobre la mosca y sin mensajes adicionales, los predecesores inmediatos de los eventos relevantes. Este problema consiste en realidad para determinar la reducción transitiva (diagrama de HASSE) del gráfico de causalidad generado por los eventos relevantes del cálculo. Resolver este problema requiere un seguimiento de la causalidad, por lo tanto, utilizando relojes vectoriales. Los trabajos anteriores han abordado la implementación eficiente de los relojes vectoriales para rastrear la dependencia causal de los eventos relevantes. Su objetivo era reducir el tamaño de las marcas de tiempo adjuntas a los mensajes. En [19] se propone una implementación eficiente de reloj vectorial adecuado para sistemas con canales FIFO. Otra implementación eficiente que no depende de la propiedad de ordenamiento de canales se describe en [11]. La noción de barrera causal se introduce en [2, 17] para reducir el tamaño de la información de control requerida para implementar multidifusión causal. Sin embargo, ninguno de estos documentos considera el problema IPT. Este problema se ha abordado por primera vez (hasta donde sabemos) en [4, 6] donde se describe un protocolo IPT, pero sin prueba de corrección. Además, en este protocolo, las marcas de tiempo adjuntas a los mensajes son de tamaño n.Esto plantea la siguiente pregunta que, hasta donde sabemos, nunca ha sido respondida: ¿hay técnicas eficientes de implementación del reloj vectorial que sean adecuadas para el problema IPT? Este documento tiene tres contribuciones principales: (1) una respuesta positiva a la pregunta abierta anterior, (2) el diseño de una familia de protocolos IPT eficientes y (3) una prueba de corrección formal de los protocolos asociados. Desde un punto de vista metodológico, el documento utiliza un enfoque de arriba hacia abajo. Establece propiedades abstractas de las cuales se derivan más propiedades y protocolos de concreto. La familia de protocolos IPT se define mediante una condición general que permite que los mensajes de aplicación sean información de control de respaldo cuyo tamaño puede ser más pequeño que el tamaño del sistema (es decir, menor que el número de procesos que componen el sistema). En ese sentido, esta familia define protocolos IPT de bajo costo cuando consideramos el tamaño del mensaje. Además de la eficiencia, el enfoque propuesto tiene una propiedad de diseño interesante. A saber, la familia se construye incrementalmente en tres pasos. El protocolo básico de reloj vectorial se enriquece primero al agregar a cada proceso un vector booleano cuya gestión permite que los procesos rastreen los eventos predecesor inmediatos. Luego, se establece una condición general para reducir el tamaño de la información de control llevada por los mensajes. Finalmente, de acuerdo con la forma en que se implementa esta condición, se obtienen tres protocolos IPT. El documento está compuesto por siete secciones. Las secciones 2 introducen el modelo de cálculo, los relojes vectoriales y la noción de eventos relevantes. La Sección 3 presenta el primer paso de la construcción que da como resultado un protocolo IPT en el que cada mensaje lleva un reloj vectorial y una matriz booleana, ambos de tamaño N (el número de procesos). La Sección 4 mejora este protocolo al proporcionar la condición general que permite que un mensaje lleve la información de control cuyo tamaño puede ser menor que n.La Sección 5 proporciona instanciaciones de esta condición. La Sección 6 proporciona un estudio de simulación que compara los comportamientos de los protocolos propuestos. Finalmente, la Sección 7 concluye el documento.(Debido a limitaciones de espacio, se omiten pruebas de lemas y teoremas. Se pueden encontrar en [1].) 2. Modelo y reloj Vector 2.1 Cálculo distribuido Un programa distribuido está compuesto por programas locales secuenciales que se comunican y sincronizan solo al intercambiar mensajes. Un cálculo distribuido describe la ejecución de un programa distribuido. La ejecución de un programa local da lugar a un proceso secuencial. Sea {P1, P2 ,..., Pn} será el conjunto finito de procesos secuenciales del cálculo distribuido. Cada par ordenado de procesos de comunicación (PI, PJ) está conectado por un canal confiable CIJ a través del cual PI puede enviar mensajes a PJ. Suponemos que cada mensaje es único y que un proceso no envía mensajes a sí mismo1. Los retrasos en la transmisión de mensajes son finitos pero impredecibles. Además, los canales no son necesariamente FIFO. Las velocidades de proceso son positivas pero arbitrarias. En otras palabras, el modelo de cálculo subyacente es asíncrono. El programa local asociado con PI puede incluir declaraciones de envío, recepción y internas. La ejecución de dicha declaración produce un evento de envío/recepción/interno correspondiente. Estos eventos se llaman eventos primitivos. Let Ex I Be the X-Th Event producido por Process Pi. La secuencia HI = E1 I E2 i...ex i...constituye la historia de Pi, denotada HI. Sea h = ∪n i = 1hi el conjunto de eventos producidos por un cálculo distribuido. Este conjunto está estructurado como un orden parcial por Lamports ocurrió antes de la relación [14] (denotado Hb →) y se define de la siguiente manera: Ex I Hb → Ey J si y solo si (i = j ∧ x + 1 = y) (precedencia local) ∨ (∃m: ex i = enviar (m) ∧ ey j = recibir (m)) (msg prec.) ∨ (∃ ez k: ex i hb → ez k ∧ e z k hb → ey j) (cierre transitivo).Max (Ex I, EY J) es una función parcial definida solo cuando se ordenan Ex I y Ey J. Se define de la siguiente manera: max (ex i, ey j) = ex i si ey j hb → ex i, max (ex i, ey j) = ey i if ex i hb → ey j. Claramente, la restricción de HB → a HI, para un dado I, es un orden total. Por lo tanto, usaremos la notación ex i <ey i iff x <y. A lo largo del documento, utilizaremos la siguiente notación: si E ∈ Hi no es el primer evento producido por PI, entonces Pred (E) denota el evento inmediatamente anterior a E en la secuencia HI. Si E es el primer evento producido por PI, entonces Pred (e) se denota por ⊥ (lo que significa que no hay tal evento), y ∀e ∈ Hi: ⊥ <e.El orden parcial BH = (H, Hb →) constituye un modelo formal del cálculo distribuido con el que se asocia.1 Esta suposición es solo para obtener protocolos simples.211 p1 p2 p3 [1, 1, 2] [1, 0, 0] [3, 2, 1] [1, 1, 0] (2, 1) [0, 0, 1] (3, 1) [2, 0, 1] (1, 1) (1, 3) (1, 2) (2, 2) (2, 3) (3, 2) [2, 2, 1] [2, 3, 1](1, 1) (1, 2) (1, 3) (2, 1) (2, 2) (2, 3) (3, 1) (3, 2) Figura 1: Eventos relevantes y gráfico de predecesores inmediatos.(Diagrama de Hasse) 2.2 Eventos relevantes para un observador dado de un cálculo distribuido, solo algunos eventos son relevantes2 [7, 9, 15]. Un ejemplo interesante de lo que es una observación es la detección de predicados en estados globales consistentes de un cálculo distribuido [3, 6, 8, 9, 13, 15]. En ese caso, un evento relevante corresponde a la modificación de una variable local involucrada en el predicado global. Otro ejemplo es el problema del punto de control donde un evento relevante es la definición de un punto de control local [10, 12, 20]. La parte izquierda de la Figura 1 muestra un cálculo distribuido utilizando el diagrama clásico de espacio-tiempo. En esta figura, solo se representan eventos relevantes. La secuencia de eventos relevantes producidos por el proceso Pi se denota por RI, y r = ∪n i = 1ri ⊆ h denota el conjunto de todos los eventos relevantes. Sea → la relación en r definida de la siguiente manera: ∀ (e, f) ∈ R × R: (E → F) ⇔ (E Hb → F). El POSET (R, →) constituye una abstracción del cálculo distribuido [7]. A continuación, consideramos un cálculo distribuido a tal nivel de abstracción. Además, sin pérdida de generalidad, consideramos que el conjunto de eventos relevantes es un subconjunto de los eventos internos (si se debe observar un evento de comunicación, se puede generar un evento interno relevante justo antes de un envío y justo después de que ocurriera un evento de comunicación de recibir). Cada evento relevante se identifica mediante un par (ID de proceso, número de secuencia) (ver Figura 1). Definición 1. El pasado causal relevante de un evento e ∈ H es el subconjunto (parcialmente ordenado) de eventos relevantes f tal que f hb → e.Se denota ↑ (e). Tenemos ↑ (e) = {f ∈ R |F Hb → E}. Tenga en cuenta que, si e ∈ R entonces ↑ (e) = {f ∈ R |F → E}. En el cálculo descrito en la Figura 1, tenemos, para el evento E identificado (2, 2): ↑ (e) = {(1, 1), (1, 2), (2, 1), (3, 1)}. Las siguientes propiedades son consecuencias inmediatas de las definiciones anteriores. Sea e ∈ H. cp1 si e no es un evento de recepción, entonces ↑ (e) = 8 <: ∅ if pred (e) = ⊥, ↑ (pred (e)) ∪ {pred (e)} si pred (e)∈ R, ↑ (pred (e)) si pred (e) ∈ R. cp2 si e es un evento de recepción (de un mensaje m), entonces ↑ (e) = 8 >> <>>: ↑ (enviar (m)) si pred (e) = ⊥, ↑ (pred (e)) ∪ ↑ (enviar (m)) ∪ {pred (e)} si pred (e) ∈ R, ↑ (pred (e)) ∪ ↑ (enviar(m)) Si pred (e) ∈ R. 2, esos eventos a veces se llaman eventos observables. Definición 2. Deje e ∈ HI. Para cada j tal que ↑ (e) ∩ rj = ∅, el último evento relevante de PJ con respecto a E es: LASTR (E, J) = Max {f |f ∈ ↑ (e) ∩ rj}. Cuando ↑ (e) ∩ rj = ∅, lastr (e, j) se denota por ⊥ (lo que significa que no hay tal evento). Consideremos el evento E identificado (2,2) en la Figura 1. Tenemos LASTR (E, 1) = (1, 2), LASTR (E, 2) = (2, 1), LASTR (E, 3) = (3, 1). Las siguientes propiedades relacionan los eventos LASTR (E, J) y LASTR (F, J) para todos los predecesores F de E en la relación Hb →. Estas propiedades siguen directamente de las definiciones. Deje e ∈ HI. Lr0 ∀e ∈ Hi: Lastre, i) = 8 <: ⊥ if pred (e) = ⊥, pred (e) si pred (e) ∈ R, lastrre (pred (e), i) si pred (e) ∈R. LR1 Si E no es un evento de recibo: ∀j = I: LASTR (E, J) = LASTR (PRED (E), J). LR2 Si E es un evento de recepción de M: ∀j = I: LASTR (E, J) = Max (LASTR (Pred (E), J), LASTR (SEND (M), J)).2.3 Definición del sistema de reloj vectorial como un concepto fundamental asociado con la teoría de la causalidad, los relojes vectoriales se han introducido en 1988, simultáneamente e independientemente por Fidge [5] y Mattern [16]. Un sistema de reloj vectorial es un mecanismo que asocia las marcas de tiempo con eventos de tal manera que la comparación de sus marcas de tiempo indica si los eventos correspondientes están o no relacionados causalmente (y, si lo son, cuál es el primero). Más precisamente, cada proceso PI tiene un vector de enteros v Ci [1..n] de tal manera que v ci [j] es el número de eventos relevantes producidos por PJ, que pertenecen al pasado causal relevante actual de PI. Tenga en cuenta que v Ci [i] cuenta el número de eventos relevantes producidos hasta ahora por PI. Cuando un proceso PI produce un evento E (relevante) E, se asocia con una marca de tiempo vectorial cuyo valor (denotado e.v c) es igual al valor actual de v Ci. Implementación del reloj vectorial La siguiente implementación de los relojes vectoriales [5, 16] se basa en la observación de que ∀i, ∀e ∈ Hi, ∀j: e.v ci [j] = y ⇔ lastr (e, j) = ey j where e.vCI es el valor de V Ci justo después de la aparición de E (esta relación resulta directamente de las propiedades LR0, LR1 y LR2). Cada proceso PI administra su reloj vector V CI [1..n] de acuerdo con las siguientes reglas: VC0 v Ci [1..n] se inicializa a [0 ,..., 0]. VC1 cada vez que produce un evento relevante E, PI incrementa su entrada de reloj Vector v Ci [i] (v Ci [i]: = v ci [i] + 1) a 212 indica que ha producido un evento más relevante, luego PiAsociado con E la marca de tiempo E.V C = V CI. VC2 Cuando un proceso Pi envía un mensaje m, se adjunta a M el valor actual de v Ci. Deje que M.V C denote este valor. VC3 Cuando Pi recibe un mensaje M, actualiza su reloj vectorial de la siguiente manera: ∀K: V Ci [k]: = max (v ci [k], m.v c [k]).3. Predecesores inmediatos En esta sección, se establece el problema de seguimiento del predecesor inmediato (IPT) (Sección 3.1). Luego, algunas propiedades técnicas de los predecesores inmediatos se declaran y proban (Sección 3.2). Estas propiedades se utilizan para diseñar el protocolo IPT básico y demostrar su corrección (Sección 3.3). Este protocolo IPT, previamente presentado en [4] sin prueba, se construye a partir de un protocolo de reloj vectorial agregando la gestión de una matriz booleana local en cada proceso.3.1 El problema IPT como se indica en la introducción, algunas aplicaciones (por ejemplo, análisis de ejecuciones distribuidas [6], la detección de propiedades distribuidas [7]) requieren determinar (sobre la marcha y sin mensajes adicionales) la reducción transitiva de larelación → (es decir, no debemos considerar la dependencia causal transitiva). Dados dos eventos relevantes F y E, decimos que F es un predecesor inmediato de E If F → E y no hay un evento relevante G tal que F → G → E.Definición 3. El problema de seguimiento del predecesor inmediato (IPT) consiste en asociarse con cada evento relevante E El conjunto de eventos relevantes que son sus predecesores inmediatos. Además, esto debe hacerse en la mosca y sin un mensaje de control adicional (es decir, sin modificar el patrón de comunicación del cálculo). Como se señaló en la introducción, el problema IPT es el cálculo del diagrama HASSE asociado con el conjunto parcialmente ordenado de los eventos relevantes producidos por un cálculo distribuido.3.2 Propiedades formales de IPT Para diseñar un protocolo que resuelva el problema IPT, es útil considerar la noción de predecesor relevante inmediato de cualquier evento, ya sea relevante o no. Primero, observamos que, por definición, el predecesor inmediato en PJ de un evento E es necesariamente el evento LASTR (E, J). Segundo, para que LASTR (E, J) sea predecesor inmediato de E, no debe haber otro evento LASTR (E, K) en un camino entre LASTR (E, J) y E.Estas observaciones se formalizan en la siguiente definición: Definición 4. Deje e ∈ HI. El conjunto de predecesores relevantes inmediatos de E (IP denotado (e)), es el conjunto de eventos relevantes LASTR (E, J) (J = 1, ..., N) de tal manera que ∀K: LASTR (E, J) ∈ ↑ (lastr (e, k)). De esta definición se deduce que ip (e) ⊆ {lastr (e, j) | j = 1 ,..., n} ⊂ ↑ (e). Cuando consideramos la Figura 1, el gráfico representado en su parte derecha describe los predecesores inmediatos de los eventos relevantes del cálculo definido en su parte izquierda, más precisamente, un borde dirigido (E, F) significa que el evento relevante es inmediatopredecesor del evento relevante F (3). Los siguientes lemas muestran cómo el conjunto de predecesores inmediatos de un evento está relacionado con los de sus predecesores en la relación Hb →. Se utilizarán para diseñar y demostrar que los protocolos resuelven el problema IPT. Para aliviar la lectura del documento, sus pruebas se presentan en el Apéndice A. El significado intuitivo del primer lema es el siguiente: si E no es un evento de recepción, todas las rutas causales que llegan a E tienen pred (e) como evento siguiente (ver CP1). Entonces, si Pred (e) es un evento relevante, todos los eventos relevantes que pertenecen a su pasado causal relevante se separan de E por pred (e), y Pred (e) se convierte en el único predecesor inmediato de e.En otras palabras, el evento pred (e) constituye un restablecimiento de W.R.T.El conjunto de predecesores inmediatos de e.Por otro lado, si el pred (e) no es relevante, no separa su pasado causal relevante de e.Lema 1. Si E no es un evento de recibir, IP (e) es igual a: ∅ if pred (e) = ⊥, {pred (e)} if pred (e) ∈ R, ip (pred (e)) si pred (e) ∈ R. El significado intuitivo de la próxima lema es el siguiente: si E es un evento de recepción Recibe (m), las rutas causales que llegan a E tienen pred (e) o envían (m) como eventos siguientes. Si Pred (e) es relevante, como se explica en el lema anterior, este evento se esconde de E todo su pasado causal relevante y se convierte en un predecesor inmediato de E.Con respecto a los últimos predecesores relevantes de SEND (M), solo aquellos que no son predecesores de Pred (E) siguen siendo predecesores inmediatos de E.Lema 2. Sea e ∈ Hi sea el evento de recepción de un mensaje m.If pred(e) ∈ Ri, then, ∀j, IP(e) ∩ Rj is equal to: {pred(e)} if j = i, ∅ if lastr(pred(e),j) ≥ lastr(send(m), j), ip (enviar (m)) ∩ rj if lastr (pred (e), j) <lastr (send (m), j). El significado intuitivo de la próxima lema es el siguiente: si E es un evento de recepción de recepción (m), y Pred (e) no es relevante, los últimos eventos relevantes en el pasado causal relevante de E se obtienen fusionando los de Pred (e) y los de enviar (m) y tomando lo último en cada proceso. Entonces, los predecesores inmediatos de E son los de Pred (E) o los de Send (M). En un proceso en el que los últimos eventos relevantes de Pred (e) y Send (m) son el mismo evento F, ninguno de los caminos de F a E debe contener otro evento relevante y, por lo tanto, F debe ser predecesor inmediato de ambos eventospred (e) y enviar (m). Lema 3. Sea e ∈ Hi sea el evento de recepción de un mensaje m.Si pred (e) ∈ Ri, entonces, ∀j, ip (e) ∩ rj es igual a: ip (pred (e)) ∩ rj if lastr (pred (e), j)> lastr (send (m),j), ip (send (m)) ∩ rj if lastr (pred (e), j) <lastr (send (m), j) ip (pred (e)) ∩ip (send (m)) ∩rj ifLASTR (Pred (E), J) = LASTR (SEND (M), J).3.3 Un protocolo IPT básico El protocolo básico propuesto aquí se asocia con cada evento relevante E, un atributo que codifica el conjunto IP (e) de sus predecesores inmediatos. De los lemas anteriores, el conjunto 3 en realidad, este gráfico es el diagrama de Hasse del orden parcial asociado con el cálculo distribuido.213 IP (e) de cualquier evento E depende de los conjuntos IP de los eventos pred (e) y/o envían (m) (cuando e = recibir (m)). De ahí la idea de introducir una estructura de datos que permita administrar los conjuntos IP inductivamente en el POSET (H, Hb →). Para tener en cuenta la información de Pred (E), cada proceso administra una matriz booleana IPI de tal manera que, ∀e ∈ Hi El valor de IPI cuando ocurre E (denotado e.IPI) es la representación de la matriz booleana del conjunto de ip (e). Más precisamente, ∀j: ipi [j] = 1 ⇔ lastr (e, j) ∈ Ip (e). Como se recordó en la Sección 2.3, el conocimiento de LASTR (E, J) (para cada E y cada J) se basa en la gestión de los vectores v CI. Por lo tanto, el set ip (e) se determina de la siguiente manera: ip (e) = {ey j |e.v ci [j] = y ∧ e.ipi [j] = 1, j = 1 ,..., n} Cada proceso PI actualiza IPI de acuerdo con los Lemmas 1, 2 y 3: 1. Resulta de Lemma 1 que, si E no es un evento de recepción, el valor actual de IPI es suficiente para determinar E.IPI. Resulta de Lemmas 2 y 3 que, si E es un evento de recepción (e = recibir (m)), entonces la determinación de E.IPI implica información relacionada con el envío del evento (m). Más precisamente, esta información involucra IP (Send (M)) y la marca de tiempo de Send (M) (necesaria para comparar los eventos LASTR (SEND (M), J) y LASTR (Pred (E), J), para cada J). Entonces, ambos vectores envían (m) .v cj y envían (m) .ipj (suponiendo que envíe (m) producido por PJ) se adjuntan al mensaje m.2. Además, IPI debe actualizarse sobre la ocurrencia de cada evento. De hecho, el valor de IPI justo después de un evento E se usa para determinar el valor succ (e) .IPI. En particular, como se indica en los lemas, la determinación de succ (e) .IPI depende de si E es relevante o no. Por lo tanto, el valor de IPI justo después de la ocurrencia del evento E debe realizar un seguimiento de este evento. El siguiente protocolo, presentado previamente en [4] sin prueba, garantiza la gestión correcta de las matrices v CI (como en la Sección 2.3) y IPI (según los lemas de la Sección 3.2). La marca de tiempo asociada con un evento relevante E se denota E.Ts. Inicialización R0: tanto V Ci [1..N] como IPI [1..n] se inicializan a [0 ,..., 0]. R1 cada vez que produce un evento relevante e: - Pi Associates con E la marca de tiempo E.TS definido de la siguiente manera e.ts = {(k, v ci [k]) |Ipi [k] = 1}, - Pi incrementa su entrada de reloj vector V Ci [i] (es decir, ejecuta v Ci [i]: = v Ci [i] + 1), - Pi restablece IPI: ∀ = i: IPI[]: = 0;Ipi [i]: = 1. R2 Cuando Pi envía un mensaje M a PJ, se adjunta a M los valores actuales de V CI (denotado M.V C) y la matriz booleana IPI (denotada M.IP). R3 Cuando recibe un mensaje M de PJ, Pi ejecuta las siguientes actualizaciones: ∀k ∈ [1..N]: Caso V Ci [K] <M.V C [K] entoncesv Ci [K]: = M.V C [K];Ipi [k]: = m.ip [k] v ci [k] = m.v c [k] y luego ipi [k]: = min (ipi [k], m.ip [k]) v ci [k]>M.V C [K] Luego omita el final de la prueba del siguiente teorema que se deduce directamente de Lemmas 1, 2 y 3. Teorema 1. El protocolo descrito en la Sección 3.3 resuelve el problema IPT: para cualquier evento relevante E, la marca de tiempo E.TS contiene los identificadores de todos sus predecesores inmediatos y ningún otro identificador de eventos.4. Una condición general que esta sección aborda un problema previamente abierto, a saber, ¿cómo resolver el problema IPT sin requerir que cada mensaje de aplicación tenga un reloj de vector completo y una matriz booleana completa? Primero, se puede omitir una condición general que caracteriza qué entradas de vectores V CI y IPI se pueden omitir a partir de la información de control adjunta a un mensaje enviado en el cálculo (Sección 4.1). Luego se muestra (Sección 4.2) que esta condición es suficiente y necesaria. Sin embargo, esta condición general no puede evaluarse localmente mediante un proceso que esté a punto de enviar un mensaje. Por lo tanto, se deben definir aproximaciones localmente evaluables de esta condición general. A cada aproximación corresponde un protocolo, implementado con estructuras de datos locales adicionales. En ese sentido, la condición general define una familia de protocolos IPT, que resuelven el problema previamente abierto. Este problema se aborda en la Sección 5. 4.1 para transmitir o no transmitir información de control, consideremos el protocolo IPT anterior (Sección 3.3). La regla R3 muestra que un proceso PJ no actualiza sistemáticamente cada entrada v CJ [k] cada vez que recibe un mensaje M de un proceso PI: no hay actualización de V CJ [K] cuando v CJ [K] ≥ M.V C [K]. En tal caso, el valor M.V C [k] es inútil y podría omitirse a partir de la información de control transmitida con M por PI a PJ. Del mismo modo, algunas entradas IPJ [k] no se actualizan cuando PI recibe un mensaje M de PI. Esto ocurre cuando 0 <v cj [k] = m.v c [k] ∧ m.ip [k] = 1, o cuando v cj [k]> m.v c [k], o cuando m.v c [k] = 0 (En el último caso, como M.IP [k] = ipi [k] = 0, entonces no es necesaria actualización de IPJ [k]). De manera diferente, algunas otras entradas se restablecen sistemáticamente a 0 (esto ocurre cuando 0 <v Cj [k] = m.v c [k] ∧ m.ip [k] = 0). Estas observaciones conducen a la definición de la condición k (m, k) que caracteriza qué entradas de vectores v Ci y IPI pueden omitirse a partir de la información de control adjunta a un mensaje M enviado por un proceso PI a un proceso PJ: Definición 5. K (m, k) ≡ (enviar (m) .v ci [k] = 0) ∨ (enviar (m) .v ci [k] <pred (recibo (m)). V cj [k]) ∨;(Enviar (m) .v Ci [k] = pred (recibir (m)). V CJ [k]) ∧ (enviar (m) .ipi [k] = 1).4.2 Una condición necesaria y suficiente, mostramos aquí que la condición k (m, k) es necesaria y suficiente para decidir qué triples de la forma (k, envía (m) .v ci [k], send (m) .ipi[k]) se puede omitir en un mensaje saliente m enviado por PI a PJ. También se denotará un triple adjunto a M (K, M.V C [K], M.IP [K]). Debido a las limitaciones de espacio, las pruebas de Lemma 4 y Lemma 5 se dan en [1].(La prueba del teorema 2 sigue directamente de estos lemas.) 214 LEMMA 4. (suficiencia) Si K (M, K) es verdadinútil con respecto a la gestión correcta de IPJ [K] y V CJ [k]. Lema 5. (necesidad) Si K (M, K) es falso, entonces el triple (K, M.V C [K], M.IP [K]) es necesario para garantizar la gestión correcta de IPJ [K] y V CJ[K]. Teorema 2. Cuando un proceso PI envía M a un proceso PJ, la condición k (m, k) es necesaria y suficiente para no transmitir el triple (k, enviar (m) .v ci [k], enviar (m) .ipi [k]).5. Una familia de protocolos IPT basados en condiciones evaluables que resulta del teorema anterior que, si Pi podría evaluar K (M, K) cuando envía M a PJ, esto nos permitiría mejorar el protocolo IPT anterior de la siguiente manera: en la reglaR2, el triple (k, v ci [k], ipi [k]) se transmite con m solo si ¬k (m, k). Además, la regla R3 se modifica adecuadamente para considerar solo los triples llevados por m.Sin embargo, como se mencionó anteriormente, Pi no puede evaluar localmente K (M, K) cuando está a punto de enviar m.Más precisamente, cuando Pi envía M a PJ, Pi conoce los valores exactos de enviar (m) .v Ci [k] y enviar (m) .ipi [k] (son los valores actuales de v ci [k] e ipi[k]). Pero, en lo que respecta al valor de Pred (recibir (M)). V CJ [K], son posibles dos casos. Caso (i): si Pred (recibir (M)) Hb → Enviar (M), entonces PI puede saber el valor de Pred (recibir (M)). V CJ [K] y, en consecuencia, puede evaluar K (M, K). Caso (ii): si Pred (recibir (M)) y enviar (M) son concurrentes, PI no puede saber el valor de Pred (recibir (M)). V CJ [K] y, en consecuencia, no puede evaluar K (M, K). Además, cuando envía M a PJ, sea cual sea el caso (i o II) que realmente ocurra, Pi no tiene forma de saber qué caso ocurre. De ahí la idea de definir aproximaciones evaluables de la condición general. Sea K (M, K) una aproximación de K (M, K), que puede evaluarse mediante un proceso PI cuando envía un mensaje m.Para ser correctos, la Condición K debe asegurarse de que, cada vez que Pi deba transmitir un triple (k, v ci [k], ipi [k]) de acuerdo con el teorema 2 (es decir, cada vez ¬k (m, k)),Entonces Pi transmite este triple cuando usa la condición k. Por lo tanto, la definición de una aproximación evaluable correcta: definición 6. Una condición k, localmente evaluable por un proceso cuando envía un mensaje m a otro proceso, es correcta si ∀ (m, k): ¬k (m, k) ⇒ ¬k (m, k) o, de manera equivalente, ∀ ((m, k): k (m, k) ⇒ k (m, k). Esta definición significa que un protocolo que evalúa K para decidir qué triples debe adjuntarse a los mensajes, no se pierde los triples cuya transmisión es requerida por el Teorema 2. Consideremos la condición constante (denotada K1), que siempre es falso, es decir, ∀ (m, k): k1 (m, k) = falso. Esta aproximación trivialmente correcta de K en realidad corresponde al protocolo IPT particular descrito en la Sección 3 (en el que cada mensaje lleva un reloj de vector completo y un vector booleano completo). La siguiente sección presenta una mejor aproximación de K (denotó K2).5.1 Una condición evaluable basada en la matriz booleana K2 se basa en la observación de que la condición K está compuesta por subcondiciones. Algunos de ellos pueden ser pj envían (m) pi v ci [k] = x ipi [k] = 1 v cj [k] ≥ x recibir (m) Figura 2: la condición evaluable K2 evaluada localmente mientras que los demás no pueden. Más precisamente, k ≡ a ∨ α ∨ (β ∧ b), donde a ≡ enviar (m) .v ci [k] = 0 y b ≡ enviar (m) .ipi [k] = 1 son localmente evaluables, mientras que α≡ enviar (m) .v ci [k] <pred (recibir (m)). V cj [k] y β ≡ send (m) .v ci [k] = pred (recibir (m)). V CJ [k] no lo son. Pero, desde el cálculo booleano fácil, A∨ ((α∨β) ∧b) = ⇒ A∨α∨ (β ∧ B) ≡ K. Esto conduce a la condición K ≡ A ∨ (γ ∧ B), donde γ = α∨ β ≡ enviar (m) .v Ci [k] ≤ pred (recibir (m)). V CJ [K], es decir, k ≡ (enviar (m) .v Ci [k] ≤ pred (recibir (m)) .V cj [k] ∧ enviar (m) .ipi [k] = 1) ∨ Enviar (m) .v ci [k] = 0. Entonces, Pi necesita aproximar el predicado envío (m) .v Ci [k] ≤ pred (recibir (m)). V CJ [K]. Para ser correctos, esta aproximación debe ser un CI de predicado localmente evaluable (J, k) de modo que, cuando Pi está a punto de enviar un mensaje M a PJ, CI (J, K) ⇒ (Envir (M) .V Ci [k] ≤ pred (recibir (m)). V CJ [k]). Informalmente, eso significa que, cuando CI (J, K) se mantenga, el contexto local de Pi permite deducir que la recepción de M por PJ no conducirá a la actualización V CJ [K] (PJ sabe tanto como PI sobre PK). Por lo tanto, la condición de concreto K2 es la siguiente: k2 ≡ enviar (m) .v ci [k] = 0 ∨ (ci (j, k) ∧ enviar (m) .ipi [k] = 1). Examinemos ahora el diseño de dicho predicado (IC denotado). Primero, el caso j = puedo ser ignorado, ya que se supone (Sección 2.1) que un proceso nunca se envía un mensaje a sí mismo. Segundo, en el caso j = k, la relación envía (m) .v ci [j] ≤ pred (recibir (m)). V CJ [J] siempre es verdadero, porque la recepción de M por PJ no puede actualizar V CJ[J]. Por lo tanto, ∀j = i: ci (j, j) debe ser verdad. Ahora, consideremos el caso donde J = I y J = K (Figura 2). Supongamos que existe un evento e = recibir (m) con e <send (m), m enviado por PJ y pidgybacking the triple (k, m .v c [k], m .ip [k]) y m .v C C[k] ≥ v ci [k] (de ahí m .v c [k] = recibir (m) .v ci [k]). Como V CJ [K] no puede disminuir, esto significa que, mientras v Ci [k] no aumente, por cada mensaje m enviado por PI a PJ tenemos lo siguiente: Enviar (m) .v ci [k] = recibir(m) .v Ci [k] = send (m) .v cj [k] ≤ recibir (m) .v cj [k], es decir, ci (j, k) debe seguir siendo verdadero. En otras palabras, una vez que Ci (J, K) es verdadero, el único evento de Pi que podría restablecerlo a Falso es la recepción de un mensaje que aumenta v ci [k] o, si k = i, la ocurrencia de unevento relevante (que aumenta v Ci [i]). Del mismo modo, una vez que CI (J, K) es falso, el único evento que puede establecerlo en verdadero es la recepción de un mensaje M de PJ, Pidgybacking the Triple (k, m .v c [k], m .ip [k]) con m .v c [k] ≥ v ci [k]. Para implementar los predicados locales CI (J, K), cada proceso PI está equipado con una matriz booleana MI (como en [11]) de modo que M [J, K] = 1 ⇔ Ci (J, K). De la discusión anterior se deduce que esta matriz se gestiona de acuerdo con las siguientes reglas (tenga en cuenta que su línea I-Th no es significativa (caso J = I), y que su diagonal siempre es igual a 1): Inicialización M0: ∀ ((J, K): Mi [J, K] se inicializa a 1. 215 m1 cada vez que produce un evento relevante e: PI restablece4 la columna ésica de su matriz: ∀j = i: mi [j, i]: = 0. M2 Cuando Pi envía un mensaje: no se produce una actualización de MI. M3 Cuando recibe un mensaje m de PJ, Pi ejecuta las siguientes actualizaciones: ∀ k ∈ [1..n]: Caso V Ci [K] <M.V C [K] Entonces ∀ = I, J, K: Mi [,k]: = 0;Mi [j, k]: = 1 v ci [k] = m.v c [k] Entonces mi [j, k]: = 1 v ci [k]> m.v c [k] Luego omita el final de los siguientes resultados de lema de las reglasM0-M3. El teorema que sigue muestra que la condición K2 (M, K) es correcta.(Ambos se prueban en [1].) Lemma 6. ∀i, ∀m enviado por Pi a PJ, ∀K, tenemos: Enviar (M) .Mi [J, K] = 1 ⇒ Send (M) .V Ci [K] ≤ Pred (recibir (M)). V CJ [K]. Teorema 3. Sea m un mensaje enviado por Pi a PJ. Sea k2 (m, k) ≡ ((enviar (m) .mi [j] = 1) ∧ (enviar (m) .ipi [k] = 1) ∨ (enviar (m) .v ci [k]= 0)). Tenemos: K2 (M, K) ⇒ K (M, K).5.2 Protocolo IPT resultante El texto completo del protocolo IPT basado en la discusión anterior sigue. Inicialización RM0: - Tanto V Ci [1..N] como IPI [1..n] están configurados en [0 ,..., 0], y ∀ (J, K): Mi [J, K] se establece en 1. RM1 Cada vez que produce un evento relevante e: - Pi asocia con E la marca de tiempo E.TS definido de la siguiente manera: e.ts = {(k, v ci [k]) |Ipi [k] = 1}, - Pi incrementa su entrada de reloj vector V Ci [i] (a saber, se ejecuta v Ci [i]: = v ci [i] + 1), - Pi restablece IPI: ∀ = i:Ipi []: = 0;Ipi [i]: = 1. - Pi restablece la columna ésima de su matriz booleana: ∀j = i: mi [j, i]: = 0. RM2 Cuando Pi envía un mensaje M a PJ, se adjunta a M el conjunto de triples (cada uno compuesto por una identificación de proceso, un entero y un booleano): {(k, v ci [k], ipi [k]) |(Mi [j, k] = 0 ∨ ipi [k] = 0) ∧ (v ci [k]> 0)}. RM3 Cuando Pi recibe un mensaje M de PJ, ejecuta las siguientes actualizaciones: ∀ (k, m.v c [k], m.ip [k]) transportado por m: caso v ci [k] <m.v c [k] entoncesV ci [k]: = m.v c [k];Ipi [k]: = m.ip [k];∀ = i, j, k: mi [, k]: = 0;4 En realidad, el valor de esta columna permanece constante después de su primera actualización. De hecho, ∀j, mi [j, i] se puede establecer en 1 solo al recibir un mensaje de PJ, llevando el valor v cj [i] (ver R3). Pero, como mj [i, i] = 1, PJ no envía v Cj [i] a pi. Por lo tanto, es posible mejorar el protocolo ejecutando este reinicio de la columna MI [∗, i] solo cuando PI produce su primer evento relevante. Mi [j, k]: = 1 v ci [k] = m.v c [k] y luego ipi [k]: = min (ipi [k], m.ip [k]);Mi [j, k]: = 1 v ci [k]> m.v c [k] Luego omita el endcase 5.3 a compensación de la condición k2 (m, k) muestra que un triple no debe transmitirse cuando (mi [j, k] = 1 ∧ ipi [k] = 1) ∨ (v ci [k]> 0). Primero observemos que la gestión de IPI [k] se rige por el programa de aplicaciones. Más precisamente, el protocolo IPT no define cuáles son los eventos relevantes, solo tiene que garantizar una gestión correcta de IPI [k]. De manera diferente, la matriz MI no pertenece a la especificación del problema, es una variable auxiliar del protocolo IPT, que lo administra para satisfacer la siguiente implicación cuando Pi envía M a PJ: (mi [j, k] = 1)⇒ (Pred (recibir (M)). V CJ [K] ≥ Enviar (M) .V CI [K]). El hecho de que la gestión de MI se rige por el protocolo y no por el programa de aplicación deja abierta la posibilidad de diseñar un protocolo donde más entradas de MI son iguales a 1. Esto puede hacer que la condición K2 (M, K) sea más a menudo satisfecha y, en consecuencia, puede permitir que el protocolo transmita menos triples. Mostramos aquí que es posible transmitir menos triples al precio de transmitir algunos vectores booleanos adicionales. El protocolo basado en matriz IPT anterior (Sección 5.2) se modifica de la siguiente manera. Las reglas RM2 y RM3 se reemplazan con las reglas modificadas RM2 y RM3 (MI [∗, K] denota la columna KTH de MI). RM2 Cuando Pi envía un mensaje M a PJ, se adjunta a M el siguiente conjunto de 4-Sples (cada uno compuesto por una identificación de proceso, un entero, un vector booleano y booleano): {(k, v ci [k], Ipi [k], mi [∗, k]) |(Mi [j, k] = 0 ∨ ipi [k] = 0) ∧ v ci [k]> 0}. RM3 Cuando Pi recibe un mensaje M de PJ, ejecuta las siguientes actualizaciones: ∀ (K, M.V C [K], M.IP [K], M.M [1..N, K]) transportada por M: Caso V CI[k] <M.V C [k] entonces v ci [k]: = m.v c [k];Ipi [k]: = m.ip [k];∀ = i: mi [, k]: = m.m [, k] v ci [k] = m.v c [k] y luego ipi [k]: = min (ipi [k], m.ip [k]);∀ = i: mi [, k]: = max (mi [, k], m.m [, k]) v ci [k]> m.v c [k] y luego omita el final de manera similar a las pruebas descritas en [1],es posible demostrar que el protocolo anterior aún satisface la propiedad probada en Lemma 6, a saber, ∀i, ∀m enviado por Pi a PJ, ∀K tenemos (envío (M) .mi [J] = 1) ⇒(Enviar (M) .v Ci [K] ≤ Pred (recibir (M)). V CJ [K]).5 Consideremos el protocolo descrito anteriormente (Sección 5.2) donde el valor de cada entrada de matriz Mi [J, K] siempre es igual a 0. El lector puede verificar fácilmente que esta configuración implementa correctamente la matriz. Además, K2 (M, K) siempre es falso: en realidad coincide con K1 (k, m) (que corresponde al caso donde los vectores completos deben transmitirse con cada mensaje).216 Intuitivamente, el hecho de que algunas columnas de matrices m se adjunten a los mensajes de aplicación permiten una transmisión transitiva de información. Más precisamente, la historia relevante de PK conocida por PJ se transmite a un proceso PI a través de una secuencia causal de mensajes de PJ a PI. En contraste, el protocolo descrito en la Sección 5.2 utilizó solo una transmisión directa de esta información. De hecho, como se explicó la Sección 5.1, el predicado C (implementado localmente por la matriz m) se basó en la existencia de un mensaje M enviado por PJ a Pi, pidgybacking the Triple (k, m .v c [k], m .ip.[k]), y m .v c [k] ≥ v ci [k], es decir, sobre la existencia de una transmisión directa de información (por el mensaje m). El protocolo IPT resultante (definido por las reglas RM0, RM1, RM2 y RM3) utiliza la misma condición K2 (M, K) que la anterior. Muestra una compensación interesante entre el número de triples (K, V Ci [K], IPI [k]) cuya transmisión se guarda y el número de vectores booleanos que también deben ser abarrotados. Es interesante notar que el tamaño de esta información adicional está limitada, mientras que cada triple incluye un entero no unido (es decir, un valor de reloj vectorial).6. Estudio experimental Esta sección compara los comportamientos de los protocolos anteriores. Esta comparación se realiza con un estudio de simulación. IPT1 denota el protocolo presentado en la Sección 3.3 que usa la condición K1 (m, k) (que siempre es igual a falso). IPT2 denota el protocolo presentado en la Sección 5.2 que utiliza la condición K2 (m, k) donde los mensajes llevan triples. Finalmente, IPT3 denota el protocolo presentado en la Sección 5.3 que también usa la condición K2 (M, K) pero donde los mensajes llevan vectores booleanos adicionales. Esta sección no tiene como objetivo proporcionar un estudio de simulación en profundidad de los protocolos, sino que presenta una opinión general sobre los comportamientos de protocolo. Con este fin, compara IPT2 e IPT3 con respecto a IPT1. Más precisamente, para IPT2, el objetivo era evaluar la ganancia en términos de triples (k, v ci [k], ipi [k]) no transmitidos con respecto a la transmisión sistemática de vectores completos como se realiza en IPT1. Para IPT3, el objetivo era evaluar la compensación entre los vectores booleanos adicionales transmitidos y el número de triples guardados. El comportamiento de cada protocolo se analizó en un conjunto de programas.6.1 Parámetros de simulación El simulador proporciona diferentes parámetros que permiten ajustar tanto la comunicación como las características de los procesos. Estos parámetros permiten establecer el número de procesos para el cálculo simulado, variar la tasa de comunicación (enviar/recibir) eventos y alterar la duración del tiempo entre dos eventos relevantes consecutivos. Además, para ser independiente de una topología particular de la red subyacente, se supone una red totalmente conectada. Los eventos internos no han sido considerados. Dado que la presencia de los triples (k, v ci [k], ipi [k]) alcanzado por un mensaje depende en gran medida de la frecuencia en que los eventos relevantes son producidos por un proceso, se han implementado distribuciones de tiempo diferentes entre dos eventos relevantes consecutivos(por ejemplo, distribuciones normales, uniformes y de Poisson). Los remitentes de mensajes se eligen de acuerdo con una ley aleatoria. Para exhibir configuraciones particulares de un cálculo distribuido, se puede proporcionar un escenario dado al simulador. Los retrasos en la transmisión de mensajes siguen una distribución normal estándar. Finalmente, el último parámetro del simulador es el número de eventos de envío que ocurrieron durante una simulación.6.2 Configuración de parámetros Para comparar el comportamiento de los tres protocolos IPT, realizamos una gran cantidad de simulaciones utilizando diferentes parámetros. Establecimos en 10 el número de procesos que participan en un cálculo distribuido. El número de eventos de comunicación durante la simulación se ha establecido en 10 000. Se ha establecido el parámetro λ de la distribución de tiempo de Poisson (λ es el número promedio de eventos relevantes en un intervalo de tiempo dado) para que los eventos relevantes se generen al comienzo de la simulación. Con la distribución de tiempo uniforme, se genera un evento relevante (en promedio) cada 10 eventos de comunicación. El parámetro de ubicación de la distribución de tiempo normal estándar se ha establecido para que la ocurrencia de eventos relevantes se cambie alrededor de la tercera parte del experimento de simulación. Como se señaló anteriormente, el simulador se puede alimentar con un escenario dado. Esto permite analizar los peores escenarios para IPT2 e IPT3. Estos escenarios corresponden al caso en el que los eventos relevantes se generan a la frecuencia máxima (es decir, cada vez que un proceso envía o recibe un mensaje, produce un evento relevante). Finalmente, los tres protocolos IPT se analizan con los mismos parámetros de simulación.6.3 Resultados de simulación Los resultados se muestran en las Figuras 3.A-3.D. Estas cifras trazan la ganancia de los protocolos en términos del número de triples que no se transmiten (eje y) con respecto al número de eventos de comunicación (eje x). A partir de estas cifras, observamos que, sea cual sea la distribución del tiempo seguida de los eventos relevantes, tanto IPT2 como IPT3 exhiben un comportamiento mejor que IPT1 (es decir, el número total de triples de piggyback es menor en IPT2 e IPT3 que en IPT1), incluso enEl peor de los casos (ver Figura 3.D). Consideremos el peor escenario. En ese caso, la ganancia se obtiene al comienzo de la simulación y dura mientras exista un proceso PJ para el cual ∀K: V CJ [k] = 0. En ese caso, la condición ∀K: K (M, K) está satisfecha. Tan pronto como ∃K: V CJ [k] = 0, tanto IPT2 como IPT3 se comportan como IPT1 (la forma de la curva se vuelve plana) ya que la condición k (m, k) ya no está satisfecha. La Figura 3.A muestra que durante los primeros eventos de la simulación, la pendiente de las curvas IPT2 e IPT3 son empinadas. Lo mismo ocurre en la Figura 3.D (que representa el peor de los casos). Luego, la pendiente de estas curvas disminuye y permanece constante hasta el final de la simulación. De hecho, tan pronto como v Cj [k] se vuelve mayor que 0, la condición ¬k (m, k) se reduce a (mi [j, k] = 0 ∨ ipi [k] = 0). Figura 3.B muestra una característica interesante. Considera λ = 100. Como los eventos relevantes se toman solo durante el comienzo de la simulación, esta figura exhibe una pendiente muy empinada como las otras figuras. La cifra muestra que, tan pronto como no se toman más eventos relevantes, en promedio, el 45% de los triples no son abarrotados por los mensajes. Esto muestra la importancia de Matrix Mi. Además, IPT3 se beneficia de transmitir vectores booleanos adicionales para ahorrar transmisiones triples. Las Figuras 3.A-3.C muestran que la ganancia promedio de IPT3 con respecto a IPT2 es cercana al 10%. Finalmente, la Figura 3.C subraya aún más la importancia 217 de Matrix Mi. Cuando se toman muy pocos eventos relevantes, IPT2 e IPT3 resultan ser muy eficientes. De hecho, esta figura muestra que, muy rápidamente, la ganancia en número de triples que se guardan es muy alta (en realidad, el 92% de los triples se guardan).6.4 Lecciones aprendidas de la simulación Por supuesto, todos los resultados de la simulación son consistentes con los resultados teóricos. IPT3 siempre es mejor o igual a IPT2, e IPT2 siempre es mejor que IPT1. Los resultados de la simulación nos enseñan más: • La primera lección que hemos aprendido las preocupaciones de Matrix Mi. Su uso es bastante significativo, pero principalmente depende de la distribución del tiempo seguida de los eventos relevantes. Por un lado, al observar la Figura 3.B donde se toman una gran cantidad de eventos relevantes en muy poco tiempo, IPT2 puede ahorrar hasta el 45% de los triples. Sin embargo, podríamos haber esperado una ganancia más sensible de IPT2 ya que la IP vectorial booleana tiende a estabilizarse para [1, ..., 1] cuando no se toman eventos relevantes. De hecho, como se discutió en la Sección 5.3, la gestión de Matrix Mi dentro de IPT2 no permite una transmisión transitiva de información, sino solo una transmisión directa de esta información. Esto explica por qué algunas columnas de MI pueden permanecer iguales a 0, mientras que potencialmente podrían ser iguales a 1. De manera diferente, como IPT3 se beneficia de transmitir vectores booleanos adicionales (que proporciona una información de transmisión transitiva) alcanza una ganancia del 50%. Por otro lado, cuando se toman muy pocos eventos relevantes en un gran período de tiempo (ver Figura 3.c), el comportamiento de IPT2 e IPT3 resulta ser muy eficiente ya que la transmisión de hasta el 92% de los triples essalvado. Esto proviene del hecho de que el IPI vectorial booleano tiende a estabilizarse a [1, ..., 1] y que Matrix Mi contiene muy pocos 0 ya que se han tomado muy pocos eventos relevantes. Por lo tanto, una transmisión directa de la información es suficiente para obtener rápidamente matrices MI igual a [1, ..., 1] ,..., [1, ..., 1].• La segunda lección se refiere a IPT3, más precisamente, la compensación entre el académico adicional de los vectores booleanos y el número de triples cuya transmisión se guarda. Con n = 10, agregar 10 booleanos a un triple no aumenta sustancialmente su tamaño. Las Figuras 3.A-3.C exhiben el número de triples cuya transmisión se guarda: la ganancia promedio (en número de triples) de IPT3 con respecto a IPT2 es de aproximadamente el 10%.7. Conclusión Este documento ha abordado un importante problema de computación distribuida relacionada con la causalidad, a saber, el problema inmediato de seguimiento de los predecesores. Ha presentado una familia de protocolos que proporciona a cada evento relevante una marca de tiempo que identifica exactamente a sus predecesores inmediatos. La familia se define mediante una condición general que permite que los mensajes de aplicación sean información de control de pildo cuyo tamaño puede ser menor que n (el número de procesos). En ese sentido, esta familia define los protocolos IPT del tamaño del tamaño del mensaje. Según la forma en que se implementa la condición general, se pueden obtener diferentes protocolos IPT. Tres de ellos han sido descritos y analizados con experimentos de simulación. Curiosamente, también se ha demostrado que la eficiencia de los protocolos (medidos en términos del tamaño de la información de control que no está abarrotada por un mensaje de aplicación) depende del patrón definido por los eventos de comunicación y los eventos relevantes. Por último, pero no menos importante, es interesante observar que si uno no está interesado en rastrear los eventos predecesores inmediatos, los protocolos presentados en el documento pueden simplificarse suprimiendo los vectores booleanos IPI (pero manteniendo las matrices booleanas MI). Los protocolos resultantes, que implementan un sistema de reloj vectorial, son particularmente eficientes en lo que respecta al tamaño de la marca de tiempo que lleva cada mensaje. Curiosamente, esta eficiencia no se obtiene al precio de supuestos adicionales (como los canales FIFO).8. Referencias [1] Anceaume E., H´Elary J.-M.y Raynal M., rastreando predecesores inmediatos en cálculos distribuidos. Res. Informe #1344, Irisa, Univ. Rennes (Francia), 2001. [2] Baldoni R., Prakash R., Raynal M. y Singhal M., Broadcastización eficiente ∆-causal. Journal of Computer Systems Science and Engineering, 13 (5): 263-270, 1998. [3] Chandy K.M.y Lamport L., instantáneas distribuidas: determinación de estados globales de sistemas distribuidos, transacciones ACM en sistemas informáticos, 3 (1): 63-75, 1985. [4] Diehl C., Jard C. y Rampon J.-X.,Análisis de accesibilidad de ejecuciones distribuidas, Proc. Tapsoft93, Springer-Verlag LNCS 668, pp. 629-643, 1993. [5] Fidge C.J., marcas de tiempo en sistemas de paseos de mensajes que preservan el pedido parcial, Proc.11ª Conferencia de Computación Australiana, pp. 56-66, 1988. [6] Fromentin E., Jard C., Jourdan G.-V.y Raynal M., Análisis sobre los cálculos distribuidos, IPL, 54: 267-274, 1995. [7] Fromentin E. y Raynal M., Estados globales compartidos en cálculos distribuidos, JCSS, 55 (3):522-528, 1997. [8] Fromentin E., Raynal M., Garg V.K.y Tomlinson A., Prueba sobre patrones regulares en cálculos distribuidos. Proc. ICPP94, vol.2: 73-76, 1994. [9] Garg V.K., Principios de sistemas distribuidos, Kluwer Academic Press, 274 páginas, 1996. [10] H´Elary J.-M., Most´efaoui A., Netzer R.H.B.y Raynal M., Prevención basada en la comunicación de Ckeckpoints inútiles en cálculos distribuidos. Computación distribuida, 13 (1): 29-43, 2000. [11] H´Elary J.-M., Melideo G. y Raynal M., Seguimiento de la causalidad en sistemas distribuidos: un conjunto de protocolos eficientes. Proc. Sirocco00, Carleton University Press, pp. 181-195, Laquila (Italia), junio de 2000. [12] H´Eltary J.-M., Netzer R. y Raynal M., Problemas de consistencia en los puntos de control distribuidos. IEEE TSE, 25 (4): 274-281, 1999. [13] Hurfin M., Mizuno M., Raynal M. y Singhal M., Detección distribuida eficiente de la conjunción de predicados locales en los cálculos de asíncho. IEEE TSE, 24 (8): 664-677, 1998. [14] Lamport L., tiempo, relojes y el orden de eventos en un sistema distribuido. Comunicación ACM, 21 (7): 558-565, 1978. [15] Marzullo K. y Sabel L., Detección eficiente de una clase de propiedades estables. Computación distribuida, 8 (2): 81-91, 1994. [16] Mattern F., Tiempo virtual y estados globales de sistemas distribuidos. Proc. En t. Conf. Algoritmos paralelos y distribuidos, (Cosnard, Quinton, Raynal, Robert eds), Holanda Norte, pp. 215-226, 1988. [17] Prakash R., Raynal M. y Singhal M., un algoritmo de orden causal adaptativo adaptado aEntorno informático móvil. JPDC, 41: 190-204, 1997. [18] Raynal M. y Singhal S., Tiempo lógico: captura de causalidad en sistemas distribuidos. IEEE Computer, 29 (2): 49-57, 1996. [19] Singhal M. y Kshemkalyani A., una implementación eficiente de relojes vectoriales. IPL, 43: 47-52, 1992. [20] Wang Y.M., puntos de control globales consistentes que contienen un conjunto dado de puntos de control locales. IEEE TOC, 46 (4): 456-468, 1997. 218 0 1000 2000 3000 4000 5000 6000 0 2000 4000 6000 8000 10000 GainnumberOfTriples COMUNICACIÓN Número de eventos IPT1 IPT2 IPT3 Eventos relevantes (a) Los eventos relevantes siguen una distribución uniforme (relación = = = = = =1/10) -5000 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 0 2000 4000 6000 8000 10000 GainnumberOfTriples Número de eventos de comunicación IPT1 IPT2 IPT3 Eventos relevantes (b) Los eventos relevantes siguen una distribución de Poisson (λ = 100) 0 10000 2000030000 40000 50000 60000 70000 80000 90000 100000 0 2000 4000 6000 8000 10000 GaininnumberOfTriples COMUNICACIÓN Número de eventos IPT1 IPT2 IPT3 Eventos relevantes (c) Los eventos relevantes siguen una distribución normal 0 50 100 150 200 250 300 350 400 450 1 10 100 1000 10000 GainnumberFtriples CommunicationNúmero de eventos IPT1 IPT2 IPT3 Eventos relevantes (D) Para cada PI, PI toma un evento relevante y transmite a todos los procesos Figura 3: Resultados experimentales 219