Uso de distribuciones asimétricas para mejorar las estimaciones de probabilidad del clasificador de texto Paul N. Bennett Departamento de Ciencias de la Computación. Universidad de Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Los clasificadores de texto abstractos que dan estimaciones de probabilidad son más fácilmente aplicables en una variedad de escenarios. Por ejemplo, en lugar de elegir un umbral de decisión establecido, se pueden usar en un modelo de riesgo bayesiano para emitir una decisión de tiempo de ejecución que minimiza una función de costo específica de usuario elegida dinámicamente en el momento de la predicción. Sin embargo, la calidad de las estimaciones de probabilidad es crucial. Revisamos una variedad de enfoques estándar para convertir puntajes (y estimaciones de probabilidad deficientes) desde clasificadores de texto hasta estimaciones de alta calidad e introducir nuevos modelos motivados por la intuición de que la distribución de la puntuación empírica para los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente son relevantes sona menudo significativamente diferente. Finalmente, analizamos el rendimiento experimental de estos modelos sobre las salidas de dos clasificadores de texto. El análisis demuestra que uno de estos modelos es teóricamente atractivo (introduciendo pocos parámetros nuevos al tiempo que aumenta la flexibilidad), computacionalmente eficiente y empíricamente preferible. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información;I.2.6 [Inteligencia artificial]: aprendizaje;I.5.2 [Reconocimiento de patrones]: Metodología de diseño Algoritmos de términos generales, experimentación, confiabilidad.1. Introducción Los clasificadores de texto que dan estimaciones de probabilidad son más flexibles en la práctica que aquellos que solo dan una clasificación simple o incluso una clasificación. Por ejemplo, en lugar de elegir un umbral de decisión establecido, se pueden usar en un modelo de riesgo bayesiano [8] para emitir una decisión de tiempo de ejecución que minimiza el costo esperado de una función de costo especificada por el usuario elegida dinámicamente en el momento de la predicción. Esto se puede utilizar para minimizar una función de costo de utilidad lineal para filtrar tareas donde los costos previamente especificados de relevantes/irrelevantes no están disponibles durante la capacitación, pero se especifican en el tiempo de predicción. Además, los costos se pueden cambiar sin volver a capacitar el modelo. Además, las estimaciones de probabilidad a menudo se usan como base para decidir qué documentos etiquetar solicitar a continuación durante el aprendizaje activo [17, 23]. El aprendizaje activo efectivo puede ser clave en muchas tareas de recuperación de información donde la obtención de datos etiquetados puede ser costoso, reduciendo severamente la cantidad de datos etiquetados necesarios para alcanzar el mismo rendimiento que cuando se solicitan nuevas etiquetas al azar [17]. Finalmente, también son susceptibles de tomar otros tipos de decisiones sensibles a los costos [26] y para combinar las decisiones [3]. Sin embargo, en todas estas tareas, la calidad de las estimaciones de probabilidad es crucial. Los modelos paramétricos generalmente utilizan suposiciones que los datos ajustan al modelo para intercambiar flexibilidad con la capacidad de estimar los parámetros del modelo con precisión con pocos datos de entrenamiento. Dado que muchas tareas de clasificación de texto a menudo tienen muy pocos datos de entrenamiento, nos centramos en los métodos paramétricos. Sin embargo, la mayoría de los métodos paramétricos existentes que se han aplicado a esta tarea tienen una suposición que encontramos indeseable. Si bien algunos de estos métodos permiten que las distribuciones de los documentos relevantes e irrelevantes para el tema tengan diferentes variaciones, generalmente hacen cumplir la restricción innecesaria de que los documentos se distribuyen simétricamente en torno a sus respectivos modos. Introducimos varios modelos paramétricos asimétricos que nos permiten relajar esta suposición sin aumentar significativamente el número de parámetros y demostrar cómo podemos ajustar de manera eficiente los modelos. Además, estos modelos pueden interpretarse como suponiendo que los puntajes producidos por el clasificador de texto tienen tres tipos básicos de comportamiento empírico, uno correspondiente a cada uno de los elementos extremadamente irrelevantes, difíciles de discriminar y obviamente relevantes. Primero revisamos el trabajo relacionado para mejorar las estimaciones de probabilidad y el modelado de puntaje en la recuperación de información. Luego, discutimos con más detalle la necesidad de modelos asimétricos. Después de esto, describimos dos modelos asimétricos específicos y, utilizando dos clasificadores de texto estándar, Na¨ıve Bayes y SVM, demostramos cómo pueden usarse de manera eficiente para recalibrar estimaciones de probabilidad deficientes o producir estimaciones de probabilidad de alta calidad de las puntuaciones sin procesar. Luego revisamos experimentos utilizando métodos propuestos previamente y los métodos asimétricos en varios corpus de clasificación de texto para demostrar las fortalezas y debilidades de los diversos métodos. Finalmente, resumimos nuestras contribuciones y discutimos direcciones futuras.2. Se han empleado modelos paramétricos de trabajo relacionados para obtener estimaciones de probabilidad en varias áreas de recuperación de información. Lewis & Gale [17] usan regresión logística para recalibrar na¨ie bayes Aunque la calidad de las estimaciones de probabilidad no se evalúa directamente;Simplemente se realiza como un paso intermedio en el aprendizaje activo. Manmatha et.Al [20] introdujo modelos apropiados para producir estimaciones de probabilidad a partir de puntajes de relevancia devueltos de los motores de búsqueda y demostraron cómo las estimaciones de probabilidad resultantes podrían emplearse posteriormente para combinar los resultados de varios motores de búsqueda. Utilizan una distribución paramétrica diferente para las clases relevantes e irrelevantes, pero no persiguen distribuciones asimétricas de dos lados para una sola clase como se describe aquí. También examinan la larga historia de modelar las puntuaciones de relevancia de los motores de búsqueda. Nuestro trabajo es similar en sabor a estos intentos anteriores de modelar las puntuaciones de los motores de búsqueda, pero apuntamos a las salidas del clasificador de texto que hemos encontrado que demuestran un tipo diferente de comportamiento de distribución de puntaje debido al papel de los datos de entrenamiento. El enfoque en mejorar las estimaciones de probabilidad ha estado creciendo últimamente. Zadrozny y Elkan [26] proporcionan una medida correctiva para los árboles de decisión (denominados reducción) y un método no paramétrico para recalibrar náicios bayes. En trabajos más recientes [27], investigan el uso de un método semi-paramétrico que utiliza un ajuste monotónico Paso-Iseconstant a los datos y aplica el método a Na¨ıve Bayes y una SVM lineal. Si bien compararon sus métodos con otros métodos paramétricos basados en la simetría, no proporcionan resultados de prueba de significación. Nuestro trabajo proporciona métodos paramétricos asimétricos que complementan los métodos no paramétricos y semi-paramétricos que proponen cuando la escasez de datos es un problema. Además, sus métodos reducen la resolución de los puntajes de salida por el clasificador (el número de resultados de valores distintos), pero los métodos aquí no tienen tal debilidad ya que son funciones continuas. Hay una variedad de otros trabajos que extiende este documento. Platt [22] utiliza un marco de regresión logística que modela etiquetas de clase ruidosas para producir probabilidades a partir de la salida sin procesar de un SVM. Su trabajo mostró que este método de postprocesamiento no solo puede producir estimaciones de probabilidad de calidad similar a las SVM directamente capacitadas para producir probabilidades (métodos de núcleo de probabilidad regularizados), sino que también tiende a producir núcleos más escasos (que se generalizan mejor). Finalmente, Bennett [1] obtuvo ganancias moderadas aplicando el método PLATTS a la recalibración de Na¨ıve Bayes, pero descubrió que había áreas más problemáticas que cuando se aplicó a SVM. Recalibrar clasificadores mal calibrados no es un problema nuevo. Lindley et.Al [19] propuso por primera vez la idea de recalibrar clasificadores, y DeGroot & Fienberg [5, 6] dio la formalización estándar ahora aceptada para el problema de evaluar la calibración iniciada por otros [4, 24].3. Definición y enfoque del problema Nuestro trabajo difiere de los enfoques anteriores principalmente en tres puntos: (1) proporcionamos modelos paramétricos asimétricos adecuados para su uso cuando hay pocos datos de entrenamiento disponibles;(2) analizamos explícitamente la calidad de las estimaciones de probabilidad que estos y los métodos competitivos producen y proporcionan pruebas de significancia para estos resultados;(3) Apuntamos a las salidas del clasificador de texto donde la mayoría de la literatura anterior se dirigió a la salida de los motores de búsqueda.3.1 Definición del problema El problema general con el que nos preocupa se destaca en la Figura 1. Un clasificador de texto produce una predicción sobre un documento y ofrece una puntuación S (d) que indica la fortaleza de su decisión de que el documento pertenece a la clase positiva (relevante para el tema). Suponemos que solo hay dos clases: la clase positiva e negativa (o irrelevante) (+ y - respectivamente). Hay dos tipos generales de enfoques paramétricos. El primero de estos intenta adaptarse directamente a la función posterior, es decir, hay una clase P (S |+) P (S | -) Bayes Rulep (+) P ( -) P (+| S (D)) Predicte la clase,c (d) = {+, -} confianza s (d) que c (d) =+ documento, d y dar figura 1 no anormalizada: nos preocupa cómo realizar la caja resaltada en gris. Las partes internas son para un tipo de enfoque.Estimador de funciones que realiza una asignación directa de la puntuación S a la probabilidad P (+| S (D)). El segundo tipo de enfoque descompone el problema como se muestra en la caja gris de la Figura 1. Se produce un estimador para cada una de las densidades condicionales de clase (es decir, P (S |+) y P (S |-)), luego la regla de Bayes y los antecedentes de clase se usan para obtener la estimación para P (+| S (D)).3.2 Motivación para distribuciones asimétricas La mayoría de los enfoques paramétricos anteriores a este problema, ya sea directa o indirectamente (cuando se ajusta solo a los posteriores) corresponden a los gaussianos ajustados a las densidades condicionales de clase;difieren solo en el criterio utilizado para estimar los parámetros. Podemos visualizar esto como se muestra en la Figura 2. Dado que el aumento de S generalmente indica una mayor probabilidad de pertenecer a la clase positiva, la distribución más derecha generalmente corresponde a P (S |+). A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 P (S | class = { +, -}) Puntuación de confianza no anormalizada S P (S | class = +) P (S | class = -) Figura 2: Vista típicaSin embargo, de la discriminación basada en los gaussianos, el uso de gaussianos estándar no puede capitalizar una característica básica comúnmente vista. Es decir, si tenemos una puntuación de salida sin procesar que puede usarse para la discriminación, entonces el comportamiento empírico entre los modos (etiqueta B en la Figura 2) es a menudo muy diferente a la fuera de los modos (etiquetas A y C en la Figura 2). Intuitivamente, el área entre los modos corresponde a los ejemplos difíciles, que son difíciles de distinguir para este clasificador, mientras que las áreas fuera de los modos son los ejemplos extremos que generalmente se distinguen fácilmente. Esto sugiere que podemos querer desacoplar la escala de los segmentos externos e internos de la distribución (como se muestra por la curva denotada como a-gaussiana en la Figura 3). Como resultado, una distribución asimétrica puede ser una opción más apropiada para la aplicación a la puntuación de salida sin procesar de un clasificador. Idealmente (es decir, la clasificación perfecta) existirá puntajes θ− y θ+ de modo que todos los ejemplos con puntaje mayor que θ+ son relevantes y todos los ejemplos con puntajes inferiores a θ− son irrelevantes. Además, no hay ejemplos entre θ− y θ+. La distancia |θ− - θ+ |corresponde al margen en algunos clasificadores, y a menudo se intenta maximizar esta cantidad. Debido a que los clasificadores de texto tienen datos de entrenamiento para separar las clases, el comportamiento final de las distribuciones de puntaje es principalmente un factor de la cantidad de datos de entrenamiento y la consiguiente separación en las clases logradas. Esto contrasta con la recuperación del motor de búsqueda, donde la distribución de puntajes es más un factor de distribución del lenguaje entre los documentos, la función de similitud y la longitud y el tipo de consulta. La clasificación perfecta corresponde al uso de dos distribuciones muy asimétricas, pero en este caso, las probabilidades son en realidad una y cero y muchos métodos funcionarán para fines típicos. Prácticamente, algunos ejemplos caerán entre θ− y θ+, y a menudo es importante estimar bien las probabilidades de estos ejemplos (ya que corresponden a los ejemplos difíciles). Se pueden dar justificaciones para tanto por qué puede encontrar más y menos ejemplos entre θ− y θ+ que fuera de ellos, pero hay pocas razones empíricas para creer que las distribuciones deberían ser simétricas. Un primer candidato natural para una distribución asimétrica es generalizar una distribución simétrica común, p.El Laplace o el Gaussiano. Se puede lograr una distribución de laplace asimétrica colocando dos exponenciales alrededor del modo de la siguiente manera: P (x | θ, β, γ) =    βγ β+γ EXP [−β (θ - x)] x≤ θ (β, γ> 0) βγ β+γ EXP [−γ (x - θ)] x> θ (1) donde θ, β y γ son los parámetros del modelo.θ es el modo de distribución, β es la escala inversa del exponencial a la izquierda del modo, y γ es la escala inversa del exponencial a la derecha. Usaremos la notación λ (x | θ, β, γ) para referirse a esta distribución.0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 P (S | class = {+, -}) Puntuación de confianza no anormalizada S Figura 3 de gaussianas Gaussianas: Gaussians vs. Gaussianos asimétricos. Una deficiencia de distribuciones simétricas: las líneas verticales muestran los modos como no paramétricamente estimados. Podemos crear un gaussiano asimétrico de la misma manera: p (x | θ, σl, σr) =    2√ 2π (σl+σr) exp - (x --θ) 2 2σ2 l x ≤ θ (σl, σr> 0) 2√ 2π (σl+σr) exp - (x --θ) 2 2σ2 r x> θ (2) donde θ, σl y σr son los parámetros del modelo. Para referirnos a este gaussiano asimétrico, usamos la notación γ (x | θ, σl, σr). Si bien estas distribuciones están compuestas de mitades, la función resultante es una sola distribución continua. Estas distribuciones nos permiten ajustar nuestros datos con una flexibilidad mucho mayor a costa de solo ajustar seis parámetros. En su lugar, podríamos probar modelos de mezcla para cada componente u otras extensiones, pero la mayoría de las otras extensiones requieren al menos tantos parámetros (y a menudo pueden ser más costosos computacionalmente). Además, la motivación anterior debería proporcionar una causa significativa para creer que las distribuciones subyacentes en realidad se comportan de esta manera. Además, esta familia de distribuciones aún puede adaptarse a una distribución simétrica y, finalmente, en la evaluación empírica, se presenta evidencia que demuestra este comportamiento asimétrico (ver Figura 4). Hasta donde sabemos, ninguna familia de distribuciones se ha utilizado previamente en aprendizaje automático o recuperación de información. Ambas se denominan generalizaciones de una Laplace asimétrica en [14], pero nos referimos a ellas como se describió anteriormente para reflejar la naturaleza de cómo los derivamos para esta tarea.3.3 Estimación de los parámetros de las distribuciones asimétricas Esta sección desarrolla el método para encontrar estimaciones de máxima probabilidad (MLE) de los parámetros para las distribuciones asimétricas anteriores. Para encontrar el MLES, tenemos dos opciones: (1) use una estimación numérica para estimar los tres parámetros a la vez (2) fijar el valor de θ y estimar los otros dos (β y γ o σl y σr) dado nuestroElección de θ, luego considere valores alternativos de θ. Debido a la simplicidad del análisis en la última alternativa, elegimos este método.3.3.1 Laplace asimétrico MLES para d = {x1, x2 ,..., xn} donde los xi son i.i.d.y x ∼ λ (x | θ, β, γ), la probabilidad es n i λ (x | θ, β, γ). Ahora, fijamos θ y calculamos la máxima probabilidad de esa elección de θ. Entonces, podemos simplemente considerar todas las opciones de θ y elegir la que tenga la máxima probabilidad sobre todas las opciones de θ. La derivación completa se omite debido al espacio, pero está disponible en [2]. Definimos los siguientes valores: nl = |{x ∈ D |x ≤ θ} |Nr = |{x ∈ D |x> θ} |Sl = x∈D | x≤θ x sr = x∈D | x> θ x dl = nlθ - sl dr = sr - nrθ. Tenga en cuenta que DL y DR son la suma de las diferencias absolutas entre las X pertenecientes a las mitades izquierda y derecha de la distribución (respectivamente) y θ. Finalmente, los MLE para β y γ para un θ fijo son: βmle = n dl + √ drdl γmle = n dr + √ drdl.(3) Estas estimaciones no son totalmente inesperadas, ya que obtendríamos NL DL si estimamos β independientemente de γ. La elegancia de las fórmulas es que las estimaciones tenderán a ser simétricas solo en la medida en que los datos lo dictan (es decir, cuanto más cercanos DL y DR serán iguales, más cerca son las escalas inversas resultantes). Por argumentos de continuidad, cuando n = 0, asignamos β = γ = 0 donde 0 es una pequeña constante que actúa para dispersar la distribución a un uniforme. Del mismo modo, cuando N = 0 y DL = 0, asignamos β = INF donde INF es una constante muy grande que corresponde a una distribución extremadamente aguda (es decir, casi toda masa en θ para esa mitad). DR = 0 se maneja de manera similar. Suponiendo que θ cae en algún rango [φ, ψ] que depende solo de los documentos observados, entonces esta alternativa también es fácilmente computable. Dado NL, SL, NR, SR, podemos calcular el posterior y los MLE en tiempo constante. Además, si los puntajes se clasifican, entonces podemos realizar todo el proceso de manera bastante eficiente. Comenzando con el mínimo θ = φ que nos gustaría probar, pasamos a través de los puntajes una vez y establecemos NL, SL, NR, SR adecuadamente. Luego aumentamos θ y pasamos los puntajes que han cambiado del lado derecho de la distribución a la izquierda. Suponiendo que el número de θs candidatos sea O (n), este proceso es o (n) y el proceso general está dominado al clasificar los puntajes, o (n log n) (o tiempo lineal esperado).3.3.2 MLES gaussianos asimétricos para d = {x1, x2 ,..., xn} donde los xi son i.i.d.y x ∼ γ (x | θ, σl, σr), la probabilidad es n i γ (x | θ, β, γ). Los MLE se pueden resolver de manera similar a la anterior. Suponemos las mismas definiciones que anteriormente (la derivación completa omitida para el espacio está disponible en [2]), y además, deje: SL2 = x∈D | x≤θ x2 sr2 = x∈D | x> θ x2 dl2 =Sl2 - slθ + θ2 nl dr2 = sr2 - srθ + θ2 nr. La solución analítica para los MLE para un θ fijo es: σl, MLE = DL2 + D 2/3 L2 D 1/3 R2 N (4) σr, MLE = DR2 + D 2/3 R2 D 1/3 L2 N.(5) Por argumentos de continuidad, cuando n = 0, asignamos σr = σl = inf, y cuando n = 0 y dl2 = 0 (resp. Dr2 = 0), asignamos σl = 0 (resp. Σr = 0). Nuevamente, el mismo análisis de complejidad computacional se aplica a la estimación de estos parámetros.4. Análisis experimental 4.1 Métodos Para cada uno de los métodos que usan una clase anterior, utilizamos una estimación de un complemento suave, es decir, P (c) = | c | +1 n+2 donde n es el número de documentos. Para los métodos que se ajustan a las densidades condicionales de clase, P (S |+) y P (S |-), las densidades resultantes se invierten utilizando la regla Bayes como se describió anteriormente. Todos los métodos a continuación se ajustan utilizando estimaciones de máxima verosimilitud. Para recalibrar un clasificador (es decir, corregir las estimaciones de probabilidad deficientes por el clasificador), es habitual utilizar los productos de registro de los clasificadores estimados como S (D). Los registros de registro se definen para ser log p (+| d) p (-| d). El umbral de decisión normal (error de minimización) en términos de log-ODDS está en cero (es decir, P (+| d) = p ( - | d) = 0.5). Dado que escala las salidas a un espacio [−∞, ∞], los log-Odds hacen que las distribuciones normales (y similares) sean aplicables [19]. Lewis y Gale [17] dan un punto de vista más motivador que ajustar los log-Odds es un efecto amortiguador para la suposición de independencia inexacta y una corrección de sesgo para las estimaciones inexactas de los antecedentes. En general, ajustar los registros de registro puede servir para aumentar o amortiguar la señal del clasificador original como dictan los datos. Gaussianos Un gaussiano es en forma para cada una de las densidades condicionales de clase, utilizando las estimaciones habituales de máxima probabilidad. Este método se denota en las tablas a continuación como Gauss. Gaussianos asimétricos Un gaussiano asimétrico se ajusta a cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima probabilidad descrito anteriormente. Los intervalos entre las puntuaciones adyacentes se dividen por 10 en la prueba de candidato θs, es decir, se prueban 8 puntos entre las puntuaciones reales en el conjunto de datos. Este método se denota como A. Gauss. Distribuciones de Laplace Aunque las distribuciones de Laplace no suelen aplicarse a esta tarea, también probamos este método para aislar por qué se obtiene el beneficio de la forma asimétrica. Los MLE habituales se utilizaron para estimar la ubicación y la escala de una distribución clásica de Laplace simétrico como se describe en [14]. Denotamos este método como Laplace a continuación. Distribuciones de Laplace asimétricos Se ajusta una Laplace asimétrica para cada una de las densidades condicionales de clase utilizando el procedimiento de estimación de máxima verosimilitud descrito anteriormente. Al igual que con el gaussiano asimétrico, los intervalos entre las puntuaciones adyacentes se dividen por 10 en la prueba de candidato θs. Este método se denota como A. Laplace a continuación. Regresión logística Este método es el primero de los dos métodos que evaluamos que se ajustan directamente al posterior, p (+| s (d)). Ambos métodos restringen el conjunto de familias a una familia sigmoidea de dos parámetros;difieren principalmente en su modelo de etiquetas de clase. A diferencia de los métodos anteriores, se puede argumentar que una bendición adicional de estos métodos es que preservan completamente la clasificación dada por el clasificador. Cuando esto se desea, estos métodos pueden ser más apropiados. Los métodos anteriores preservarán principalmente las clasificaciones, pero pueden desviarse si los datos los dictan. Por lo tanto, pueden modelar el comportamiento de los datos mejor a costa de apartarse de una restricción de monotonicidad en la producción del clasificador. Lewis y Gale [17] usan regresión logística para recalibrar na¨ ven bayes para su uso posterior en el aprendizaje activo. El modelo que usan es: P ( + | S (D)) = Exp (A + B S (D)) 1 + Exp (A + B S (D)).(6) En lugar de usar las probabilidades de salida directamente por el clasificador, usan la relación loglikeliliosidad de las probabilidades, log p (d |+) p (d | -), como la puntuación S (d). En lugar de usar esto a continuación, usaremos la relación Logodds. Esto no afecta al modelo, ya que simplemente cambia todos los puntajes por una constante determinada por los Priors. Nos referimos a este método como logregir a continuación. La regresión logística con etiquetas de clase ruidosas Platt [22] propone un marco que extiende el modelo de regresión logística anterior para incorporar etiquetas de clase ruidosas y lo utiliza para producir estimaciones de probabilidad a partir de la salida sin procesar de una SVM. Este modelo difiere del modelo de logreg solo en cómo se estiman los parámetros. Los parámetros aún se ajustan utilizando la estimación de máxima verosimilitud, pero se usa un modelo de etiquetas de clase ruidosas además de permitir la posibilidad de que la clase fuera mal etiquetada. El ruido se modela asumiendo que existe una probabilidad finita de etiquetar mal un ejemplo positivo y de etiquetar mal un ejemplo negativo;Estas dos estimaciones de ruido están determinadas por el número de ejemplos positivos y el número de ejemplos negativos (utilizando la regla de Bayes para inferir la probabilidad de una etiqueta incorrecta). A pesar de que no se espera que el rendimiento de este modelo se desvíe mucho del logreg, lo evaluamos para completar. Nos referimos a este método a continuación como LR+Noise.4.2 Datos examinamos varios corpus, incluido el Directorio web de MSN, Reuters y TREC-AP. Directorio web MSN El Directorio web MSN es una gran colección de páginas web heterogéneas (de una instantánea web de mayo de 1999) que se han clasificado jerárquicamente. Utilizamos la misma división de tren/prueba de 50078/10024 documentos como el informado en [9]. La jerarquía web de MSN es una jerarquía de siete niveles;Utilizamos las 13 categorías de nivel superior. Las proporciones de clase en el establecimiento de capacitación varían de 1.15% a 22.29%. En el conjunto de pruebas, varían de 1.14% a 21.54%. Las clases son materias generales como Health & Fitness and Travel & Hazing. Los indexadores humanos asignaron los documentos a cero o más categorías. Para los experimentos a continuación, utilizamos solo las 1000 palabras principales con información mutua más alta para cada clase;Aproximadamente 195k palabras aparecen en al menos tres documentos de entrenamiento. Reuters The Reuters 21578 Corpus [16] contiene artículos de noticias de Reuters de 1987. Para este conjunto de datos, utilizamos la división de trenes/ pruebas estándar de modapte de los documentos 9603/3299 (8676 documentos no utilizados). Las clases son temas económicos (por ejemplo, ACQ para adquisiciones, ganan ganancias, etc.) que los etiquetadores humanos aplicaron al documento;Un documento puede tener múltiples temas. En realidad, hay 135 clases en este dominio (solo 90 de las cuales ocurren en el conjunto de entrenamiento y prueba);Sin embargo, solo examinamos las diez clases más frecuentes, ya que un pequeño número de ejemplos de pruebas dificultan la interpretación de algunas medidas de rendimiento debido a la alta varianza.1 Limitar a las diez clases más grandes nos permite comparar nuestros resultados con los resultados publicados anteriormente [10, 13, 21, 22]. Las proporciones de clase en el establecimiento de capacitación varían de 1.88% a 29.96%. En el conjunto de pruebas, varían de 1.7% a 32.95%. Para los experimentos a continuación, usamos solo las 300 palabras principales con información mutua más alta para cada clase;Aproximadamente 15k palabras aparecen en al menos tres documentos de entrenamiento. TREC-AP El Corpus TREC-AP es una colección de noticias AP de 1988 a 1990. Utilizamos la misma división de tren/prueba de los documentos 142791/66992 que se usaron en [18]. Como se describe en [17] (ver también [15]), las categorías se definen mediante palabras clave en un campo de palabras clave. El título y los campos corporales se utilizan en los experimentos a continuación. Hay veinte categorías en total. Las proporciones de la clase en el conjunto de entrenamiento varían de 0.06% a 2.03%. En el conjunto de pruebas, varían de 0.03% a 4.32%. Para los experimentos descritos a continuación, usamos solo las 1000 palabras principales con la información mutua más alta para cada clase;Aproximadamente 123k palabras aparecen en al menos 3 documentos de entrenamiento.4.3 Clasificadores seleccionamos dos clasificadores para la evaluación. Un clasificador SVM lineal que es un clasificador discriminativo que normalmente no genera valores de probabilidad, y un clasificador de Bayes Na¨ıve cuyas salidas de probabilidad a menudo son pobres [1, 7] pero se pueden mejorar [1, 26, 27].1 También se realizó una comparación separada de solo logreg, ruido LR+y A. Laplace sobre las 90 categorías de Reuters. Después de contabilizar la varianza, esa evaluación también respaldó las reclamaciones hechas aquí. SVM para SVM lineales, utilizamos el kit de herramientas SMOX que se basa en el algoritmo de optimización mínima secuencial de Platts. Las características se representaron como valores continuos. Utilizamos la puntuación de salida sin procesar del SVM como S (D) ya que se ha demostrado que es apropiado antes [22]. El umbral de decisión normal (suponiendo que estamos tratando de minimizar los errores) para este clasificador está en cero. Na¨ıve Bayes El modelo de clasificador Na¨ıve Bayes es un modelo multinomial [21]. Alisamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la palabra anterior) y una condima m de Laplace, respectivamente. Utilizamos los registros de registro estimados por el clasificador como S (D). El umbral de decisión normal está en cero.4.4 Medidas de rendimiento Usamos la pérdida de registro [12] y el error al cuadrado [4, 6] para evaluar la calidad de las estimaciones de probabilidad. Para un documento d con clase C (d) ∈ { +, -} (es decir, los datos tienen etiquetas conocidas y no probabilidades), el logloss se define como δ (c (d), +) log p ( +| d) +δ(c (d), -) log p ( - | d) donde δ (a, b).= 1 si a = by 0 de lo contrario. El error al cuadrado es δ (c (d), +) (1 - p ( +| d)) 2 +δ (c (d), -) (1 - p ( - | d)) 2. Cuando la clase de un documento se predice correctamente con una probabilidad de uno, la pérdida de registro es cero y el error al cuadrado es cero. Cuando la clase de un documento se predice incorrectamente con una probabilidad de uno, la pérdida de registro es −∞ y el error al cuadrado es uno. Por lo tanto, ambas medidas evalúan qué tan cerca se produce una estimación para predecir correctamente la clase de ítems, pero varía en la penalización de las predicciones duramente incorrectas. Reportamos solo la suma de estas medidas y omitimos los promedios para el espacio. Sus promedios, la pérdida de registro promedio y el error cuadrático medio (MSE), se pueden calcular a partir de estos totales dividiendo por el número de decisiones binarias en un corpus. Además, también comparamos el error de los clasificadores en sus umbrales predeterminados y con las probabilidades. Esto evalúa cómo las estimaciones de probabilidad han mejorado con respecto al umbral de decisión P (+| d) = 0.5. Por lo tanto, el error solo indica cómo funcionarían los métodos si un falso positivo se penalizara igual que un falso negativo y no la calidad general de las estimaciones de probabilidad. Se presenta simplemente para proporcionar al lector una comprensión más completa de las tendencias empíricas de los métodos. Utilizamos una prueba de micro signo emparejada estándar [25] para determinar la significación estadística en la diferencia de todas las medidas. Solo los pares en los que los métodos no están de acuerdo se utilizan en la prueba de signo. Esta prueba compara pares de puntajes de dos sistemas con la hipótesis nula en que el número de elementos en los que no están de acuerdo se distribuyen binomialmente. Usamos un nivel de significancia de P = 0.01.4.5 Metodología experimental Como las categorías bajo consideración en los experimentos no son mutuamente excluyentes, la clasificación se realizó entrenando a clasificadores binarios, donde n es el número de clases. Para generar los puntajes que cada método utiliza para adaptarse a sus estimaciones de probabilidad, utilizamos una validación cruzada de cinco veces en los datos de entrenamiento. Observamos que, aunque es computacionalmente eficiente realizar una validación cruzada de dejar uno para el clasificador Na¨ıve Bayes, esto puede no ser deseable ya que la distribución de puntajes puede ser sesgada como resultado. Por supuesto, como con cualquier aplicación de validación cruzada N-pliegues, también es posible sesgar los resultados al mantener N demasiado bajo y subestimando el rendimiento del clasificador final.4.6 Resultados y discusión Los resultados para recalibrar los bayes na¨ıve se dan en la Tabla 1A. La Tabla 1B proporciona resultados para producir salidas probabilísticas para SVM. Error de error de registro de registro2 Errores MSN Web Gauss -60656.41 10503.30 10754 A.Gauss -57262.26 8727.47 9675 Laplace -45363.84 8617.59 10927 A.Laplace -36765.88 6407.84 † 8350 Log -364470.99.99 6522.99 55. 36468.18 6534.61 8563 Na¨ıve Bayes -1098900.8317117.50 17834 Reuters Gauss -5523.14 1124.17 1654 A.GAUSS -4929.12 652.67 888 Laplace -5677.68 1157.33 1416 A.Laplace -3106.95 ‡ 554.37 ‡ 726 logreg -3375.63 603.20 786 RUY Na¨ıve Bayes -52184.52 1969.41 2121 TREC-AP Gauss -57872.57 8431.89 9705 a.gauss -66009.43 7826.99 8865 Laplace -61548.42 9571.29 11442 a.Laplace -48711.55 7251.87 ‡ 8642 logreg -48250.81 7540.60.60 87970 87970 87970 87970 87970 8797 87970 87970 87970 870 870 8797 870 870 87970 87970 87970 870 870 870 870 879T .51 7544.84 8801 Na¨ıve Bayes -1903487.10 41770.21 43661 Log -MoSSSError2 errores MSN Web Gauss -54463.32 9090.57 10555 A. Gauss -44363.70 6907.79 8375 Laplace -42429.25 7669.75 10201 A. Laplace -31133.83 5003.32 6170 logreg --30209.36 5158.74 6480 6551 SVM lineal N/A N/A 6602 Reuters Gauss-3955.33 589.25 735 A. Gauss -4580.46 428.21 532 Laplace -3569.36 640.19 770 A. Laplace -2599.28 412.75 505 Logreg -2575.85 407.48 509 LR+ruido -2567.68 408.82 516 516 AP Gauss -54620.94 6525.717321 A. Gauss -77729.49 6062.64 6639 Laplace -54543.19 7508.37 9033 A. Laplace -48414.39 5761.25 ‡ 6572 ‡ logreg -48285.56 5914.04 6791 lr+ruido -48214.96 5919.25 6794 6794 lineal 6718 Tabla 1: (a) Resultados paraNa¨ıve Bayes (izquierda) y (b) SVM (derecha). La mejor entrada para un corpus está en negrita. Las entradas que son estadísticamente significativamente mejores que todas las demás entradas están subrayadas. A † Denota el método es significativamente mejor que todos los demás métodos, excepto los bayes na¨ıve. A ‡ denota la entrada es significativamente mejor que todos los demás métodos, excepto A. Gauss (y Na¨ıve Bayes para la mesa a la izquierda). La razón de esta distinción en las pruebas de significancia se describe en el texto. Comenzamos con observaciones generales que resultan de examinar el desempeño de estos métodos sobre los diversos corpus. El primero es que A. Laplace, LR+Noise y Logreg, superan claramente los otros métodos. Por lo general, hay poca diferencia entre el rendimiento del ruido LR+y el logreg (tanto como se muestra aquí como en una decisión por decisión), pero esto no es sorprendente ya que LR+Noise solo agrega etiquetas de clase ruidosas al modelo de logregir. Con respecto a las tres medidas diferentes, el ruido LR+y el logreg tienden a funcionar ligeramente mejor (pero nunca significativamente) que A. Laplace en algunas tareas con respecto al error de pérdida de registro y cuadrado. Sin embargo, A. Laplace siempre produce el menor número de errores para todas las tareas, aunque a veces el grado de mejora no es significativo. Para darle al lector una mejor sensación del comportamiento de estos métodos, las Figuras 4-5 muestran los ajustes producidos por los más competitivos de estos métodos versus el comportamiento de los datos real (como se estima no paramétricamente por binning) para la clase Gane en Reuters. La Figura 4 muestra las densidades condicionales de clase y, por lo tanto, solo se muestra A. Laplace ya que el logreg se ajusta directamente a la posterior. La Figura 5 muestra las estimaciones de los log-Odds (es decir, log P (gana | s (d)) p (¬earn | s (d))). Ver los ODD de registro (en lugar de los posteriores) generalmente permite que los errores en estimación sean detectados por el ojo más fácilmente. Podemos desglosar las cosas como lo hace la prueba de signo y solo mirar las victorias y las pérdidas en los elementos en los que los métodos no están de acuerdo. Miró de esta manera solo dos métodos (Na¨ıve Bayes y A. Gauss) tienen más victorias por pares que A. Laplace;Esos dos a veces tienen más victorias por pares en el error de pérdida de registro y cuadrado a pesar de que el total nunca gana (es decir, son arrastrados por penalizaciones fuertes). Además, esta comparación de las victorias por pares significa que para aquellos casos en los que el logreg y el ruido LR+tienen mejores puntajes que A. Laplace, no se consideraría significativo mediante la prueba de signo en cualquier nivel, ya que no tienen más victorias. Por ejemplo, de las decisiones binarias de 130k sobre el conjunto de datos web de MSN, A. Laplace tuvo aproximadamente 101K victorias por pares versus logreg y ruido LR+. Ningún método tiene más victorias por pares que A. Laplace para la comparación de errores ni ningún método logra un total mejor. La observación básica realizada sobre na¨ıve bayes en trabajos anteriores es que tiende a producir estimaciones muy cercanas a cero y una [1, 17]. Esto significa que si tiende a ser lo suficientemente correcto del tiempo, producirá resultados que no parecen significativos en una prueba de signo que ignora el tamaño de la diferencia (como el aquí). Los totales del error al cuadrado y la pérdida de log la observación anterior de que cuando está mal está realmente mal. También hay varios puntos interesantes sobre el rendimiento de las distribuciones asimétricas. Primero, A. Gauss funciona mal porque (similar a Na¨ıve Bayes) hay algunos ejemplos en los que se penaliza una gran cantidad. Este comportamiento resulta de una tendencia general a funcionar como la imagen que se muestra en la Figura 3 (tenga en cuenta el crossover en las colas). Si bien el gaussiano asimétrico tiende a colocar el modo con mucho más precisión que un gaussiano simétrico, su flexibilidad asimétrica combinada con su función de distancia hace que distribuya demasiada masa a las colas exteriores mientras no se ajusta alrededor del modo lo suficientemente preciso como para compensar. La Figura 3 es en realidad el resultado de ajustar las dos distribuciones a datos reales. Como resultado, en las colas puede haber una gran discrepancia entre la probabilidad de pertenecer a cada clase. Por lo tanto, cuando no hay valores atípicos A. Gauss puede funcionar de manera bastante competitiva, pero cuando hay un 0 0.002 0.004 0.006 0.008 0.01 0.012 -600 -400 -200 0 200 400 P (s (D) | class = {+, -})s (d) = Prueba de trenes de registro de registro de Bayes Naive A.Laplace 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 -15 -10 -5 0 5 10 15 P (s (D) | class = {+, -})S (D) = Prueba de tren de puntaje sin procesar SVM lineal A.Laplace Figura 4: La distribución empírica de los puntajes del clasificador para los documentos en la capacitación y la prueba establecida para la clase Gann in Reuters. También se muestra el ajuste de la distribución asimétrica de Laplace a la distribución del puntaje de entrenamiento. La clase positiva (es decir Gane) es la distribución de la derecha en cada gráfico, y la clase negativa (es decir, Earn) es la de la izquierda en cada gráfico.-6 -4 -2 0 2 4 6 8 -250 -200 -150 -100 -50 0 50 100 150 150 logodds = logp (+| s (d)) -logp ( -| s (d)) s (d)= Prueba de trenes de log -Odds de Bayes ingenuos a.Laplace logreg -5 0 5 10 15 -4 -2 0 2 4 6 logodds = logp (+| s (d)) -logp ( -| s (d)) s (D) = Prueba de trenes de puntaje sin procesar SVM lineal A.Laplace Logreg Figura 5: El ajuste producido por varios métodos en comparación con los registros empíricos de los datos de entrenamiento para la clase Gane In Reuterers.Outlier A. Gauss está penalizado bastante. Hay suficientes casos en general en general que parece claramente inferior a los tres métodos principales. Sin embargo, el Laplace asimétrico pone mucho más énfasis alrededor del modo (Figura 4) debido a la función de distancia diferente (piense en el pico agudo de un exponencial). Como resultado, la mayor parte de la masa se mantiene centradas en el modo, mientras que los parámetros asimétricos aún permiten más flexibilidad que el Laplace estándar. Dado que el laplace estándar también corresponde a un ajuste por partes en el espacio log -Odds, esto resalta que parte de la potencia de los métodos asimétricos es su sensibilidad al colocar los nudos en los modos reales, en lugar de la suposición simétrica de que las medias corresponden alos modos. Además, los métodos asimétricos también tienen una mayor flexibilidad para ajustar las pendientes de los segmentos de línea. Incluso en los casos en que la distribución de la prueba difiere de la distribución de entrenamiento (Figura 4), A. Laplace todavía produce una solución que proporciona un mejor ajuste que el logreg (Figura 5), el siguiente mejor competidor. Finalmente, podemos hacer algunas observaciones sobre la utilidad de las diversas métricas de rendimiento. Primero, Log-Loss solo otorga una cantidad finita de crédito, ya que el grado en que algo es correcto mejora (es decir, hay rendimientos decrecientes a medida que se acerca a cero), pero puede penalizar infinitamente para una estimación incorrecta. Por lo tanto, es posible que un caso atípico sesga los totales, pero clasificar mal este ejemplo puede no importar para ninguna, excepto un puñado de funciones de utilidad reales utilizadas en la práctica. En segundo lugar, el error al cuadrado tiene una debilidad en la otra dirección. Es decir, su penalización y recompensa están limitadas en [0, 1], pero si el número de errores es lo suficientemente pequeño, es posible que un método parezca mejor cuando produce lo que generalmente consideramos estimaciones de probabilidad inútiles. Por ejemplo, considere un método que solo estima las probabilidades como cero o uno (que tiende a los Bayes, pero no alcanza si usa suavizado). Este método podría ganar de acuerdo con el error al cuadrado, pero con solo un error nunca funcionaría mejor en la pérdida de registro que cualquier método que asigne cierta probabilidad de cero a cada resultado. Por estas razones, recomendamos que ninguno de estos se use de forma aislada, ya que cada uno ofrece ideas ligeramente diferentes a la calidad de las estimaciones producidas. Estas observaciones son directas de las definiciones, pero están subrayadas por la evaluación.5. Trabajo futuro Una extensión prometedora del trabajo presentado aquí es una distribución híbrida de un gaussiano (en las pendientes exteriores) y los exponenciales (en las laderas internas). A partir de la evidencia empírica presentada en [22], la expectativa es que tal distribución podría permitir más énfasis en la masa de probabilidad alrededor de los modos (como con el exponencial) al tiempo que proporciona estimaciones más precisas hacia las colas. Así como la regresión logística permite que las notas de registro de la distribución posterior se ajusten directamente con una línea, podríamos ajustar directamente los log-nods de la posterior con una línea de tres piezas (una spline) en lugar de hacer lo mismo indirectamente.Ajuste el laplace asimétrico. Este enfoque puede proporcionar más potencia ya que conserva la suposición de asimetría, pero no la suposición de que las densidades condicionales de clase son de una Laplace asimétrica. Finalmente, extender estos métodos a las salidas de otros clasificadores discriminativos es un área abierta. Actualmente estamos evaluando la idoneidad de estos métodos para la producción de un perceptrón votado [11]. Por analogía con los log-Odds, el puntaje operativo que parece prometedor es la votación de Perceptrones de peso de log + Perceptrones de peso Votación-.6. Resumen y conclusiones Hemos revisado una amplia variedad de métodos paramétricos para producir estimaciones de probabilidad a partir de las puntuaciones sin procesar de un clasificador discriminativo y para recalibrar un clasificador probabilístico no calibrado. Además, hemos introducido dos nuevas familias que intentan capitalizar el comportamiento asimétrico que tiende a surgir al aprender una función de discriminación. Hemos dado una forma eficiente de estimar los parámetros de estas distribuciones. Si bien estas distribuciones intentan lograr un equilibrio entre el poder de generalización de las distribuciones paramétricas y la flexibilidad que dan los parámetros asimétricos agregados, el gaussiano asimétrico parece tener un énfasis demasiado grande de los modos. En un contraste sorprendente, la distribución asimétrica de Laplace parece ser preferible en varios dominios de texto grandes y una variedad de medidas de rendimiento a los métodos paramétricos competitivos primarios, aunque el rendimiento comparable a veces se logra con una de las dos variedades de regresión logística. Dada la facilidad de estimar los parámetros de esta distribución, es una buena primera opción para producir estimaciones de probabilidad de calidad. Agradecimientos Agradecemos a Francisco Pereira por el código de prueba de signo, Anton Likhodedov por el código de regresión logística y John Platt por el soporte del código para el clasificador SVM Lineal Kits Kit SMOX. Además, agradecemos sinceramente a Chris Meek y John Platt por los consejos muy útiles proporcionados en las primeras etapas de este trabajo. Gracias también a Jaime Carbonell y John Lafferty por sus útiles comentarios sobre las versiones finales de este documento.7. Referencias [1] P. N. Bennett. Evaluación de la calibración de estimaciones posteriores de Bayes ingenuos. Informe técnico CMU-CS-00-155, Carnegie Mellon, School of Computer Science, 2000. [2] P. N. Bennett. Uso de distribuciones asimétricas para mejorar las probabilidades del clasificador: una comparación de métodos paramétricos nuevos y estándar. Informe técnico CMU-CS-02-126, Carnegie Mellon, Escuela de Informática, 2002. [3] H. Bourlard y N. Morgan. Un sistema continuo de reconocimiento de voz que incorpora MLP en HMM. En NIPS 89, 1989. [4] G. Brier. Verificación de pronósticos expresados en términos de probabilidad. Revisión del clima mensual, 78: 1-3, 1950. [5] M. H. DeGroot y S. E. Fienberg. La comparación y evaluación de los pronosticadores. Statistician, 32: 12-22, 1983. [6] M. H. DeGroot y S. E. Fienberg. Comparación de pronosticadores de probabilidad: conceptos binarios básicos y extensiones multivariadas. En P. Goel y A. Zellner, editores, Inferencias bayesianas y técnicas de decisión. Elsevier Science Publishers B.V., 1986. [7] P. Domingos y M. Pazzani. Más allá de la independencia: condiciones para la optimización del clasificador bayesiano simple. En ICML 96, 1996. [8] R. Duda, P. Hart y D. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [9] S. T. Dumais y H. Chen. Clasificación jerárquica de contenido web. En Sigir 00, 2000. [10] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, 1998. [11] Y. Freund y R. Schapire. Gran clasificación de margen utilizando el algoritmo Perceptron. Aprendizaje automático, 37 (3): 277-296, 1999. [12] I. Bien. Decisiones racionales. Revista de la Royal Statistical Society, Serie B, 1952. [13] T. Joachims. Categorización de texto con máquinas vectoriales de soporte: aprendizaje con muchas características relevantes. En ECML 98, 1998. [14] S. Kotz, T. J. Kozubowski y K. Podgorski. La distribución y generalizaciones de Laplace: una revisión con aplicaciones a comunicaciones, economía, ingeniería y finanzas. Birkh¨auser, 2001. [15] D. D. Lewis. Un algoritmo secuencial para los clasificadores de texto de entrenamiento: corrigendo y datos adicionales. Sigir Forum, 29 (2): 13-19, otoño de 1995. [16] D. D. Lewis. Reuters-21578, Distribución 1.0.http://www.daviddlewis.com/resources/ testCollections/Reuters21578, enero de 1997. [17] D. D. Lewis y W. A. Gale. Un algoritmo secuencial para los clasificadores de texto de entrenamiento. En Sigir 94, 1994. [18] D. D. Lewis, R. E. Schapire, J. P. Callan y R. Papka. Algoritmos de entrenamiento para clasificadores de texto lineal. En Sigir 96, 1996. [19] D. Lindley, A. Tversky y R. Brown. Sobre la reconciliación de evaluaciones de probabilidad. Journal of the Royal Statistical Society, 1979. [20] R. Manmatha, T. Rath y F. Feng. Modelado de distribuciones de puntaje para combinar las salidas de los motores de búsqueda. En Sigir 01, 2001. [21] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto de Naive Bayes. En AAAI 98, Taller sobre el aprendizaje para la categorización de texto, 1998. [22] J. C. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparaciones con métodos de probabilidad regularizados. En A. J. Smola, P. Bartlett, B. Scholkopf y D. Schuurmans, editores, avances en clasificadores de gran margen. MIT Press, 1999. [23] M. Saar-Tsechansky y F. Provost. Aprendizaje activo para la estimación y clasificación de probabilidad de clase. En Ijcai 01, 2001. [24] R. L. Winkler. Reglas de puntuación y la evaluación de evaluadores de probabilidad. Journal of the American Statistical Association, 1969. [25] Y. Yang y X. Liu. Un reexamen de los métodos de categorización de texto. En Sigir 99, 1999. [26] B. Zadrozny y C. Elkan. Obteniendo estimaciones de probabilidad calibradas de árboles de decisión y clasificadores bayesianos ingenuos. En ICML 01, 2001. [27] B. Zadrozny y C. Elkan. Reducción de multiclas a binarios mediante el acoplamiento de estimaciones de probabilidad. En KDD 02, 2002.