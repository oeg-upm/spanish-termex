Un algoritmo de búsqueda distribuido basado en el aprendizaje de refuerzo para los sistemas jerárquicos de recuperación de información entre pares a igual Haizheng Zhang College of Information Science and Technology Pennsylvania State University University Park, PA 16803 hzhang@ist.psu.edu Victor Menor Departamento de Ciencias de la Computación de la Universidad de Massachusetts Amstersts Amster, MA 01003 Lesser@cs.umass.edu Resumen Las estrategias de enrutamiento dominantes existentes empleadas en los sistemas de recuperación de información (IR) basados en Peerto-Peer (P2P) son enfoques basados en la similitud. En estos enfoques, los agentes dependen de la similitud de contenido entre las consultas entrantes y sus agentes vecinos directos para dirigir las sesiones de búsqueda distribuidas. Sin embargo, tal heurística es miope, ya que los agentes vecinos pueden no estar conectados con agentes más relevantes. En este documento, se desarrolla un enfoque basado en el aprendizaje de refuerzo en línea para aprovechar las características dinámicas del tiempo de ejecución de los sistemas P2P IR representados por información sobre sesiones de búsqueda anteriores. Específicamente, los agentes mantienen estimaciones sobre las habilidades de los agentes posteriores para proporcionar documentos relevantes para consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de comentarios devuelta de las sesiones de búsqueda anteriores. Según esta información, los agentes derivan las políticas de enrutamiento correspondientes. Posteriormente, estos agentes enrutan las consultas en función de las políticas aprendidas y actualizan las estimaciones basadas en las nuevas políticas de enrutamiento. Los resultados experimentales demuestran que el algoritmo de aprendizaje mejora considerablemente el rendimiento de enrutamiento en dos conjuntos de recolecciones de prueba que se han utilizado en una variedad de estudios de IR distribuidos. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial distribuida]: Algoritmos de términos generales de sistemas multiagentes, rendimiento, experimentación 1. Introducción En los últimos años, ha habido intereses crecientes en el estudio de cómo controlar los procesos de búsqueda en sistemas de recuperación de información (IR) basados en pares (P2P) [6, 13, 14, 15]. En esta línea de investigación, uno de los problemas centrales que concierne a los investigadores es enrutar eficientemente las consultas de los usuarios en la red a los agentes que están en posesión de documentos apropiados. En ausencia de información global, las estrategias dominantes para abordar este problema son los enfoques basados en la similitud de contenido [6, 13, 14, 15]. Si bien la similitud de contenido entre consultas y nodos locales parece ser un indicador acreditable para el número de documentos relevantes que residen en cada nodo, estos enfoques están limitados por varios factores. En primer lugar, las métricas basadas en la similitud pueden ser miope, ya que los nodos localmente relevantes pueden no estar conectados a otros nodos relevantes. En segundo lugar, los enfoques basados en la similitud no tienen en cuenta las características del tiempo de ejecución de los sistemas IR P2P, incluidos los parámetros ambientales, el uso de ancho de banda y la información histórica de las sesiones de búsqueda anteriores, que proporcionan información valiosa para los algoritmos de enrutamiento de consultas. En este artículo, desarrollamos un enfoque IR basado en el aprendizaje de refuerzo para mejorar el rendimiento de los algoritmos de búsqueda IR distribuidos. Los agentes pueden adquirir mejores estrategias de búsqueda recopilando y analizando la información de retroalimentación de sesiones de búsqueda anteriores. En particular, los agentes mantienen estimaciones, a saber, la utilidad esperada, en las capacidades de los agentes posteriores de proporcionar documentos relevantes para tipos específicos de consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de comentarios devuelta de las sesiones de búsqueda anteriores. Según la información actualizada de la utilidad esperada, los agentes derivan las políticas de enrutamiento correspondientes. Posteriormente, estos agentes enrutan las consultas en función de las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Este proceso se realiza de manera iterativa. El objetivo del algoritmo de aprendizaje, a pesar de que consume algún ancho de banda de red, es acortar el tiempo de enrutamiento para que se procesen más consultas por unidad de tiempo y al mismo tiempo encontrar documentos más relevantes. Esto contrasta con los enfoques basados en la similitud de contenido donde se repiten operaciones similares para cada consulta entrante y el tiempo de procesamiento mantiene en gran medida constante con el tiempo. Otra forma de ver este documento es que nuestro enfoque básico para la búsqueda de IR distribuida es construir una red de superposición jerárquica (organización de agentes) basada en la medida de similitud de contenido entre los agentes de las colecciones de documentos de una manera de abajo hacia arriba. En el trabajo pasado, hemos demostrado que esta organización mejora significativamente el rendimiento de la búsqueda. Sin embargo, esta estructura organizativa no tiene en cuenta los patrones de llegada de consultas, incluidas su frecuencia, tipos y donde ingresan al sistema, ni el ancho de banda de comunicación disponible de la red y las capacidades de procesamiento de los agentes individuales. La intención del aprendizaje de refuerzo es adaptar las decisiones de enrutamiento de los agentes a las situaciones de red dinámicas y aprender de las sesiones de búsqueda anteriores. Específicamente, las contribuciones de este documento incluyen: (1) un enfoque basado en el aprendizaje de refuerzo para que los agentes adquieran políticas de enrutamiento satisfactorias basadas en las estimaciones de la contribución potencial de sus agentes vecinos;(2) Dos estrategias para acelerar el proceso de aprendizaje. Hasta nuestro mejor conocimiento, esta es una de las primeras aplicaciones de aprendizaje de refuerzo para abordar los problemas de intercambio de contenido distribuido y es indicativo de algunos de los problemas para aplicar el refuerzo en una aplicación compleja. El resto de este documento se organiza de la siguiente manera: la Sección 2 revisa los sistemas jerárquicos de intercambio de contenido y el algoritmo de búsqueda de dos fases basado en dicha topología. La Sección 3 describe un enfoque basado en el aprendizaje de refuerzo para dirigir el proceso de enrutamiento;La Sección 4 detalla la configuración experimental y analiza los resultados. La Sección 5 discute estudios relacionados y la Sección 6 concluye el documento.2. Buscar en sistemas jerárquicos de IR P2P Esta sección revisa brevemente nuestros enfoques básicos para los sistemas IR jerárquicos P2P. En un sistema IR jerárquico P2P ilustrado en la Fig.1, los agentes están conectados entre sí a través de tres tipos de enlaces: enlaces hacia arriba, enlaces hacia abajo y enlaces laterales. En las siguientes secciones, denotamos el conjunto de agentes que están directamente conectados al agente ai como directconn (ai), que se define como directconn (ai) = nei (ai) ∪ par (ai) ∪ chl (ai), donde nei(Ai) es el conjunto de agentes vecinos conectados a la IA a través de enlaces laterales;PAR (AI) es el conjunto de agentes a los que el agente AI está conectado a través de enlaces ascendentes y CHL (AI) es el conjunto de agentes a los que el agente AI se conecta a través de enlaces hacia abajo. Estos enlaces se establecen a través de un proceso de agrupación distribuido basado en la similitud de contenido ascendente [15]. Los agentes usan estos enlaces para localizar otros agentes que contienen documentos relevantes para las consultas dadas. Un agente típico AI en nuestro sistema usa dos colas: una cola de búsqueda local, LSI y una cola de reenvío de mensajes MFI. Los estados de las dos colas constituyen los estados internos de un agente. La búsqueda de búsqueda local LSI almacena sesiones de búsqueda que están programadas para el procesamiento local. Es una cola prioritaria y el agente AI siempre selecciona las consultas más prometedoras para procesar para maximizar la utilidad global. MFI consiste en un conjunto de consultas para avanzar y se procesa de manera FIFO (primero en primera vez). Para la primera consulta en MFI, el agente IA determina qué subconjunto de sus agentes vecinos la reenvía en función de la política de enrutamiento de los agentes πi. Estas decisiones de enrutamiento determinan cómo se realiza el proceso de búsqueda en la red. En este documento, llamamos a AI como agente aguas arriba de AJS y AJ como agente aguas abajo de AIS si a4 a5 a6 a7 a2 a3 a9 nei (a2) = {a3} par (a2) = {a1} chl (a2) = {a4, a5, a5)} A1 A8 Figura 1: Una fracción de un sistema P2PIR jerárquico Un agente AI enruta una consulta al agente AJ. El protocolo de búsqueda distribuido de nuestra organización de agentes jerárquicos está compuesto por dos pasos. En el primer paso, al recibir una consulta QK en el momento, TL de un usuario, el agente AI inicia una sesión de búsqueda Si sondeando a sus agentes vecinos aj ∈ Nei (ai) con la sonda de mensaje para el valor de similitud SIM (QK, AJ)entre QK y AJ. Aquí, la IA se define como el iniciador de la consulta de la sesión de búsqueda SI. En el segundo paso, AI selecciona un grupo de los agentes más prometedores para comenzar el proceso de búsqueda real con la búsqueda de mensajes. Estos mensajes de búsqueda contienen un parámetro TTL (Time To Live) además de la consulta. El valor TTL disminuye en 1 después de cada salto. En el proceso de búsqueda, los agentes descargan esas consultas que se han procesado previamente o cuyo TTL cae a 0, lo que evita que las consultas sean el bucle en el sistema para siempre. La sesión de búsqueda termina cuando todos los agentes que reciben la consulta lo dejan caer o TTL disminuye a 0. Al recibir mensajes de búsqueda de QK, los agentes programan actividades locales que incluyen búsqueda local, reenvío de QK a sus vecinos y devuelven los resultados de búsqueda al iniciador de consultas. Este proceso y algoritmos relacionados se detallan en [15, 14].3. Un enfoque de búsqueda basado en refuerzo básico en el algoritmo de búsqueda distribuido mencionado anteriormente, las decisiones de enrutamiento de un agente AI dependen de la comparación de similitud entre consultas entrantes y agentes vecinos de AIS para enviar esas consultas a agentes relevantes sin inundar la red con mensajes de consulta innecesarios. Sin embargo, esta heurística es miope porque un vecino directo relevante no está necesariamente conectado con otros agentes relevantes. En esta sección, proponemos un enfoque más general al enmarcar este problema como una tarea de aprendizaje de refuerzo. En la búsqueda de una mayor flexibilidad, los agentes pueden cambiar entre dos modos: modo de aprendizaje y modo de no aprendizaje. En el modo de no aprendizaje, los agentes operan de la misma manera que en los procesos de búsqueda distribuidos normales descritos en [14, 15]. Por otro lado, en el modo de aprendizaje, en paralelo con las sesiones de búsqueda distribuidas, los agentes también participan en un proceso de aprendizaje que se detallará en esta sección. Tenga en cuenta que en el protocolo de aprendizaje, el proceso de aprendizaje no interfiere con el proceso de búsqueda distribuido. Los agentes pueden optar por iniciar y dejar de aprender procesos sin afectar el rendimiento del sistema. En particular, dado que el proceso de aprendizaje consume recursos de red (especialmente el ancho de banda), los agentes pueden optar por iniciar el aprendizaje solo cuando la carga de la red es relativamente baja, minimizando así los costos de comunicación adicionales incurridos por el algoritmo de aprendizaje. La sección está estructurada de la siguiente manera, la sección 3.1 describe 232 el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), un modelo basado en el aprendizaje de refuerzo. La Sección 3.2 describe un protocolo para implementar el algoritmo de aprendizaje en la red. La Sección 3.3 discute la convergencia del algoritmo de aprendizaje.3.1 La política de enrutamiento del Modelo A Agente toma el estado de una sesión de búsqueda como entrada y salida Las acciones de enrutamiento para esa consulta. En nuestro trabajo, el estado de una sesión de búsqueda SJ se estipula como: QSJ = (QK, TTLJ) donde TTLJ es el número de lúpulo que queda para la sesión de búsqueda SJ, QK es la consulta específica. QL es un atributo de QK que indica a qué tipo de consultas QK probablemente pertenecen. El conjunto de QL se puede generar ejecutando un algoritmo de clasificación en línea simple en todas las consultas que han sido procesadas por los agentes, o un algoritmo de ine en un conjunto de capacitación prediseñado. La suposición aquí es que el conjunto de tipos de consultas se aprende con anticipación y pertenece al conocimiento común de los agentes en la red. El trabajo futuro incluye explorar cómo se puede lograr el aprendizaje cuando esta suposición no se mantiene. Dado el conjunto de tipos de consulta, una consulta entrante Qi se puede clasificar en una clase de consulta Q (Qi) mediante la fórmula: Q (Qi) = Arg Max Qj P (Qi | Qj) (1) Donde P (Qi | Qj) indicaLa probabilidad de que la consulta Qi sea generada por la clase de consulta QJ [8]. El conjunto de acciones de enrutamiento atómico de un agente AI se denota como {αi}, donde {αi} se define como αi = {αi0, αi1, ..., αin}. Un elemento αij representa una acción para enrutar una consulta dada al agente vecino Aij ∈ DirectConn (AI). La política de enrutamiento πi del agente AI es estocástica y su resultado para una sesión de búsqueda con QSJ de estado se define como: πi (QSJ) = {(αi0, πi (QSI, αi0)), (αi1, πi (QSI, αi1)))), ...} (2) Tenga en cuenta que el operador πi está sobrecargado para representar la política probabilística para una sesión de búsqueda con QSJ de estado, denotado como πi (QSJ);o la probabilidad de reenviar la consulta a un agente vecino específico aik ∈ DirectConn (ai) bajo la política πi (QSJ), denotada como πi (QSJ, αik). Por lo tanto, la ecuación (2) significa que la probabilidad de reenviar la sesión de búsqueda al agente AI0 es πi (QSI, αi0) y así sucesivamente. Bajo esta política estocástica, la acción de enrutamiento no es determinista. La ventaja de tal estrategia es que los mejores agentes vecinos no se seleccionarán repetidamente, mitigando así las posibles situaciones de puntos calientes. La utilidad esperada, Un I (QSJ), se utiliza para estimar la posible ganancia de utilidad de la consulta de enrutamiento tipo QSJ al agente ai bajo la política πn i. El Superscript N indica el valor en la enésima iteración en un proceso de aprendizaje iterativo. La utilidad esperada proporciona orientación de enrutamiento para futuras sesiones de búsqueda. En el proceso de búsqueda, cada agente AI mantiene observaciones parciales de sus estados vecinos, como se muestra en la figura 2. La observación parcial incluye información no local, como la estimación de utilidad potencial de su vecino AM para el estado de consulta QSJ, denotado como UM (QSJ), así como la información de carga, LM. Estas observaciones son actualizadas periódicamente por los vecinos. La información de utilidad estimada se utilizará para actualizar la utilidad esperada AIS para su política de enrutamiento. Información de carga Utilidad esperada para diferentes tipos de consultas Agentes vecinos ... A0 A1 A3 A2 UN 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1)Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ... Figura 2: Agente AIS Observación parcial sobre sus vecinos (A0, A1 ...) La información de carga de AM, LM, se define como lm = | mfm |CM, donde | MFM |es la longitud de la cola de mensaje y CM es la tasa de servicio de la cola de mensajes del agente AMS. Por lo tanto, LM caracteriza la utilización de un canal de comunicación de agentes y, por lo tanto, proporciona información no local para que los vecinos de AMS ajusten los parámetros de su política de enrutamiento para evitar inundar a sus agentes posteriores. Tenga en cuenta que, según las características de las consultas que ingresan a las capacidades del sistema y los agentes, la carga de agentes puede no ser uniforme. Después de recopilar la información de la tasa de utilización de todos sus vecinos, el agente AI calcula Li como una medida única para evaluar la condición de carga promedio de su vecindario: li = p k lk | directconn (ai) |Los agentes explotan el valor de LI para determinar la probabilidad de enrutamiento en su política de enrutamiento. Tenga en cuenta que, como se describe en la Sección 3.2, la información sobre los agentes vecinos está abarrotada con el mensaje de consulta propagado entre los agentes siempre que sea posible para reducir la sobrecarga del tráfico.3.1.1 Actualizar la política Se introduce un proceso de actualización iterativo para que los agentes aprendan una política de enrutamiento estocástica satisfactoria. En este proceso iterativo, los agentes actualizan sus estimaciones sobre la utilidad potencial de sus políticas de enrutamiento actuales y luego propagan las estimaciones actualizadas a sus vecinos. Sus vecinos luego generan una nueva política de enrutamiento basada en la observación actualizada y, a su vez, calculan la utilidad esperada en función de las nuevas políticas y continúan este proceso iterativo. En particular, en el momento n, dado un conjunto de utilidad esperada, un agente IA, cuyo conjunto de agentes directamente conectados es directconn (ai) = {ai0, ..., AIM}, determina su política de enrutamiento estocástica correspondiente para una sesión de búsqueda deEstado QSJ Basado en los siguientes pasos: (1) AI primero selecciona un subconjunto de agentes como los posibles agentes aguas abajo de Set DirectConn (AI), denotado como PDN (AI, QSJ). El tamaño del agente posterior aguas abajo se especifica como | PDN (AI, QSJ) |= min (| nei (ai), dn i + k) |donde k es constante y se establece en 3 en este documento;Dn I, el ancho delantero, se define como el número esperado del sexto INTL. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 233 agentes vecinos que el agente AI puede reenviar en el tiempo n.Esta fórmula especifica que el potencial agente aguas abajo establece PDN (AI, QSJ) es el subconjunto de agentes vecinos con DN I + K más alto valor de utilidad esperado para QSJ de estado entre todos los agentes en DirectConn (AI) o todos sus agentes vecinos. El K se introduce en base a la idea de una política de enrutamiento estocástica y hace que la probabilidad de reenvío del agente más alto DN I +K sea menos del 100%. Tenga en cuenta que si queremos limitar el número de agentes aguas abajo para la sesión de búsqueda SJ como 5, la probabilidad de reenviar la consulta a todos los agentes vecinos debe sumar hasta 5. Configurar el valor DN I correctamente puede mejorar la tasa de utilización del ancho de banda de la red cuando gran parte de la red está inactiva al mitigar la carga de tráfico cuando la red está altamente cargada. El valor DN+1 I se actualiza en función de DN I, las observaciones anteriores y actuales sobre la situación del tráfico en el vecindario. Específicamente, la fórmula de actualización para dn+1 i es dn+1 i = dn i ∗ (1+1 - li | directconn (ai) |) En esta fórmula, el ancho directo se actualiza en función de las condiciones de tráfico del vecindario del agente AIS, es decir, Li y su valor anterior.(2) Para cada agente AIK en el PDN (AI, QSJ), la probabilidad de reenviar la consulta a AIK se determina de la siguiente manera para asignar una mayor probabilidad de reenvío a los agentes vecinos con un valor de utilidad esperado más alto: πn+1I (QSJ, αik) = Dn+1 i | Pdn (AI, QSJ) |+ β ∗ `UIK (QSJ) - PDU (AI, QSJ) | PDN (AI, QSJ) |´ (3) donde pdun (ai, qsj) = x o∈P dn (ai, qsj) uo (QSJ) y QSJ es el estado posterior del agente aik después del agente AI reenvía la sesión de búsqueda con QSJ estatal a su agente vecino AIK;Si qsj = (qk, ttl0), entonces qsj = (qk, ttl0 - 1). En la Fórmula 3, el primer término a la derecha de la ecuación, dn+1 i | p dn (ai, qsj) |, se usa para determinar la probabilidad de reenvío distribuyendo igualmente el ancho directo, DN+1 I, a los agentes en el conjunto PDN (AI, QSJ). El segundo término se usa para ajustar la probabilidad de ser elegido para que los agentes con valores de utilidad esperados más altos se favorecen.β se determina de acuerdo con: β = min `m - dn+1 i m ∗ umax - pdun (ai, qsj), dn+1 i pdun (ai, qsj) - m ∗ echa ´ (4) donde m = | pdn ((Ai, qsj) |, umax = max o∈P dn (ao, qsj) uo (qsj) y umin = min o∈P dn (ao, qsj) uo (qsj) Esta fórmula garantiza que la final πn+1 i (El valor QSJ, αik) está bien definido, es decir, 0 ≤ πn+1 I (QSJ, αik) ≤ 1 y X I πn+1 I (QSJ, αik) = Dn+1 I Sin embargo, dicha solución no explora todaslas posibilidades. Para equilibrar la explotación y la exploración, se adopta un enfoque de grantedia λ. En el enfoque de grantre de λ, además de asignar una mayor probabilidad a aquellos agentes con un valor de utilidad esperado más alto, como en la ecuación (3). Los agentes que parecen ser opciones no tan buenas también se enviarán consultas basadas en una tasa de exploración dinámica. En particular, para los agentes en el conjunto PDN (AI, QSJ), πn+1 i1 (QSJ) se determina de la misma manera que la anterior, con la única diferencia que Dn+1 I se reemplaza con DN+1 I ∗(1 - λn). El ancho de banda de búsqueda restante se usa para aprender asignando probabilidad λn de manera uniforme a los agentes AI2 en el conjunto DirectConn (AI) - PDN (AI, QSJ).πn+1 i2 (QSJ, αik) = Dn+1 I ∗ λn | DirectConn (AI) - PDN (AI, QSJ) |(5) Donde PDN (AI, QSJ) ⊂ DirectConn (AI). Tenga en cuenta que la tasa de exploración λ no es una constante y disminuye las horas extras. El λ se determina de acuerdo con la siguiente ecuación: λn+1 = λ0 ∗ e - c1n (6) donde λ0 es la tasa de exploración inicial, que es una constante;C1 también es una constante para ajustar la tasa de disminución de la tasa de exploración;n es la unidad de tiempo actual.3.1.2 Actualización de la utilidad esperada Una vez que la política de enrutamiento en el paso n+1, πn+1 i se determina en función de la fórmula anterior, el agente ai puede actualizar su propia utilidad esperada, un+1 i (QSI), basada en elLa política de enrutamiento actualizada resultó de la fórmula 5 y los valores U actualizados de sus agentes vecinos. Bajo el supuesto de que después de reenviar una consulta a los vecinos de AIS, las sesiones de búsqueda posteriores son independientes, la fórmula de actualización es similar a la fórmula de actualización de Bellman en Q-learning: un+1 i (QSJ) = (1-θi) ∗(QSJ) + θi ∗ (Rn + 1 I (QSJ) + X K πn + 1 I (QSJ, αik) Un K (QSJ)) (7) donde QSJ = (QJ, TTL - 1) es el siguiente estado deQSJ = (QJ, TTL);Rn+1 I (QSJ) es la recompensa local esperada para la clase de consulta QK en Agent AI bajo la Política de enrutamiento πn+1 i;θi es el coeficiente para decidir cuánto peso se otorga al valor anterior durante el proceso de actualización: cuanto menor valor θi sea, cuanto más rápido se espera que el agente aprenda el valor real, mientras que la mayor volatilidad del algoritmo y viceversa. RN+1 (s) se actualiza de acuerdo con la siguiente ecuación: 234 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) RN +1 I (QSJ) = RN I (QSJ) +γi ∗ (R (QSJ)-RN I (QSJ)) ∗ P (QJ | QJ) (8)donde R (QSJ) es la recompensa local asociada con la sesión de búsqueda. P (QJ | QJ) indica cuán relevante es la consulta QJ para el tipo de consulta QJ, y γi es la tasa de aprendizaje para el agente AI. Dependiendo de la similitud entre una consulta específica Qi y su consulta correspondiente Tipo de Qi, la recompensa local asociada con la sesión de búsqueda tiene un impacto diferente en la estimación de RN I (QSJ). En la fórmula anterior, este impacto se refleja en el coeficiente, el valor P (QJ | QJ).3.1.3 Función de recompensa Después de que una sesión de búsqueda se detiene cuando sus valores TTL expiran, todos los resultados de búsqueda se devuelven al usuario y se comparan con el juicio de relevancia. Suponiendo que el conjunto de resultados de búsqueda es sr, la recompensa rew (sr) se define como: rew (sr) = j 1 if | rel (sr) |> C | Rel (Sr) |c de lo contrario.Donde SR es el conjunto de resultados de búsqueda devueltos, REL (SR) es el conjunto de documentos relevantes en los resultados de búsqueda. Esta ecuación especifica que los usuarios dan una recompensa 1.0 si el número de documentos relevantes devueltos alcanza un número predefinido c.De lo contrario, la recompensa es en proporción al número de documentos relevantes devueltos. Esta justificación para establecer dicho valor de corte es que la importancia de la relación de recuperación disminuye con la abundancia de documentos relevantes en el mundo real, por lo tanto, los usuarios tienden a centrarse solo en un número limitado de resultados buscados. Los detalles del protocolo de enrutamiento real se introducirán en la Sección 3.2 cuando presentemos cómo se implementa el algoritmo de aprendizaje en sistemas reales.3.2 Implementación del algoritmo de aprendizaje Esta sección describe cómo se puede usar el algoritmo de aprendizaje en un proceso de búsqueda monofásico o de dos fases. En el algoritmo de búsqueda monofásica, las sesiones de búsqueda comienzan desde los iniciadores de las consultas. Por el contrario, en el algoritmo de búsqueda de dos pasos, el iniciador de consulta primero intenta buscar un punto de partida más apropiado para la consulta introduciendo un paso exploratorio como se describe en la Sección 2. A pesar de la diferencia en la calidad de los puntos de partida, la mayor parte del proceso de aprendizaje para los dos algoritmos es en gran medida la misma que se describe en los siguientes párrafos. Antes de que comience el aprendizaje, cada agente inicializa el valor de utilidad esperado para todos los estados posibles como 0. Posteriormente, al recibir una consulta, además de las operaciones normales descritas en la sección anterior, un agente IA también establece un temporizador para esperar los resultados de búsqueda devueltos de sus agentes aguas abajo. Una vez que el temporizador expira o ha recibido la respuesta de todos sus agentes aguas abajo, IA fusiona y reenvía los resultados de búsqueda acumulados de sus agentes posteriores a su agente aguas arriba. Configurar el temporizador acelera el aprendizaje porque los agentes pueden evitar esperar demasiado para que los agentes posteriores devuelvan los resultados de búsqueda. Tenga en cuenta que estos resultados detallados y la información del agente correspondiente aún se almacenarán en AI hasta que la información de retroalimentación se pase de su agente aguas arriba y se puede evaluar el rendimiento de sus agentes aguas abajo. La duración del temporizador está relacionada con el valor TTL. En este artículo, establecemos el temporizador en ttimer = ttli ∗ 2 + tf, donde ttli ∗ 2 es la suma del tiempo de viaje de las consultas en la red, y TF es el período de tiempo esperado que a los usuarios les gustaría esperar. Los resultados de búsqueda eventualmente se devolverán al iniciador de la sesión de búsqueda A0. Se compararán con el juicio de relevancia proporcionado por los usuarios finales (como se describe en la sección del experimento, el juicio de relevancia para el conjunto de consultas se proporciona junto con las colecciones de datos). La recompensa se calculará y se propagará hacia atrás a los agentes en la forma en que se aprobaron los resultados de búsqueda. Este es un proceso inverso de la propagación de resultados de búsqueda. En el proceso de propagación de la recompensa hacia atrás, los agentes actualizan las estimaciones de su propio valor de utilidad potencial, generan una política actualizada y pasan sus resultados actualizados a los agentes vecinos en función del algoritmo descrito en la Sección 3. Tras el cambio del valor de utilidad esperado, el Agente AI envía su estimación de utilidad actualizada a sus vecinos para que puedan actuar sobre la utilidad esperada cambiada y el estado correspondiente. Este mensaje de actualización incluye la recompensa potencial, así como el estado correspondiente QSI = (QK, TTLL) del Agente AI. Cada agente vecino, AJ, reacciona a este tipo de mensaje de actualización actualizando el valor de utilidad esperado para QSJ de estado (QK, TTLL + 1) de acuerdo con el valor de utilidad esperado cambiado recientemente. Una vez que completen la actualización, los agentes nuevamente informarían a los vecinos relacionados para actualizar sus valores. Este proceso continúa hasta que el valor TTL en el mensaje de actualización aumenta al límite TTL. Para acelerar el proceso de aprendizaje, mientras se actualiza los valores de utilidad esperados de un agente AIS Agentes vecinos, especificamos que UM (QK, TTL0)> = Um (QK, TTL1) IFF TTL0> TTL1 Así, cuando el agente AI recibe una utilidad esperada actualizada.Valor con TTL1, también actualiza los valores de utilidad esperados con cualquier TTL0> TTL1 si um (QK, TTL0) <um (QK, TTL1) para acelerar la convergencia. Esta heurística se basa en el hecho de que la utilidad de una sesión de búsqueda es una función no decreciente del tiempo t.3.3 Discusión En la formalización del sistema de enrutamiento de contenido como tarea de aprendizaje, se hacen muchos supuestos. En los sistemas reales, estos supuestos pueden no mantenerse y, por lo tanto, el algoritmo de aprendizaje no puede converger. Dos problemas son de particular, (1) este problema de enrutamiento de contenido no tiene propiedades de Markov. A diferencia del enrutamiento de paquetes basado en el nivel de IP, la decisión de enrutamiento de cada agente para una sesión de búsqueda particular SJ depende del historial de enrutamiento de SJ. Por lo tanto, la suposición de que todas las sesiones de búsqueda posteriores son independientes no se mantiene en la realidad. Esto puede conducir a un problema de conteo doble de que los documentos relevantes de algunos agentes se contarán más de una vez para el estado donde el valor TTL es más de 1. Sin embargo, en el contexto de las organizaciones de agentes jerárquicos, dos factores mitigan estos problemas: primero, los agentes en cada grupo de contenido forman una estructura similar a un árbol. Con la ausencia de los ciclos, las estimaciones dentro del árbol estarían cerca del valor preciso. En segundo lugar, la naturaleza estocástica de la política de enrutamiento remedia en parte este problema. El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 235 (2) Otro desafío para este algoritmo de aprendizaje es que en un entorno de red real, las observaciones de los agentes vecinos pueden no ser actualizados a tiempo debido al retraso de comunicación u otras situaciones. Además, cuando los agentes vecinos actualizan sus estimaciones al mismo tiempo, la oscilación puede surgir durante el proceso de aprendizaje [1]. Este artículo explora varios enfoques para acelerar el proceso de aprendizaje. Además de la estrategia mencionada de actualizar los valores de utilidad esperados, también empleamos una estrategia de actualización activa donde los agentes notifican a sus vecinos cada vez que se actualiza su utilidad esperada. Por lo tanto, se puede lograr una velocidad de convergencia más rápida. Esta estrategia contrasta con la actualización perezosa, donde los agentes solo hacen eco de sus agentes vecinos con su cambio de utilidad esperado cuando intercambian información. La compensación entre los dos enfoques es la carga de la red versus la velocidad de aprendizaje. La ventaja de este algoritmo de aprendizaje es que una vez que se aprende una política de enrutamiento, los agentes no tienen que comparar repetidamente la similitud de las consultas siempre que la topología de la red permanezca sin cambios. En cambio, el agente solo tiene que determinar la clasificación de la consulta correctamente y seguir las políticas aprendidas. La desventaja de este enfoque basado en el aprendizaje es que el proceso de aprendizaje debe realizarse cada vez que cambie la estructura de la red. Hay muchas extensiones potenciales para este modelo de aprendizaje. Por ejemplo, actualmente se usa una sola medida para indicar la carga de tráfico para el vecindario de un agente. Una extensión simple sería realizar un seguimiento de la carga individual para cada vecino del agente.4. Experimentos Settings y resultados Los experimentos se realizan en el kit de herramientas de simulación Trano con dos conjuntos de conjuntos de datos, TREC-VLC-921 y TREC123-100. Las siguientes subsecciones introducen el TRANO Testbed, los conjuntos de datos y los resultados experimentales.4.1 Trano Testbed Trano (Ruting de tareas en la organización de redes de agentes) es un Bebed de prueba de recuperación de información basado en la red de múltiples agentes. Trano se basa en la granja [4], un simulador distribuido basado en el tiempo que proporciona un marco de difusión de datos para organizaciones basadas en redes de agentes distribuidos a gran escala. Trano apoya la importación y exportación de perfiles de organización de agentes, incluidas conexiones topológicas y otras características. Cada agente de Trano está compuesto por una estructura de vista de agente y una unidad de control. En la simulación, cada agente se pulsa regularmente y el agente verifica las colas de mensajes entrantes, realiza operaciones locales y luego reenvía mensajes a otros agentes.4.2 Configuración experimental En nuestro experimento, utilizamos dos conjuntos de datos estándar, TRECVLC-921 y TREC-123-100 conjuntos de datos, para simular las colecciones alojadas en los agentes. Los conjuntos de datos TREC-VLC-921 y TREC123-100 fueron creados por el Instituto Nacional de Tecnología Estándar (NIST) de EE. UU. Para sus conferencias TREC. En el dominio de recuperación de información distribuida, las dos colecciones de datos se dividen en 921 y 100 subcolecciones. Se observa que el conjunto de datos TREC-VLC-921 es más heterogéneo que TREC-123-100 en términos de fuente, longitud del documento y distribución de documentos relevantes de las estadísticas de las dos colecciones de datos enumeradas en [13]. Por lo tanto, TREC-VLC-921 está mucho más cerca de las distribuciones de documentos reales en entornos P2P. Además, TREC-123-100 se divide en dos conjuntos de 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Número de consulta ARSS versus el número de consultas entrantes para TREC-VLC-921 SSLA-921 SSNA-921 Figura 3: ARSS (recompensa promedio por sesión de búsqueda) versus el número de sesiones de búsqueda para la búsqueda de 1 fase en TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Número de consulta-VLC-921 TSLA-921 TSNA-921 Figura 4: ARSS (recompensa promedio por sesión de búsqueda) versus el número de sesiones de búsqueda para la búsqueda de 2 fases en las subcolecciones TREC-VLC-921 de dos maneras: al azar y por fuente. Las dos particiones se denotan como TREC-123-100-Random y TREC-123-100-Source respectivamente. Los documentos en cada subcolección en el conjunto de datos TREC-123-100-Source son más coherentes que los de TREC-123-100-Random. Los dos conjuntos diferentes de particiones nos permiten observar cómo el algoritmo de aprendizaje distribuido se ve afectado por la homogeneidad de las colecciones. La organización del agente jerárquico es generada por el algoritmo descrito en nuestro algoritmo anterior [15]. Durante el proceso de generación de topología, la información de grado de cada agente se estima mediante el algoritmo introducido por Palmer et al.[9] con parámetros α = 0.5 y β = 0.6. En nuestros experimentos, estimamos el límite ascendente y el límite de grado descendente utilizando factores de descuento lineal 0.5, 0.8 y 1.0. Una vez que se construye la topología, las consultas seleccionadas al azar del conjunto de consultas 301-350 en TREC-VLC-921 y el conjunto de consultas 1- 50 en TREC-123-100-Random y TREC-123-100-Source se inyectan al sistema basado en el sistema.En una distribución de Poisson p (n (t) = n) = (λt) n n!e - λ 236 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 CumulativeUtility Consulte Número de utilidad acumulativa sobre el número de consultas entrantes TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figura 5: La utilidad acumulada versus el número de sesiones de búsqueda TREC-VLC-921 Además, suponemos que todos los agentes tienen las mismas posibilidades de obtener consultas del entorno, es decir, λ es lo mismo para cada agente. En nuestros experimentos, λ se establece como 0.0543 para que la media de las consultas entrantes del entorno a la red de agentes sea 50 por unidad de tiempo. El tiempo de servicio para la cola de comunicación y la cola de búsqueda local, es decir, TQIJ y TRS, se establece como una unidad de tiempo 0.01 y 0.05 unidades de tiempo respectivamente. En nuestros experimentos, hay diez tipos de consultas adquiridas al agrupar el conjunto de consultas 301 - 350 y 1 - 50. 4.3 Análisis de resultados y evaluación La Figura 3 demuestra el ARS (recompensa promedio por sesión de búsqueda) versus el número de consultas entrantes con el tiempo para el tiempoEl algoritmo no de aprendizaje basado en un solo paso (SSNA) y el algoritmo de aprendizaje de un solo paso (SSLA) para la recopilación de datos TREC-VLC-921. Muestra que la recompensa promedio para el algoritmo SSNA varía de 0.02 - 0.06 y el rendimiento cambia poco con el tiempo. La recompensa promedio por el enfoque SSLA comienza en el mismo nivel con el algoritmo SSNA. Pero el rendimiento aumenta con el tiempo y la ganancia promedio de rendimiento se estabiliza en aproximadamente un 25% después del rango de consulta 2000 - 3000. La Figura 4 muestra el ARSS (recompensa promedio por sesión de búsqueda) versus el número de consultas entrantes a lo largo del tiempo para el algoritmo no de aprendizaje basado en dos pasos (TSNA) y el algoritmo de aprendizaje de dos pasos (TSLA) para la recolección de datos TREC-VLC-921. El enfoque TSNA tiene un rendimiento relativamente consistente con la recompensa promedio de 0.05 - 0.15. La recompensa promedio para el enfoque TSLA, donde el algoritmo de aprendizaje se explota, comienza al mismo nivel con el algoritmo TSNA y mejora la recompensa promedio con el tiempo hasta las consultas 2000-2500 que se unen al sistema. Los resultados muestran que la ganancia de rendimiento promedio para el enfoque TSLA sobre el enfoque TNLA es del 35% después de la estabilización. La Figura 5 muestra la utilidad acumulada versus el número de consultas entrantes con el tiempo para SSNA, SSLA, TSNA y TSLA, respectivamente. Ilustra que la utilidad acumulada de los algoritmos de no aprendizaje aumenta en gran medida linealmente con el tiempo, mientras que las ganancias de los algoritmos basados en el aprendizaje se aceleran cuando más consultas ingresan al sistema. Estos resultados experimentales demuestran que los enfoques basados en el aprendizaje funcionan constantemente mejor que el algoritmo de enrutamiento basado en el no aprendizaje. Además, el algoritmo basado en el aprendizaje en dos fases es mejor que el algoritmo de aprendizaje basado en un solo fase porque la recompensa máxima que un agente puede recibir al buscar en su vecindario dentro del lúpulo TTL está relacionado con el número total de documentos relevantes en esa área. Por lo tanto, incluso la política de enrutamiento óptima puede hacer poco más allá de alcanzar estos documentos relevantes más rápido. Por el contrario, el algoritmo de aprendizaje de dos escasos puede reubicar la sesión de búsqueda en un vecindario con documentos más relevantes. El TSLA combina los méritos de ambos enfoques y los supera. La Tabla 1 enumera la utilidad acumulada para conjuntos de datos TREC123-100-Random y TREC-123-100-Source con organizaciones jerárquicas. Las cinco columnas muestran los resultados para cuatro enfoques diferentes. En particular, la columna TSNA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSNA. La columna TSLA-Random muestra los resultados para el conjunto de datos TREC-123-100-Random con el enfoque TSLA. Hay dos números en cada celda en la columna TSLA-Random. El primer número es la utilidad acumulada real, mientras que el segundo número es el porcentaje de ganancia en términos de la utilidad sobre el enfoque TSNA. Las columnas-fuente TSNA y la fuente TSLA muestran los resultados para el conjunto de datos TREC-123-100-fuente con enfoques TSNA y TSLA respectivamente. La Tabla 1 muestra que la mejora del rendimiento para TREC-123-100-Random no es tan significativa como los otros conjuntos de datos. Esto se debe a que los documentos en la subcolección de TREC-123-100-Random se seleccionan al azar, lo que hace que el modelo de recolección, la firma de la colección, sea menos significativa. Dado que ambos algoritmos están diseñados en función de la suposición de que las colecciones de documentos pueden estar bien representadas por su modelo de recolección, este resultado no es sorprendente. En general, las Figuras 4, 5 y la Tabla 1 demuestran que el enfoque basado en el aprendizaje de refuerzo puede mejorar considerablemente el rendimiento del sistema para ambas colecciones de datos. Sin embargo, sigue siendo un trabajo futuro para descubrir la correlación entre la magnitud de las ganancias de rendimiento y el tamaño de la recopilación de datos y/o la extensión de la heterogeneidad entre las subcolecciones.5. Trabajo relacionado El problema de enrutamiento de contenido difiere del enrutamiento de nivel de red en las redes de comunicación de paquetes con un paquete en las redes de nivel de aplicación. Además, los agentes de destino en nuestros algoritmos de reducción de contenido son múltiples y las direcciones no se conocen en el proceso de enrutamiento. Los problemas de enrutamiento a nivel de IP han sido atacados desde la perspectiva de aprendizaje de refuerzo [2, 5, 11, 12]. Estos estudios han explorado algoritmos completamente distribuidos que pueden, sin coordinación central para difundir el conocimiento sobre la red, encontrar las rutas más cortas de manera robusta y eficiente frente a las topologías de red cambiantes y los costos de los enlaces cambiantes. Hay dos clases principales de algoritmos adaptados de enrutamiento de paquetes distribuidos en la literatura: algoritmos de vector de distancia y algoritmos de estado de enlace. Si bien esta línea de estudios lleva una cierta similitud con nuestro trabajo, se ha centrado principalmente en las redes de comunicación de paquetes. En este dominio, el destino de un paquete es determinista y único. Cada agente mantiene estimaciones, probabilística o determinista, en la distancia a un determinado destino a través de sus vecinos. Una variante de las técnicas de Q-Learning se implementa el sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 237 Tabla 1: Utilidad acumulada para conjuntos de datos TREC-123-100-Random y TREC-123-100-Source con organización jerárquica;Los números porcentuales en las columnas TSLA-Random y TSLA-Source demuestran la ganancia de rendimiento sobre el algoritmo sin aprendizaje Número de consulta TSNA-Random TSLA-Random-Source TSLA-Source 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20 20 20 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% a la actualización de la actualización a la actualización de la actualización de la actualización de la actualización de la actualización de la actualización de la actualización.distancias. Se ha descubierto que la propiedad de la localidad es una característica importante de los sistemas de recuperación de información en los estudios de modelado de usuarios [3]. En los sistemas de intercambio de contenidos basados en P2P, esta propiedad se ejemplifica con el fenómeno que los usuarios tienden a enviar consultas que representan solo un número limitado de temas y, por el contrario, los usuarios en el mismo vecindario probablemente compartan intereses comunes y envíen consultas similares [10]. Se percibe que el enfoque basado en el aprendizaje es más beneficioso para los sistemas de recuperación de información distribuidos reales que exhiben propiedad de localidad. Esto se debe a que los patrones de tráfico y consulta de los usuarios pueden reducir el espacio estatal y acelerar el proceso de aprendizaje. El trabajo relacionado para aprovechar esta propiedad incluye [7], donde los autores intentaron abordar este problema mediante técnicas de modelado de usuarios.6. Conclusiones En este documento, se desarrolla un enfoque basado en el aprendizaje de refuerzo para mejorar el rendimiento de los algoritmos de búsqueda IR distribuidos. En particular, los agentes mantienen estimaciones, a saber, la utilidad esperada, en la capacidad de los agentes aguas abajo de proporcionar documentos relevantes para consultas entrantes. Estas estimaciones se actualizan gradualmente al aprender de la información de comentarios devuelta de las sesiones de búsqueda anteriores. Según la información actualizada de la utilidad esperada, los agentes modifican sus políticas de enrutamiento. Posteriormente, estos agentes enrutan las consultas en función de las políticas aprendidas y actualizan las estimaciones sobre la utilidad esperada en función de las nuevas políticas de enrutamiento. Los experimentos en dos conjuntos de datos IR distribuidos diferentes ilustran que el enfoque de aprendizaje de refuerzo mejora considerablemente la utilidad acumulada con el tiempo.7. Referencias [1] S. Abdallah y V. Lesser. Aprender el juego de asignación de tareas. En AAMAS 06: Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagentes, Páginas 850-857, Nueva York, NY, EE. UU., 2006. ACM Press.[2] J. A. Boyan y M. L. Littman. Enrutamiento de paquetes en redes cambiantes dinámicamente: un enfoque de aprendizaje de refuerzo. En Avances en Sistemas de Procesamiento de Información Neural, Volumen 6, páginas 671-678. Morgan Kaufmann Publishers, Inc., 1994. [3] J. C. French, A. L. Powell, J. P. Callan, C. L. Viles, T. Emmitt, K. J. Prey e Y. Mou. Comparando el rendimiento de los algoritmos de selección de la base de datos. En investigación y desarrollo en recuperación de información, páginas 238-245, 1999. [4] B. Horling, R. Mailler y V. Lesser. Granja: un entorno escalable para el desarrollo y evaluación de múltiples agentes. En Avances en Ingeniería de Software para sistemas de múltiples agentes, páginas 220-237, Berlín, 2004. Springer-Verlag.[5] M. Littman y J. Boyan. Un esquema de aprendizaje de refuerzo distribuido para el enrutamiento de red. En Actas del Taller Internacional sobre Aplicaciones de Redes Neurales a Telecomunicaciones, 1993. [6] J. Lu y J. Callan. Búsqueda federada de bibliotecas digitales basadas en texto en redes jerárquicas de igual a igual. En Ecir05, 2005. [7] J. Lu y J. Callan. Modelado de usuarios para la búsqueda federada de texto completo en redes entre pares. En ACM Sigir 2006. ACM Press, 2006. [8] C. D. Manning y H. Sch¨utze. Fundamentos del procesamiento estadístico del lenguaje natural. The MIT Press, Cambridge, Massachusetts, 1999. [9] C. R. Palmer y J. G. Steffan. Generando topologías de red que obedecen las leyes de poder. En Actas de Globecom 2000, noviembre de 2000. [10] K. Sripanidkulchai, B. Maggs y H. Zhang. Ubicación de contenido eficiente utilizando localidad basada en intereses en sistemas de tope de pares. En Infocom, 2003. [11] D. Subramanian, P. Druschel y J. Chen. Hormigas y aprendizaje de refuerzo: un estudio de caso en el enrutamiento en redes dinámicas. En Actas de la Decimocoria Conferencia Conjunta Internacional sobre Inteligencia Artificial, páginas 832-839, 1997. [12] J. N. Tao y L. Weaver. Un enfoque de gradiente de políticas de múltiples agentes para el enrutamiento de redes. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Autor, 2001. [13] H. Zhang, W. B. Croft, B. Levine y V. Lesser. Un enfoque de múltiples agentes para la recuperación de información entre pares. En Actas de la Tercera Conferencia Conjunta Internacional sobre Agentes Autónomos y Sistemas de Aguos de Multi-Agentes, julio de 2004. [14] H. Zhang y V. Lesser. Sistemas de recuperación de información entre pares basados en múltiples agentes con sesiones de búsqueda concurrentes. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas de Aguos Multi-Agentes, mayo de 2006. [15] H. Zhang y V. R. Lesser. Una organización de agentes jerárquicos formados dinámicamente para un sistema de intercambio de contenido distribuido. En 2004, la Conferencia Internacional IEEE/WIC/ACM sobre tecnología de agentes inteligentes (IAT 2004), 20-24 de septiembre de 2004, Beijing, China, páginas 169-175. IEEE Computer Society, 2004. 238 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07)