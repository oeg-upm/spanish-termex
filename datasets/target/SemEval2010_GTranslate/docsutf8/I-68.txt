Sobre técnicas oportunistas para resolver procesos descentralizados de decisión de Markov con restricciones temporales Janusz Marecki y Milind Tambe de Departamento de Ciencias de la Computación Universidad del Sur de California 941 W 37th Place, Los Ángeles, CA 90089 {Marecki, Tambe}@usc.Edu Resumen de la decisión de decenticación de Markov (Dec. (-MDPS) son un modelo popular de problemas de coordinación de agentes en dominios con incertidumbre y limitaciones de tiempo, pero muy difíciles de resolver. En este documento, mejoramos un método de solución heurística de última generación para Dec-MDP, llamado OC-DEC-MDP, que recientemente se ha demostrado que se reduce a Dec-MDPS más grandes. Nuestro método de solución heurística, llamada propagación de la función de valor (VFP), combina dos mejoras ortogonales de OC-DEC-MDP. Primero, acelera OC-DECMDP por un orden de magnitud manteniendo y manipulando una función de valor para cada estado (en función del tiempo) en lugar de un valor separado para cada par de sate e intervalo de tiempo. Además, logra mejores cualidades de solución que OC-DEC-MDP porque, como muestran nuestros resultados analíticos, no sobreestima la recompensa total esperada como OC-DP-DP. Probamos ambas mejoras de forma independiente en un dominio de gestión de crisis, así como para otros tipos de dominios. Nuestros resultados experimentales demuestran una aceleración significativa de VFP sobre OC-DEC-MDP, así como cualidades de solución más altas en una variedad de situaciones. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial]: Sistemas de inteligencia artificiales distribuidos Algoritmos generales de sistemas generales, Teoría 1. Introducción El desarrollo de algoritmos para la coordinación efectiva de múltiples agentes que actúan como un equipo en dominios inciertos y críticos en el tiempo se ha convertido recientemente en un campo de investigación muy activo con posibles aplicaciones que van desde la coordinación de agentes durante una misión de rescate de rehenes [11] hasta la coordinación de autónomos.Marte Exploration Rovers [2]. Debido a las características inciertas y dinámicas de tales dominios, los modelos teóricos de decisión han recibido mucha atención en los últimos años, principalmente gracias a su expresividad y la capacidad de razonar sobre la utilidad de las acciones a lo largo del tiempo. Los modelos teóricos de decisión clave que se han vuelto populares en la literatura incluyen procesos descentralizados de decisión de Markov (DECMDP) y procesos de decisión de Markov descentralizados y parcialmente observables (DEC-POMDPS). Desafortunadamente, se ha demostrado que resolver estos modelos de manera óptima es completar NEXP [3], por lo tanto, las subclases más manejables de estos modelos han sido objeto de investigaciones intensivas. En particular, la red distribuyó el POMDP [13] que supone que no todos los agentes interactúan entre sí, la transición Dec-MDP independiente [2] que supone que la función de transición se descompone en funciones de transición locales o DEC-MDP con interacciones impulsadas por eventos [1 1] que suponen que las interacciones entre los agentes ocurren en puntos de tiempo fijos constituyen buenos ejemplos de tales subclases. Aunque los algoritmos óptimos a nivel mundial para estas subclases han demostrado resultados prometedores, los dominios en los que se ejecutan estos algoritmos siguen siendo pequeños y los horizontes temporales están limitados a solo unos pocos tiempos. Para remediar eso, se han propuesto algoritmos localmente óptimos [12] [4] [5]. En particular, el costo de oportunidad Dec-MDP [4] [5], denominado OC-DEC-MDP, es particularmente notable, ya que se ha demostrado que se reduce a dominios con cientos de tareas y horizontes temporales de dos dígitos. Además, OC-DEC-MDP es único en su capacidad para abordar las restricciones temporales y las duraciones de ejecución de métodos inciertos, que es un factor importante para los dominios del mundo real. OC-DEC-MDP puede escalar a tales dominios principalmente porque en lugar de buscar la solución globalmente óptima, lleva a cabo una serie de iteraciones de políticas;En cada iteración realiza una iteración de valor que reutiliza los datos calculados durante la iteración de política anterior. Sin embargo, OC-DEC-MDP sigue siendo lento, especialmente porque el horizonte temporal y el número de métodos abordan grandes valores. La razón de los altos tiempos de ejecución de OC-DEC-MDP para dichos dominios es una consecuencia de su enorme espacio de estado, es decir, OC-DEC-MDP introduce un estado separado para cada posible par de intervalo de ejecución de métodos y métodos. Además, OC-DEC-MDP sobreestima la recompensa que un método espera recibir para habilitar la ejecución de métodos futuros. Esta recompensa, también conocida como el costo de oportunidad, juega un papel crucial en la toma de decisiones de los agentes, y como mostramos más adelante, su sobreestimación conduce a políticas altamente subóptimas. En este contexto, presentamos VFP (= valor de la función de valor), una técnica de solución eficiente para el modelo DEC-MDP con restricciones temporales y duraciones de ejecución de métodos inciertos, que se basa en el éxito de OC-DEC-MDP. VFP presenta nuestras dos ideas ortogonales: primero, de manera similar a [7] [9] y [10], mantenemos 830 978-81-904262-7-5 (RPS) c 2007 Ifaamas y manipulamos una función de valor con el tiempo para cada métodoen lugar de un valor separado para cada par de método e intervalo de tiempo. Dicha representación nos permite agrupar los puntos de tiempo para los cuales la función de valor cambia a la misma velocidad (= su pendiente es constante), lo que resulta en una propagación rápida y funcional de las funciones de valor. En segundo lugar, demostramos (tanto teóricamente como empíricamente) que el OC-DP-DP sobreestima el costo de oportunidad, y para remediar eso, presentamos un conjunto de heurísticas que corrigen el problema de sobreestimación del costo de oportunidad. Este documento está organizado de la siguiente manera: en la Sección 2 motivamos esta investigación mediante la introducción de un dominio de rescate civil donde un equipo de brigadas de bomberos debe coordinar para rescatar a los civiles atrapados en un edificio en llamas. En la Sección 3 proporcionamos una descripción detallada de nuestro modelo DEC-MDP con restricciones temporales y en la Sección 4 discutimos cómo se pueden resolver los problemas codificados en nuestro modelo utilizando solucionadores globalmente óptimos y localmente óptimos. Las secciones 5 y 6 discuten las dos mejoras ortogonales al algoritmo OC-DEC-MDP de última generación que implementa nuestro algoritmo VFP. Finalmente, en la sección 7 demostramos empíricamente el impacto de nuestras dos mejoras ortogonales, es decir, mostramos que: (i) las nuevas heurísticas corrigen el problema de sobreestimación de costos de oportunidad que conducen a políticas de mayor calidad y (ii) al permitir una compensación sistemáticaDe calidad de solución para el tiempo, el algoritmo VFP funciona mucho más rápido que el algoritmo OC-DEC-MDP 2. Ejemplo motivador Estamos interesados en dominios donde múltiples agentes deben coordinar sus planes con el tiempo, a pesar de la incertidumbre en la duración y el resultado de la ejecución del plan. Un dominio de ejemplo es el desastre a gran escala, como un fuego en un rascacielos. Debido a que puede haber cientos de civiles dispersos en numerosos pisos, se deben enviar múltiples equipos de rescate, y los canales de comunicación por radio pueden ser saturados e inútiles. En particular, los pequeños equipos de brigadas de bomberos deben enviarse en misiones separadas para rescatar a los civiles atrapados en docenas de diferentes lugares. Imagine un pequeño plan de misión de la Figura (1), donde se les asignó a tres bienes de fuego una tarea para rescatar a los civiles atrapados en el Sitio B, a los que se accede desde el Sitio A (por ejemplo, una oficina a la que se accede desde el piso) 1. Los procedimientos generales de lucha contra incendios implican ambos: (i) sacando las llamas y (ii) ventilar el sitio para dejar que los gases tóxicos y a alta temperatura escapen, con la restricción de que la ventilación no debe realizarse demasiado rápido para evitar el incendioextensión. El equipo estima que los civiles tienen 20 minutos antes de que el incendio en el Sitio B se vuelva insoportable, y que el incendio en el Sitio A tiene que ser presentado para abrir el acceso al Sitio B. Como ha sucedido en el pasado en desastres a gran escala, la comunicación a menudo se descompone;y por lo tanto asumimos en este dominio que no hay comunicación entre las brigadas de fuego 1,2 y 3 (denotadas como FB1, FB2 y FB3). En consecuencia, FB2 no sabe si ya es seguro ventilar el sitio A, FB1 no sabe si ya es seguro ingresar al sitio A y comenzar a combatir el fuego en el Sitio B, etc. Asignamos la recompensa 50 por evacuar a los civiles del sitio B, y una recompensa más pequeña 20 por la ventilación exitosa del Sitio A, ya que los mismos civiles podrían lograr el sitio B. Uno puede ver claramente el dilema, que FB2 enfrenta: solo puede estimar las duraciones del incendio de la lucha en el sitio, un métodos para ser ejecutados por FB1 y FB3, y al mismo tiempo FB2 sabe que el tiempo se está agotando para los civiles. Si FB2 ventila el sitio A demasiado temprano, el fuego se extenderá fuera de control, mientras que si FB2 espera con el método de ventilación durante demasiado tiempo, el fuego en el Sitio B se volverá insoportable para los civiles. En general, los agentes tienen que realizar una secuencia de tales 1, explicamos el EST y dejamos la notación en la Sección 3 Figura 1: Dominio de rescate civil y un plan de misión. Las flechas punteadas representan restricciones de precedencia implícitas dentro de un agente.decisiones difíciles;En particular, el proceso de decisión de FB2 implica primero elegir cuándo comenzar la ventilación del sitio A, y luego (dependiendo del tiempo que llevó ventilar el sitio A), eligiendo cuándo comenzar a evacuar a los civiles del sitio B. Tal secuencia de decisiones constituye la política de un agente, y debe encontrarse rápidamente porque el tiempo se está agotando.3. Descripción del modelo ENCODEDO nuestros problemas de decisión en un modelo al que nos referimos como MDP descentralizado con restricciones temporales 2. Cada instancia de nuestros problemas de decisión puede describirse como una tupla m, a, c, p, r donde m = {mi} | m |i = 1 es el conjunto de métodos, y a = {ak} | a |K = 1 es el conjunto de agentes. Los agentes no pueden comunicarse durante la ejecución de la misión. Cada agente AK se asigna a un conjunto MK de métodos, de modo que s | a |k = 1 mk = my ∀i, j; i = jmi ∩ mj = Ø. Además, cada método de agente AK puede ejecutarse solo una vez, y el agente AK puede ejecutar solo un método a la vez. Los tiempos de ejecución del método son inciertos y p = {pi} | m |i = 1 es el conjunto de distribuciones de duraciones de ejecución de métodos. En particular, Pi (t) es la probabilidad de que la ejecución del método mi consuma tiempo t.C es un conjunto de restricciones temporales en el sistema. Los métodos se ordenan parcialmente y cada método tiene ventanas de tiempo fijas dentro de las cuales se puede ejecutar, es decir, C = C≺ ∪ C [] donde C≺ es el conjunto de restricciones predecesoras y C [] es el conjunto de restricciones de ventana de tiempo. Para c ∈ C≺, c = mi, MJ significa que el método MI precede al método MJ, es decir, la ejecución de MJ no puede comenzar antes de que MI termine. En particular, para un agente AK, todos sus métodos forman una cadena vinculada por restricciones predecesoras. Suponemos que el gráfico G = M, C≺ es acíclico, no tiene nodos desconectados (el problema no puede descomponerse en subproblemas independientes), y sus vértices de origen y sumidero identifican los métodos de fuente y sumidero del sistema. Para c ∈ C [], c = mi, est, medios que la ejecución de MI solo puede comenzar después de la hora de inicio más temprana y debe terminar antes de la última hora de finalización.Permitimos que los métodos tengan múltiples restricciones de ventana de tiempo disjunto. Aunque las distribuciones PI pueden extenderse a los horizontes de tiempo infinitos, dadas las limitaciones de la ventana de tiempo, el horizonte de planificación δ = max m, τ, τ ∈C [] τ se considera como la fecha límite de la misión. Finalmente, r = {ri} | m |i = 1 es el conjunto de recompensas no negativas, es decir, RI se obtiene tras la ejecución exitosa de MI. Dado que no se permite la comunicación, un agente solo puede estimar las probabilidades de que sus métodos ya se habilitan 2 también se podría usar el marco OC-DEC-MDP, que modela las restricciones de tiempo y recursos de la Sexta INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 831 por otros agentes. En consecuencia, si mj ∈ Mk es el siguiente método que el agente es ejecutar y la hora actual es t ∈ [0, Δ], el agente debe tomar una decisión si ejecutar el método MJ (denotado como E), oesperar (denotado como w). En caso de que el agente AK decida esperar, permanece inactivo para un pequeño tiempo arbitrario y reanuda la operación en el mismo lugar (= a punto de ejecutar el método MJ) en el momento t +. En caso de que el agente AK decida ejecutar el siguiente método, son posibles dos resultados: éxito: el agente AK recibe recompensa RJ y avanza a su siguiente método (si dicho método existe) siempre que las siguientes condiciones se mantengan: (i) Todas las todas las condicionesMétodos {mi |mi, mj ∈ C≺} que habilita directamente el método MJ ya se ha completado, (ii) la ejecución del método MJ comenzó en una ventana de tiempo de método MJ, es decir, ∃ Mj, τ, τ ∈C [] tales que t ∈ [τ, τ] y (iii) ejecución del método MJ terminó dentro de la misma ventana de tiempo, es decir, el agente AK Método completado MJ en tiempo menor o igual a τ - t.Falla: si alguna de las condiciones mencionadas anteriormente no se mantiene, el agente AK detiene su ejecución. Otros agentes pueden continuar su ejecución, pero los métodos mk ∈ {m |MJ, M ∈ C≺} nunca se habilitará. La política πk de un agente AK es una función πk: mk × [0, Δ] → {w, e} y πk (m, t) = a medias, que si ak está en el método m en el tiempo t, lo haráElija realizar la acción a. Una política conjunta π = [πk] | a |K = 1 se considera óptimo (denotado como π ∗), si maximiza la suma de las recompensas esperadas para todos los agentes.4. Técnicas de solución 4.1 Algoritmos óptimos Política conjunta óptima π ∗ generalmente se encuentra utilizando el principio de actualización de Bellman, es decir, para determinar la política óptima para el método MJ, las políticas óptimas para los métodos mk ∈ {m |se usan mj, m ∈ C≺}. Desafortunadamente, para nuestro modelo, la política óptima para el método MJ también depende de las políticas para los métodos mI ∈ {m |m, mj ∈ C≺}. Esta doble dependencia resulta del hecho de que la recompensa esperada por comenzar la ejecución del método MJ en el momento t también depende de la probabilidad de que el método MJ se habilite por el tiempo t.En consecuencia, si el tiempo se discretiza, uno debe considerar δ | M |Políticas candidatas para encontrar π ∗. Por lo tanto, los algoritmos óptimos globalmente utilizados para resolver problemas del mundo real es poco probable que terminen en un tiempo razonable [11]. La complejidad de nuestro modelo podría reducirse si consideramos su versión más restringida;En particular, si cada método se permitiera habilitar en los puntos de tiempo t ∈ Tj ⊂ [0, δ], se podría usar el algoritmo de conjunto de cobertura (CSA) [1]. Sin embargo, la complejidad de CSA es doble exponencial en el tamaño de Ti, y para nuestros dominios TJ puede almacenar todos los valores que van desde 0 a δ.4.2 Algoritmos localmente óptimos Después de la aplicabilidad limitada de los algoritmos óptimos globalmente para DEC-MDP con restricciones temporales, los algoritmos localmente óptimos parecen más prometedores. Especialmente, el algoritmo OC-DEC-MDP [4] es particularmente significativo, ya que ha demostrado escalar fácilmente a dominios con cientos de métodos. La idea del algoritmo OC-DECMDP es comenzar con la primera política de tiempo de inicio π0 (según el cual un agente comenzará a ejecutar el método M tan pronto como M tenga una posibilidad no cero de estar habilitado), y luego lo mejorará.iterativamente, hasta que no sea posible una mejora adicional. En cada iteración, el algoritmo comienza con alguna política π, que determina de manera única las probabilidades Pi, [τ, τ] que el método MI se realizará en el intervalo de tiempo [τ, τ]. Luego realiza dos pasos: Paso 1: se propaga de los métodos de sumidero para obtener los métodos de obtención de los valores vi, [τ, τ], que representan la utilidad esperada para ejecutar el método MI en el intervalo de tiempo [τ, τ]. Esta propagación utiliza las probabilidades Pi, [τ, τ] de la iteración de algoritmo anterior. Llamamos a este paso una fase de propagación de valor. Paso 2: Dados los valores vi, [τ, τ] del paso 1, el algoritmo elige los intervalos de ejecución de métodos más rentables que se almacenan en una nueva política π. Luego propaga las nuevas probabilidades Pi, [τ, τ] de los métodos de origen a los métodos de hundimiento. Llamamos a este paso una fase de propagación de probabilidad. Si la política π no mejora π, el algoritmo termina. Hay dos deficiencias del algoritmo OC-DEC-MDP que abordamos en este documento. Primero, cada uno de los estados OC-DEC-MDP es un par MJ, [τ, τ], donde [τ, τ] es un intervalo de tiempo en el que se puede ejecutar el método MJ. Si bien dicha representación estatal es beneficiosa, ya que el problema puede resolverse con un algoritmo de iteración de valor estándar, desdibuja el mapeo intuitivo de la recompensa total esperada por comenzar la ejecución de MJ en el tiempo T.En consecuencia, si algún método MI permite el método MJ, y los valores VJ, [τ, τ] ∀τ, τ ∈ [0, δ] son conocidos, la operación que calcula los valores vi, [τ, τ] ∀τ, τ, τ, τ∈ [0, δ] (durante la fase de propagación del valor), se ejecuta en el tiempo O (I2), donde I es el número de intervalos de tiempo 3. Dado que el tiempo de ejecución de todo el algoritmo es proporcional al tiempo de ejecución de esta operación, especialmente para Big Time Horizons δ, el algoritmo OC-DECMDP funciona lento. En segundo lugar, si bien OC-DEC-MDP enfatiza en el cálculo preciso de los valores VJ, [τ, τ], no aborda un problema crítico que determina cómo los valores VJ, [τ, τ] se dividen dado que el método MJ tiene múltiplesMétodos habilitadores. Como mostramos más adelante, OC-DEC-MDP divide VJ, [τ, τ] en partes que pueden sobreestimar VJ, [τ, τ] cuando se resume nuevamente. Como resultado, los métodos que preceden al método MJ sobreestiman el valor para habilitar MJ que, como mostramos más adelante, pueden tener consecuencias desastrosas. En las siguientes dos secciones, abordamos ambas deficiencias.5. Propagación de la función de valor (VFP) El esquema general del algoritmo VFP es idéntico al algoritmo OCDEC-MDP, ya que realiza una serie de iteraciones de mejora de políticas, cada una implica una fase de propagación de valor y probabilidad. Sin embargo, en lugar de propagar valores separados, VFP mantiene y propaga las funciones completas, por lo tanto, nos referimos a estas fases como la fase de propagación de la función de valor y la fase de propagación de la función de probabilidad. Para este fin, para cada método mi ∈ M, definimos tres nuevas funciones: la función de valor, denotada como vi (t), que mapea el tiempo t ∈ [0, Δ] a la recompensa total esperada para comenzar la ejecución de método mi entiempo t.Función de costo de oportunidad, denotada como VI (t), que mapea el tiempo t ∈ [0, δ] a la recompensa total esperada por comenzar la ejecución del método MI en el momento t suponiendo que MI esté habilitado. Función de probabilidad, denotada como pi (t), que mapea el tiempo t ∈ [0, δ] a la probabilidad de que el método MI se complete antes del tiempo t.Dicha representación funcional nos permite leer fácilmente la política actual, es decir, si un agente AK está en el método MI en el momento t, entonces esperará siempre que la función de valor vi (t) sea mayor en el futuro. Formalmente: πk (mi, t) = j w if ∃t> t tal que vi (t) <vi (t) e de lo contrario. Ahora desarrollamos una técnica analítica para realizar la función de valor y las fases de propagación de la función de probabilidad.3 De manera similar para la fase de propagación de probabilidad 832, la sexta intl. Conf.Suponga que los agentes autónomos y los sistemas de múltiples agentes (AAMAS 07) 5.1 Fase de propagación de la función de valor, que estamos realizando una fase de propagación de la función de valor durante la cual las funciones de valor se propagan desde los métodos de sumidero a los métodos de origen. En cualquier momento durante esta fase, encontramos una situación que se muestra en la Figura 2, donde se conocen las funciones de costo de oportunidad [VJN] n n = 0 de los métodos [mjn] n n = 0, y el costo de oportunidad VI0 de método se deriva. Sea PI0 la función de distribución de probabilidad del método MI0 Duración de ejecución, y ri0 es la recompensa inmediata por comenzar y completar la ejecución del método Mi0 dentro de un intervalo de tiempo [τ, τ] tal que mi0 τ, τ ∈ C []. La función VI0 se deriva de RI0 y los costos de oportunidad Vjn, i0 (t) n = 1, ..., n de los métodos futuros. Formalmente: vi0 (t) = 8 >> <>>: r τ −t 0 pi0 (t) (ri0 + pn n = 0 vjn, i0 (t + t)) dt si ∃ mi0 τ, τ ∈C []tal que t ∈ [τ, τ] 0 de lo contrario (1) Tenga en cuenta que para t ∈ [τ, τ], si h (t): = ri0 + pn n = 0 vjn, i0 (τ −t) entonces vi0 is es is es is esUna convolución de P y H: Vi0 (t) = (PI0 ∗ H) (τ −t). Suponga por ahora que VJN, i0 representa un costo de oportunidad total, posponiendo la discusión sobre diferentes técnicas para dividir el costo de oportunidad VJ0 en [VJ0, Ik] K K = 0 hasta la sección 6. Ahora mostramos cómo derivar VJ0, i0 (derivación de VJN, i0 para n = 0 sigue el mismo esquema). Figura 2: Fragmento de un MDP del agente AK. Las funciones de probabilidad se propagan hacia adelante (de izquierda a derecha) mientras que las funciones de valor se propagan hacia atrás (derecha a izquierda). Sea V j0, i0 (t) el costo de oportunidad de comenzar la ejecución del método MJ0 en el momento t dado que el método MI0 se ha completado. Se deriva multiplicando VI0 por las funciones de probabilidad de todos los métodos que no sean MI0 que permitan MJ0. Formalmente: V J0, i0 (t) = VJ0 (t) · ky k = 1 pik (t). Donde de manera similar a [4] y [5] ignoramos la dependencia de [PLK] K K = 1. Observe que V J0, i0 no tiene que estar disminuyendo monotónicamente, es decir, retrasar la ejecución del método MI0 a veces puede ser rentable. Por lo tanto, el costo de oportunidad VJ0, i0 (t) de método de habilitación MI0 en el momento t debe ser mayor o igual a V j0, i0. Además, VJ0, i0 debería no aumentar. Formalmente: vj0, i0 = min f∈F f (2) donde f = {f |F ≥ V J0, I0 y F (T) ≥ F (t) ∀t <t}. Sabiendo que la oportunidad cuesta VI0, podemos derivar fácilmente la función de valor VI0. Deje que AK sea un agente asignado al método MI0. Si AK está a punto de comenzar la ejecución de MI0, significa que AK debe haber completado su parte del plan de misión hasta el método MI0. Dado que AK no sabe si otros agentes han completado los métodos [mlk] k = k k = 1, para derivar VI0, tiene que multiplicar VI0 por las funciones de probabilidad de todos los métodos de otros agentes que habilitan MI0. Formalmente: vi0 (t) = vi0 (t) · ky k = 1 plk (t) donde también se ignora la dependencia de [plk] k k = 1. En consecuencia, hemos mostrado un esquema general sobre cómo propagar las funciones de valor: saber [vjn] n n = 0 y [vjn] n n = 0 de métodos [mjn] n n = 0 podemos derivar vi0 y vi0 del método mi0. En general, el esquema de propagación de la función de valor comienza con los nodos de sumidero. Luego visita en cada vez un método m, de modo que todos los métodos que M habilitan ya se han marcado como se visitó. La fase de propagación de la función de valor termina cuando todos los métodos de origen se han marcado según lo visitado.5.2 Lectura de la política para determinar la política del agente AK para el método MJ0 Debemos identificar el conjunto ZJ0 de los intervalos [Z, Z] ⊂ [0, ..., δ], de modo que: ∀t∈ [Z,z] πk (MJ0, t) = W. Uno puede identificar fácilmente los intervalos de ZJ0 observando los intervalos de tiempo en los que la función de valor VJ0 no disminuye monotónicamente.5.3 Fase de propagación de la función de probabilidad Suponga ahora que las funciones de valor y los valores de los costos de oportunidad se han propagado de métodos de sumidero a nodos de origen y los conjuntos ZJ para todos los métodos mj ∈ M se han identificado. Dado que la fase de propagación de la función de valor estaba utilizando probabilidades Pi (t) para métodos mi ∈ M y tiempos t ∈ [0, δ] encontrado en la iteración de algoritmo anterior, ahora tenemos que encontrar nuevos valores Pi (t), para preparar el algoritmopara su próxima iteración. Ahora mostramos cómo en el caso general (Figura 2) propagamos las funciones de probabilidad hacia adelante a través de un método, es decir, suponemos que las funciones de probabilidad [pik] k k = 0 de los métodos [mik] k k = 0 son conocidos, y lasFunción de probabilidad PJ0 del método MJ0 debe derivarse. Sea PJ0 la función de distribución de probabilidad de la duración de la ejecución del método MJ0, y ZJ0 sea el conjunto de intervalos de inactividad para el método MJ0, encontrado durante la fase de propagación de la función de último valor. Si ignoramos la dependencia de [pik] k k = 0, entonces la probabilidad pj0 (t) de que la ejecución del método mj0 comienza antes del tiempo t por: pj0 (t) = (qk k = 0 pik (τ) si ∃ ∃(τ, τ) ∈ Zj0 s.t. t ∈ (τ, τ) qk k = 0 pik (t) de lo contrario. Dado pj0 (t), la probabilidad pj0 (t) de ese método mj0 se completará por el tiempo t se deriva por: pj0 (t) = z t 0 z t 0 (∂pj0 ∂t) (t) · pj0 (t −t) dt dt (3) que se puede escribir compactamente como ∂pj0 ∂t = pj0 ∗ ∂p J0 ∂t. En consecuencia, hemos mostrado cómo propagar las funciones de probabilidad [PIK] K K = 0 de los métodos [Mik] K K = 0 para obtener la función de probabilidad PJ0 del método MJ0. El general, la fase de propagación de la función de probabilidad comienza con los métodos de origen MSI para los cuales sabemos que psi = 1 ya que están habilitados de forma predeterminada. Luego visitamos en cada vez un método m de tal manera que todos los métodos que permitan el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 833 M ya se han marcado como se lo visitó. La fase de propagación de la función de probabilidad termina cuando todos los métodos de sumidero se han marcado según lo visitado.5.4 El algoritmo De manera similar al algoritmo OC-DEC-MDP, VFP comienza las iteraciones de mejora de políticas con la Política de tiempo de inicio más temprana π0. Luego, en cada iteración: (i) propaga las funciones de valor [vi] | m |i = 1 usando las antiguas funciones de probabilidad [PI] | M |i = 1 de la iteración de algoritmo anterior y establece los nuevos conjuntos [Zi] | M |i = 1 de los intervalos de inactividad del método, y (ii) propaga las nuevas funciones de probabilidad [pi] | m |i = 1 usando los conjuntos recién establecidos [Zi] | M |i = 1. Estas nuevas funciones [pi] | m |I = 1 se usan luego en la siguiente iteración del algoritmo. De manera similar a OC-DEC-MDP, VFP termina si una nueva política no mejora la política de la iteración de algoritmo anterior.5.5 Implementación de operaciones de función Hasta ahora, hemos obtenido las operaciones funcionales para la función de la función de valor y la propagación de la función de probabilidad sin elegir ninguna representación de función. En general, nuestras operaciones funcionales pueden manejar el tiempo continuo, y uno tiene libertad para elegir una técnica de aproximación de función deseada, como aproximación lineal por partes [7] o constante por partes [9]. Sin embargo, dado que uno de nuestros objetivos es comparar VFP con el algoritmo OC-DP-DP existente, que funciona solo para un tiempo discreto, también discretizamos el tiempo y elegimos aproximar las funciones de valor y las funciones de probabilidad con funciones lineales por partes (PWL). Cuando el algoritmo VFP propaga las funciones de valor y las funciones de probabilidad, constantemente lleva a cabo operaciones representadas por las ecuaciones (1) y (3) y ya hemos demostrado que estas operaciones son convoluciones de algunas funciones P (T) y H (t). Si el tiempo está discretizado, las funciones P (t) y H (t) son discretas;Sin embargo, H (t) se puede aproximar bien con una función PWL BH (t), que es exactamente lo que hace VFP. Como resultado, en lugar de realizar multiplicaciones O (Δ2) para calcular F (t), VFP solo necesita realizar multiplicaciones O (k · δ) para calcular F (t), donde k es el número de segmentos lineales de BH (t (t) (Tenga en cuenta que dado que h (t) es monotónico, bh (t) generalmente está cerca de h (t) con k δ). Dado que los valores de PI están en rango [0, 1] y los valores de VI están en el rango [0, p mi∈M ri], sugerimos aproximar VI (t) con BVI (t) dentro del error V y PI (t) conBPI (t) dentro del error p. Ahora demostramos que el error de aproximación general acumulado durante la fase de propagación de la función de valor puede expresarse en términos de P y V: Teorema 1. Sea C≺ un conjunto de restricciones de precedencia de un DEC-MDP con restricciones temporales, y P y V sean la función de probabilidad y los errores de aproximación de la función de valor respectivamente. El error general π = maxv supt∈ [0, δ] | V (t) - bv (t) |de la función de propagación de la función de valor se limita a: | C≺ |V + ((1 + P) | C≺ | - 1) P Mi∈M RI. PRUEBA. Para establecer el límite para π, primero probamos mediante la inducción en el tamaño de C≺, que el error general de la fase de propagación de la función de probabilidad, π (p) = maxp supt∈ [0, δ] | p (t) -−BP (t) |está limitado por (1 + P) | C≺ |- 1. Base de inducción: si n = 1 solo hay dos métodos presentes, y realizaremos la operación identificada por la ecuación (3) solo una vez, introduciendo el error π (p) = p = (1 + p) | C≺ |- 1. Paso de inducción: supongamos que π (p) para | c≺ |= n está limitado por (1 + p) n - 1, y queremos demostrar que esta declaración es válida para | c≺ |= n.Sea g = m, c≺ un gráfico con la mayoría de los bordes n + 1, y g = m, c≺ un subgrafio de g, de modo que c≺ = c≺ - {mi, mj}, donde mj ∈ M esUn nodo de sumidero en G. de la suposición de inducción que tenemos, que C≺ introduce el error de fase de propagación de probabilidad limitado por (1 + P) N - 1. Ahora agregamos el enlace {MI, MJ} a C≺, que afecta el error de una sola función de probabilidad, a saber, PJ, por un factor de (1 + P). Dado que el error de fase de propagación de probabilidad en C≺ se limitó por (1 + P) N - 1, en C≺ = C≺ ∪ {mi, MJ} puede ser como máximo ((1 + p) n - 1) (1 +P) <(1 + p) n + 1 - 1. Por lo tanto, si las funciones de costo de oportunidad no se sobreestiman, están limitadas por p mi∈M ri y el error de una operación de propagación de la función de valor único será como máximo z δ 0 p (t) (v + ((1+ p) |C≺ | −1) x mi∈M ri) dt <v + ((1+ p) | c≺ | −1) x mi∈M ri. Dado que el número de operaciones de propagación de la función de valor es | C≺ |, el error total π de la fase de propagación de la función de valor está limitado por: | C≺ |V + ((1 + P) | C≺ | - 1) P Mi∈M RI.6. Dividir las funciones de costos de oportunidad En la Sección 5, dejamos de lado la discusión sobre cómo la función de costo de oportunidad VJ0 del método MJ0 se divide en las funciones de costo de oportunidad [VJ0, IK] K K = 0 enviado de vuelta a los métodos [Mik] K K = 0,que habilitan directamente el método MJ0. Hasta ahora, hemos adoptado el mismo enfoque que en [4] y [5] en que la función de costo de oportunidad VJ0, IK que el método Mik envía al método MJ0 es una función mínima y no aumentable que domina la función V J0, ik (t) = (vj0 · q k ∈ {0, ..., k} k = k pik) (t). Nos referimos a este enfoque, como heurística H 1,1. Antes de demostrar que esta heurística sobreestima el costo de oportunidad, discutimos tres problemas que pueden ocurrir al dividir las funciones del costo de oportunidad: (i) sobreestimación, (ii) subestimación y (iii) inanición. Considere la situación en la Figura 3: división de la función de valor del método MJ0 entre los métodos [mik] k k = 0.(3) Cuando se realiza la propagación de la función de valor para los métodos [mik] k k = 0. Para cada k = 0, ..., k, la ecuación (1) deriva la función de costo de oportunidad VIK de la recompensa inmediata RK y la función de costo de oportunidad VJ0, IK. Si M0 es los únicos métodos que precede al método MK, entonces V ik, 0 = VIK se propaga al método M0 y, en consecuencia, el costo de oportunidad para completar el método M0 en el momento t es igual a PK K = 0 VIK, 0 (t). Si se sobreestima este costo, entonces un agente A0 al método M0 tendrá demasiado incentivo para finalizar la ejecución de M0 en el momento t.En consecuencia, aunque la probabilidad de P (t) de que M0 estará habilitado por otros agentes por el tiempo T es bajo, el agente A0 aún podría encontrar la utilidad esperada de comenzar la ejecución de M0 en el momento T más alto que la utilidad esperada de hacerlo más tarde. Como resultado, elegirá en el momento t comenzar a ejecutar el método M0 en lugar de esperar, lo que puede tener consecuencias desastrosas. Del mismo modo, si se subestima PK K = 0 VIK, 0 (t), el Agente A0 podría perder el interés en habilitar los métodos futuros [Mik] K K = 0 y simplemente centrarse en 834 el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) maximizando la posibilidad de obtener su recompensa inmediata R0. Dado que esta posibilidad se incrementa cuando el Agente A0 Waits4, considerará en el momento T más rentable esperar, en lugar de comenzar la ejecución de M0, que puede tener consecuencias igualmente desastrosas. Finalmente, si VJ0 se divide de alguna manera, que para algunos k, vj0, ik = 0, es el método Mik el que subestima el costo de oportunidad de habilitar el método MJ0, y se aplica el razonamiento similar. Llamamos a tal problema un hambre del método MK. Esa breve discusión muestra la importancia de dividir la función de costo de oportunidad VJ0 de tal manera que se evita la sobreestimación, la subestimación y el problema de inanición. Ahora demostramos que: Teorema 2. Heuristic H 1,1 puede sobreestimar el costo de oportunidad. PRUEBA. Probamos el teorema mostrando un caso en el que ocurre la sobreestimación. Para el plan de misión de la figura (3), deje que h 1,1 divida VJ0 en [V j0, ik = vj0 · q k ∈ {0, ..., k} k = k pik] k k = 0 enviado a métodos[mik] k k = 0 respectivamente. Además, suponga que los métodos [mik] k k = 0 no proporcionan recompensa local y tienen las mismas ventanas de tiempo, es decir, rik = 0;Estik = 0, letik = δ para k = 0, ..., k. Para demostrar la sobreestimación del costo de oportunidad, debemos identificar t0 ∈ [0, ..., Δ] de modo que la oportunidad coste PK K = 0 VIK(t) Para los métodos [mik] k k = 0 en el tiempo t ∈ [0, .., δ] es mayor que el costo de oportunidad VJ0 (t). De la ecuación (1) tenemos: VIK (T) = Z δ - T 0 PIK (T) VJ0, IK (T + T) DT suma sobre todos los métodos [Mik] K K = 0 Obtenemos: KX K = 0 VIK(t) = kx k = 0 z Δ - t 0 pik (t) vj0, ik (t + t) dt (4) ≥ kx k = 0 z δ - t 0 pik (t) v j0, ik (t +t) dt = kx k = 0 z δ - t 0 pik (t) vj0 (t + t) y k ∈ {0, ..., k} k = k pik (t + t) dt dejar c ∈ (0, 1] ser constante y t0 ∈ [0, Δ] ser tal que ∀t> t0 y ∀k = 0, .., k tenemos q k ∈ {0, ..., k} k = k pik (t)> C Ahora, supongamos que existe T1 ∈ (T0, δ], de modo que PK K = 0 R T1 - T0 0 PIK (T) DT> VJ0 (T0) C · VJ0 (T1). Dado que disminuir el límite superior de la función integral sobre positiva también disminuye la integral, tenemos: KX K = 0 VIK (T0)> C KX K = 0 Z T1 T0 PIK (T - T0) VJ0 (T) DT y desde VJ0(t) no es creciente: tenemos: kx k = 0 vik (t0)> c · vj0 (t1) kx k = 0 z t1 t0 pik (t-t0) dt (5) = c · vj0 (t1) kxk = 0 z t1 - t0 0 pik (t) dt> c · vj0 (t1) vj (t0) c · vj (t1) = vj (t0) 4 suponiendo que 0 en consecuencia, el costo de oportunidad pk k = 0 VIK (t0) de comenzar la ejecución de métodos [mik] k k = 0 en el tiempo t ∈ [0, .., δ] es mayor que el costo de oportunidad VJ0 (T0) que prueba el teorema. Figura 4 muestra que la sobreestimación de la oportunidadEl costo es fácilmente observable en la práctica. Para remediar el problema de la sobreestimación de los costos de oportunidad, proponemos tres heurísticas alternativas que dividen las funciones del costo de oportunidad: • Heuristic H 1,0: solo un método, Mik obtiene la recompensa completa esperada por habilitar el método MJ0, es decir, V J0, IK (IK (t) = 0 para k ∈ {0, ..., k} \ {k} y v j0, ik (t) = (vj0 · q k ∈ {0, ..., k} k = k pik) (t).• Heuristic H 1/2,1/2: cada método [Mik] K K = 0 Obtiene el costo de oportunidad total para habilitar el método MJ0 dividido por el número K de métodos que habilitan el método MJ0, es decir, V J0, IK (T)= 1 k (vj0 · q k ∈ {0, ..., k} k = k pik) (t) para k ∈ {0, ..., k}.• Heuristic BH 1,1: Esta es una versión normalizada de la heurística H 1,1 en que cada método [Mik] K K = 0 inicialmente obtiene el costo de oportunidad total para habilitar el método MJ0. Para evitar la sobreestimación de los costos de oportunidad, normalizamos las funciones divididas cuando su suma excede la función de costo de oportunidad a dividirse. Formalmente: V J0, Ik (T) = 8> <>: V H 1,1 J0, Ik (T) si Pk K = 0 V H 1,1 J0, Ik (T) <VJ0 (T) Vj0 (T) V H1,1 J0, Ik (T) Pk K = 0 V H 1,1 J0, Ik (t) De lo contrario, donde v H 1,1 J0, Ik (t) = (VJ0 · Q k ∈ {0, ..., k} k = k pjk) (t). Para la nueva heurística, ahora probamos que: Teorema 3. Heuristics H 1,0, H 1/2,1/2 y BH 1,1 no sobreestiman el costo de oportunidad. PRUEBA. Cuando Heuristic H 1,0 se usa para dividir la función de costo de oportunidad VJ0, solo un método (por ejemplo, Mik) obtiene el costo de oportunidad para habilitar el método MJ0. Por lo tanto: kx k = 0 VIK (t) = z δ-t 0 pik (t) vj0, ik (t + t) dt (6) y dado que vj0 no aumenta ≤ z Δ-t 0 pik (t) vj0(t + t) · y k ∈ {0, ..., k} k = k pjk (t + t) dt ≤ z Δ - t 0 pik (t) vj0 (t + t) dt ≤ vj0 (t)La última desigualdad también es una consecuencia del hecho de que VJ0 no aumenta. Para la heurística H 1/2,1/2, de manera similar, tenemos: kx k = 0 vik (t) ≤ kx k = 0 z Δ - t 0 pik (t) 1 k vj0 (t + t) y k ∈ {0,..., k} k = k pjk (t + t) dt ≤ 1 k kx k = 0 z Δ - t 0 pik (t) vj0 (t + t) dt ≤ 1 k · k · vj0 (t) = =VJ0 (t). Para BH 1,1 heurística, la función de costo de oportunidad VJ0 está, por definición, dividida de tal manera, que Pk K = 0 VIK (t) ≤ VJ0 (t). En consecuencia, hemos demostrado que nuestra nueva heurística H 1,0, H 1/2,1/2 y BH 1,1 evitan la sobreestimación del costo de oportunidad. El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 835 La razón por la que hemos introducido las tres nuevas heurísticas es la siguiente: dado que H 1,1 sobreestima el costo de oportunidad, uno tiene que elegir qué método recibirá la recompensa por habilitarEl método MJ0, que es exactamente lo que hace la heurística H 1,0. Sin embargo, la heurística H 1,0 deja métodos K - 1 que preceden al método MJ0 sin ninguna recompensa que conduzca a la inanición. Se puede evitar el hambre si las funciones de costo de oportunidad se dividen utilizando H 1/2,1/2 heurística, que proporciona recompensa a todos los métodos habilitadores. Sin embargo, la suma de las funciones de costo de oportunidad dividida para la heurística H 1/2,1/2 puede ser más pequeña que la función de costo de oportunidad de división distinta de cero para la heurística H 1,0, que es claramente indeseable. Dicha situación (Figura 4, Heuristic H 1,0) ocurre porque la media F+G 2 de dos funciones F, G no es más pequeña que F ni G solo si F = G.Es por eso que hemos propuesto la heurística BH 1,1, que por definición evita los problemas de sobreestimación, subestimación y inanición.7. Evaluación de evaluación experimental ya que el algoritmo VFP que presentamos proporciona dos mejoras ortogonales sobre el algoritmo OC-DEC-DP-MDP, la evaluación experimental que realizamos constaba en dos partes: en la Parte 1, probamos empíricamente la calidad de las soluciones queOC-DEC-MDP o VFP) se encuentra, dado que utiliza diferentes funciones de costo de oportunidad dividiendo la heurística, y en la Parte 2, comparamos los tiempos de ejecución de los algoritmos VFP y OC-Dec-MDP para una variedad de configuraciones de planes de misión. Parte 1: Primero ejecutamos el algoritmo VFP en una configuración de plan de misión genérica de la Figura 3, donde solo estaban presentes los métodos MJ0, MI1, MI2 y M0. Las ventanas de tiempo de todos los métodos se establecieron en 400, la duración PJ0 del método MJ0 fue uniforme, es decir, PJ0 (T) = 1 400 y duraciones PI1, PI2 de los métodos MI1, MI2 fueron distribuciones normales, es decir, PI1 = N (μ = 250 = 250, σ = 20), y Pi2 = N (μ = 200, σ = 100). Asumimos que solo el método MJ0 proporcionó recompensa, es decir, RJ0 = 10 fue la recompensa por terminar la ejecución del método MJ0 antes del tiempo t = 400. Mostramos nuestros resultados en la Figura (4) donde el eje X de cada uno de los gráficos representa el tiempo, mientras que el eje y representa el costo de oportunidad. El primer gráfico confirma que cuando la función de costo de oportunidad VJ0 se dividió en las funciones de costo de oportunidad VI1 y VI2 utilizando la heurística H 1,1, la función VI1 +VI2 no siempre estaba por debajo de la función VJ0. En particular, VI1 (280) + VI2 (280) excedió VJ0 (280) en un 69%. Cuando la heurística H 1,0, H 1/2,1/2 y BH 1,1 se usaron (gráficos 2,3 y 4), la función VI1 + VI2 siempre estaba por debajo de VJ0. Luego cambiamos nuestra atención al dominio de rescate civil introducido en la Figura 1 para la cual probamos todas las duraciones de ejecución de acción de la distribución normal n = (μ = 5, σ = 2)). Para obtener la línea de base para el rendimiento heurístico, implementamos un solucionador óptimo globalmente, que encontró una verdadera recompensa total esperada para este dominio (Figura (6a)). Luego comparamos esta recompensa con una recompensa total esperada encontrada por un solucionador localmente óptimo guiado por cada una de las heurísticas discutidas. Figura (6a), que traza sobre el eje Y, la recompensa total esperada de una política complementa nuestros resultados anteriores: H 1,1 Heuristic sobreestimó la recompensa total esperada en un 280%, mientras que las otras heurísticas pudieron guiar el solucionador localmente óptimo.a una verdadera recompensa total esperada. PARTE 2: Luego elegimos H 1,1 para dividir las funciones de costos de oportunidad y realizamos una serie de experimentos destinados a probar la escalabilidad de VFP para varias configuraciones del plan de misión, utilizando el rendimiento del algoritmo OC-DEC-MDP como un punto de referencia. Comenzamos las pruebas de escalabilidad VFP con una configuración de la Figura (5a) asociada con el dominio de rescate civil, para el cual las duraciones de ejecución de métodos se extendieron a distribuciones normales n (μ = Figura 5: Configuraciones del plan de misión: (a) dominio de rescate civil (((b) Cadena de N Métodos, (c) Árbol de N Figura 6: rendimiento de VFP en el dominio de rescate civil.30, σ = 5), y la fecha límite se extendió a δ = 200. Decidimos probar el tiempo de ejecución del algoritmo VFP que se ejecuta con tres niveles diferentes de precisión, es decir, se eligieron diferentes parámetros de aproximación P y V, de modo que el error acumulativo de la solución encontrado por VFP permaneció dentro del 1%, 5% y 10%de la solución encontrada por el algoritmo Oc- DEC-MDP. Luego ejecutamos ambos algoritmos para un total de 100 iteraciones de mejora de políticas. La Figura (6b) muestra el rendimiento del algoritmo VFP en el dominio de rescate civil (el eje Y muestra el tiempo de ejecución en milisegundos). Como vemos, para este pequeño dominio, VFP se ejecuta un 15% más rápido que OCDEC-MDP al calcular la política con un error de menos del 1%. A modo de comparación, la resolución óptima globalmente no terminó dentro de las primeras tres horas de su tiempo de ejecución, lo que muestra la fuerza de los solucionadores oportunistas, como OC-DEC-MDP. Luego decidimos probar cómo se desempeña VFP en un dominio más difícil, es decir, con métodos que forman una cadena larga (Figura (5B)). Probamos cadenas de 10, 20 y 30 métodos, aumentando al mismo tiempo de tiempo en las ventanas de tiempo a 350, 700 y 1050 para garantizar que se puedan alcanzar métodos posteriores. Mostramos los resultados en la Figura (7a), donde variamos en el eje x el número de métodos y trazamos en el eje Y el tiempo de ejecución del algoritmo (observe la escala logarítmica). Como observamos, la ampliación del dominio revela el alto rendimiento de VFP: dentro del error del 1%, se ejecuta hasta 6 veces más rápido que OC-DECMDP. Luego probamos cómo se escala VFP, dado que los métodos están dispuestos en un árbol (Figura (5C)). En particular, consideramos árboles con factor de ramificación de 3, y profundidad de 2, 3 y 4, aumentando al mismo tiempo el horizonte temporal de 200 a 300, y luego a 400. Mostramos los resultados en la figura (7b). Aunque las velocidades son más pequeñas que en el caso de una cadena, el algoritmo VFP todavía funciona hasta 4 veces más rápido que OC-DEC-MDP al calcular la política con un error de menos del 1%. Finalmente probamos cómo VFP maneja los dominios con métodos dispuestos en una malla n × n, es decir, c≺ = {mi, j, mk, j+1} para i = 1, ..., n;k = 1, ..., n;J = 1, ..., n - 1. En particular, consideramos 836 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 4: Visualización de heurísticas para la división de costos de oportunidad. Figura 7: Experimentos de escalabilidad para OC-DEC-MDP y VFP para diferentes configuraciones de red.Métodos de malla de 3 × 3, 4 × 4 y 5 × 5. Para tales configuraciones, debemos aumentar en gran medida el horizonte temporal ya que las probabilidades de habilitar los métodos finales en un tiempo particular disminuyen exponencialmente. Por lo tanto, variamos los horizontes temporales de 3000 a 4000, y luego a 5000. Mostramos los resultados en la Figura (7c) donde, especialmente para mallas más grandes, el algoritmo VFP se extiende hasta un orden de magnitud más rápido que OC-DEC-MDP mientras se encuentra una política que está dentro de menos del 1% de la política encontrada por OC- DECMDP.8. Conclusiones El proceso de decisión descentralizado de Markov (DEC-MDP) ha sido muy popular para el modelado de problemas de coordinación de agentes, es muy difícil de resolver, especialmente para los dominios del mundo real. En este artículo, mejoramos un método de solución heurística de última generación para Dec-MDP, llamado OC-DEC-MDP, que recientemente se ha demostrado que se reduce a grandes Dec-MDP. Nuestro método de solución heurística, llamada propagación de la función de valor (VFP), proporcionó dos mejoras ortogonales de OC-DEC-MDP: (i) aceleró OC-DECMDP por un orden de magnitud al mantener y manipular una función de valor para cada método en lugar deUn valor separado para cada par de método e intervalo de tiempo, y (ii) logró mejores cualidades de solución que OC-DEC-MDP porque corrigió la sobreestimación del costo de oportunidad de OC-DEC-MDP. En términos de trabajo relacionado, hemos discutido ampliamente el algoritmo OCDEC-MDP [4]. Además, como se discutió en la Sección 4, hay algoritmos globalmente óptimos para resolver Dec-MDP con restricciones temporales [1] [11]. Desafortunadamente, no pueden escalar a los dominios a gran escala en la actualidad. Más allá de OC-DEC-MDP, hay otros algoritmos localmente óptimos para Dec-MDPS y DECPOMDPS [8] [12], [13], sin embargo, tradicionalmente no han tratado con tiempos de ejecución inciertos y restricciones temporales. Finalmente, las técnicas de función de valor se han estudiado en el contexto de los MDP de agente único [7] [9]. Sin embargo, de manera similar a [6], no abordan la falta de conocimiento del estado global, que es un tema fundamental en la planificación descentralizada. Agradecimientos Este material se basa en el trabajo respaldado por el Programa de Coordinadores DARPA/IPTO y el Laboratorio de Investigación de la Fuerza Aérea bajo el No. FA875005C0030. Los autores también quieren agradecer a Sven Koenig y a los revisores anónimos por sus valiosos comentarios.9. Referencias [1] R. Becker, V. Lesser y S. Zilberstein. MDP descentralizados con interacciones basadas en eventos. En Aamas, páginas 302-309, 2004. [2] R. Becker, S. Zilberstein, V. Lesser y C. V. Goldman. Procesos de decisión descentralizados de Markov descentralizados independientes de la transición. En Aamas, páginas 41-48, 2003. [3] D. S. Bernstein, S. Zilberstein y N. Inmerman. La complejidad del control descentralizado de los procesos de decisión de Markov. En UAI, páginas 32-37, 2000. [4] A. Beynier y A. Mouaddib. Un algoritmo polinomial para los procesos de decisión de Markov descentralizados con restricciones temporales. En Aamas, páginas 963-969, 2005. [5] A. Beynier y A. Mouaddib. Un algoritmo iterativo para resolver procesos de decisión descentralizados de Markov restringidos. En AAAI, páginas 1089-1094, 2006. [6] C. Boutilier. Optimización y coordinación secuenciales en sistemas múltiples. En IJCAI, páginas 478-485, 1999. [7] J. Boyan y M. Littman. Soluciones exactas para MDP dependientes del tiempo. En NIPS, páginas 1026-1032, 2000. [8] C. Goldman y S. Zilberstein. Optimización del intercambio de información en sistemas cooperativos de múltiples agentes, 2003. [9] L. Li y M. Littman. Aproximación perezosa para resolver MDP continuos de horizón finito. En AAAI, páginas 1175-1180, 2005. [10] Y. Liu y S. Koenig. Planificación sensible al riesgo con funciones de utilidad de un interruptor: iteración de valor. En AAAI, páginas 993-999, 2005. [11] D. Musliner, E. Durfee, J. Wu, D. Dolgov, R. Goldman y M. Boddy. Gestión de planes coordinados utilizando MDP multiagentes. En el Simposio de Primavera AAAI, 2006. [12] R. Nair, M. Tambe, M. Yokoo, D. Pynadath y S. Marsella. Taming POMDP descentralizado: hacia un cálculo de políticas eficiente para configuraciones multiagentes. En Ijcai, páginas 705-711, 2003. [13] R. Nair, P. Varakantham, M. Tambe y M. Yokoo. POMDP distribuidos en red: una sinergia de optimización de restricciones distribuidas y POMDPS. En IJCAI, páginas 1758-1760, 2005. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 837