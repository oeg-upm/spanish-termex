Programación de recursos combinatorios para MDP múltiples Dmitri A. Dolgov, Michael R. James y Michael E. Investigación técnica de AI y Robotics Group, Toyota Technical Center USA {Ddolgov, Michael.R.James, Michael.samples}@gmail.comLa programación óptima de los recursos en sistemas múltiples es una tarea computacionalmente desafiante, particularmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre los agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDP). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por los MDP. Sin embargo, este trabajo anterior se ha centrado en los problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDP de Horizon Infinite. Extendemos esos modelos existentes al problema de la programación de recursos combinatorios, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidas), que requieren recursos solo para esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular las asignaciones de recursos óptimas a nivel mundial a los agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio estocástico de cambio de empleo. Categorías y descriptores de sujetos I.2.8 [Inteligencia artificial]: resolución de problemas, métodos de control y búsqueda;I.2.11 [Inteligencia artificial]: Sistemas de inteligencia artificial distribuida Sistemas de términos generales Algoritmos, rendimiento, diseño 1. Introducción Las tareas de asignación y programación de recursos óptimos son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos a un agente no es aditivo (como suele ser el caso con los recursos que son sustitutos o complementos), la función de utilidad podría deberse definirse en un espacio exponencialmente grande de paquetes de recursos queMuy rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que no es cero solo en un pequeño subconjunto de los posibles paquetes de recursos, obtener una asignación óptima aún es computacionalmente prohibitiva, a medida que el problema se convierte en NP-complete [14]. Tales problemas computacionales han generado recientemente varios hilos de trabajo en el uso de modelos compactos de preferencias de agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de manera compacta, a través de, por ejemplo, fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos por los cuales un agente podría utilizar los recursos y definir la función de utilidad como el pago de estos procesos. En particular, si un agente usa recursos para actuar en un entorno estocástico, su función de utilidad puede modelarse naturalmente con un proceso de decisión de Markov, cuyo conjunto de acciones se parametriza con los recursos disponibles. Esta representación se puede usar para construir algoritmos de asignación de recursos muy eficientes que conducen a una aceleración exponencial sobre un problema de optimización sencilla con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre la asignación de recursos con preferencias inducidas por los MDP parametrizados por recursos asume que los recursos solo se asignan una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDP infinitos. Esta suposición de que no es posible una reasignación de recursos puede ser limitante en los dominios donde los agentes llegan y salen dinámicamente. En este documento, ampliamos el trabajo sobre la asignación de recursos bajo las preferencias inducidas por MDP a problemas de programación de tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden usar recursos dentro de estos intervalos. En particular, los agentes llegan y salen en tiempos arbitrarios (predefinidos) y dentro de estos intervalos usan recursos para ejecutar tareas en los MDP de horizones finitos. Abordamos el problema de la programación de recursos a nivel mundial, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximiza la suma de las recompensas esperadas que obtienen. En este contexto, nuestra contribución principal es una formulación de programación de información mixta del problema de programación que elige asignaciones de recursos, tiempos de inicio y de ejecución globalmente óptimos para todos los agentes (dentro de su llegada1220 978-81-904262-7-5 (RPS) C)2007 Intervalos de salida de Ifaamas). Analizamos y comparamos empíricamente dos sabores del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticos dentro de sus MDP de horizones finitos, y otro, donde los recursos pueden reasignarse dinámicamente entre los agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y la declaración de problema formal en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptimo. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de empleo en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método.2. Antecedentes De manera similar al modelo utilizado en trabajos anteriores sobre recursos de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos a un agente como el valor de la mejor política de MDP que es realizable, dados esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo está en los problemas de programación, y una gran parte del problema de optimización es decidir cómo se asignan los recursos en el tiempo entre los agentes con tiempos de llegada y salida finitos, modelamos los agentes que planean problemas como MDP de oraciones finitas., en contraste con los trabajos anteriores que usaron MDP con descuento de Horizon Infinite-Horizon. En el resto de esta sección, primero introducimos algunos antecedentes necesarios sobre los MDP de horizones finitos y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como un punto de referencia de comparación para el nuevo modelo desarrollado aquí.2.1 Procesos de decisión de Markov Un MDP estacionario, de dominio finito y en tiempo discreto (ver, por ejemplo, [13] para un desarrollo exhaustivo y detallado) puede describirse como S, A, P, R, donde: S es un conjunto finitode estados del sistema;A es un conjunto finito de acciones que están disponibles para el agente;P es una función de transición estocástica estacionaria, donde p (σ | s, a) es la probabilidad de transición al estado σ al ejecutar la acción A en el estado s;r es una función de recompensa estacionaria, donde R (s, a) especifica la recompensa obtenida al ejecutar la acción A en el estado s.Dado dicho MDP, un problema de decisión bajo un horizonte finito T es elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida de los agentes (finitos). La política óptima de los agentes es una función de los estados actuales y la hora hasta el horizonte. Una política óptima para tal problema es actuar con avidez con respecto a la función de valor óptimo, definida de manera recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: V (S, T) = Max a R, A) +X σ p (σ | s, a) v (σ, t + 1), ∀s ∈ S, t ∈ [1, t - 1];v (s, t) = 0, ∀s ∈ S;donde v (s, t) es el valor óptimo de estar en el estado s en el momento t ∈ [1, t]. Esta función de valor óptimo se puede calcular fácilmente utilizando una programación dinámica, lo que lleva a la siguiente política óptima π, donde π (s, a, t) es la probabilidad de ejecutar la acción A en el estado s en el tiempo t: π (s, a, t) = (1, a = argmaxa r (s, a) + p σ p (σ | s, a) v (σ, t + 1), 0, de lo contrario. Lo anterior es la forma más común de calcular la función de valor óptimo (y, por lo tanto, una política óptima) para un MDP de horillo finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (de manera similar al Dual LP para MDP con descuento de Horizon Infinite [13, 6, 7]): Max x s x a r (s, a) x t x (s, a a a a A, t) sujeto a: x a x (σ, a, t + 1) = x s, a p (σ | s, a) x (s, a, t) ∀σ, t ∈ [1, t - 1];X a x (s, a, 1) = α (s), ∀s ∈ S;(1) donde α (s) es la distribución inicial sobre el espacio de estado, y x es la medida de ocupación (no estacionaria) (x (s, a, t) ∈ [0, 1] es el número total esperado de vecesLa acción A se ejecuta en el estado en el momento t). Se obtiene una política óptima (no estacionaria) de la medida de ocupación de la siguiente manera: π (s, a, t) = x (s, a, t)/ x a x (s, a, t) ∀s ∈ S, t∈ [1, t].(2) Tenga en cuenta que el MDP de horizonte finito sin restricciones estándar, como se describió anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier α (s) de distribución inicial). Por lo tanto, se puede obtener una política óptima utilizando una constante α (s)> 0 arbitraria (en particular, α (s) = 1 dará como resultado x (s, a, t) = π (s, a, t)). Sin embargo, para los MDP con restricciones de recursos (como se define a continuación en la Sección 3), no existen políticas uniformemente óptimas en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido por los MDP de horizonte infinito con varios tipos de restricciones [1, 6], y también es válida para nuestro modelo de horizonte finito, que puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]2.2 Programación de recursos combinatorios Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, los MDP de Horizon finito) sería que cada agente enumere todas las posibles asignaciones de recursos sobretiempo y, para cada uno, calcule su valor resolviendo el MDP correspondiente. Luego, cada agente proporcionaría valoraciones para cada posible paquete de recursos con el tiempo a un coordinador centralizado, que calcularía las tareas óptimas de recursos a través del tiempo en función de estas valoraciones. Cuando los recursos se pueden asignar en diferentes momentos a diferentes agentes, cada agente debe enviar valoraciones para cada combinación de posibles horizontes temporales. Deje que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de partida de llegada τ ∈ [τa m, τd m]. Por lo tanto, el agente M ejecutará un MDP con horizonte de tiempo no mayor que tm = τd m - τa m+1. Sea Bτ el horizonte de tiempo global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos τd m <bτ, ∀m ∈ M. El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1221 Para el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDP de oraciones finitas, los agentes proporcionan una valoración para cada paquete de recursos para cada posible horizonte de tiempo (de [1, TM]) que pueden usar. Sea Ω el conjunto de recursos que se asignarán entre los agentes. Un agente obtendrá como máximo un paquete de recursos para uno de los horizontes temporales. Deje que la variable ψ ∈ ψm enumere todos los pares posibles de paquetes de recursos y horizontes temporales para el agente M, por lo que hay 2 | Ω |× valores TM para ψ (el espacio de los paquetes es exponencial en el número de tipos de recursos | ω |). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par ψ (recurso, horizonte de tiempo) a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ se asigna al agente m.Para el tiempo τ y el recurso Ω, la función nm (ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ usa el recurso Ω en el tiempo τ (supuse que los agentes tienen requisitos de recursos binarios). Este problema de asignación es completado NP, incluso cuando se considera solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso Ω asignado a todos los agentes no excede la cantidad disponible Bϕ (Ω) puede expresarse como el siguiente programa entero: max x m∈M x ψ∈ψmzψ mvψ m sujeto a: x ψ∈ no zψ m ≤ 1, ∀m ∈ M;X m∈M x ψ∈ no Zψ mnm (ψ, τ, Ω) ≤ bϕ (ω), ∀τ ∈ [1, bτ], ∀Ω ∈ ω;(3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total de recursos ω no exceda, en ningún momento, el límite de recursos. Para el problema de programación donde los agentes pueden reasignar dinámicamente los recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Deje que la variable ψ ∈ ψm en este caso enumere todos los paquetes de recursos posibles para los cuales, como máximo, se puede asignar un paquete al agente M en cada paso de tiempo. Por lo tanto, en este caso hay p t∈ [1, tm] (2 | Ω |) t ∼ 2 | ω | tm posibilidades de paquetes de recursos asignados a diferentes ranuras de tiempo, para los horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) puede usarse para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores ψ es exponencial en cada agente que planee Horizon TM, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver estos dos problemas de programación requiere una enumeración y solución de 2 | Ω |Tm (asignación estática) o p t∈ [1, tm] 2 | ω | t (reasignación dinámica) MDP para cada agente, que rápidamente se vuelve intratable con el crecimiento del número de recursos | ω |o el horizonte de tiempo tm.3. Modelo y declaración del problema ahora presentamos formalmente nuestro modelo del problema de reducción de recursos. La entrada del problema consiste en los siguientes componentes: • M, ω, Bϕ, τa m, τd m, bτ son como se definió anteriormente en la Sección 2.2.• {θm} = {s, a, pm, rm, αm} son los MDP de todos los agentes m ∈ M. Sin pérdida de generalidad, suponemos que los espacios de estado y de acción de todos los agentes son los mismos, pero cada uno tiene su propiofunción de transición PM, función de recompensa RM y condiciones iniciales αm.• ϕM: A × Ω → {0, 1} es la asignación de acciones a los recursos para el agente m.ϕm (a, ω) indica si la acción A del agente M necesita recurso Ω. Un agente M que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP cualquier acción A para la cual ϕm (a, Ω) = 0. Asumimos que todos los requisitos de recursos son binarios;Como se discute a continuación en la Sección 6, esta suposición no es limitante. Dada la entrada anterior, el problema de optimización que consideramos es encontrar la maximización óptima globalmente óptima, la suma de las recompensas esperadas de los recursos a los agentes para los pasos de tiempo: δ: τ × m × ω → {0, 1}. Es factible una solución si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: x m Δm (τ, Ω) ≤ bϕ (ω), ∀ω ∈ ω, τ ∈ [1, bτ].(4) Consideramos dos sabores del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre los agentes en cada tiempo dentro de sus vidas. La Figura 1 muestra un problema de programación de recursos con tres agentes M = {M1, M2, M3}, tres recursos ω = {ω1, ω2, ω3}, y un horizonte de problemas global de Bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Se muestra una solución a este problema a través de barras horizontales dentro de cada caja de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1A muestra una solución a un problema de programación estática. Según la solución mostrada, el agente M1 comienza la ejecución de su MDP en el momento τ = 1 y tiene un bloqueo en los tres recursos hasta que termina la ejecución en el momento τ = 3. Tenga en cuenta que el Agente M1 renuncia a su control sobre los recursos antes de su tiempo de salida anunciado de τd M1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el momento τ = 4, los recursos ω1 y ω3 se asignan al Agente M2, quien los usa para ejecutar su MDP (utilizando solo acciones soportadas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente M3 contiene el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden reasignarse entre los agentes en cada paso de tiempo. Por ejemplo, el agente M1 da el uso de recursos ω2 en el momento τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Observe que un agente no puede detener y reiniciar su MDP, por lo que el agente M1 solo puede continuar ejecutándose en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren ningún recurso (ϕm (a, Ω)= 0). Claramente, el modelo y el estado del problema descrito anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseadas. Discutimos algunos de esos supuestos y sus implicaciones en la Sección 6. 1222 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) Asignaciones de recursos estáticos (las asignaciones de recursos son constantes dentro de los agentesLifetimes; b) Asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo).4. Programación de recursos de nuestro algoritmo de programación de recursos en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta el agente MDP;Este proceso se describe en la Sección 4.1. En segundo lugar, utilizando estos MDP aumentados construimos un problema de optimización global, que se describe en la Sección 4.2.4.1 Agentes de aumento MDP en el modelo descrito en la sección anterior, suponemos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente deja el sistema. En otras palabras, los MDP no pueden detenerse y reanudarse. Por ejemplo, en el problema que se muestra en la Figura 1A, el Agente M1 libera todos los recursos después del tiempo τ = 3, momento en el cual se detiene la ejecución de su MDP. Del mismo modo, los agentes M2 y M3 solo ejecutan sus MDP en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema global de toma de decisiones es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutar su MDP). Para lograr esto, aumentamos cada MDP de los agentes con dos estados nuevos (los estados de inicio y finalización SB, SF, respectivamente) y una nueva acción de inicio/parada A ∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado de inicio SB hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada A ∗ y las transiciones al espacio de estado del MDP original con la probabilidad de transición que corresponde aLa distribución inicial original α (s). Por ejemplo, en la Figura 1a, para el agente M2 esto sucedería en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente M2 en la Figura 1A), realiza la acción de inicio/parada, lo que lo lleva al estado del sumidero SF en el momento τ = 7. Más precisamente, dado un MDP S, A, PM, RM, αM, definimos un MDP S, A, PM, RM, αM como sigue: S = S ∪ Sb ∪ SF;A = a ∪ a ∗;p (s | sb, a ∗) = α (s), ∀s ∈ S;P (Sb | Sb, a) = 1.0, ∀a ∈ A;p (sf | s, a ∗) = 1.0, ∀s ∈ S;p (σ | s, a) = p (σ | s, a), ∀s, σ ∈ S, a ∈ A;r (sb, a) = r (sf, a) = 0, ∀a ∈ A;r (s, a) = r (s, a), ∀s ∈ S, a ∈ A;α (SB) = 1;α (s) = 0, ∀s ∈ S;donde se supone que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m-1. Esto no afectará la asignación de recursos debido a que las limitaciones de recursos solo se aplican para los estados originales de MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDP aumentados que se muestran en la Figura 2b (que comienza en el estado SB en el momento τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estado: el agente comienza en el estado SB, se transforma al espacio de estado del MDP original y finalmente existe en el estado de sumidero SF. Tenga en cuenta que si quisiéramos modelar un problema en el que los agentes pudieran detener sus MDP en pasos de tiempo arbitrarios (lo que podría ser útil para dominios donde es posible la reasignación dinámica), podríamos lograrlo fácilmente al incluir una acción adicional que pasa de cada estado a sí mismo a sí mismo.con cero recompensa.4.2 MILP para la programación de recursos dado un conjunto de MDP aumentados, como se definió anteriormente, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y debajo, se supone que todos los MDP son los MDP aumentados como se define en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y la aumentamos con limitaciones que aseguran que la asignación de recursos correspondiente entre los agentes y el tiempo sea válida. El problema de optimización resultante luego resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos incrementalmente un programa entero mixto (MILP) que logra esto. En ausencia de restricciones de recursos, los agentes Finitehorizon MDPS son completamente independientes, y la solución globalmente óptima se puede obtener de manera trivial a través del siguiente LP, que es simplemente una agregación de LPS de Hinitehorizon de un solo agente: Max X M X S X A RM (RM (RM (s, a) x t xm (s, a, t) sujeto a: x a xm (σ, a, t + 1) = x s, a pm (σ | s, a) xm (s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, tm - 1];X a xm (s, a, 1) = αm (s), ∀m ∈ M, s ∈ S;(12) donde xm (s, a, t) es la medida de ocupación del agente m, y el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración del aumento de un MDP para permitir tiempos de inicio y detención de variables: a) (izquierda) El MDP de dos estados original con un soloacción;(derecha) el MDP aumentado con los nuevos estados SB y SF y la nueva acción a ∗ (tenga en cuenta que las transiciones de origianl no se cambian en el proceso de aumento);b) El MDP aumentado se muestra como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada. Función objetivo (suma de recompensas esperadas sobre todos los agentes) Max x m x s x a rm (s, a) x t xm (s, a, t) (5) Significa implicaciones de restricciones lineales unte x a θ. El agente solo está activo cuando la medida de ocupación no es cero en los estados originales de MDP.θm (τ) = 0 = ⇒ xm (s, a, τ −τa m+1) = 0 ∀s /∈ {sb, sf}, a ∈ A x s /∈ {sb, sf} x a xm (s,a, t) ≤ θm (τa m + t - 1) ∀m ∈ M, ∀t ∈ [1, tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm (τ) = =0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) no puede usar recursos cuando no está activo θm (τ) = 0 = ⇒ ΔM (τ, ω) = 0 ∀τ ∈ [0, bτ],ω ∈ ω ΔM (τ, ω) ≤ θm (τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ ω (8) Ate x a δ (fuerzas de cero x correspondientes δ para ser nonoss)) Δm (τ, ω) = 0, ϕm (a, ω) = 1 = ⇒ xm (s, a, τ - τa m + 1) = 0 ∀s /∈ {sb, sf} 1 /| a |X a ϕm (a, ω) x s/∈ {sb, sf} xm (s, a, t) ≤ Δm (t + τa m - 1, ω) ∀m ∈ M, ω ∈ ω, t ∈ [1, Tm] (9) límites de recursos x m Δm (τ, ω) ≤ bϕ (Ω) ∀ω ∈ ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programar con tareas estáticas.θm (τ) = 1 y θm (τ + 1) = 1 = ⇒ ΔM (τ, ω) = ΔM (τ + 1, ω) ΔM (τ, ω) - z (1 - θm (τ + 1))≤ Δm (τ + 1, Ω) + z (1 - θm (τ)) Δm (τ, ω) + z (1 - θm (τ + 1)) ≥ ΔM (τ + 1, ω) - z (1- θm (τ)) ∀m ∈ M, Ω ∈ ω, τ ∈ [0, Bτ] (11) Tabla 1: MILP para la programación de recursos óptimo globalmente. Tm = τd m - τa m + 1 es el horizonte temporal para los agentes MDP. Utilizando este LP como base, lo aumentamos con limitaciones que aseguran que el uso de recursos implícito por las medidas de ocupación de los agentes {XM} no viole los requisitos de recursos globales Bϕ en ningún momento τ ∈ [0, bτ]. Para formular estas restricciones de recursos, usamos las siguientes variables binarias: • ΔM (τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], Ω ∈ ω, que sirve como variables indicadoras queDefina si el agente M posee recursos Ω en el momento τ. Estas son análogas a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6].• θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutar su MDP) en el momento τ. El significado de las variables de uso de recursos δ se ilustra en la Figura 1: ΔM (τ, ω) = 1 Solo si el recurso Ω se asigna al agente M en el momento τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente M está en el estado de inicio SB o en el estado de finalización SF, el correspondiente θm = 0, pero una vez que el agente se activa y entra en uno de los otros estados, nosotros, nosotros, nosotrosestablecer θm = 1. Este significado de θ puede aplicarse con una restricción lineal que sincroniza los valores de la ocupación de los agentes medidas XM y la actividad 1224 la sexta intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que tenemos que agregar porque los indicadores de actividad θ se definen en la línea de tiempo global τ-IS para hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de partida de llegada. Esto se logra por restricción (7) en la Tabla 1. Además, los agentes no deberían usar recursos mientras están inactivos. Esta restricción también se puede aplicar a través de una desigualdad lineal en θ y δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación XM. De manera similar, debemos asegurarnos de que las variables de uso de recursos δ también se sincronizen con la medida de ocupación XM. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que hace cumplir el significado de δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda las cantidades de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema donde las asignaciones de recursos son estáticas durante la vida útil de un agente, agregamos una restricción que asegura que las variables de uso de recursos δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde z ≥ 2 es una constante que se usa para desactivar las restricciones cuando θm (τ) = 0 o θm (τ + 1) = 0. Esta restricción no se usa para la formulación de problemas dinámicos, donde los recursos pueden reasignarse entre los agentes en cada paso de tiempo. Para resumir, la Tabla 1 junto con la conservación de las restricciones de flujo de (12) define el MILP que calcula simultáneamente una asignación de recursos óptima para todos los agentes a lo largo del tiempo, así como las políticas MDP de Horizon finitas óptimas que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables y restricciones de optimización. Sea tm = p tm = p m (τa m-τd m + 1) la suma de las longitudes de las ventanas de partida de llegada en todos los agentes. Entonces, el número de variables de optimización es: tm + bτ | m || Ω |+ bτ | m |, tm de los cuales son continuos (xm) y bτ | m || ω |+ bτ | m |son binarios (δ y θ). Sin embargo, observe que todos menos tm | m |del θ se establecen en cero por restricción (7), lo que también obliga inmediatamente a todos menos tm | m || ω |del δ para ser cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: tm + tm | ω |+ bτ | ω |+ bτ | m || ω |. A pesar del hecho de que la complejidad del MILP es, en el peor de los casos, exponencial1 en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) más baja que la del MILP con funciones de utilidad planas, descrita en la Sección 2.2. Este resultado hace eco de las ganancias de eficiencia informadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva delOptimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5. 1 Estrictamente hablando, resolver MILP a la optimización es NPComplete en el número de variables enteras.5. Resultados experimentales Aunque la complejidad de resolver MILP es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILP que permiten que nuestro algoritmo escala bien para los parámetros comunes a los problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio problemático: el problema de la pantalla de reparación utilizado para evaluar empíricamente nuestros algoritmos escalabilidad en términos del número de agentes | M |, el número de recursos compartidos | Ω |qué agentes pueden ingresar y salir del sistema. El problema del taller de reparación es un MDP parametrizado simple que adopta la metáfora de un taller de reparación vehicular. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que producen recompensas solo cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio estatal solo se permiten si el agente posee ciertos recursos que están disponibles públicamente para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede tener los recursos limitados para tomar medidas y obtener recompensas individuales. Cada tarea a completar se asocia con una sola acción, aunque el agente debe repetir la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a las acciones necesarias, un tiempo global durante el cual los agentes pueden llegar y partir, y una longitud máxima para el númeroPasos de tiempo Un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILP en una computadora Pentium4 con 2 GB de RAM. Los ensayos se realizaron en la versión estática y dinámica del problema de reducción de recursos, como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes al conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para las escalas MILP a medida que aumentamos el número de agentes | M |, el horizonte de tiempo global Bτ y el número de recursos | ω |. El aumento del número de agentes conduce a la escala exponencial de complejidad, lo que se espera para un problema completado de NP. Sin embargo, aumentando el límite de tiempo global Bτ o el número total de tipos de recursos | Ω |-mientras mantiene el número de agentes constantes no conducen a una disminución del rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se limitan, lo que también es un fenómeno común para los problemas completos de NP. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios de asignación de recursos óptimos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como se esperaba, ya que la recompensa por la versión dinámica no es siempre menor que la recompensa de la versión estática). Debemos señalar que estos gráficos no deben verse como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas sino a diferentes problemas), sino como observaciones sobre cómo cambian la calidad de las soluciones óptimas a medida que se permite más flexibilidad enla reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en los que las variables de entrada comunes se escalan. Esto permite el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 10 1 10 2 10 3 10 4 Número de agentes | M |Cputime, sec | ω |= 5, τ = 50 dinámica estática 50 100 150 200 10 −2 10 −1 10 0 10 10 1 10 2 10 3 Límite de tiempo global τ CPutime, Sec | M |= 5, | Ω |= 5 dinámica estática 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de recursos | Ω |Cputime, sec | m |= 5, τ = 50 dinámica estática 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de agentes | M |Valor | Ω |= 5, τ = 50 dinámica estática 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Tiempo global Límite τ Valor | M |= 5, | Ω |= 5 dinámica estática 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de recursos | Ω |Valor | M |= 5, τ = 50 dinámica estática Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de la ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de la CPU, y la fila inferior muestra la recompensa conjunta de las políticas de los agentes MDP. Las barras de error muestran los cuartiles 1er y tercero (25% y 75%).2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de agentes | M |Cputime, sec τ = 10 | m |Dinámica estática 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 3 10 4 Número de agentes | M |Cputime, sec | ω |= 2 | m |Dinámica estática 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 3 10 4 Número de agentes | M |Cputime, sec | ω |= 5 | m |Dinámica estática 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de agentes | M |Valor τ = 10 | m |Dinámica estática 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de agentes | M |Valor | Ω |= 2 | m |Dinámica estática 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de agentes | M |Valor | Ω |= 5 | m |Dinámica estática Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda rastrea el rendimiento y el tiempo de la CPU a medida que el número de agentes y la ventana de tiempo global aumentan (bτ = 10 | m |). La columna central y correcta rastrea el rendimiento y el tiempo de la CPU como el número de recursos y el número de agentes aumentan juntos como | ω |= 2 | m |y | Ω |= 5 | m |, respectivamente. Las barras de error muestran los cuartiles 1er y tercero (25% y 75%).1226 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) para explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o el horizonte de tiempo global, mientras mantiene constante la densidad de agente promedio (por unidad de tiempo global) o el número promedio de recursos por agente (que se produce comúnmente en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede usarse para resolver efectivamente problemas de tamaño no trivial.6. Discusión y conclusiones En todo el documento, hemos hecho una serie de suposiciones en nuestro algoritmo de modelo y solución;Discutimos sus implicaciones a continuación.• Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciones a State SF), sale del sistema y no puede devolver. Es fácil relajar esta suposición para los dominios donde los MDP de los agentes pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que transique de un estado dado a sí mismo, y tiene cero recompensa.• Indiferencia a la hora de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de augatización MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por ralentí al asignar una recompensa negativa no cero al estado de inicio SB.• Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm (a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a las asignaciones de recursos no binarias, análogos al procedimiento utilizado en [5].• Agentes cooperativos. El procedimiento de optimización discutido en este documento se desarrolló en el contexto de los agentes cooperativos, pero también se puede utilizar para diseñar un mecanismo para programar recursos entre los agentes interesados. Este procedimiento de optimización se puede incrustar en una subasta Vickreyclarke-Groves, completamente análoga a la forma en que se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se trasladan directamente al dominio de programación discutido en este documento, lo que requiere solo pequeñas modificaciones para tratar con los MDP de Horizon Finite• Los tiempos de llegada y salida deterministas conocidas. Finalmente, hemos asumido que los agentes de llegada y tiempos de salida (τa my τd m) son deterministas y conocidos a priori. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios en los que esta suposición es válida, en muchos casos los agentes llegan y salen dinámicamente y sus tiempos de llegada y salida solo pueden predecirse probabilísticamente, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de los agentes egoístas, esto se convierte en una versión interesante de un problema de diseño de mecanismo en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema de reducción de recursos combinatorios donde los valores de los agentes para posibles asignaciones de recursos están definidos por los MDP de Horizon de Finite. Este resultado extiende el trabajo anterior ([6, 7]) en la asignación estática de recursos de un solo disparo bajo las preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Como tal, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajarse la suposición de la llegada determinista y los tiempos de salida de los agentes es un foco de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus perspicaces comentarios y sugerencias.7. Referencias [1] E. Altman y A. Shwartz. Control adaptativo de las cadenas de Markov restringidas: criterios y políticas. Annals of Operations Research, Número especial sobre los procesos de decisión de Markov, 28: 101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. Resolución de problemas de subasta combinatorios expresados de manera concisa. En Proc.de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Idiomas de licitación para subastas combinatorias. En Proc.de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación y planificación de recursos integrados en entornos múltiples estocásticos. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov libremente acoplados. En Proc.de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para la asignación de recursos en MDP débilmente acoplados. En Proc.de Aamas-05, Nueva York, NY, EE. UU., 2005. ACM Press.[8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por los MDP factorizados. En Proc.de Aamas-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismo y agentes deliberativos. En Proc.de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press.[10] N. Nisan. Licitación y asignación en subastas combinatorias. En Electronic Commerce, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el diseño del mecanismo en línea. En Proc.de la Conferencia Anual de los Decideteseciosidades sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismo en línea aproximadamente eficiente. En Proc.de la Decimoctavo Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinacionales manejables computacionalmente. Management Science, 44 (8): 1131-1147, 1998. [15] T. Sandholm. Un algoritmo para una determinación óptima del ganador en subastas combinatorias. En Proc.de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1227