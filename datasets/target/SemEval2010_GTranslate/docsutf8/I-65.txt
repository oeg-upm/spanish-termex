Modelos gráficos para soluciones en línea para POMDPS interactivos Prashant Doshi Dept. de la Universidad de Informática de Georgia Atenas, GA 30602, EE. UU. Pdoshi@cs.uga.edu Yifeng Zeng Dept. de Computer Science Aalborg Universidad DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Informática Nacional Univ.de Singapur 117543, Singapur chenqy@comp.nus.edu.sg Resumen Desarrollamos una nueva representación gráfica para los procesos de decisión de Markov parcialmente observables interactivos (I POMDPS) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DID) buscan modelar explícitamente la estructura que a menudo está presente en los problemas del mundo real al descomponer la situación en variables casuales y de decisión, y las dependencias entre las variables. Los I-DID generalizan los DID, que pueden verse como representaciones gráficas de POMDPS, a configuraciones multiagentes de la misma manera que los I-POMDP generalizan POMDPS. Los I-DID se pueden usar para calcular la política de un agente en línea como el agente actúa y observa en un entorno poblado por otros agentes interactivos. Usando varios ejemplos, mostramos cómo se pueden aplicar I-Dids y demostrar su utilidad. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial distribuida]: Sistemas Multiagentes Términos generales Teoría 1. Introducción Los procesos de decisión Markov parcialmente observables interactivos (IPOMDPS) [9] proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan POMDPS [13] a entornos múltiples al incluir los modelos computables de otros agentes en el espacio estatal junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluidas sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. Los I POMDP adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco teórico de decisión que toma una perspectiva de toma de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactiva (I-DID) como las representaciones computacionales de I-POMDPS. Los I-DID generalizan los DID [12], que pueden verse como contrapartes computacionales de POMDPS, a la configuración de múltiples giros de la misma manera que los I-POMDP generalizan POMDPS. Los I-Did contribuyen a una línea de trabajo creciente [19] que incluye diagramas de influencia de múltiples agentes (mucamas) [14], y más recientemente, redes de diagramas de influencia (NID) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en los problemas del mundo real al descomponer la situación en variables de casualidad y decisión, y las dependencias entre las variables. Las criadas proporcionan una alternativa a las formas de juego normales y extensas utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y los nodos casuales que capturan la información privada de los agentes. Las criadas analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio Nash explotando la estructura de independencia. NIDS extiende a las criadas para incluir la incertidumbre de los agentes sobre el juego que se juega y sobre los modelos de los otros agentes. Cada modelo es una criada y la red de mucamas se colapsan, abajo, en una sola criada para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Formalismos gráficos como mucamas y NID abren un área prometedora de investigación que tiene como objetivo representar interacciones multiagentes de manera más transparente. Sin embargo, las mucamas proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos se limita a los juegos estáticos de juego único. Las cosas son más complejas cuando consideramos interacciones que se extienden con el tiempo, donde las predicciones sobre otras acciones futuras deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DID abordan esta brecha al permitir la representación de otros modelos de agentes como los valores de un nodo modelo especial. Tanto los modelos de otros agentes como las creencias de los agentes originales sobre estos modelos se actualizan con el tiempo utilizando implementaciones de uso especial. En este artículo, mejoramos la representación preliminar anterior del I-DID que se muestra en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID como multiplexores para representar el nodo modelo y, posteriormente, el I-ID, de manera más transparente. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-Did por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación anterior de la I-DID, la actualización de la creencia de los agentes sobre los modelos de otros a medida que los agentes actúan y reciben observaciones se denotó utilizando un enlace especial llamado enlace de actualización del modelo que conectó los nodos modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo se puede implementar utilizando los enlaces de dependencia tradicionales entre los nodos casuales que constituyen los nodos modelo. El resultado neto es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de implementarse utilizando los algoritmos estándar para resolver DIDS. Mostramos cómo se pueden usar IDID para modelar la incertidumbre de los agentes sobre otros modelos, que pueden ser I-Dids. La solución al I-Did es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogo a los DIDS, los I-DID se pueden usar para calcular la política de un agente en línea como el agente actúa y observa en un entorno poblado por otros agentes interactivos.2. Antecedentes: los POMDP interactivos de IPOMDPS finitamente anidados generalizan POMDPS en entornos múltiples al incluir otros modelos de agentes como parte del espacio estatal [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio estatal interactivo está estratégicamente anidado;Contiene creencias sobre otros modelos de agentes y sus creencias sobre los demás. Por simplicidad de presentación consideramos un agente, I, que está interactuando con otro agente, j. Un i-POMDP finitamente anidado del Agente I con un nivel de estrategia L se define como la tupla: I-POMDPI, L = ISI, L, A, Ti, ωi, Oi, Ri donde: • ISI, L denota un conjunto de interactivosestados definidos como, isi, l = s × mj, l - 1, donde mj, l - 1 = {θj, l - 1 ∪ smj}, para l ≥ 1, e isi, 0 = s, donde s es el conjuntode los estados del entorno físico. Θj, L - 1 es el conjunto de modelos intencionales computables del agente J: θj, l - 1 = bj, l - 1, ˆθj donde el marco, ˆθj = a, ωj, tj, oj, rj, ocj. Aquí, J es Bayes Rational y OCJ es JS Optimity Criterion. SMJ es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo de no información [10] y un modelo de juego ficticio [6], los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estado interactivo a continuación. Isi, 0 = s, θj, 0 = {bj, 0, ˆθj |bj, 0 ∈ δ (isj, 0)} ISI, 1 = S × {θj, 0 ∪ Smj}, θj, 1 = {bj, 1, ˆθj |bj, 1 ∈ δ (isj, 1)}...... Isi, l = s × {θj, l - 1 ∪ smj}, θj, l = {bj, l, ˆθj |bj, l ∈ δ (isj, l)} formulaciones similares de espacios anidados han aparecido en [1, 3].• A = AI × AJ es el conjunto de acciones conjuntas de todos los agentes en el medio ambiente;• Ti: S × A × S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del medio ambiente;• ωi es el conjunto de observaciones del agente I;• Oi: S × A × ωi → [0, 1] da la probabilidad de las observaciones dada el estado físico y la acción articular;• RI: ISI × A → R describe el agente es preferencias sobre sus estados interactivos. Por lo general, solo los estados físicos importarán. El agente es la política es el mapeo, Ω ∗ i → δ (ai), donde ω ∗ i es el conjunto de todas las historias de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede representarse como un mapeo del conjunto de todas las creencias del Agente I a una distribución sobre sus acciones, δ (ISI) → δ (AI).2.1 Actualización de creencias análoga a POMDPS, un agente dentro del marco I-POMDP actualiza su creencia como actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, es la predicción de cómo se debe hacer los cambios en el estado físico en función de su predicción de las acciones JS. En segundo lugar, los cambios en los modelos JS deben incluirse en la actualización de la creencia. Específicamente, si J es intencional, entonces se debe incluir una actualización de las creencias JS debido a su acción y observación. En otras palabras, tengo que actualizar su creencia en función de su predicción de lo que J observaría y cómo J actualizaría su creencia. Si el modelo JS es subintencional, entonces las observaciones probables de JS se agregan al historial de observación contenido en el modelo. Formalmente, tenemos: Pr (ist | AT - 1 I, BT - 1 I, L) = β IST - 1: MT - 1 J = θt J BT - 1 I, L (ist - 1) × AT - 1 JPR (AT - 1 J | θT - 1 J, L - 1) OI (ST, AT - 1 I, AT - 1 J, OT I) × TI (ST - 1, AT - 1 I, AT - 1 J,st) ot J Oj (ST, AT - 1 I, AT - 1 J, OT J) × τ (SEθt J (BT - 1 J, L - 1, AT - 1 J, OT J) - BT J, L−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0 de lo contrario es 0, PR (AT - 1 J | θt - 1 J, L - 1) es la probabilidad de que AT - 1 J ISBayes racional para el agente descrito por el modelo θt - 1 J, L - 1 y SE (·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo JS es subinstendente, ver [9]. Si el Agente J también se modela como un I POMDP, entonces la actualización de creencias invoca la actualización de creencias de JS (a través del término Seθt J (BT-1 J, L-1, At-1 J, OT J)), que a su vez podríaInvoke es la actualización de creencias, etc. Esta recursión en las creencias que anidan en el nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de POMDP.1 Para las ilustraciones de la actualización de creencias, detalles adicionales sobre i-POMDPS y cómo se comparan con otros marcos multiagentes, ver [9].2.2 Valor iteración Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja el pago máximo que el agente puede esperar en este estado de creencia: un (bi, l, θi) = max ai∈Ai is∈Isi, l eri ((is, ai) bi, l (is)+ γ oi∈ωi pr (oi | ai, bi, l) un - 1 (seθi (bi, l, ai, oi), θi) (2) donde, eri (is (is (is (is (is (is (is (is (is (iss, ai) = aj ri (is, ai, aj) pr (aj | mj, l - 1) (ya que es = (s, mj, l - 1)). Ec.2 es una base para la iteración de valor en i-POMDPS. El agente es una acción óptima, a ∗ i, para el caso del horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT (θi), definido como: Opt (bi, l, θi) = argmaxai∈Ai is∈Isi, l eri (is, ai) bi, l (is) +γ oi∈ωi pr (oi | ai, bi, l) un (seθi (bi, l, ai, oi), θi)(3) 3. InteractiveInfluencedAgrams Una extensión ingenua de los diagramas de influencia (ID) a la configuración poblada por múltiples agentes es posible tratando a otros agentes como autómatas, representados usando nodos casuales. Sin embargo, este enfoque supone que las acciones de los agentes se controlan utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactiva (I-IDS) adoptan un enfoque más sofisticado al generalizar las ID para hacerlos aplicables a la configuración compartida con otros agentes que pueden actuar y observar, y actualizar sus creencias.3.1 Sintaxis Además de la oportunidad habitual, la decisión y los nodos de utilidad, los IID incluyen un nuevo tipo de nodo llamado nodo modelo. Mostramos un nivel L I-ID general en la Fig. 1 (a), donde el nodo modelo (MJ, L-1) se denota usando un hexágono. Observamos que la distribución de probabilidad sobre el nodo casual, S y el nodo modelo juntos representa el agente es la creencia sobre sus estados interactivos. Además del Modelo 1, el modelo de nivel 0 es un POMDP: otras acciones de agentes se tratan como eventos exógenos y se doblan en las funciones T, O y R. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 815 Figura 1: (a) Un nivel genérico L I-ID para el agente I situado con otro agente j. El hexágono es el nodo modelo (MJ, L - 1) cuya estructura mostramos en (b). Los miembros del nodo modelo son I-IDS (M1 J, L-1, M2 J, L-1; diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de oportunidad correspondientes (A1 J, A2 J). Dependiendo del valor del nodo, Mod [MJ], la distribución de cada uno de los nodos de posibilidades se asigna al nodo AJ.(c) El I-ID transformado con el nodo modelo reemplazado por los nodos casuales y las relaciones entre ellos.Nodo, I-IDS difieren de las ID al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo modelo y un nodo casual, AJ, que representa la distribución sobre las acciones de otros agentes dado su modelo. En ausencia de otros agentes, el nodo modelo y el nodo Chance, AJ, Vanish e I-ID colapsan en ID tradicionales. El nodo modelo contiene los modelos computacionales alternativos atribuidos por I al otro agente del conjunto, θj, L - 1 ∪ SMJ, donde θj, L - 1 y SMJ se definieron previamente en la Sección 2. Por lo tanto, un modelo en el nodo modelo puede ser un I-ID o ID, y la recursión termina cuando un modelo es una ID o subintencionales. Debido a que el nodo modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDS que cuando se resuelven generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de oportunidad correspondiente, digamos A1 J, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas resolviendo el I-ID (o ID), entonces Pr (AJ ∈ A1 J) = 1| OP T |Si aj ∈ Opt, 0 de lo contrario. Pediendo prestados ideas de trabajos anteriores [8], observamos que el nodo modelo y el enlace de política discontinua que lo conecta al nodo casual, AJ, podría representarse como se muestra en la Fig. 1 (b). El nodo de decisión de cada nivel L-1 I-ID se transforma en un nodo casual, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se asignan probabilidades uniformes en el nodo casual, mientras que el resto se asigna ceroprobabilidad. Los diferentes nodos casuales (A1 J, A2 J), uno para cada modelo, y además, el nodo probable etiquetado Mod [MJ] forman los padres del nodo casual, AJ. Por lo tanto, hay tantos nodos de acción (A1 J, A2 J) en MJ, L - 1, ya que el número de modelos en el apoyo del agente son las creencias. La tabla de probabilidad condicional del nodo casual, AJ, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 J, A2 J) dependiendo del valor de Mod [MJ]. Los valores de Mod [MJ] denotan los diferentes modelos de j. En otras palabras, cuando Mod [MJ] tiene el valor M1 J, L - 1, el nodo casual AJ asume la distribución del nodo A1 J, y AJ asume la distribución de A2 J cuando Mod [MJ] tiene el valor M2 J, L - 1. La distribución sobre el nodo, Mod [MJ], es que el agente es la creencia sobre los modelos de J dado un estado físico. Para más agentes, tendremos tantos nodos modelo como agentes. Observe que la Fig. 1 (b) aclara la semántica del enlace de la política y muestra cómo se puede representar utilizando los enlaces de dependencia tradicionales. En la Fig. 1 (c), mostramos el I-ID transformado cuando el nodo modelo se reemplaza por los nodos casuales y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de política de uso especial, sino que el I-ID está compuesto solo por aquellos tipos de nodos que se encuentran en las ID tradicionales y las relaciones de dependencia entre los nodos. Esto permite que los I-ID se representen e implementen utilizando herramientas de aplicación convencionales que se dirigen a IDS. Tenga en cuenta que podemos ver el nivel L I-ID como un NID. Específicamente, cada uno de los modelos de nivel L - 1 dentro del nodo modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es una identificación tradicional, de lo contrario si l> 1, cada bloque dentro del NID puede ser un NID. Tenga en cuenta que dentro de los I-ID (o IDS) en cada nivel, solo hay un solo nodo de decisión. Por lo tanto, nuestro NID no contiene ninguna criada. Figura 2: A Nivel L I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son las creencias sobre los modelos JS condicionados en un estado físico.3.2 SOLUCIÓN La solución de un I-ID se realiza de manera ascendente, y se implementa de manera recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son ID tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de otros agentes, que se ingresan en los nodos de posibilidad correspondientes que se encuentran en el nodo modelo del ID I Nivel 1. La asignación de los nodos de decisión de los modelos de nivel 0 a los nodos de casualidad se lleva a cabo para que las acciones con el mayor valor en el nodo de decisión se les asigne probabilidades uniformes en el nodo casual, mientras que al resto se le asigna una probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el ID I-ID de nivel 1 se transforma como se muestra en la Fig. 1 (c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, AJ, se pobla de tal manera que el nodo asume la distribución de cada uno de los nodos de posibilidades dependiendo del valor del nodo, Mod [MJ]. Como mencionamos anteriormente, los valores del Nodo Mod [MJ] denotan los diferentes modelos del otro agente, y su distribución es que el agente es la creencia sobre los modelos de J condicionados en el estado físico. El ID ID de Nivel 1 transformado es una identificación tradicional que se puede resolver US816 el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) (a) (b) Figura 3: (a) Un nivel genérico de dos niveles de tiempo L i-Did para el agente I en un entorno con otro agente j. Observe el enlace de actualización del modelo punteado que denota la actualización de los modelos de J y la distribución sobre los modelos a lo largo del tiempo.(b) La semántica del enlace de actualización del modelo.en el método estándar de maximización de utilidad esperado [18]. Este procedimiento se lleva a cabo hasta el nivel L I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Tenga en cuenta que análogo a las ID, los I-ID son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes.4. Diagramas de influencia dinámica interactiva Los diagramas de influencia dinámica interactiva (I-DID) extienden I-IDS (y NIDS) para permitir la toma de decisiones secuenciales en varios pasos de tiempo. Así como las DID son representaciones gráficas estructuradas de POMDPS, los I-DID son los análogos gráficos en línea para i-POMDP finitamente anidados. Los I-Did se pueden usar para optimizar a través de una apariencia finita dadas las creencias iniciales mientras interactúan con otros agentes, posiblemente similares.4.1 Sintaxis representamos un I-Did de tono de tiempo general en la Fig. 3 (a). Además de los nodos modelo y el enlace de política discontinua, lo que diferencia un I-DID de A DO es el enlace de actualización del modelo que se muestra como una flecha punteada en la Fig. 3 (a). Explicamos la semántica del nodo modelo y el enlace de política en la sección anterior;Describimos las actualizaciones del modelo a continuación. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo T, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerde de la Sección 2 que un modelo intencional de agentes incluye su creencia. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente puede recibir cualquiera de | ωj |Posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo | mt j, l - 1 || aj || ωj |modelos. Aquí, | Mt J, L - 1 |es el número de modelos en el paso de tiempo T, | AJ |y | ωj |son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. En segundo lugar, calculamos la nueva distribución sobre los modelos actualizados dada la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que condujo al modelo actualizado. Estos pasos son parte de la actualización de la creencia formalizada por la ecuación.1. En la Fig. 3 (b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel L - 1 atribuidos a J en el paso T del tiempo T da como resultado una acción, y J podría hacer una de las dos observaciones posibles, entonces el nodo del modelo en el paso de tiempo T + 1 contiene cuatro modelos actualizados (MT + 1, 1 J, L - 1, MT+1,2 J, L - 1, MT+1,3 J, L - 1 y MT+1,4 J, L - 1). Estos modelos difieren en sus creencias iniciales, cada uno de los cuales es el resultado de J actualizando sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DID o DID que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos modelo y el enlace de actualización del modelo reemplazado por los nodos de posibilidades y las relaciones (en negrita).Nodos del azar, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (se calcula la distribución sobre el mod de nodo Chance [MT+1 J] en MT+1 J, L - 1). La probabilidad de que el modelo actualizado JS sea, por ejemplo, MT+1,1 J, L - 1, depende de la probabilidad de realizar la acción y recibir la observación que condujo a este modelo, y la distribución previa sobre los modelos en el paso de tiempo T.Debido a que el nodo casual en J asume la distribución de cada uno de los nodos de acción en función del valor de Mod [MT J], la probabilidad de la acción viene dada por este nodo casual. Para obtener la probabilidad de JS Posible observación, introducimos el Nodo Nodo OJ, que dependiendo del valor de MOD [MT J] supone la distribución del nodo de observación en el modelo de nivel inferior denotado por MOD [MT J]. Debido a que la probabilidad de observaciones JS depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo OJ está vinculado con ST+1, en J, y en i.2 Análogo a AT J, la tabla de probabilidad condicional de OJ también es un multiplexor modulado por MOD [MT J]. Finalmente, la distribución sobre los modelos anteriores en el tiempo t se obtiene del nodo casual, mod [mt j] en mt j, l - 1. En consecuencia, los nodos casuales, mod [mt j], en j, y OJ, forman los padres de mod [mt+1 j] en mt+1 j, l - 1. Observe que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos casuales que constituyen los nodos modelo en los dos cortes de tiempo. En la Fig. 4 mostramos los dos I-Did de corte de tiempo con los nodos modelo reemplazados por los nodos casuales y las relaciones entre ellos. Los nodos casuales y los enlaces de dependencia que no están en negrita son estándar, generalmente se encuentran en DIDS. La expansión del I-Did en más pasos de tiempo requiere la repetición de los dos pasos para actualizar el conjunto de modelos que forman la nota de 2 que OJ representa la observación JS en el tiempo t + 1. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 817 Valores del nodo modelo y agregando las relaciones entre los nodos casuales, tantas veces como hay enlaces de actualización del modelo. Observamos que el posible conjunto de modelos del otro agente J crece exponencialmente con la cantidad de pasos de tiempo. Por ejemplo, después de los pasos t, puede haber como máximo | mt = 1 j, l - 1 | (| aj || ωj |) t −1 modelos candidatos que residen en el nodo modelo.4.2 Solución análoga a I-IDS, la solución a un nivel L I-Did para el agente que expandí durante los pasos de tiempo T se puede llevar a cabo de manera recursiva. Para el propósito de la ilustración, deje l = 1 y t = 2. El método de solución utiliza la técnica estándar de apariencia, proyectando las secuencias de acción y observación de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el próximo paso de tiempo. Debido a que el Agente I también cree sobre los modelos JS, el LookAhead incluye descubrir los posibles modelos que J podría tener en el futuro. En consecuencia, cada uno de los modelos JS Subintencions o Nivel 0 (representados usando un DIDA estándar) en el primer paso debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de posibles observaciones que J podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de J.Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos de inferencia estándar utilizando las relaciones de dependencia entre los nodos modelo como se muestra en la Fig. 3 (b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de la resolución es el nivel 1 I-Did, JS Nivel 0 DIDS debe resolverse. Si la anidación de los modelos es más profunda, todos los modelos en todos los niveles a partir de 0 se resuelven de manera ascendente. Esbozamos brevemente el algoritmo recursivo para resolver el agente es el algoritmo para resolver la entrada I-DID: Nivel L ≥ 1 I-ID o ID de nivel 0, T Fase 1 de expansión. Para t de 1 a t - 1 do 2. Si L ≥ 1, entonces llenue MT+1 J, L - 1 3. Para cada MT J en el rango (Mt J, L - 1) do 4. Algoritmo de llamada recursivamente con el I-ID (o ID) recursivamente que representa MT J y el horizonte, t-t + 1 5. Mapee el nodo de decisión del ID (o ID) resuelto, OPT (MT J), a un nodo casual AJ 6. Para cada AJ en Opt (Mt J) do 7. Para cada OJ en OJ (parte de Mt J) do 8. ACTUALIZAR JS BREVIST, BT+1 J ← SE (BT J, AJ, OJ) 9. MT+1 J ← NUEVO I-ID (o ID) con BT+1 J como la creencia inicial 10. Rango (mt+1 j, l - 1) ∪ ← {mt+1 j} 11. Agregue el nodo modelo, MT+1 J, L - 1 y los enlaces de dependencia entre MT J, L - 1 y Mt+1 J, L - 1 (que se muestra en la Fig. 3 (b)) 12. Agregue la oportunidad, la decisión y los nodos de utilidad para T + 1 Time Slice y los enlaces de dependencia entre ellos 13. Establezca los CPT para cada nodo casual y de nodo de utilidad. Aplique el método estándar de apariencia y respaldo para resolver el I-DID expandido Figura 5: Algoritmo para resolver un nivel l ≥ 0 I-DID.Nivel L I-Did expandido durante los pasos de tiempo T con otro agente J en la figura 5. Adoptamos un enfoque de dos fases: dado un I-ID de nivel L (descrito anteriormente en la Sección 3) con todos los modelos de nivel inferior también representados como I-ID o IDS (IF Nivel 0), el primer paso es expandir el nivelL I-ID durante t Pasos de tiempo Agregar los enlaces de dependencia y las tablas de probabilidad condicionales para cada nodo. Nos centramos particularmente en establecer y poblar los nodos modelo (líneas 3-11). Tenga en cuenta que el rango (·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo modelo). En la segunda fase, utilizamos una técnica de aspecto estándar que proyecta la acción y las secuencias de observación en los pasos de tiempo T en el futuro y respaldamos los valores de utilidad de las creencias accesibles. Similar a I-IS, los I-Dids se reducen a DIDS en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0-° son los DID tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modeladas en ese nivel a I-Dids en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDID de nivel 1 se pueden resolver como dids y proporcionar distribuciones de probabilidad a modelos de nivel más alto. Suponga que el número de modelos considerados en cada nivel está sujeto a un número, M. Resolviendo un I-Did de nivel L en entonces equivalente a resolver O (ml) DIDS.5. Ejemplo de aplicaciones Para ilustrar la utilidad de los I-DID, las aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas para resolverlo.5.1 Seguimiento de la vida útil En el problema de tigre multiagente Comenzamos nuestras ilustraciones de usar I-ID e I-DID con una versión ligeramente modificada del problema de tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (o), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan crujidos (desde la izquierda (CL), desde la derecha (CR), o ninguna (s) crujir (s)), que indican ruidosamente los otros agentes que abren una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente I escucha gruñidos con una confiabilidad del 65% y cruje con una confiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente I escucha el agente j abriendo puertas de manera más confiable que los tigres gruñen. Esto sugiere que podría usar las acciones JS como una indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como I-Dids es el surgimiento de comportamientos sociales realistas en sus recetas. En entornos del persistente problema de tigre múltiple que reflejan situaciones del mundo real, demostramos seguidores entre los agentes y, como se muestra en [15], el engaño entre los agentes que creen que están en una relación con el líder de seguidores. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su aparición. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, las preferencias y los posibles comportamientos del otro, y se da cuenta de que es mejor que siga las acciones de los demás para maximizar sus pagos. Consideremos una configuración particular del problema del tigre en el que el agente creo que las preferencias de JS están alineadas con la suya, ambos solo quieren obtener el oro, y la audición JS es más confiable en comparación con sí misma. Como ejemplo, suponga que J, en escuchar puede discernir la ubicación de los Tigres, el 95% de las veces en comparación con el 65% de precisión. Además, el Agente I no tiene información inicial sobre la ubicación de Tigers. En otras palabras, es la creencia anidada de un solo nivel, BI, 1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considero dos modelos de J, que difieren en JS Flat Level 0 Creencias iniciales. Esto se representa en el ID I-ID de Nivel 1 que se muestra en la Fig. 6 (a). Según un modelo, J asigna una probabilidad de 0.9 que el tigre está detrás de la puerta izquierda, mientras que el otro 818 el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del Agente I, (b) Dos ID de nivel 0 del Agente J cuyos nodos de decisión se asignan a los nodos casuales, A1 J,A2 J, en (a).El modelo asigna 0.1 a esa ubicación (ver Fig. 6 (b)). El agente I está indeciso en estos dos modelos de j. Si variamos es la capacidad auditiva y resolvemos el ID de nivel 1 correspondiente expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas que se muestran en la Fig. 7 que exhiben un comportamiento de seguidores. Si es la probabilidad de escuchar correctamente los gruñidos es 0.65, entonces como se muestra en la política en la Fig. 7 (a), comienzo a seguir condicionalmente las acciones de JS: abre la misma puerta que jeinó anteriormente IFF es una evaluación propia de la ubicación de los tigres.confirma JS Pick. Si pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a J y abre la misma puerta que J abrió anteriormente (Fig. 7 (b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita.* es un comodín y denota cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de otros, era suficiente para que surgiera seguidores en el problema del tigre. Sin embargo, los requisitos epistemológicos para la aparición del liderazgo son más complejos. Para que un agente, digamos J, que emerja como líder, los seguidores primero deben surgir en el otro agente i. Como mencionamos anteriormente, si estoy seguro de que sus preferencias son idénticas a las de J, y cree que J tiene un mejor sentido de audición, seguiré las acciones de JS con el tiempo. El agente J emerge como un líder si cree que lo seguiré, lo que implica que la creencia JS debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darme cuenta de que seguiré a los regalos con la oportunidad de influir en las acciones en beneficio del bien colectivo o su interés propio solo. Por ejemplo, en el problema del tigre, consideremos una configuración en la que tanto I y J abren la puerta correcta, cada uno obtiene una recompensa de 20 que es el doble del original. Si J solo selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta equivocada, sus sanciones se cortan a la mitad. En este entorno, es tanto en el mejor interés de JS como en el mejoramiento colectivo para que J use su experiencia en la selección de la puerta correcta y, por lo tanto, sea un buen líder. Sin embargo, considere un problema ligeramente diferente en el que J gana de la pérdida de IS y se penaliza si gana. Específicamente, deje que se pague de JS, lo que indica que J es antagónico hacia I - si J elige la puerta correcta y yo la que es la pérdida de 100 se convierte en JS. El agente J cree que creo incorrectamente que las preferencias de JS son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza donde está el tigre. Debido a que cree que sus preferencias son similares a las de J, y que J comienza creyendo casi seguramente que uno de los dos es la ubicación correcta (dos modelos de nivel 0 de J), comenzaré siguiendo las acciones de JS. Mostramos que es una política normativa para resolver su I-Did anidada individual en tres pasos de tiempo en la Fig. 8 (a). La política demuestra que seguiré ciegamente las acciones de JS. Dado que el tigre persiste en su ubicación original con una probabilidad de 0.95, seleccionaré nuevamente la misma puerta. Si J comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolviendo JS I-Did anidada dos niveles de profundidad, da como resultado la política que se muestra en la Fig. 8 (b). Aunque J está casi seguro de que OL es la acción correcta, comenzará seleccionando o, seguido de OL. La intención del agente JS es engañar a I Who Who, cree, seguirá las acciones de JS, a fin de ganar $ 110 en el segundo paso, que es más de lo que J ganaría si fuera honesto. Figura 8: Emergencia del engaño entre los agentes en el problema del tigre. Los comportamientos de interés están en negrita.* denota como antes.(a) El agente es una política que demuestra que seguirá ciegamente las acciones de JS.(b) Aunque J está casi seguro de que el tigre está a la derecha, comenzará seleccionando o, seguido de OL, para engañar I.5.2 Altruismo y reciprocidad En el problema público del bien público El problema del bien público (PG) [7], consiste en un grupo de agentes M, cada uno de los cuales debe contribuir con algunos recursos a una olla pública o mantenerlo por sí mismos. Dado que los recursos contribuidos a la olla pública se comparten entre todos los agentes, son menos valiosos para el agente cuando están en la olla pública. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa a cada agente es más que si nadie contribuye. Dado que un agente obtiene su parte de la olla pública, independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y, en cambio, sea un viaje gratuito en las contribuciones de otros. Sin embargo, los comportamientos de los jugadores humanos en las simulaciones empíricas del problema PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen una gran cantidad a la olla pública y continúan contribuyendo cuando el problema de PG se juega repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente a la olla pública, incluso cuando todos los demás están deserviendo. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de otros. Para simplificar, suponemos que el juego se juega entre los agentes M = 2, I y J. Deje que cada agente esté inicialmente dotado de la cantidad de recursos XT. Si bien la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acción al permitir dos posibles acciones. Cada agente puede optar por contribuir (c) una cantidad fija de los recursos o no contribuir. La última acción es Dethe Sixth Intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 819 señalados como defecto (D). Suponemos que las acciones no son observables para otros. El valor de los recursos en el bote público es descontado por CI para cada agente I, donde CI es el retorno privado marginal. Suponemos que CI <1 para que el agente no se beneficie lo suficiente como para que contribuya a la olla pública para obtener ganancias privadas. Simultáneamente, CIM> 1, haciendo que la contribución colectiva Pareto sea óptima.I/J C D C 2Cixt, 2CJXT CIXT-CP, XT + CJXT-P D XT + CIXT-P, CJXT-CP XT, XX Tabla 1: El juego PG de un disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los pasajeros libres pero incurren en un pequeño costo por administrar el castigo. Sea P el castigo impartido al agente de defectos y CP el costo no cero de castigar al agente contribuyente. Para simplificar, suponemos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la mesa.1. Deje Ci = CJ, CP> 0, y si P> Xt - CIXT, entonces la deserción ya no es una acción dominante. Si p <xt - cixt, entonces la deserción es la acción dominante para ambos. Si P = Xt-CIXT, entonces el juego no es solucionable de dominio. Figura 9: (a) Nivel 1 I-ID del Agente I, (b) IDS de nivel 0 del Agente J con nodos de decisión asignados a los nodos casuales, A1 J y A2 J, en (A). Formulamos una versión secuencial del problema PG con el castigo desde la perspectiva del agente i. Aunque en el juego PG repetido, la cantidad en el bote público se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta a los agentes. Cada agente puede contribuir con una cantidad fija, XC o defecto. Un agente sobre la realización de una acción recibe una observación de abundancia (py) o escasa (MR) que simboliza el estado de la olla pública. Observe que las observaciones también son indirectamente indicativas de las acciones del agente JS porque el estado de la olla pública está influenciado por ellas. La cantidad de recursos en el agente es la olla privada, es perfectamente observable para i. Los pagos son análogos a la mesa.1. Tomando prestados de las investigaciones empíricas del problema PG [5], construimos ID de nivel 0 para J que modelan tipos altruistas y no altruistas (Fig. 9 (b)). Específicamente, nuestro agente altruista tiene un alto rendimiento privado marginal (CJ está cerca de 1) y no castiga a otros que defectan. Deje que XC = 1 y el agente de nivel 0 se castigan a la mitad de las veces que defecta. Con una acción restante, ambos tipos de agentes eligen contribuir a evitar ser castigados. Con dos acciones por recorrer, el tipo altruista elige contribuir, mientras que los otros defectos. Esto se debe a que CJ para el tipo altruista está cerca de 1, por lo tanto, el castigo esperado, 0.5p> (1 - CJ), que el tipo altruista evita. Debido a que CJ para el tipo no altruista es menor, prefiere no contribuir. A tres pasos por recorrer, el agente altruista contribuye a evitar el castigo (0.5p> 2 (1-CJ)) y los defectos de tipo no altruista. Para más de tres pasos, mientras que el agente altruista continúa contribuyendo a la olla pública dependiendo de qué tan cerca esté su rendimiento privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista que modelé utilizando un I-Did de nivel 1 expandido en 3 pasos de tiempo.Afirmé los dos modelos de nivel 0, mencionados anteriormente, a J (ver Fig. 9). Si cree con una probabilidad 1 que J es altruista, elige contribuir para cada uno de los tres pasos. Este comportamiento persiste cuando no tengo conocimiento de si J es altruista (Fig. 10 (a)), y cuando asigno una alta probabilidad de que J sea el tipo no altruista. Sin embargo, cuando cree con una probabilidad 1 de que J no es altruista y, por lo tanto, seguramente desertará, elige desertar para evitar ser castigado y porque su retorno privado marginal es inferior a 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja a que se encuentran experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de la probabilidad de que el otro agente sea altruista. Analizamos el comportamiento de un tipo de agente recíproco que coincide con la cooperación o deserción esperadas. El rendimiento privado marginal de los tipos recíprocos es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso cuando el agente recíproco I no está seguro de si J es altruista y cree que es probable que la olla pública esté medio llena. Para esta creencia previa, elijo desertar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defecto (Fig. 10 (b)). Esto se debe a que una observación de abundancia señala que es probable que el bote esté más de medio lleno, lo que resulta de la acción JS para contribuir. Por lo tanto, entre los dos modelos atribuidos a J, es probable que su tipo sea altruista, lo que es probable que J contribuya nuevamente en el próximo paso de tiempo. Por lo tanto, el agente I elige contribuir a la acción de reciprocar JS. Un razonamiento análogo lleva a I para desertar cuando observa una olla escasa. Con una acción por recorrer, creo que J contribuye, elegirá contribuir también para evitar el castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye.(b) Un agente recíproco I comienza por defecto seguido de la elección de contribuir o defectos en función de su observación de abundancia (lo que indica que J es probable que sea altruista) o escaso (J no es altruista).5.3 Estrategias en el póker de póker de dos jugadores es un popular juego de cartas de suma cero que ha recibido mucha atención entre la comunidad de investigación de inteligencia artificial como un testeble [2]. El póker se juega entre M ≥ 2 jugadores en los que cada jugador recibe una mano de cartas de un mazo. Si bien existen varios sabores de póker con una complejidad variable, consideramos una versión simple en la que cada jugador tiene tres capas durante las cuales el jugador puede intercambiar una carta (e), mantener la mano existente (k), pliegue (f) y retirarse deEl juego, o llamar (c), requerir que todos los jugadores muestren sus manos. Para mantener las cosas simples, deje que M = 2, y cada jugador reciba una mano que consiste en una sola carta extraída del mismo traje. Por lo tanto, durante un enfrentamiento, el jugador que tiene la carta numéricamente más grande (2 es la más baja, ACE es la más alta) gana la olla. Durante un intercambio de tarjetas, la tarjeta desechada se coloca en la pila L, lo que indica al otro agente que era una tarjeta de bajo número de 8 años, o en el 820 el sexto INTL. Conf.en agentes autónomos y sistemas de agentes múltiples (AAMAS 07) H Pila, lo que indica que la tarjeta tenía un rango mayor o igual a 8. Observe que, por ejemplo, si se descarta una tarjeta numerada más baja, ahora se reduce la probabilidad de recibir una tarjeta baja a cambio. Mostramos el ID de Nivel 1 para el póker simplificado de dos jugadores en la figura 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una tarjeta numerada en su mano. Por otro lado, el agente agresivo J cree con una alta probabilidad de que su oponente tenga una carta de menor número. Por lo tanto, los dos tipos difieren en sus creencias sobre sus oponentes. En ambos modelos de nivel 0, se supone que el oponente realiza sus acciones después de una distribución fija y uniforme. Con tres acciones por recorrer, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su tarjeta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una tarjeta baja, lo que mejora sus posibilidades de obtener una tarjeta alta durante el intercambio. El agente conservador elige mantener su tarjeta, sin importar su mano porque sus posibilidades de obtener una tarjeta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre JS Mano del paso de tiempo anterior, (b) ID de nivel 0 del Agente J cuyos nodos de decisión se asignan a los nodos casuales, A1 J, A2 J, en (A). La política de un agente de Nivel 1 que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en JS Hand (tipo de personalidad neutral) y J podría ser un tipo agresivo o conservador, se muestra en la Fig. 12. Es propio.La mano contiene la tarjeta numerada 8. El agente comienza manteniendo su tarjeta. Al ver que J no intercambió una tarjeta (N), cree con la probabilidad 1 de que J es conservador y, por lo tanto, mantendrá sus cartas.Responde manteniendo su tarjeta o intercambiándola porque J es igualmente probable que tenga una tarjeta más baja o superior. Si observo que J desechó su tarjeta en la pila L o H, creo que J es agresivo. Al observar L, me doy cuenta de que J tenía una tarjeta baja y es probable que tenga una tarjeta alta después de su intercambio. Debido a que la probabilidad de recibir una tarjeta baja es alta ahora, elige mantener su tarjeta. Al observar H, creyendo que la probabilidad de recibir una tarjeta numerada es alta, elige intercambiar su tarjeta. En el paso final, elige llamar independientemente de su historial de observación porque su creencia de que J tiene una tarjeta más alta no es lo suficientemente alta como para concluir que es mejor doblar y renunciar a la recompensa. Esto se debe en parte al hecho de que una observación de, por ejemplo, restablecer el agente son las creencias de pasos de tiempo anteriores sobre JS mano a las tarjetas con numeradas bajas solamente.6. Discusión Mostramos cómo los DIDS pueden extenderse a I-DID que permiten la toma de decisiones secuenciales en línea en entornos múltiples inciertos. Nuestra representación gráfica de I-DID mejora en la Figura 12 anterior: un agente de nivel 1 es una política de tres pasos en el problema de póker.Comienzo creyendo que J es igualmente probable que sea agresivo o conservador y podría tener cualquier tarjeta en su mano con igual probabilidad.Trabaje significativamente por ser más transparente, semánticamente claro y capaz de resolverse utilizando algoritmos estándar que el objetivo es DID. Los I-DID extienden NIDS para permitir la toma de decisiones secuenciales en múltiples pasos de tiempo en presencia de otros agentes interactivos. Los I-DID pueden verse como representaciones gráficas concisas para IPOMDP que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea como el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-Dids aproximadamente con límites comprobables en la calidad de la solución. Reconocimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea reconocer el apoyo de una subvención de Ugarf.7. Referencias [1] R. J. Aumann. Epistemología interactiva I: Conocimiento. International Journal of Game Theory, 28: 263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Journal of Economic Theory, 59: 189-198, 1993. [4] C. Camerer. Teoría del juego conductual: experimentos en la interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. American Economic Review, 90 (4): 980-994, 2000. [6] D. Fudenberg y D. K. Levine. La teoría del aprendizaje en los juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juego. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un idioma para modelar procesos de toma de decisiones de los agentes de modelado en los juegos. En Aamas, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. Jair, 24: 49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos de múltiples agentes. Jaamas, 3 (4): 319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugadas por jugadores bayesianos. Management Science, 14 (3): 159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, los principios y aplicaciones del análisis de decisiones. Grupo de decisiones estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Journal de inteligencia artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia de múltiples agentes para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia dinámica interactiva. En GTDT Workshop, Aamas, 2006. [16] B. Rathnas., P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia de comportamiento. En Agentes Autónomos y Conferencia de Sistemas de Aguos Multi-Agentes (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia artificial: un enfoque moderno (segunda edición). Prentice Hall, 2003. [18] R. D. Shachter. Evaluar los diagramas de influencia. Operations Research, 34 (6): 871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Modelos de aprendizaje de otros agentes que utilizan diagramas de influencia. En Um, 1999. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 821