Adarank: un algoritmo de impulso para la recuperación de información Jun Xu Microsoft Research Asia No. 49 Zhichun Road, Haidian Distints Beijing, China 100080 junxu@microsoft.com Hang Li Microsoft Research Asia No. 49 Zhichun Road, Haidian Distint Distints Beijing, China 100080 Hangli@microsoft.com Resumen En este documento Nos dirigimos el problema de aprender a clasificar para clasificarRecuperación de documentos. En la tarea, un modelo se crea automáticamente con algunos datos de capacitación y luego se utiliza para la clasificación de documentos. La bondad de un modelo generalmente se evalúa con medidas de rendimiento como MAP (precisión promedio media) y NDCG (ganancia acumulativa con descuento normalizada). Idealmente, un algoritmo de aprendizaje entrenaría un modelo de clasificación que podría optimizar directamente las medidas de rendimiento con respecto a los datos de capacitación. Sin embargo, los métodos existentes solo pueden entrenar modelos de clasificación minimizando las funciones de pérdida relacionadas con las medidas de rendimiento. Por ejemplo, clasificación de modelos de clasificación SVM y RankBoost Train minimizando los errores de clasificación en los pares de instancias. Para lidiar con el problema, proponemos un nuevo algoritmo de aprendizaje dentro del marco de impulso, que puede minimizar una función de pérdida directamente definida en las medidas de rendimiento. Nuestro algoritmo, denominado Adarank, construye repetidamente a los rango débil sobre la base de los datos de entrenamiento re-ponderados y finalmente combina linealmente a los rankers débiles para hacer predicciones de clasificación. Probamos que el proceso de entrenamiento de Adarank es exactamente el de mejorar la medida de rendimiento utilizada. Los resultados experimentales en cuatro conjuntos de datos de referencia muestran que Adarank supera significativamente los métodos de referencia de BM25, clasificación de SVM y RankBoost. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: modelos de recuperación Algoritmos de términos generales, experimentación, teoría 1. La introducción recientemente el aprendizaje para clasificar ha ganado una atención cada vez mayor tanto en los campos de recuperación de información como en el aprendizaje automático. Cuando se aplica a la recuperación de documentos, el aprendizaje para clasificar se convierte en una tarea de la siguiente manera. En la capacitación, se construye un modelo de clasificación con datos que consisten en consultas, sus documentos recuperados correspondientes y niveles de relevancia dados por los humanos. En la clasificación, dada una nueva consulta, los documentos recuperados correspondientes se clasifican utilizando el modelo de clasificación capacitado. En la recuperación de documentos, los resultados de clasificación generalmente se evalúan en términos de medidas de rendimiento como MAP (precisión promedio media) [1] y NDCG (ganancia acumulativa con descuento normalizada) [15]. Idealmente, se crea la función de clasificación para que se maximice la precisión de la clasificación en términos de una de las medidas con respecto a los datos de entrenamiento. Varios métodos para aprender a clasificar se han desarrollado y aplicado para la recuperación de documentos. Por ejemplo, Herbrich et al.[13] propone un algoritmo de aprendizaje para la clasificación sobre la base de las máquinas de vectores de soporte, llamado Ranking SVM. Freund et al.[8] adopta un enfoque similar y realiza el aprendizaje utilizando el impulso, denominado RankBoost. Todos los métodos existentes utilizados para la recuperación de documentos [2, 3, 8, 13, 16, 20] están diseñados para optimizar las funciones de pérdida relacionadas con las medidas de rendimiento IR, no las funciones de pérdida directamente en función de las medidas. Por ejemplo, clasificación de modelos de clasificación SVM y RankBoost Train minimizando los errores de clasificación en los pares de instancias. En este documento, nuestro objetivo es desarrollar un nuevo algoritmo de aprendizaje que pueda optimizar directamente cualquier medida de rendimiento utilizada en la recuperación de documentos. Inspirados en el trabajo de Adaboost para la clasificación [9], proponemos desarrollar un algoritmo de impulso para la recuperación de información, denominado Adarank. Adarank utiliza una combinación lineal de rankers débiles como su modelo. En el aprendizaje, repite el proceso de volver a alojar la muestra de entrenamiento, crear un ranker débil y calcular un peso para el ranker. Mostramos que el algoritmo Adarank puede optimizar iterativamente una función de pérdida exponencial basada en cualquiera de las medidas de rendimiento IR. Se proporciona un límite inferior del rendimiento en los datos de capacitación, lo que indica que la precisión de la clasificación en términos de la medida de rendimiento puede mejorarse continuamente durante el proceso de capacitación. Adarank ofrece varias ventajas: facilidad en la implementación, solidez teórica, eficiencia en la capacitación y alta precisión en la clasificación. Los resultados experimentales indican que Adarank puede superar los métodos de referencia de BM25, clasificar SVM y RankBoost, en cuatro conjuntos de datos de referencia, incluidos Ohsumed, WSJ, AP y .gov. Los modelos de clasificación de ajuste utilizando ciertos datos de entrenamiento y una medida de rendimiento es una práctica común en IR [1]. A medida que el número de características en el modelo de clasificación se hace mayor y la cantidad de datos de entrenamiento se hace más grande, la afinación se vuelve más difícil. Desde el punto de vista de IR, Adarank se puede ver como un método de aprendizaje automático para el ajuste del modelo de clasificación. Recientemente, la optimización directa de las medidas de rendimiento en el aprendizaje se ha convertido en un tema de investigación candente. Se han propuesto varios métodos para la clasificación [17] y la clasificación [5, 19]. Adarank se puede ver como un método de aprendizaje automático para la optimización directa de las medidas de rendimiento, basado en un enfoque diferente. El resto del documento está organizado de la siguiente manera. Después de un resumen del trabajo relacionado en la Sección 2, describimos el algoritmo Adarank propuesto en detalles en la Sección 3. Los resultados experimentales y las discusiones se dan en la Sección 4. La Sección 5 concluye este documento y ofrece trabajo futuro.2. Trabajo relacionado 2.1 Recuperación de información El problema clave para la recuperación de documentos es la clasificación, específicamente, cómo crear el modelo de clasificación (función) que puede ordenar documentos en función de su relevancia para la consulta dada. Es una práctica común en IR ajustar los parámetros de un modelo de clasificación utilizando algunos datos etiquetados y una medida de rendimiento [1]. Por ejemplo, los métodos de vanguardia de BM25 [24] y LMIR (modelos de lenguaje para la recuperación de información) [18, 22] tienen parámetros que sintonizar. A medida que los modelos de clasificación se vuelven más sofisticados (se utilizan más características) y se están disponibles más datos etiquetados, cómo sintonizar o entrenar modelos de clasificación resulta ser un problema desafiante. Recientemente se han aplicado métodos de aprendizaje para clasificar a la construcción del modelo de clasificación y se han obtenido algunos resultados prometedores. Por ejemplo, Joachims [16] aplica SVM de clasificación para documentar la recuperación. Utiliza datos de clic para deducir datos de capacitación para la creación del modelo. Cao et al.[4] Adapte la clasificación de SVM para documentar la recuperación modificando la función de pérdida de bisagra para cumplir mejor los requisitos de IR. Específicamente, introducen una función de pérdida de bisagra que penaliza fuertemente los errores en la parte superior de las listas de clasificación y los errores de consultas con menos documentos recuperados. Burges et al.[3] Emplea la entropía relativa como función de pérdida y descenso de gradiente como un algoritmo para capacitar a un modelo de red neuronal para clasificar en la recuperación de documentos. El método se conoce como RankNet.2.2 Aprendizaje automático Hay tres temas en el aprendizaje automático que están relacionados con nuestro trabajo actual. Están aprendiendo a clasificar, impulsar y optimización directa de las medidas de rendimiento. Aprender a clasificarse es crear automáticamente una función de clasificación que asigne puntajes a las instancias y luego clasificar las instancias utilizando los puntajes. Se han propuesto varios enfoques para abordar el problema. Un enfoque importante para aprender a clasificar es el de transformarlo en clasificación binaria en pares de instancias. Este enfoque pareal se ajusta bien a la recuperación de información y, por lo tanto, se usa ampliamente en IR. Los métodos típicos del enfoque incluyen clasificar SVM [13], RankBoost [8] y RankNet [3]. Para otros enfoques para aprender a clasificarse, consulte [2, 11, 31]. En el enfoque de pareja para la clasificación, la tarea de aprendizaje se formaliza como un problema de clasificar los pares de instancias en dos categorías (clasificada correctamente y clasificada incorrectamente). En realidad, se sabe que reducir los errores de clasificación en los pares de instancias es equivalente a maximizar un límite inferior del mapa [16]. En ese sentido, los métodos existentes para clasificar SVM, RankBoost y RankNet solo pueden minimizar las funciones de pérdida que están libremente relacionadas con las medidas de rendimiento IR. El impulso es una técnica general para mejorar las precisiones de los algoritmos de aprendizaje automático. La idea básica de aumentar es construir repetidamente a los alumnos débiles al volver a alojar los datos de capacitación y formar un conjunto de estudiantes débiles de modo que se aumente el rendimiento total del conjunto. Freund y Schapire han propuesto el primer algoritmo de impulso bien conocido llamado Adaboost (impulso adaptativo) [9], que está diseñado para la clasificación binaria (predicción 0-1). Más tarde, Schapire & Singer ha introducido una versión generalizada de Adaboost en la que los alumnos débiles pueden dar puntajes de confianza en sus predicciones en lugar de tomar decisiones 0-1 [26]. Se han hecho extensiones para tratar los problemas de clasificación de múltiples clases [10, 26], regresión [7] y clasificación [8]. De hecho, Adaboost es un algoritmo que construye ingeniosamente un modelo lineal minimizando la función de pérdida exponencial con respecto a los datos de entrenamiento [26]. Nuestro trabajo en este documento se puede ver como un método de impulso desarrollado para la clasificación, particularmente para la clasificación en IR. Recientemente, varios autores han propuesto realizar una optimización directa de medidas de rendimiento multivariadas en el aprendizaje. Por ejemplo, Joachims [17] presenta un método SVM para optimizar directamente las medidas de rendimiento multivariadas no lineales como la medida F1 para la clasificación. Cossock y Zhang [5] encuentran una manera de optimizar aproximadamente la medida de rendimiento de clasificación DCG [15]. Metzler et al.[19] también propone un método para maximizar directamente las métricas basadas en rango para la clasificación sobre la base del aprendizaje múltiple. Adarank también es uno que intenta optimizar directamente las medidas de rendimiento multivariadas, pero se basa en un enfoque diferente. Adarank es único en el sentido de que emplea una función de pérdida exponencial basada en medidas de rendimiento de IR y una técnica de impulso.3. Nuestro método: ADARANK 3.1 Marco general, primero describimos el marco general de aprender a clasificarse para la recuperación de documentos. En la recuperación (prueba), dada una consulta, el sistema devuelve una lista de clasificación de documentos en orden descendente de los puntajes de relevancia. Los puntajes de relevancia se calculan con una función de clasificación (modelo). En el aprendizaje (capacitación), se dan una serie de consultas y sus documentos recuperados correspondientes. Además, también se proporcionan los niveles de relevancia de los documentos con respecto a las consultas. Los niveles de relevancia se representan como rangos (es decir, categorías en un orden total). El objetivo del aprendizaje es construir una función de clasificación que logre los mejores resultados en la clasificación de los datos de entrenamiento en el sentido de minimización de una función de pérdida. Idealmente, la función de pérdida se define sobre la base de la medida de rendimiento utilizada en las pruebas. Supongamos que y = {r1, r2, · · ·, r} es un conjunto de rangos, donde denota el número de rangos. Existe un orden total entre los rangos R R −1 · · · R1, donde denota una relación de preferencia. En el entrenamiento, se da un conjunto de consultas q = {Q1, Q2, · · ·, QM}. Cada consulta Qi está asociada con una lista de documentos recuperados di = {Di1, Di2, · · ·, di, n (qi)} y una lista de etiquetas yi = {yi1, yi2, · · ·, yi, n (Qi)}, donde n (qi) denota los tamaños de las listas di e yi, dij denota el documento jth en di, y yij ∈ Y denota el rango de documento di j. Se crea un vector de características xij = ψ (qi, di j) ∈ X a partir de cada par de documentos de consulta (qi, di j), i = 1, 2, · · ·, m;j = 1, 2, · · ·, n (qi). Por lo tanto, el conjunto de entrenamiento se puede representar como S = {(qi, di, yi)} m i = 1. El objetivo del aprendizaje es crear una función de clasificación F: x →, de modo que para cada consulta a los elementos en su lista de documentos correspondientes se les pueda asignar puntajes de relevancia utilizando la función y luego clasificarse de acuerdo con los puntajes. Específicamente, creamos una permutación de enteros π (qi, di, f) para consulta qi, la lista correspondiente de documentos di y la función de clasificación f.Deje que di = {di1, di2, · · ·, di, n (qi)} se identifique por la lista de enteros {1, 2, · · ·, n (qi)}, luego permutación π (qi, di, f) se define como una biyección de {1, 2, · · ·, n (qi)} a sí misma. Usamos π (j) para denotar la posición del ítem j (es decir, di j). El proceso de aprendizaje resulta ser el de minimizar la función de pérdida que representa el desacuerdo entre la permutación π (Qi, Di, F) y la lista de rangos Yi, para todas las consultas. Tabla 1: Notaciones y explicaciones. Explicaciones de notaciones qi ∈ Q ith consulta di = {di1, di2, · · ·, di, n (qi)} Lista de documentos para qi yi j ∈ {r1, r2, · · ·, r} rango de di j w.r.t.qi yi = {yi1, yi2, · · ·, yi, n (qi)} Lista de rangos para qi s = {(qi, di, yi)} m i = 1 Conjunto de entrenamiento xij = ψ (qi, dij) ∈ XVector de características para (qi, di j) f (xij) ∈ Modelo de clasificación π (qi, di, f) permutación para qi, di y f ht (xi j) ∈ Tth Ranker débil e (π (qi, di, f), yi) ∈ [−1, +1] función de medida de rendimiento En el documento, definimos el modelo de rango como una combinación lineal de rankers débiles: f (x) = t t = 1 αTHT (x), donde ht (x) es un ranker débil, αT es su peso y t es el número de rankers débiles. En la recuperación de la información, las medidas de rendimiento basadas en consultas se utilizan para evaluar la bondad de una función de clasificación. Por medida basada en la consulta, nos referimos a una medida definida en una lista de clasificación de documentos con respecto a una consulta. Estas medidas incluyen MAP, NDCG, MRR (rango recíproco medio), WTA (los ganadores toman todo) y Precision@N [1, 15]. Utilizamos una función general E (π (Qi, Di, F), yi) ∈ [−1, +1] para representar las medidas de rendimiento. El primer argumento de E es la permutación π creada usando la función de clasificación F en DI. El segundo argumento es la lista de rangos yi dada por humanos. E mide el acuerdo entre π y yi. La Tabla 1 proporciona un resumen de las notaciones descritas anteriormente. A continuación, como ejemplos de medidas de rendimiento, presentamos las definiciones de MAP y NDCG. Dada una consulta Qi, la lista correspondiente de rangos yi, y una permutación πi en di, la precisión promedio para qi se define como: avgpi = n (qi) j = 1 pi (j) · yij n (qi) j = 1 yij, (1) donde YIJ toma 1 y 0 como valores, que representan ser relevantes o irrelevantes y Pi (j) se define como precisión en la posición de dij: pi (j) = k: πi (k) ≤πi (j)yik πi (j), (2) donde πi (j) denota la posición de di j. Dada una consulta Qi, la lista de rangos yi y una permutación πi en di, ndcg en la posición m para qi se define como: ni = ni · j: πi (j) ≤m 2yi j - 1 log (1 + πi (j)), (3) donde Yij asume rangos como valores y Ni es una constante de normalización.Ni se elige para que una puntuación de clasificación π π i s ndcg en la posición m sea 1. 3.2 algoritmo inspirado en el algoritmo AdaBoost para la clasificación, hemos ideado un algoritmo novedoso que puede optimizar una función de pérdida basada en las medidas de rendimiento IR. El algoritmo se conoce como Adarank y se muestra en la Figura 1. Adarank toma un conjunto de entrenamiento S = {(qi, di, yi)} m i = 1 como entrada y toma la función de medida de rendimiento e y el número de iteraciones t como parámetros. Adarank corre t rondas y en cada ronda crea un ranker ht débil (t = 1, · · ·, t). Finalmente, genera un modelo de clasificación F combinando linealmente a los rankers débiles. En cada ronda, Adarank mantiene una distribución de pesos sobre las consultas en los datos de entrenamiento. Denotamos la distribución de la entrada de pesos: s = {(qi, di, yi)} m i = 1, y los parámetros e y t inicializan p1 (i) = 1/m. Para t = 1, · · ·, t • Crear ranker ht débil con distribución ponderada PT en datos de entrenamiento s.• Elija αT αT = 1 2 · ln m i = 1 pt (i) {1 + e (π (qi, di, ht), yi)} m i = 1 pt (i) {1 - e (π (qi, di, di, ht), yi)}.• Crear ft ft (x) = t k = 1 αKHK (x).• Actualizar pt+1 pt+1 (i) = exp {−e (π (qi, di, ft), yi)} m j = 1 exp {−e (π (qj, dj, ft), yj)}. Fin para el modelo de clasificación de salida: F (x) = ft (x). Figura 1: El algoritmo Adarank.en la ronda T como PT y el peso en la I -ésima consulta de entrenamiento Qi en la ronda T como PT (i). Inicialmente, Adarank establece pesos igual a las consultas. En cada ronda, aumenta los pesos de esas consultas que no están bien clasificadas por FT, el modelo creado hasta ahora. Como resultado, el aprendizaje en la próxima ronda se centrará en la creación de un ranker débil que puede trabajar en la clasificación de esas consultas difíciles. En cada ronda, se construye un HT de Ranker débil en función de datos de entrenamiento con distribución de peso PT. La bondad de un ranker débil se mide por la medida de rendimiento e ponderada por Pt: M I = 1 Pt (I) E (π (Qi, Di, Ht), Yi). Se pueden considerar varios métodos para la construcción del rango débil. Por ejemplo, se puede crear un ranker débil utilizando un subconjunto de consultas (junto con su lista de documentos y lista de etiquetas) muestreadas de acuerdo con la distribución PT. En este artículo, utilizamos características individuales como rankers débiles, como se explicará en la Sección 3.6. Una vez que se construye un Ranker HT débil, Adarank elige un peso αT> 0 para el ranker débil. Intuitivamente, αT mide la importancia de HT. Se crea un modelo de clasificación FT en cada ronda combinando linealmente los rankers débiles construidos hasta ahora H1, · · ·, ht con pesos α1, · · ·, αT.FT se usa luego para actualizar la distribución PT+1.3.3 Análisis teórico Los algoritmos de aprendizaje existentes para el intento de clasificación de minimizar una función de pérdida basada en pares de instancias (pares de documentos). En contraste, Adarank intenta optimizar una función de pérdida basada en consultas. Además, la función de pérdida en Adarank se define sobre la base de medidas generales de rendimiento IR. Las medidas pueden ser MAP, NDCG, WTA, MRR o cualquier otra medida cuyo rango esté dentro de [−1, +1]. Luego explicamos por qué este es el caso. Idealmente, queremos maximizar la precisión de la clasificación en términos de una medida de rendimiento en los datos de entrenamiento: max f∈F m i = 1 e (π (qi, di, f), yi), (4) donde f es el conjunto de posiblesFunciones de clasificación. Esto es equivalente a minimizar la pérdida en los datos de entrenamiento min f∈F m i = 1 (1 - E (π (qi, di, f), yi)).(5) Es difícil optimizar directamente la pérdida, porque E es una función no continua y, por lo tanto, puede ser difícil de manejar. En su lugar, intentamos minimizar un límite superior de la pérdida en (5) min f∈F m i = 1 exp {−e (π (qi, di, f), yi)}, (6) porque e - x ≥ 1 −X se sostiene para cualquier x ∈. Consideramos el uso de una combinación lineal de rankers débiles como nuestro modelo de clasificación: F (x) = t t = 1 αTHT (x).(7) La minimización en (6) luego resulta ser min ht∈H, αT∈ + L (ht, αT) = m i = 1 exp {−e (π (qi, di, ft - 1 + αTHT),,yi)}, (8) donde h es el conjunto de posibles rankers débiles, αT es un peso positivo y (ft - 1 + αTHT) (x) = ft - 1 (x) + αTHT (x). Se pueden considerar varias formas de calcular los coeficientes αT y los rankers débiles HT. Siguiendo la idea de Adaboost, en Adarank adoptamos el enfoque del modelado aditivo en escena hacia adelante [12] y obtenemos el algoritmo en la Figura 1. Se puede demostrar que existe un límite inferior en la precisión de clasificación para Adarank en los datos de entrenamiento, como se presenta en el Teorema 1. T 1. El siguiente límite se mantiene en la precisión de clasificación del algoritmo Adarank en los datos de entrenamiento: 1 m m i = 1 e (π (qi, di, ft), yi) ≥ 1 - t t = 1 e - quin min 1 - ϕ (t)2, donde ϕ (t) = m i = 1 pt (i) e (π (qi, di, ht), yi), Δt min = mini = 1, ···, m Δt i y Δt i = e (π (qi, di, ft - 1 + αTht), yi) - e (π (qi, di, ft - 1) −αTE (π (qi, di, ht), yi), para todos i = =1, 2, · · ·, my t = 1, 2, · · ·, T. Una prueba del teorema se puede encontrar en el apéndice. El teorema implica que la precisión de la clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre que E - Δt min 1 - ϕ (t) 2 <1 se mantenga.3.4 Ventajas Adarank es un método simple pero poderoso. Más importante aún, es un método que puede justificarse desde el punto de vista teórico, como se discutió anteriormente. Además, Adarank tiene varias otras ventajas en comparación con los métodos de aprendizaje existentes para clasificar, como clasificar SVM, RankBoost y RankNet. Primero, Adarank puede incorporar cualquier medida de rendimiento, siempre que la medida esté basada en la consulta y en el rango de [−1, +1]. Observe que las principales medidas IR cumplen con este requisito. Por el contrario, los métodos existentes solo minimizan las funciones de pérdida que están libremente relacionadas con las medidas IR [16]. En segundo lugar, el proceso de aprendizaje de Adarank es más eficiente que el de los algoritmos de aprendizaje existentes. La complejidad del tiempo de Adarank es de orden o ((k+t) · m · n log n), donde k denota el número de características, el número de rondas, el número de consultas en los datos de entrenamiento, y n es elNúmero máximo de documentos para consultas en datos de capacitación. La complejidad del tiempo de RankBoost, por ejemplo, es de orden O (t · m · n2) [8]. En tercer lugar, Adarank emplea un marco más razonable para realizar la tarea de clasificación que los métodos existentes. Específicamente en Adarank, las instancias corresponden a consultas, mientras que en los métodos existentes las instancias corresponden a pares de documentos. Como resultado, Adarank no tiene las siguientes deficiencias que afectan los métodos existentes.(a) Los métodos existentes deben hacer una suposición fuerte de que los pares de documentos de la misma consulta se distribuyen de forma independiente. En realidad, este claramente no es el caso y este problema no existe para Adarank.(b) La clasificación de los documentos más relevantes en la parte superior de las listas de documentos es crucial para la recuperación de documentos. Los métodos existentes no pueden centrarse en la capacitación en la parte superior, como se indica en [4]. Se han propuesto varios métodos para rectificar el problema (por ejemplo, [4]), sin embargo, no parecen resolver fundamentalmente el problema. Por el contrario, Adarank puede centrarse naturalmente en el entrenamiento en la parte superior de las listas de documentos, porque las medidas de rendimiento utilizadas a favor de las clasificaciones para los cuales los documentos relevantes están en la parte superior.(c) En los métodos existentes, los números de pares de documentos varían de consulta a consulta, lo que resulta en la creación de modelos sesgados hacia consultas con más pares de documentos, como se señala en [4]. Adarank no tiene este inconveniente, porque trata consultas en lugar de pares de documentos como unidades básicas en el aprendizaje.3.5 Las diferencias de Adaboost Adarank es un algoritmo de impulso. En ese sentido, es similar a Adaboost, pero también tiene varias diferencias sorprendentes con respecto a Adaboost. Primero, los tipos de instancias son diferentes. Adarank hace uso de consultas y sus listas de documentos correspondientes como instancias. Las etiquetas en los datos de capacitación son listas de rangos (niveles de relevancia). Adaboost utiliza los vectores de características como instancias. Las etiquetas en los datos de entrenamiento son simplemente +1 y −1. En segundo lugar, las medidas de rendimiento son diferentes. En Adarank, la medida de rendimiento es una medida genérica, definida en la lista de documentos y la lista de rango de una consulta. En ADABOOST, la medida de rendimiento correspondiente es una medida específica para la clasificación binaria, también conocida como margen [25]. Tercero, las formas de actualizar los pesos también son diferentes. En Adaboost, la distribución de pesos en las instancias de entrenamiento se calcula de acuerdo con la distribución actual y el rendimiento del alumno débil actual. En Adarank, en contraste, se calcula de acuerdo con el rendimiento del modelo de clasificación creado hasta ahora, como se muestra en la Figura 1. Tenga en cuenta que Adaboost también puede adoptar el método de actualización de peso utilizado en Adarank. Para Adaboost son equivalentes (cf., [12] Página 305). Sin embargo, esto no es cierto para Adarank.3.6 Construcción de Ranker débil Consideramos una implementación eficiente para la construcción de rango débil, que también se usa en nuestros experimentos. En la implementación, como Ranker débil, elegimos la característica que tiene el rendimiento ponderado óptimo entre todas las características: max k m i = 1 pt (i) e (π (qi, di, xk), yi). Creando clasificadores débiles de esta manera, el proceso de aprendizaje resulta ser el de seleccionar repetidamente características y combinar linealmente las características seleccionadas. Tenga en cuenta que las características que no se seleccionan en la fase de entrenamiento tendrán un peso de cero.4. Resultados experimentales realizamos experimentos para probar el rendimiento de Adarank utilizando cuatro conjuntos de datos de referencia: Ohsumed, WSJ, AP y .gov. Tabla 2: Características utilizadas en los experimentos en conjuntos de datos OHSUMED, WSJ y AP. C (w, d) representa la frecuencia de la palabra w en el documento d;C representa toda la colección;n denota el número de términos en la consulta;|· |denota la función de tamaño;e ID F (·) denota la frecuencia de documentos inversos.1 wi∈Q d ln (c (wi, d) + 1) 2 wi∈Q d ln (| c | c (wi, c) + 1) 3 wi∈Q d ln (id f (wi)) 4 wi∈Q d ln(c (wi, d) | d | + 1) 5 wi∈Q d ln (c (wi, d) | d | · id f (wi) + 1) 6 wi∈Q d ln (c (wi, d) · ·| C | | d | · c (wi, c) + 1) 7 ln (puntaje BM25) 0.2 0.3 0.4 0.4 0.5 0.6 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm rarnkboost adarank.map adarank.ndcgFigura 2: precisiones de clasificación en datos de ohsumed.4.1 Clasificación del experimento Ranking SVM [13, 16] y RankBoost [8] se seleccionaron como líneas de base en los experimentos, porque son los métodos de vanguardia para el aprendizaje de los arte. Además, BM25 [24] se utilizó como línea de base, que representa el método IR de última generación (en realidad utilizamos la herramienta Lemur1). Para Adarank, el parámetro T se determinó automáticamente durante cada experimento. Específicamente, cuando no hay mejora en la precisión de clasificación en términos de la medida de rendimiento, la iteración se detiene (y se determina T). Como se utilizaron la medida E, MAP y NDCG@5. Los resultados para Adarank usando MAP y NDCG@5 como medidas en el entrenamiento se representan como adarank.map y adarank.ndcg, respectivamente.4.2 Experimento con datos de ohsumed En este experimento, utilizamos el conjunto de datos de OHSUMED [14] para probar el rendimiento de Adarank. El conjunto de datos de OHSUMed consta de 348,566 documentos y 106 consultas. Hay en total 16.140 pares de documentos de consulta sobre los cuales se realizan juicios de relevancia. Los juicios de relevancia son D (definitivamente relevantes), P (posiblemente relevantes) o N (no relevantes). Los datos se han utilizado en muchos experimentos en IR, por ejemplo [4, 29]. Como características, adoptamos los utilizados en la recuperación de documentos [4]. La Tabla 2 muestra las características. Por ejemplo, TF (frecuencia de término), IDF (frecuencia de documento inversa), DL (longitud del documento) y combinaciones de ellos se definen como características. La puntuación BM25 en sí también es una característica. Se eliminaron las palabras de parada y se realizó la derecha en los datos. Dividimos al azar consultas en cuatro subconjuntos pares y realizamos experimentos de validación cruzada de 4 veces. Sintonizamos los parámetros para BM25 durante una de las pruebas y los aplicamos a los otros ensayos. Los resultados informados en la Figura 2 son los promediados en cuatro ensayos. En el cálculo del mapa, definimos el rango D como relevante y 1 http://www.lemurproject.com Tabla 3: Estadísticas en conjuntos de datos WSJ y AP. Conjunto de datos # consultas # # documentos recuperados # documentos por consulta ap 116 24,727 213.16 WSJ 126 40,230 319.29 0.40 0.45 0.50 0.55 0.60 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 clasificación svm rankoost adarank.map adarank.ndc figuraprecisiones en el conjunto de datos WSJ.Los otros dos rangos son irrelevantes. De la Figura 2, vemos que tanto adarank.map como adarank.ndcg superan a BM25, clasificando SVM y RankBoost en términos de todas las medidas. Realizamos pruebas significativas (prueba t) en las mejoras de Adarank.map sobre BM25, clasificación de SVM y RankBoost en términos de mapa. Los resultados indican que todas las mejoras son estadísticamente significativas (valor p <0.05). También realizamos la prueba t en las mejoras de adarank.ndcg sobre BM25, clasificación de SVM y RankBoost en términos de NDCG@5. Las mejoras también son estadísticamente significativas.4.3 Experimente con los datos de WSJ y AP En este experimento, utilizamos los conjuntos de datos WSJ y AP de la pista de recuperación AD-hoc trec, para probar las actuaciones de Adarank. WSJ contiene 74,520 artículos de Wall Street Journals de 1990 a 1992, y AP contiene 158,240 artículos de Associated Press en 1988 y 1990. 200 consultas se seleccionan de los temas de TREC (No.101 ∼ No.300). Cada consulta tiene una serie de documentos asociados y están etiquetados como relevantes o irrelevantes (para la consulta). Después de la práctica en [28], las consultas que tienen menos de 10 documentos relevantes se descartaron. La Tabla 3 muestra las estadísticas en los dos conjuntos de datos. De la misma manera que en la Sección 4.2, adoptamos las características enumeradas en la Tabla 2 para la clasificación. También realizamos experimentos de valoración cruzada de 4 veces. Los resultados informados en la Figura 3 y 4 son los promediaron en cuatro ensayos en conjuntos de datos WSJ y AP, respectivamente. De la Figura 3 y 4, podemos ver que Adarank.Map y Adarank.ndcg superan a BM25, clasificando SVM y RankBoost en términos de todas las medidas en WSJ y AP. Realizamos pruebas t sobre las mejoras de adarank.map y adarank.ndcg sobre BM25, clasificando SVM y RankBoost en WSJ y AP. Los resultados indican que todas las mejoras en términos de MAP son estadísticamente significativas (valor p <0.05). Sin embargo, solo algunas de las mejoras en términos de NDCG@5 son estadísticamente significativas, aunque en general las mejoras en las puntuaciones NDCG son bastante altas (1-2 puntos).4.4 Experimente con datos .gov En este experimento, utilizamos los datos de TREC .gov para probar el rendimiento de Adarank para la tarea de recuperación web. El corpus es un rastreo del dominio .gov a principios de 2002, y se ha utilizado en la pista web de TREC desde 2002. Hay un total de 0.40 0.45 0.50 0.55 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm rankboost adarank.map adarank.ndcg Figura 4: precisiones de clasificación en dataSet AP.0.1 0.2 0.3 0.4 0.5 0.6 0.7 mapa ndcg@1 ndcg@3 ndcg@5 ndcg@10 bm25 ranking svm rankboost adarank.map adarank.ndcg Figura 5: precisiones de clasificación en .gov dataSet. Tabla 4: Características utilizadas en los experimentos en el conjunto de datos .gov.1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 Hostrank [30] 5 Propagación de relevancia [23] (10 características) de 1,053,110 páginas web con 11,164,829 hipervínculos en los datos. Se utilizaron las 50 consultas en la tarea de destilación del tema en la pista web de TREC 2003 [6]. Las verdades de tierra para las consultas son proporcionadas por el Comité TREC con juicio binario: relevante o irrelevante. El número de páginas relevantes varía de consulta a consulta (de 1 a 86). Extrajimos 14 características de cada par de documentos de consulta. La Tabla 4 ofrece una lista de las características. Son las salidas de algunos algoritmos (sistemas) bien conocidos. Estas características son diferentes de las de la Tabla 2, porque la tarea es diferente. Nuevamente, realizamos experimentos de 4 veces de validación cruzada. Los resultados promediados en cuatro ensayos se informan en la Figura 5. A partir de los resultados, podemos ver que Adarank.Map y Adarank.ndcg superan a todas las líneas de base en términos de todas las medidas. Realizamos ttests en las mejoras de adarank.map y adarank.ndcg sobre BM25, clasificación de SVM y RankBoost. Algunas de las mejoras no son estadísticamente significativas. Esto se debe a que solo tenemos 50 consultas utilizadas en los experimentos, y el número de consultas es demasiado pequeña.4.5 Discusiones Investigamos las razones por las que Adarank supera los métodos de referencia, utilizando los resultados del conjunto de datos de ohsumido como ejemplos. Primero, examinamos la razón por la que Adarank tiene actuaciones más altas que en clasificar SVM y RankBoost. Específicamente, com0.58 0.60 0.62 0.64 0.66 0.68 D-N D-P P-N Tipo de pareja de precisión Ranking SVM RankBoost Adarank.map adarank.ndcg Figura 6: Precisión en los pares de documentos de clasificación con un conjunto de datos de suma.0 2 4 6 8 10 12 Número del número de pares de documentos por consulta Figura 7: Distribución de consultas con diferentes pares de pares de documentos en datos de entrenamiento de prueba 1. Apare las tasas de error entre los diferentes pares de rango realizados por la clasificación de SVM, RankBoost, Adarank.MAP, y adarank.ndcg en los datos de prueba. Los resultados promediados en cuatro ensayos en la validación cruzada de 4 veces se muestran en la Figura 6. Usamos D-N para defender los pares entre definitivamente relevantes y no relevantes, D-P los pares entre definitivamente relevantes y parcialmente relevantes, y P-N los pares entre parcialmente relevantes y no relevantes. De la Figura 6, podemos ver que Adarank.Map y Adarank.ndcg cometen menos errores para D-N y D-P, que están relacionados con la parte superior de las clasificaciones y son importantes. Esto se debe a que ADARANK.MAP y ADARANK.NDCG pueden centrarse naturalmente en el entrenamiento en las partes superiores optimizando MAP y NDCG@5, respectivamente. También hicimos estadísticas sobre el número de pares de documentos por consulta en los datos de capacitación (para el juicio 1). Las consultas se agrupan en diferentes grupos en función del número de sus pares de documentos asociados. La Figura 7 muestra la distribución de los grupos de consultas. En la figura, por ejemplo, 0-1K es el grupo de consultas cuyo número de pares de documentos se encuentran entre 0 y 999. Podemos ver que los números de pares de documentos realmente varían de consulta a consulta. A continuación, evaluamos las precisiones de Adarank.Map y RankBoost en términos de mapa para cada uno de los grupos de consultas. Los resultados se informan en la Figura 8. Descubrimos que el mapa promedio de Adarank.map sobre los grupos es dos puntos más alto que RankBoost. Además, es interesante ver que Adarank.Map funciona particularmente mejor que RankBoost para consultas con pequeños números de pares de documentos (por ejemplo, 0-1K, 1K-2K y 2K-3K). Los resultados indican que ADARANK.MAP puede evitar la creación de un modelo sesgado hacia consultas con más pares de documentos. Para adarank.ndcg, se pueden observar resultados similares.0.2 0.3 0.4 0.5 MAP GRUPO DE MAP RANCOAT ADARANK. MAP Figura 8: Diferencias en MAP para diferentes grupos de consultas.0.30 0.31 0.32 0.33 0.34 Prueba 1 Prueba 2 Prueba 3 Prueba 4 mapa adarank.map adarank.ndcg Figura 9: mapa en el conjunto de entrenamiento cuando el modelo está entrenado con map o ndcg@5. Además, realizamos un experimento para ver si Adarank tiene la capacidad de mejorar la precisión de la clasificación en términos de una medida utilizando la medida en el entrenamiento. Específicamente, entrenamos modelos de clasificación con adarank.map y adarank.ndcg y evaluamos sus precisiones en el conjunto de datos de entrenamiento en términos de map y ndcg@5. El experimento se realizó para cada prueba. La Figura 9 y la Figura 10 muestran los resultados en términos de MAP y NDCG@5, respectivamente. Podemos ver que, adarank.map entrenado con mapa funciona mejor en términos de mapa, mientras que adarank.ndcg entrenado con ndcg@5 funciona mejor en términos de ndcg@5. Los resultados indican que Adarank puede mejorar el rendimiento de la clasificación en términos de una medida utilizando la medida en el entrenamiento. Finalmente, tratamos de verificar la corrección del teorema 1. Es decir, la precisión de clasificación en términos de la medida de rendimiento puede mejorarse continuamente, siempre que E - Δt min 1 - ϕ (t) 2 <1 se mantenga. Como ejemplo, la Figura 11 muestra la curva de aprendizaje de Adarank.map en términos de mapa durante la fase de entrenamiento en una prueba de la validación cruzada. De la figura, podemos ver que la precisión de clasificación de Adarank.map mejora constantemente, a medida que avanza el entrenamiento, hasta que llega al pico. El resultado está de acuerdo bien con el teorema 1. 5. Conclusión y trabajo futuro En este documento hemos propuesto un algoritmo novedoso para los modelos de clasificación de aprendizaje en la recuperación de documentos, denominado Adarank. A diferencia de los métodos existentes, Adarank optimiza una función de pérdida que se define directamente en las medidas de rendimiento. Emplea una técnica de impulso en la clasificación del aprendizaje del modelo. Adarank ofrece varias ventajas: facilidad de implementación, solidez teórica, eficiencia en la capacitación y alta precisión en la clasificación. Los resultados experimentales basados en cuatro conjuntos de datos de referencia muestran que Adarank puede superar significativamente los métodos de referencia de BM25, Ranking SVM y RankBoost.0.49 0.50 0.51 0.52 0.53 Prueba 1 Prueba 2 Prueba 3 Prueba 4 NDCG@5 Adarank.map adarank.ndcg Figura 10: NDCG@5 en el conjunto de entrenamiento cuando el modelo está entrenado con MAP o NDCG@5.0.29 0.30 0.31 0.32 0 50 100 150 200 250 300 350 Mapa Número de rondas Figura 11: Curva de aprendizaje de Adarank. El trabajo futuro incluye un análisis teórico sobre el error de generalización y otras propiedades del algoritmo Adarank, y más evaluaciones empíricas del algoritmo, incluidas las comparaciones con otros algoritmos que pueden optimizar directamente las medidas de rendimiento.6. Agradecimientos Agradecemos a Harry Shum, Wei-Ying MA, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire y Andrew Arnold por sus valiosos comentarios y sugerencias a este documento.7. Referencias [1] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison Wesley, mayo de 1999. [2] C. Burges, R. Ragno y Q. Le. Aprender a clasificarse con funciones de costo no suave. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificarse usando descenso de gradiente. En ICML 22, páginas 89-96, 2005. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W.Excmo Adaptación de ranking SVM para la recuperación de documentos. En Sigir 29, páginas 186-193, 2006. [5] D. Cossock y T. Zhang. Ranking de subconjunto usando regresión. En Colt, páginas 605-619, 2006. [6] N. Craswell, D. Hawking, R. Wilkinson y M. Wu. Descripción general de la pista web TREC 2003. En Trec, páginas 78-92, 2003. [7] N. Duffy y D. Helmbold. Aumento de métodos para la regresión. Mach. Learn., 47 (2-3): 153-200, 2002. [8] Y. Freund, R. D. Iyer, R. E. Schapire e Y. Cantante. Un algoritmo de impulso eficiente para combinar las preferencias. Journal of Machine Learning Research, 4: 933-969, 2003. [9] Y. Freund y R. E. Schapire. Una generalización teórica de decisión del aprendizaje en línea y una aplicación para aumentar. J. Comput. Syst. Sci., 55 (1): 119-139, 1997. [10] J. Friedman, T. Hastie y R. Tibshirani. Regresión logística aditiva: una visión estadística del impulso. The Annals of Statistics, 28 (2): 337-374, 2000. [11] G. Fung, R. Rosales y B. Krishnapuram. Rankings de aprendizaje a través de la separación de casco convexo. En Avances en Sistemas de Procesamiento de Información Neural 18, páginas 395-402. MIT Press, Cambridge, MA, 2006. [12] T. Hastie, R. Tibshirani y J. H. Friedman. Los elementos del aprendizaje estadístico. Springer, agosto de 2001. [13] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de margen grande para la regresión ordinal. MIT Press, Cambridge, MA, 2000. [14] W. Hersh, C. Buckley, T. J. Leone y D. Hickam. OHSUMED: una evaluación de recuperación interactiva y una nueva recopilación de pruebas grandes para la investigación. En Sigir, páginas 192-201, 1994. [15] K. Jarvelin y J. Kekalainen. IR Métodos de evaluación para recuperar documentos altamente relevantes. En Sigir 23, páginas 41-48, 2000. [16] T. Joachims. Optimización de los motores de búsqueda utilizando datos de clics. En Sigkdd 8, páginas 133-142, 2002. [17] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En ICML 22, páginas 377-384, 2005. [18] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En Sigir 24, páginas 111-119, 2001. [19] D. A. Metzler, W. B. Croft y A. McCallum. Maximización directa de métricas basadas en rango para la recuperación de información. Informe técnico, CIIR, 2005. [20] R. Nallapati. Modelos discriminativos para la recuperación de información. En Sigir 27, páginas 64-71, 2004. [21] L. Page, S. Brin, R. Motwani y T. Winograd. Ranking de citas de PageRank: traer orden a la web. Informe técnico, Proyecto de Tecnologías de la Biblioteca Digital de Stanford, 1998. [22] J. M. Ponte y W. B. Croft. Un enfoque de modelado de idiomas para la recuperación de información. En Sigir 21, páginas 275-281, 1998. [23] T. Qin, T.-Y. Liu, X.-D.Zhang, Z. Chen y W.-Y. Mamá. Un estudio de propagación de relevancia para la búsqueda web. En Sigir 28, páginas 408-415, 2005. [24] S. E. Robertson y D. A. Cáscara. El informe final de la pista de filtrado TREC-9. En Trec, páginas 25-40, 2000. [25] R. E. Schapire, Y. Freund, P. Barlett y W. S. Lee. Aumentando el margen: una nueva explicación para la efectividad de los métodos de votación. En ICML 14, páginas 322-330, 1997. [26] R. E. Schapire e Y. Cantante. Algoritmos de impulso mejorados utilizando predicciones con clasificación de confianza. Mach. Learn., 37 (3): 297-336, 1999. [27] R. Song, J. Wen, S. Shi, G. Xin, T. Yan Liu, T. Qin, X. Zheng, J. Zhang,G. Xue y W.-Y. Mamá. Microsoft Research Asia en Web Track y Terabyte Track de TREC 2004. En Trec, 2004. [28] A. Trotman. Aprendiendo a clasificar. Inf. Retr., 8 (3): 359-381, 2005. [29] J. Xu, Y. Cao, H. Li e Y. Huang. Aprendizaje sensible a los costos de SVM para la clasificación. En ECML, páginas 833-840, 2006. [30] G.-R.Xue, Q. Yang, H.-J. Zeng, Y. Yu y Z. Chen. Explotando la estructura jerárquica para el análisis de enlaces. En Sigir 28, páginas 186-193, 2005. [31] H. Yu. Muestreo selectivo SVM para clasificar con la aplicación a la recuperación de datos. En Sigkdd 11, páginas 354-363, 2005. Apéndice aquí damos la prueba del teorema 1. P. Establecer zt = m i = 1 exp {−e (π (qi, di, ft), yi)} y φ (t) = 1 2 (1 + ϕ (t)). Según la definición de αT, sabemos que eαt = φ (t) 1 - φ (t). Zt = m i = 1 exp {−e (π (qi, di, ft - 1 + αt ht), yi)} = m i = 1 exp −e (π (qi, di, ft - 1), yi) - αtE (π (qi, di, ht), yi) - Δt i ≤ m i = 1 exp {−e (π (qi, di, ft - 1), yi)} exp {−αt e (π (qi, di, ht), yi)} e - Δt min = e - quirado min zt - 1 m i = 1 exp {−e (π (qi, di, ft - 1), yi)} zt - 1 exp {−αt e (π (Qi, Di, Ht), yi)} = e - quirólo Zt - 1 m i = 1 pt (i) exp {−αt e (π (qi, di, ht), yi)}. Además, si e (π (qi, di, ht), yi) ∈ [−1, +1] entonces, zt ≤ e - quir, ht), yi) 2 e - αt + 1 - e (π (qi, di, ht), yi) 2 eαt = e - quirφ (t) φ (t) + (1 - φ (t)) φ (t) 1 - φ (t)   = zt - 1e - Δt min 4φ (t) (1 - φ(T)) ≤ zt - 2 t t = t - 1 e -quir)) = m m i = 1 1 m exp {−e (π (qi, di, α1h1), yi)} t t = 2 e - quirm exp {−α1e (π (qi, di, h1), yi) - Δ1 i} t t = 2 e - quirm exp {−α1e (π (qi, di, h1), yi)} t t = 2 e -quirφ (1)) t t = 2 e - Δt min 4φ (t) (1 - φ (t)) = m t t = 1 e - quirado min 1 - ϕ (t) 2.∴ 1 m m i = 1 e (π (qi, di, ft), yi) ≥ 1 m m i = 1 {1 - exp (−e (π (qi, di, ft), yi))} ≥ 1 - t = t =1 E - poro min 1 - ϕ (t) 2.