Una definición basada en la frecuencia y basada en Poisson de la probabilidad de ser informativo del Departamento de Ciencias de la Computación de Thomas Roelleke, Queen Mary, Universidad de Londres, thor@dcs.qmul.ac.uk Resumen Este documento informa sobre investigaciones teóricas sobre los supuestos que subyacen al documento inversoFrecuencia (IDF). Mostramos que una función de probabilidad intuitiva basada en IDF para la probabilidad de que un término sea informativo asume eventos de documentos disjuntos. Al asumir que los documentos son independientes en lugar de disjuntos, llegamos a una probabilidad basada en Poisson de ser informativos. El marco es útil para comprender y decidir la estimación de parámetros y la combinación en modelos de recuperación probabilística. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: Modelos de recuperación Términos generales Teoría 1. Introducción y antecedentes La frecuencia de documentos inverso (IDF) es uno de los parámetros más exitosos para una clasificación basada en la relevancia de objetos recuperados. Siendo n el número total de documentos, y n (t) es el número de documentos en los que ocurre el término t, el IDF se define de la siguiente manera: IDF (t): = - log n (t) n, 0 <= IDF(t) <∞ La clasificación basada en la suma de los valores IDF de los términos de consulta que ocurren en los documentos recuperados funciona bien, esto se ha mostrado en numerosas aplicaciones. Además, es bien sabido que la combinación de un peso de término específico de documento y las FDI funcionan mejor que las FDI solas. Este enfoque se conoce como TF-IDF, donde tf (t, d) (0 <= tf (t, d) <= 1) es la llamada frecuencia térmica del término t en el documento d.La FDI refleja el poder discriminatorio (informatividad) de un término, mientras que el TF refleja la ocurrencia de un término. La FDI por sí sola funciona mejor que el TF solo. Una explicación podría ser el problema de TF con los términos que ocurren en muchos documentos;Nos referemos a esos términos como términos ruidosos. Utilizamos la noción de términos ruidosos en lugar de términos frecuentes, ya que los términos frecuentes deja abiertos si nos referimos a la frecuencia del documento de un término en una colección o a la llamada frecuencia de término (también conocida como con frecuencia de indocumento) de un término en un término en undocumento. Asociamos el ruido con la frecuencia del documento de un término en una colección, y asociamos la ocurrencia con la frecuencia de inincumento de un término. El TF de un término ruidoso podría ser alto en un documento, pero los términos ruidosos no son buenos candidatos para representar un documento. Por lo tanto, la eliminación de términos ruidosos (conocidos como eliminación de palabras de parada) es esencial al aplicar TF. En un enfoque TF-IDF, la eliminación de las palabras de parada es conceptualmente obsoleta, si las palabras de parada son solo palabras con IDF bajas. Desde un punto de vista probabilístico, TF es un valor con una interpretación probabilística basada en la frecuencia, mientras que IDF tiene una interpretación informativa en lugar de una probabilística. La interpretación probabilística faltante de las FDI es un problema en los modelos de recuperación probabilística donde combinamos un conocimiento incierto de diferentes dimensiones (por ejemplo .: Información de los términos, estructura de documentos, calidad de documentos, edad de documentos, etc.) de modo que una buena estimación de la estimación de laSe logra la probabilidad de relevancia. Una solución intuitiva es una normalización de IDF de tal manera que obtenemos valores en el intervalo [0;1]. Por ejemplo, considere una normalización basada en el valor máximo de IDF. Sea t el conjunto de términos que ocurren en una colección. Pfreq (t es informativo): = idf (t) maxidf maxidf: = max ({idf (t) | t ∈ T}), maxidf <= - log (1/n) minidf: = min ({idf (t)| t ∈ T}), minidf> = 0 minidf maxidf ≤ pfreq (t es informativo) ≤ 1.0 Esta función de probabilidad basada en frecuencia cubre el intervalo [0;1] Si la IDF mínima es igual a cero, que es el caso si tenemos al menos un término que ocurre en todos los documentos. ¿Podemos interpretar PFREQ, las IDF normalizadas, como la probabilidad de que el término sea informativo? Al investigar la interpretación probabilística de las 227 IDF normalizadas, hicimos varias observaciones relacionadas con la desargación e independencia de los eventos de documentos. Estas observaciones se informan en la Sección 3. Mostramos en la Sección 3.1 que la probabilidad de ruido basada en la frecuencia N (t) N utilizada en la definición clásica de IDF puede explicarse por tres supuestos: ocurrencia de término binario, contención constante de documentos y desjodidad de los eventos de contención de documentos. En la Sección 3.2 mostramos que al suponer la independencia de los documentos, obtenemos 1 - E - 1 ≈ 1 - 0.37 como el límite superior de la probabilidad de ruido de un término. El valor E - 1 está relacionado con el logaritmo e investigamos en la Sección 3.3 el enlace a la teoría de la información. En la Sección 4, vinculamos los resultados de las secciones anteriores a la teoría de la probabilidad. Mostramos los pasos desde posibles mundos hasta distribución binomial y distribución de Poisson. En la Sección 5, enfatizamos que el marco teórico de este documento es aplicable tanto para IDF como para TF. Finalmente, en la Sección 6, basamos la definición de la probabilidad de ser informativo sobre los resultados de las secciones anteriores y comparar definiciones basadas en frecuencia y basadas en Poisson.2. Antecedentes La relación entre frecuencias, probabilidades y teoría de la información (entropía) ha sido el foco de muchos investigadores. En esta sección de antecedentes, nos centramos en el trabajo que investiga la aplicación de la distribución de Poisson en IR ya que una parte principal del trabajo presentado en este documento aborda los supuestos subyacentes de Poisson.[4] propone un modelo de 2 poisson que tenga en cuenta la naturaleza diferente de los documentos relevantes y no relevantes, términos raros (palabras de contenido) y términos frecuentes (términos ruidosos, palabras de función, palabras de parada).[9] muestra experimentalmente que la mayoría de los términos (palabras) en una colección se distribuyen de acuerdo con un modelo N-poisson de baja dimensión.[10] utiliza un modelo de 2 poisson para incluir probabilidades basadas en frecuencia a término en el modelo de recuperación probabilística. La escala no lineal de la función Poisson mostró una mejora significativa en comparación con una probabilidad lineal basada en la frecuencia. El modelo Poisson se aplicó aquí al término frecuencia de un término en un documento. Generalizaremos la discusión señalando que la frecuencia del documento y la frecuencia de término son parámetros duales en el espacio de recopilación y el espacio de documentos, respectivamente. Nuestra discusión sobre la distribución de Poisson se centra en la frecuencia del documento en una colección en lugar de en el término frecuencia en un documento.[7] y [6] abordan la desviación de IDF y Poisson, y aplican mezclas de Poisson para lograr mejores estimaciones basadas en Poisson. Los resultados probaron nuevamente experimentalmente que un Poisson ontimensional no funciona para términos raros, por lo tanto, se proponen mezclas de poisson y parámetros adicionales.[3], Sección 3.3, ilustra y resume integralmente las relaciones entre frecuencias, probabilidades y Poisson. Las diferentes definiciones de IDF se ponen en contexto y se define una noción de ruido, donde el ruido se ve como el complemento de las FDI. Usamos en nuestro documento una noción diferente de ruido: consideramos un ruido basado en la frecuencia que corresponde a la frecuencia del documento, y consideramos un ruido de término que se basa en la independencia de los eventos de documentos.[11], [12], [8] y [1] Enlace frecuencias y estimación de probabilidad a la teoría de la información.[12] establece un marco en el que los modelos de recuperación de información se formalizan en función de la inferencia probabilística. Un componente clave es el uso de un espacio de eventos disjuntos, donde el marco utiliza principalmente términos como eventos disjuntos. La probabilidad de ser informativa definida en nuestro documento puede verse como la probabilidad de los términos disjuntos en el término espacio de [12].[8] Dirección de entropía y distribuciones bibliométricas. La entropía es máxima si todos los eventos son equiprobables y la ley de Lotka basada en la frecuencia (N/iλ es el número de científicos que han escrito las publicaciones I, donde N y λ son parámetros de distribución), ZIPF y la distribución de Pareto están relacionadas. La distribución de Pareto es el caso continuo de Lotka y Lotka y Zipf Show equivalencias. La distribución de Pareto es utilizada por [2] para la normalización de la frecuencia de término. La distribución de Pareto se compara con la distribución de Poisson en el sentido de que Pareto es de cola de grasa, i.mi.Pareto asigna mayores probabilidades a grandes cantidades de eventos que las distribuciones de Poisson. Esto hace que Pareto sea interesante ya que se siente que Poisson es demasiado radical en eventos frecuentes. Restringimos en este documento a la discusión de Poisson, sin embargo, nuestros resultados muestran que, de hecho, una distribución más suave que Poisson promete ser un buen candidato para mejorar la estimación de las probabilidades en la recuperación de la información.[1] establece un vínculo teórico entre TF-IDF y la teoría de la información y la investigación teórica sobre el significado de TF-IDF aclara el modelo estadístico en el que las diferentes medidas se basan comúnmente. Esta motivación coincide con la motivación de nuestro artículo: investigamos teóricamente los supuestos de las FDI y Poisson clásicas para una mejor comprensión de la estimación y combinación de parámetros.3. De disjunto a independiente definimos y discutimos en esta sección tres probabilidades: la probabilidad de ruido basada en la frecuencia (Definición 1), la probabilidad de ruido total para documentos disjuntos (definición 2).y la probabilidad de ruido para documentos independientes (definición 3).3.1 Ocurrencia binaria, contención constante y disgusto de los documentos que mostramos en esta sección, que la probabilidad de ruido basada en la frecuencia n (t) n en la definición de IDF puede explicarse como una probabilidad total con ocurrencia de término binario, contención constante de documentos y disgusto deConsejos de documento. Nos referimos a una función de probabilidad como binaria si para todos los eventos la probabilidad es 1.0 o 0.0. La probabilidad de ocurrencia P (t | d) es binaria, si P (t | d) es igual a 1.0 si t ∈ D, y p (t | d) es igual a 0.0, de lo contrario. P (t | d) es binario: ⇐⇒ p (t | d) = 1.0 ∨ p (t | d) = 0.0 Nos referimos a una función de probabilidad como constante si para todos los eventos la probabilidad es igual. La probabilidad de contención del documento refleja la posibilidad de que un documento ocurra en una colección. Esta probabilidad de contención es constante si no tenemos información sobre la contención del documento o ignoramos que los documentos difieren en contención. La contención podría derivarse, por ejemplo, del tamaño, calidad, edad, enlaces, etc. de un documento. Para una contención constante en una colección con n documentos, 1 n a menudo se supone como la probabilidad de contención. Generalizamos esta definición e introducimos la constante λ donde 0 ≤ λ ≤ N. La contención de un documento D depende de la colección C, esto se refleja en la notación p (d | c) utilizada para la contención 228 de un documento. P (d | c) es constante: ⇐⇒ ∀d: p (d | c) = λ n para documentos disjuntos que cubren todo el espacio de eventos, establecemos λ = 1 y obtenemos èd p (d | c) = 1.0. A continuación, definimos la probabilidad de ruido basada en la frecuencia y la probabilidad de ruido total para documentos disjuntos. Presentamos la notación del evento que t es ruidosa y se produce para marcar la diferencia entre la probabilidad de ruido P (t es ruidoso | c) en una colección y la probabilidad de ocurrencia P (t ocurre | d) en un documento más explícito, manteniéndose asíTenga en cuenta que la probabilidad de ruido corresponde a la probabilidad de ocurrencia de un término en una colección. Definición 1. La probabilidad de ruido del término basado en frecuencia: PFREQ (t es ruidoso | c): = n (t) n definición 2. La probabilidad de ruido de término total para documentos disjunto: PDIS (t es ruidoso | c): = d p (t ocurre | d) · p (d | c) Ahora, podemos formular un teorema que hace que supuestaciones explícitas que explicen las IDF clásicas. Teorema 1. Suposiciones de IDF: si la probabilidad de ocurrencia P (t | d) del término t en los documentos d es binario, y la probabilidad de contención p (d | c) de los documentos d es constante, y las contenedores de documentos son eventos disjuntos, entonces la probabilidad de ruido paraDocumentos disjuntos es igual a la probabilidad de ruido basada en la frecuencia. PDIS (t es ruidoso | c) = pfreq (t es ruidoso | c) prueba. Los supuestos son: ∀d: (p (t ocurre | d) = 1 ∨ p (t ocurre | d) = 0) ∧ p (d | c) = λ n ∧ d p (d | c) = 1.0 obtenemos: Pdis (t es ruidoso | c) = d | t∈D 1 n = n (t) n = pfreq (t es ruidoso | c) El resultado anterior no es una sorpresa, pero es una formulación matemática de supuestos que pueden serSe usa para explicar las IDF clásicas. Los supuestos hacen explícitos que los diferentes tipos de tipos de términos ocurren en documentos (frecuencia de un término, importancia de un término, posición de un término, parte del documento donde ocurre el término, etc.) y los diferentes tipos de contención de documentos (tamaño, calidad de calidad, la edad, etc.) se ignoran y las contenedores de documentos se consideran eventos disjuntos. De los supuestos, podemos concluir que las FDI (ruido basado en frecuencia, respectivamente) es una estimación relativamente simple pero estricta. Aún así, FDF funciona bien. Esto podría explicarse por un efecto de apalancamiento que justifica la ocurrencia binaria y la contención constante: el término ocurrencia para documentos pequeños tiende a ser mayor que para documentos grandes, mientras que la contención de documentos pequeños tiende a ser más pequeño que para documentos grandes. Desde ese punto de vista, IDF significa que P (T ∧ D | C) es constante para todos los D en el que ocurre T, y P (T ∧ D | C) es cero de otra manera. La ocurrencia y la contención pueden ser específicos. Por ejemplo, establecer P (t∧d | c) = 1/nd (c) si t ocurre en d, donde nd (c) es el número de documentos en la colección c (usamos antes de solo n). Elegimos una ocurrencia dependiente del documento P (t | d): = 1/nt (d), i.mi.La probabilidad de ocurrencia es igual al inverso de NT (d), que es el número total de términos en el documento d.A continuación, elegimos la contención p (d | c): = nt (d)/nt (c) · nt (c)/nd (c) donde nt (d)/nt (c) es una normalización de longitud del documento (númerode términos en el documento D dividido por el número de términos en la colección c), y nt (c)/nd (c) es un factor constante de la colección (número de términos en la colección c dividido por el número de documentos en la colección c). Obtenemos P (T∧D | C) = 1/nd (c). En una función tf -IDF -rprieval, el componente TF refleja la probabilidad de ocurrencia de un término en un documento. Esta es una explicación adicional por qué podemos estimar la FDI con una P (T | D) simple, ya que el TF-IDF combinado contiene la probabilidad de ocurrencia. La probabilidad de contención corresponde a una normalización del documento (normalización de la longitud del documento, longitud del documento giratado) y normalmente se adjunta al componente TF o al producto TF -IDF. La suposición de desargación es típica de las probabilidades basadas en frecuencia. Desde el punto de vista de la teoría de la probabilidad, podemos considerar los documentos como eventos disjuntos, para lograr un modelo teórico sólido para explicar las IDF clásicas. Pero, ¿la desargación refleja el mundo real donde la contención de un documento parece ser independiente de la contención de otro documento? En la siguiente sección, reemplazamos la suposición de desargación por la suposición de independencia.3.2 El límite superior de la probabilidad de ruido para documentos independientes para documentos independientes, calculamos la probabilidad de una disyunción como de costumbre, a saber, como el complemento de la probabilidad de la conjunción de los eventos negados: P (D1 ∨ ... ∨ DN)= 1 - p (¬D1 ∧ ... ∧ ¬DN) = 1 - d (1 - p (d)) La probabilidad de ruido puede considerarse como la conjunción del término ocurrencia y la contención del documento. P (t es ruidoso | c): = P (t ocurre ∧ (d1 ∨ ... ∨ dn) | c) Para documentos disjuntos, esta visión de la probabilidad de ruido condujo a la definición 2. Para documentos independientes, ahora usamos la conjunción de eventos negados. Definición 3. El término probabilidad de ruido para documentos independientes: el pin (t es ruidoso | c): = d (1 - p (t ocurre | d) · p (d | c)) con ocurrencia binaria y una contención constante p (d | c): = λ/n, obtenemos el término ruido de un término t que ocurre en n (t) documentos: pin (t es ruidoso | c) = 1 - 1 - λ n n (t) 229 para ocurrencia binaria y documentos disjunto, la probabilidad de contención fue 1/N. Ahora, con documentos independientes, podemos usar λ como parámetro de colección que controla la probabilidad de contención promedio. Mostramos a través del siguiente teorema que el límite superior de la probabilidad de ruido depende de λ.Teorema 2. El límite superior de ser ruidoso: si la ocurrencia p (t | d) es binaria, y la contención p (d | c) es constante, y los contenedores de documentos son eventos independientes, entonces 1 - e - λ es el límite superior delProbabilidad de ruido.∀T: PIN (t es ruidoso | c) <1 - e - λ prueba. El límite superior de la probabilidad de ruido independiente se deduce desde el límite limn → ∞ (1 + x n) n = ex (ver cualquier libro de matemáticas integral, por ejemplo, [5], para la ecuación de convergencia de la función Euler). Con x = −λ, obtenemos: lim n → ∞ 1 - λ n n = e - λ para el término ruido, tenemos: pin (t es ruidoso | c) = 1 - 1 - λ n n (t) pin (T es ruidoso | c) es estrictamente monótono: el ruido de un término TN es menor que el ruido de un término TN+1, donde se produce TN en n documentos y Tn+1 ocurre en documentos n+1. Por lo tanto, un término con n = n tiene la mayor probabilidad de ruido. Para una colección con muchos documentos infinitos, el límite superior de la probabilidad de ruido para los términos que ocurre en todos los documentos se convierte en: lim n → ∞ pin (tn es ruidoso) = lim n → ∞ 1 - 1 - λ n n = 1 - e−λ aplicando una independencia en lugar de una suposición de desargación, obtenemos la probabilidad e - 1 de que un término no sea ruidoso incluso si el término ocurre en todos los documentos. En el caso disjunto, la probabilidad de ruido es una para un término que ocurre en todos los documentos. Si vemos p (d | c): = λ/n como la contención promedio, entonces λ es grande para un término que ocurre principalmente en documentos grandes, y λ es pequeño para un término que ocurre principalmente en documentos pequeños. Por lo tanto, el ruido de un término t es grande si t ocurre en n (t) documentos grandes y el ruido es menor si se produce en documentos pequeños. Alternativamente, podemos asumir una contención constante y una ocurrencia dependiente de término. Si asumimos p (d | c): = 1, entonces p (t | d): = λ/n puede interpretarse como la probabilidad promedio de que t represente un documento. La suposición común es que la probabilidad promedio de contención u ocurrencia es proporcional a N (t). Sin embargo, aquí hay potencial adicional: las leyes estadísticas (ver [3] sobre Luhn y ZIPF) indican que la probabilidad promedio podría seguir una distribución normal, i.mi.Probabilidades pequeñas para N (t) pequeños y N (t) grandes, y mayores probabilidades para mediano N (t). Para el caso monótono que investigamos aquí, el ruido de un término con n (t) = 1 es igual a 1 - (1 - λ/n) = λ/n y el ruido de un término con n (t) = n escerca de 1- e - λ. En la siguiente sección, relacionamos el valor E - λ a la teoría de la información.3.3 La probabilidad de una señal informativa máxima La probabilidad E - 1 es especial en el sentido de que una señal con esa probabilidad es una señal con información máxima derivada de la definición de entropía. Considere la definición de la contribución de entropía H (t) de una señal t.H (t): = p (t) · - ln p (t) Formamos la primera derivación para calcular el óptimo.∂h (t) ∂p (t) = - ln p (t) + −1 p (t) · p (t) = - (1 + ln p (t)) para obtener óptimo, usamos: 0 = −(1 + ln p (t)) La contribución de entropía H (t) es máxima para p (t) = e - 1. Este resultado no depende de la base del logaritmo como vemos a continuación: ∂h (t) ∂p (t) = - logb p (t) + −1 p (t) · ln b · p (t) = −1 ln b + logb p (t) = - 1 + ln p (t) ln b resumimos este resultado en el siguiente teorema: Teorema 3. La probabilidad de una señal informativa máxima: la probabilidad PMAX = E - 1 ≈ 0.37 es la probabilidad de una señal informativa máxima. La entropía de una señal informativa máxima es Hmax = E - 1. Prueba. La probabilidad y entropía se deducen de la derivación anterior. El complemento de la probabilidad de ruido máxima es e - λ y ahora estamos buscando una generalización de la definición de entropía de tal manera que e - λ es la probabilidad de una señal informativa máxima. Podemos generalizar la definición de entropía calculando la integral de λ+ ln p (t), i.mi.Esta derivación es cero para e - λ. Obtenemos una entropía generalizada: - (λ + ln p (t)) d (p (t)) = p (t) · (1 - λ - ln p (t)) La entropía generalizada corresponde para λ = 1 alEntropía clásica. Al pasar de los documentos disjuntos a independientes, hemos establecido un vínculo entre el complemento de la probabilidad de ruido de un término que ocurre en todos los documentos y la teoría de la información. A continuación, vinculamos documentos independientes con la teoría de la probabilidad.4. El vínculo con la teoría de la probabilidad que revisamos para documentos independientes tres conceptos de teoría de probabilidad: posibles mundos, distribución binomial y distribución de Poisson.4.1 Posibles mundos Cada conjunción de eventos de documentos (para cada documento, consideramos dos eventos de documentos: el documento puede ser verdadero o falso) está asociado con un llamado mundo posible. Por ejemplo, considere los ocho mundos posibles para tres documentos (n = 3).230 World W Conjunción W7 D1 ∧ D2 ∧ D3 W6 D1 ∧ D2 ∧ ∧ ¬D1∧ ¬D2 ∧ D3 W0 ¬D1 ∧ ¬D2 ∧ ¬D3 Con cada mundo W, asociamos una probabilidad µ (W), que es igual al producto de las probabilidades únicas de los eventos de documentos.Probabilidad del mundo W µ (W) W7 λ n ¡3 · 1 - λ n ¡0 w6 λ n ¡2 · 1 - λ n ¡1 W5 λ n ¡2 · 1 - λ n ¡1 W4 λ n ¡1 · 1- λ n ¡2 W3 λ n ¡2 · 1 - λ n ¡1 1 w2 λ n ¡1 · 1 - λ n ¡2 w1 λ n ¡1 · 1 - λ n ¡2 w0 λ n ¡0 · 1 - λ n.N ¡3 La suma sobre los posibles mundos en los que los documentos k son verdaderos y los documentos N -k son falsos es igual a la función de probabilidad de la distribución binomial, ya que el coeficiente binomial produce el número de mundos posibles en los que los documentos K son ciertos.4.2 Distribución binomial La función de probabilidad binomial produce la probabilidad de que K de n eventos sean verdaderos donde cada evento es cierto con la probabilidad de evento único p.P (k): = binom (n, k, p): = n k pk (1 - p) n −k La probabilidad de evento único generalmente se define como p: = λ/n, i.mi.P es inversamente proporcional a N, el número total de eventos. Con esta definición de P, obtenemos para un número infinito de documentos el siguiente límite para el producto del coeficiente binomial y PK: lim n → ∞ n k pk = = lim n → ∞ n · (n −1) ·...· (N −k +1) k!λ n k = λk k! El límite está cerca del valor real para k << N. Para K grande, el valor real es menor que el límite. El límite de (1 - P) n −k sigue desde el límite limn → ∞ (1+ x n) n = ex.lim n → ∞ (1 - p) n - k = lim n → ∞ 1 - λ n n −k = lim n → ∞ e - λ · 1 - λ n −k = e - λ nuevamente, el límite está cerca delValor real para k << N. Para K grande, el valor real es mayor que el límite.4.3 Distribución de Poisson Para un número infinito de eventos, la función de probabilidad de Poisson es el límite de la función de probabilidad binomial.lim n → ∞ binom (n, k, p) = λk k!· E - λ p (k) = poisson (k, λ): = λk k!· E - λ La probabilidad Poisson (0, 1) es igual a E - 1, que es la probabilidad de una señal informativa máxima. Esto muestra la relación de la distribución de Poisson y la teoría de la información. Después de ver la convergencia de la distribución binomial, podemos elegir la distribución de Poisson como una aproximación de la probabilidad de ruido de término independiente. Primero, definimos la probabilidad de ruido de Poisson: definición 4. La probabilidad de ruido del término de Poisson: PPOI (t es ruidoso | c): = e - λ · n (t) k = 1 λk k! Para documentos independientes, la distribución de Poisson se aproxima a la probabilidad de la disyunción para N (t) grande, ya que la probabilidad de ruido del término independiente es igual a la suma sobre las probabilidades binomiales donde al menos uno de los eventos de contención de documentos de N (t) es cierto. Pin (t es ruidoso | c) = n (t) k = 1 n (t) k pk (1 - p) n −k pin (t es ruidoso | c) ≈ pPoi (t es ruidoso | c) hemos definidoUna probabilidad basada en frecuencia y basada en Poisson de ser ruidosa, donde este último es el límite de la probabilidad basada en la independencia de ser ruidoso. Antes de presentar en la sección final el uso de la probabilidad de ruido para definir la probabilidad de ser informativo, enfatizamos en la siguiente sección que los resultados se aplican al espacio de recolección, así como al espacio del documento.5. El espacio de recopilación y el espacio del documento consideran las definiciones duales de parámetros de recuperación en la Tabla 1. Asociamos un espacio de colección D × T con una colección C donde D es el conjunto de documentos y t es el conjunto de términos en la colección. Sea nd: = | d |y nt: = | t |ser el número de documentos y términos, respectivamente. Consideramos un documento como un subconjunto de t y un término como un subconjunto de D. Sea nt (d): = | {t | d ∈ T} |ser el número de términos que ocurren en el documento d, y dejar nd (t): = | {d | t ∈ D} |ser el número de documentos que contienen el término t.De una manera dual, asociamos un espacio de documento L × T con un documento D donde L es el conjunto de ubicaciones (también denominadas posiciones, sin embargo, usamos las letras L y L y no P y P para evitar confusiones con probabilidades) y t es el conjunto de términos en el documento. La dimensión del documento en un espacio de colección corresponde a la dimensión de ubicación (posición) en un espacio de documento. La definición hace explícita que la noción clásica de la frecuencia del término de un término en un documento (también conocido como la frecuencia del término dentro del documento) en realidad corresponde a la frecuencia de ubicación de un término en un documento. Para el 231 Space Collection Document Dimensions Documents y Términos ubicaciones y términos Documento/frecuencia de ubicación ND (t, c): Número de documentos en los que el término T ocurre en la colección C nl (t, d): número de ubicaciones (posiciones) en las cualesEl término T ocurre en el documento d nd (c): número de documentos en la colección c nl (d): número de ubicaciones (posiciones) en el documento d frecuencia de término nt (d, c): número de términos que el documento d contiene en la colección cNT (L, D): Número de términos que la ubicación L contiene en el documento d nt (c): número de términos en la colección c nt (d): número de términos en el documento d ruido/ocurrencia p (t | c) (términoruido) p (t | d) (término ocurrencia) contención p (d | c) (documento) p (l | d) (ubicación) INFORMATISln p (d | c) - ln p (l | d) p (informativo) ln (p (t | c))/ ln (p (tmin, c)) ln (p (t | d))/ ln (P (tmin, d)) p (conciso) ln (p (d | c))/ ln (p (dmin | c)) ln (p (l | d))/ ln (p (lmin | d)) mesa1: Parámetros de recuperación Valor de frecuencia de término real, es común usar la ocurrencia máxima (número de ubicaciones;Deje que LF sea la frecuencia de ubicación).tf (t, d): = lf (t, d): = pfreq (t ocurre | d) pfreq (tmax ocurre | d) = nl (t, d) nl (tmax, d) Una dualidad adicional es entre la informatividad yConcisión (escasez de documentos o ubicaciones): la información se basa en la ocurrencia (ruido), la concisión se basa en la contención. Hemos destacado en esta sección la dualidad entre el espacio de colección y el espacio de documentos. Nos concentramos en este documento sobre la probabilidad de que un término sea ruidoso e informativo. Esas probabilidades se definen en el espacio de recolección. Sin embargo, los resultados con respecto al término ruido e informatividad se aplican a sus homólogos duales: ocurrencia de término e información en un documento. Además, los resultados se pueden aplicar a la contención de documentos y ubicaciones.6. La probabilidad de ser informativa mostramos en las secciones anteriores que la suposición de desargación conduce a probabilidades basadas en frecuencia y que la suposición de independencia conduce a las probabilidades de Poisson. En esta sección, formulamos una definición basada en frecuencia y una definición basada en Poisson de la probabilidad de ser informativa y luego comparamos las dos definiciones. Definición 5. La probabilidad basada en frecuencia de ser informativa: PFREQ (t es informativo | C): =-Ln n (t) n-ln 1 n =-logn n (t) n = 1-logn n (t) = 1-lnn (t) ln n Definimos la probabilidad basada en Poisson de ser informativo de forma análoga a la probabilidad basada en la frecuencia de ser informativo (ver definición 5). Definición 6. La probabilidad basada en Poisson de ser informativa: PPOI (t es informativo | c): =-ln e-λ · èn (t) k = 1 λk k!- ln (e - λ · λ) = λ - ln èn (t) k = 1 λk k!λ - ln λ Para la expresión de suma, el siguiente límite se mantiene: lim n (t) → ∞ n (t) k = 1 λk k!= eλ - 1 para λ >> 1, podemos alterar el ruido e informatividad Poisson comenzando la suma de 0, desde eλ >> 1. Entonces, la información mínima de Poisson es Poisson (0, λ) = e - λ. Obtenemos una probabilidad simplificada de Poisson de ser informativo: PPOI (t es informativo | c) ≈ λ - ln èn (t) k = 0 λk k!λ = 1 - ln èn (t) k = 0 λk k!λ El cálculo de la suma de Poisson requiere una optimización para N (t) grande. La implementación para este documento explota la naturaleza de la densidad de Poisson: la densidad de Poisson produce solo valores significativamente mayores que cero en un intervalo alrededor de λ. Considere la ilustración de las definiciones de ruido e información en la Figura 1. Las funciones de probabilidad que se muestran se resumen en la Figura 2 donde el Poisson simplificado se usa en los gráficos de ruido e información. El ruido basado en la frecuencia corresponde a la curva sólida lineal en la figura de ruido. Con una suposición de independencia, obtenemos la curva en el triángulo inferior de la figura de ruido. Al cambiar el parámetro p: = λ/n de la probabilidad de independencia, podemos levantar o bajar la curva de independencia. La figura de ruido muestra el levantamiento del valor λ: = ln n ≈ 9.2. La configuración λ = ln n es especial en el sentido de que la información basada en frecuencia y basada en Poisson tiene el mismo denominador, a saber, ln n, y la suma de Poisson converge a λ. Si podemos sacar más conclusiones de esta configuración es una pregunta abierta. Podemos concluir que el levantamiento es deseable si sabemos para una colección que los términos que ocurren en relativamente pocos DOC232 0 0.2 0.4 0.6 0.6 0.8 1 0 2000 4000 6000 8000 10000 Probabilidad de la nocturna N (t): número de documentos con la independencia de la frecuencia T a término: Independencia:1/n Independencia: LN (N)/N Poisson: 1000 Poisson: 2000 Poisson: 1000,2000 0 0.2 0.4 0.6 0.8 0.8 1 0 2000 4000 6000 8000 10000 Probabilidad de la formación de N (t): Número de documentos con Término T de la frecuencia Térmica: 1/N Independencia: LN (N)/N Poisson: 1000 Poisson: 2000 Poisson: 1000,2000 Figura 1: Función de probabilidad de ruido e informatividad Informatividad de ruido Freeq Def N (T)/N Ln (N (T)/N)/N)Intervalo LN (1/N) 1/N ≤ PFREQ ≤ 1.0 0.0 ≤ PFREQ ≤ 1.0 Pin de independencia DEF 1 - (1 - P) N (T) Ln (1 - (1 - P) N (T))/Ln (p) intervalo p ≤ pin <1 - e - λ ln (p) ≤ pin ≤ 1.0 poisson ppoi def e - λ èn (t) k = 1 λk k!(λ - ln èn (t) k = 1 λk k!)/(λ - ln λ) intervalo e - λ · λ ≤ ppoi <1 - e - λ (λ - ln (eλ - 1))/(λ -ln λ) ≤ PPOI ≤ 1.0 Poisson PPOI Simplificado Def e - λ èn (t) k = 0 λk k!(λ - ln èn (t) k = 0 λk k!)/λ intervalo e - λ ≤ pPOI <1.0 0.0 <pPOI ≤ 1.0 Figura 2: Las funciones de probabilidad no son garantía para encontrar documentos relevantes, i.mi.Suponemos que los términos raros siguen siendo relativamente ruidosos. Por lo contrario, podríamos bajar la curva al suponer que los términos frecuentes no son demasiado ruidosos, i.mi.Se considera que aún son significativamente discriminativos. Las probabilidades de Poisson se aproximan a las probabilidades de independencia para N (t) grande;La aproximación es mejor para λ más grande. Para n (t) <λ, el ruido es cero, mientras que para n (t)> λ el ruido es uno. Este comportamiento radical se puede suavizar utilizando una distribución de Poisson multidimensional. La Figura 1 muestra un ruido de Poisson basado en un Poisson bidimensional: Poisson (k, λ1, λ2): = π · e-λ1 · λk 1 k!+ (1 - π) · e - λ2 · λk 2 k! El Poisson bidimensional muestra una meseta entre λ1 = 1000 y λ2 = 2000, usamos aquí π = 0.5. La idea detrás de este entorno es que los términos que ocurren en menos de 1000 documentos se consideran no ruidosos (es decir, son informativos), que los términos entre 1000 y 2000 son medio ruidosos, y que los términos con más de 2000 son definitivamente ruidosos. Para la información, observamos que se conserva el comportamiento radical de Poisson. La meseta aquí está aproximadamente a 1/6, y es importante darse cuenta de que esta meseta no se obtiene con el ruido de Poisson multidimensional usando π = 0.5. El logaritmo del ruido se normaliza mediante el logaritmo de un número muy pequeño, a saber, 0.5 · E - 1000 + 0.5 · E - 2000. Es por eso que la información estará solo cerca de una por muy poco ruido, mientras que por un poco de ruido, la información caerá a cero. Este efecto puede controlarse utilizando valores pequeños para π de tal manera que el ruido en el intervalo [λ1;λ2] sigue siendo muy poco. La configuración π = E - 2000/6 conduce a valores de ruido de aproximadamente E - 2000/6 en el intervalo [λ1;λ2], los logaritmos conducen luego a 1/6 para la información. Las funciones de informatividad basadas en la independencia y basadas en la frecuencia no difieren tanto como las funciones de ruido. Sin embargo, para la probabilidad basada en la independencia de ser informativos, podemos controlar la informatividad promedio por la definición p: = λ/n, mientras que el control en la frecuencia basada en la frecuencia es limitado a medida que abordamos a continuación. Para las IDF basadas en la frecuencia, el gradiente está disminuyendo monótonamente y obtenemos para diferentes colecciones las mismas distancias de valores de IDF, i.mi.El parámetro N no afecta la distancia. Para una ilustración, considere la distancia entre el valor IDF (TN+1) de un término TN+1 que ocurre en documentos n+1, y el valor IDF (TN) de un término TN que ocurre en n documentos.IDF (TN + 1) - IDF (TN) = Ln n N + 1 Los primeros tres valores de la función de distancia son: IDF (T2) - IDF (T1) = Ln (1/(1 + 1)) = 0.69 IDF (T3)-IDF (T2) = Ln (1/(2 + 1)) = 0.41 IDF (T4)-IDF (T3) = Ln (1/(3 + 1)) = 0.29 para la informatividad basada en Poisson, elEl gradiente disminuye primero lentamente para N (t) pequeño, luego rápidamente cerca de N (t) ≈ λ y luego vuelve a crecer lentamente para N (t) grande. En conclusión, hemos visto que la definición basada en Poisson proporciona más posibilidades de control y parámetros que 233 la definición basada en frecuencia. Mientras que más control y parámetro promete ser positivo para la personalización de los sistemas de recuperación, tiene al mismo tiempo el peligro de demasiados parámetros. El marco presentado en este documento aumenta la conciencia sobre los significados probabilísticos y teóricos de la información de los parámetros. Las definiciones paralelas de la probabilidad basada en frecuencia y la probabilidad basada en Poisson de ser informativa hicieron explícitos los supuestos subyacentes. La probabilidad basada en la frecuencia puede explicarse por ocurrencia binaria, contención constante y disgusto de los documentos. La independencia de los documentos lleva a Poisson, donde debemos ser conscientes de que Poisson se aproxima a la probabilidad de una disyunción para una gran cantidad de eventos, pero no para un pequeño número. Este resultado teórico explica por qué las investigaciones experimentales sobre Poisson (ver [7]) muestran que una estimación de Poisson funciona mejor para términos frecuentes (malos, ruidosos) que para términos raros (buenos, informativos). Además de la configuración de parámetros de toda la colección, el marco presentado aquí permite la configuración dependiente del documento, como se explica para la probabilidad de independencia. Esto es en particular interesante para las colecciones heterogéneas y estructuradas, ya que los documentos son de naturaleza diferente (tamaño, calidad, documento raíz, sub documento) y, por lo tanto, la ocurrencia binaria y la contención constante son menos apropiados que en colecciones relativamente homogéneas.7. Resumen La definición de la probabilidad de ser informativa transforma la interpretación informativa de las FDI en una interpretación probabilística, y podemos usar la probabilidad basada en IDF en los enfoques de recuperación probabilística. Mostramos que la definición clásica del ruido (frecuencia del documento) en la frecuencia inversa del documento puede explicarse por tres supuestos: el término probabilidad de ocurrencia dentro del documento es binaria, la probabilidad de contención del documento es constante y los eventos de contención del documento son disjuntos. Al formular explícita y matemáticamente los supuestos, demostramos que la definición clásica de las FDI no tiene en cuenta parámetros como la naturaleza diferente (tamaño, calidad, estructura, etc.) de documentos en una colección o la naturaleza diferente de los términos ((cobertura, importancia, posición, etc.) en un documento. Discutimos que la ausencia de esos parámetros es compensada por un efecto de apalancamiento de la probabilidad de ocurrencia del término dentro del documento y la probabilidad de contención del documento. Al aplicar una independencia más bien una suposición de desargación para la contención del documento, podríamos establecer un vínculo entre la probabilidad de ruido (término ocurrencia en una colección), teoría de la información y Poisson. De las probabilidades basadas en frecuencia y basadas en Poisson de ser ruidosos, derivamos las probabilidades basadas en frecuencia y basadas en Poisson de ser informativos. La probabilidad basada en la frecuencia es relativamente suave, mientras que la probabilidad de Poisson es radical para distinguir entre ruidoso o no ruidoso, e informativo o no informativo, respectivamente. Mostramos cómo suavizar el comportamiento radical de Poisson con un Poisson multidimensional. La formulación explícita y matemática de las IDF y las supuestas de Poisson es el resultado principal de este documento. Además, el documento enfatiza la dualidad de IDF y TF, espacio de colección y espacio de documentos, respectivamente. Por lo tanto, el resultado se aplica a la ocurrencia y la contención del documento en una colección, y se aplica a la ocurrencia y la contención de posición en un documento. Este marco teórico es útil para comprender y decidir la estimación y combinación de parámetros en los modelos de recuperación probabilística. Los enlaces entre el ruido basado en la independencia como frecuencia de documentos, interpretación probabilística de las FDI, la teoría de la información y el Poisson descritos en este documento pueden conducir a definiciones y combinaciones de IDF y TF probabilísticas variables según sea necesario en los sistemas de recuperación de información avanzados y personalizados. Reconocimiento: Me gustaría agradecer a Mounia Lalmas, Gabriella Kazai y Theodora Tsikrika por sus comentarios sobre las piezas pesadas como dijeron. Mi agradecimiento también ir al meta-revisión que me aconsejó que mejorara la presentación para que sea menos formidable y más accesible para aquellos sin una inclinación teórica. Este trabajo fue financiado por una beca de investigación de la Universidad Queen Mary de Londres.8. Referencias [1] A. Aizawa. Una perspectiva teórica de información de medidas TF-IDF. Procesamiento y gestión de la información, 39: 45-65, enero de 2003. [2] G. Amati y C. J. Rijsbergen. Normalización de frecuencia de término a través de distribuciones de Pareto. En el 24º Coloquio europeo de BCS-IRSG sobre investigación IR, Glasgow, Escocia, 2002. [3] R. K. Belew. Averiguar sobre. Cambridge University Press, 2000. [4] A. Bookstein y D. Swanson. Modelos probabilísticos para la indexación automática. Journal of the American Society for Information Science, 25: 312-318, 1974. [5] I. N. Bronstein. Taschenbuch der Mathematik. Harri Deutsch, Thun, Frankfurt am Main, 1987. [6] K. Church y W. Gale. Mezclas de Poisson. Ingeniería del Lenguaje Natural, 1 (2): 163-190, 1995. [7] K. W. Church y W. A. Gale. Frecuencia inversa del documento: una medida de desviaciones de Poisson. En el tercer taller sobre corpus muy grandes, ACL Anthology, 1995. [8] T. Lafouge y C. Michel. Enlaces entre la construcción de información y la ganancia de información: entropía y distribución bibliométrica. Journal of Information Science, 27 (1): 39-49, 2001. [9] E. Margulis. Modelado de documentos N-Poisson. En Actas de la 15ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 177-189, 1992. [10] S. E. Robertson y S. Walker. Algunas aproximaciones simples efectivas al modelo de 2 poisson para la recuperación ponderada probabilística. En Actas de la 17ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 232-241, Londres, et al., 1994. Springer-Verlag.[11] S. Wong y Y. Yao. Una medida teórica de la información de la especificidad del término. Journal of the American Society for Information Science, 43 (1): 54-61, 1992. [12] S. Wong e Y. Yao. Sobre la recuperación de información de modelado con inferencia probabilística. Transacciones ACM en Sistemas de Información, 13 (1): 38-68, 1995. 234