Agrupación regularizada para documentos ∗ Fei Wang, Changshui Zhang State Key Lab of Intelligent Tech. y del Departamento de Automatización de Sistemas, Universidad de Tsinghua Beijing, China, 100084 feiwang03@gmail.com Tao Li Escuela de Ciencias de la Computación Florida Universidad Internacional de Miami, FL 33199, EE. UU.Cada vez más atenciones como una técnica importante y fundamental para la organización de documentos no supervisada, la extracción automática de temas y la recuperación o el filtrado de información rápida. En este documento, proponemos un método novedoso para agrupar documentos utilizando regularización. A diferencia de los métodos tradicionales de agrupación globalmente regularizados, nuestro método construye primero un predictor de etiqueta lineal regularizada local para cada vector de documento, y luego combina todos esos regularizadores locales con un regularizador global de suavidad. Así que llamamos a nuestra agrupación de algoritmo con regularización local y global (CLGR). Mostraremos que las membresías del clúster de los documentos se pueden lograr mediante la descomposición del valor propio de una matriz simétrica dispersa, que puede resolverse eficientemente mediante métodos iterativos. Finalmente, nuestras evaluaciones experimentales en varios conjuntos de datos se presentan para mostrar las superioridades de CLGR sobre los métodos tradicionales de agrupación de documentos. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: búsqueda de información y agrupación de recuperación;I.2.6 [Inteligencia artificial]: Algoritmos de términos generales de aprendizaje de concepto de aprendizaje 1. Introducción La agrupación de documentos ha recibido más y más atenciones como una técnica importante y fundamental para la organización de documentos no supervisada, la extracción automática de temas y la recuperación o el filtrado de información rápida. Un buen enfoque de agrupación de documentos puede ayudar a las computadoras a organizar automáticamente el corpus de documentos en una jerarquía de clúster significativa para navegación y navegación eficientes, lo cual es muy valioso para complementar las deficiencias de las tecnologías de recuperación de información tradicionales. Como se señaló por [8], las necesidades de recuperación de información pueden expresarse mediante un espectro que varió desde una búsqueda estrecha basada en la combinación de palabras clave hasta una amplia navegación de información, como cuáles son los principales eventos internacionales en los últimos meses. Los motores tradicionales de recuperación de documentos tienden a encajar bien con el final de la búsqueda del espectro, es decir, generalmente proporcionan una búsqueda especificada de documentos que coinciden con la consulta de los usuarios, sin embargo, es difícil para ellos satisfacer las necesidades del resto del espectro en la que más bien un bienSe necesita información amplia o vaga. En tales casos, la navegación eficiente a través de una buena jerarquía de clúster será definitivamente útil. En general, los métodos de agrupación de documentos se pueden clasificar principalmente en dos clases: métodos jerárquicos y métodos de partición. Los métodos jerárquicos agrupan los puntos de datos en una estructura de árbol jerárquico utilizando enfoques de abajo hacia arriba o de arriba hacia abajo. Por ejemplo, la agrupación aglomerativa jerárquica (HAC) [13] es un método de agrupación jerárquica típica de ascenso. Se toma cada punto de datos como un solo clúster para comenzar y luego construye grupos cada vez más grandes al agrupar puntos de datos similares hasta que todo el conjunto de datos se encapsule en un clúster final. Por otro lado, los métodos de partición descomponen el conjunto de datos en una serie de grupos de disjunto que generalmente son óptimos en términos de algunas funciones de criterio predefinidas. Por ejemplo, K-means [13] es un método de partición típico que tiene como objetivo minimizar la suma de la distancia al cuadrado entre los puntos de datos y sus centros de clúster correspondientes. En este artículo, nos centraremos en los métodos de partición. Como sabemos, hay dos problemas principales existentes en los métodos de partición (como Kmeans y el modelo de mezcla gaussiana (GMM) [16]): (1) El criterio predefinido suele ser no convexo, lo que causa muchas soluciones óptimas locales;(2) El procedimiento iterativo (p. Ej. En las últimas décadas, se han propuesto muchos métodos para superar los problemas anteriores de los métodos de partición [19] [28]. Recientemente, otro tipo de métodos de partición basados en la agrupación en los gráficos de datos ha despertado intereses considerables en la comunidad de aprendizaje automático y minería de datos. La idea básica detrás de estos métodos es primero modelar todo el conjunto de datos como un gráfico ponderado, en el que los nodos gráficos representan los puntos de datos, y los pesos en los bordes corresponden a las similitudes entre los puntos por pares. Luego, las asignaciones de clúster del conjunto de datos se pueden lograr optimizando algunos criterios definidos en el gráfico. Por ejemplo, la agrupación espectral es un tipo de enfoques de agrupación más representativos basados en gráficos, generalmente tiene como objetivo optimizar algún valor de corte (p. Ej. Corte normalizado [22], corte de relación [7], corte min-max [11]) definido en un gráfico no dirigido. Después de algunas relajaciones, estos criterios generalmente se pueden optimizar a través de decomposiciones propias, lo que garantiza que es global óptimo. De esta manera, la agrupación espectral evita de manera eficiente los problemas de los métodos de partición tradicionales como introdujimos en el último párrafo. En este artículo, proponemos un algoritmo de agrupación de documentos novedoso que hereda la superioridad de la agrupación espectral, es decir, los resultados finales del clúster también se pueden obtener explotando la estructura propia de una matriz simétrica. Sin embargo, a diferencia de la agrupación espectral, que solo impone una restricción de suavidad en las etiquetas de datos en todo el colector de datos [2], nuestro método construye primero un predictor de etiqueta lineal regularizado para cada punto de datos desde su vecindario como en [25], y luego combina, y luego combina.Los resultados de todos estos predictores de etiquetas locales con un regularizador de suavidad de etiqueta global. Entonces llamamos a nuestra agrupación de métodos con regularización local y global (CLGR). La idea de incorporar información local y global en la predicción de la etiqueta se inspira en los trabajos recientes sobre el aprendizaje semi-supervisado [31], y nuestras evaluaciones experimentales en varios conjuntos de datos de documentos reales muestran que CLGR funciona mejor que muchos de última generación.Métodos de agrupación. El resto de este documento está organizado de la siguiente manera: en la Sección 2 presentaremos nuestro algoritmo CLGR en detalle. Los resultados experimentales en varios conjuntos de datos se presentan en la Sección 3, seguidos de las conclusiones y discusiones en la Sección 4. 2. El algoritmo propuesto en esta sección, presentaremos nuestra agrupación con el algoritmo de regularización local y global (CLGR) en detalle. Primero veamos cómo se representan los documentos a lo largo de este documento.2.1 Representación del documento En nuestro trabajo, todos los documentos están representados por los vectores de frecuencia de término ponderado. Sea w = {w1, w2, · · ·, wm} el conjunto de vocabulario completo del corpus de documentos (que está preprocesado por la eliminación de las palabras de parada y las palabras de las operaciones vistas). El vector de frecuencia de término Xi del documento DI se define como xi = [xi1, xi2, · · ·, xim] t, xik = tik log n idfk, donde tik es la frecuencia del término de wk ∈ W, n es el tamaño deDocument Corpus, IDFK es el número de documentos que contienen Word WK. De esta manera, Xi también se llama TFIDF Representation of Document Di. Además, también normalizamos cada Xi (1 I n) para tener una longitud de unidad, de modo que cada documento esté representado por un vector TF-IDF normalizado.2.2 Regularización local Como su nombre lo indica, CLGR está compuesto por dos partes: regularización local y regularización global. En esta subsección presentaremos la parte de regularización local en detalle.2.2.1 Motivación Como sabemos que la agrupación es un tipo de técnicas de aprendizaje, su objetivo es organizar el conjunto de datos de manera razonable. En términos generales, el aprendizaje se puede plantear como un problema de estimación de funciones, de la cual podemos obtener una buena función de clasificación que asignará etiquetas al conjunto de datos de capacitación e incluso al conjunto de datos de prueba invisible con algún costo minimizado [24]. Por ejemplo, en el escenario de clasificación de dos clases1 (en el que conocemos exactamente la etiqueta de cada documento), un clasificador lineal con un ajuste de mínimo cuadrado tiene como objetivo aprender un vector de columna w de tal manera que el costo al cuadrado j = 1 n (wt xi--yi) 2 (1) se minimiza, donde yi ∈ {+1, −1} es la etiqueta de xi. Al tomar ∂j /∂w = 0, obtenemos la solución w ∗ = n i = 1 xext i −1 n i = 1 xiyi, (2) que se puede escribir en su forma matricial como w ∗ = xxt −1 xy,(3) donde x = [x1, x2, · · ·, xn] es una matriz de documentos m × n, y = [y1, y2, · · ·, yn] t es el vector de etiqueta. Luego, para un documento de prueba t, podemos determinar su etiqueta mediante l = signo (w ∗ t u), (4) donde el signo (·) es la función de signo. Un problema natural en la ecuación.(3) es que la matriz xxt puede ser singular y, por lo tanto, no invertible (por ejemplo, cuando m n). Para evitar tal problema, podemos agregar un término de regularización y minimizar el siguiente criterio j = 1 n n i = 1 (wt xi - yi) 2 + λ w 2, (5) donde λ es un parámetro de regularización. Luego, la solución óptima que minimiza J viene dada por W ∗ = xxt + λni −1 xy, (6) donde i es una matriz de identidad m × m. Se ha informado que el clasificador lineal regularizado puede lograr muy buenos resultados en los problemas de clasificación de texto [29]. Sin embargo, a pesar de su éxito empírico, el clasificador lineal regularizado es en la Tierra un clasificador global, es decir, W ∗ se estima utilizando todo el conjunto de capacitación. Según [24], esta puede no ser una idea inteligente, ya que una w ∗ única puede no ser lo suficientemente buena para predecir las etiquetas de todo el espacio de entrada. Para obtener mejores predicciones, [6] propuso entrenar clasificadores localmente y usarlos para clasificar los puntos de prueba. Por ejemplo, el clasificador local se clasificará con un punto de prueba entrenado utilizando los puntos de entrenamiento ubicados en la vecindad 1 En las siguientes discusiones, todos asumimos que los documentos provenientes de solo dos clases. Las generalizaciones de nuestro método a los casos de múltiples clases se discutirán en la Sección 2.5.de eso. Aunque este método parece lento y estúpido, se informa que puede obtener mejores actuaciones que usar un clasificador global único en ciertas tareas [6].2.2.2 Construcción de los predictores regularizados locales inspirados en su éxito, propusimos aplicar los algoritmos de aprendizaje locales para la agrupación. La idea básica es que, para cada documento Vector XI (1 I n), capacitamos a un predictor de etiqueta local basado en su NI del vecindario K-Nearest, y luego lo usamos para predecir la etiqueta de XI. Finalmente, combinaremos todos esos predictores locales minimizando la suma de sus errores de predicción. En esta subsección presentaremos cómo construir esos predictores locales. Debido a la simplicidad y efectividad del clasificador lineal regularizado que hemos introducido en la Sección 2.2.1, elegimos que sea nuestro predictor de etiqueta local, de modo que para cada documento XI, el siguiente criterio se minimiza ji = 1 ni xj ∈Niwt i xj - qj 2 + λi wi 2, (7) donde ni = | ni |es la cardinalidad de Ni, y QJ es la membresía de clúster de XJ. Luego usando la ecuación.(6), podemos obtener la solución óptima es w ∗ i = xixt i + λinii −1 xiqi, (8) donde xi = [xi1, xi2, · · · ·, xini], y usamos XIK para denotar el k-El vecino más cercano de Xi.Qi = [Qi1, Qi2, · · ·, Qini] t con qik que representa la asignación de clúster de XIK. El problema aquí es que Xixt I es una matriz M × M con M Ni, es decir, debemos calcular el inverso de una matriz M × M para cada vector de documentos, que se prohíbe computacionalmente. Afortunadamente, tenemos el siguiente teorema: Teorema 1. W ∗ I en la ecuación.(8) se puede reescribir como w ∗ i = xi xt i xi + λiniii −1 qi, (9) donde II es una matriz de identidad Ni × Ni. Prueba. Dado que w ∗ i = xext i + λinii −1 xiqi, entonces xext i + λinii w ∗ i = xiqi = ⇒ xext i w ∗ i + λiniw ∗ i = xiqi = ⇒ w ∗ i = (λini) −1 xi qi - utt - ut.i w ∗ i. Sea β = (λini) −1 qi - xt i w ∗ i, entonces w ∗ i = xiβ = ⇒ λiniβ = qi - xt i w ∗ i = qi - xt i xiβ = ⇒ qi = xt i xi + λiniii β = β β β β= Xt I xi + λiniii −1 qi. Por lo tanto, w ∗ i = xiβ = xi xt i xi + λiniii −1 qi 2 usando el teorema 1, solo necesitamos calcular el inverso de una matriz Ni × Ni para cada documento para entrenar un predictor de etiqueta local. Además, para un nuevo punto de prueba U que cae en NI, podemos clasificarlo mediante el signo de qu = w ∗ t i u = ut wi = ut xi xt i xi + λiniii −1 qi. Esta es una expresión atractiva ya que podemos determinar la asignación del clúster de U utilizando los productos internos entre los puntos en {U ∪ Ni}, lo que sugiere que un regularizador tan local puede ser fácilmente kernelizado [21] siempre que definamos AFunción de núcleo adecuada.2.2.3 Combinación de los predictores regularizados locales Después de que todos los predictores locales se hayan construido, los combinaremos minimizando jl = n i = 1 w ∗ t i xi - qi 2, (10) que representa la suma de los errores de predicciónpara todos los predictores locales. Combinando la ecuación.(10) con la ecuación.(6), podemos obtener jl = n i = 1 w ∗ t i xi - qi 2 = n i = 1 xt i xi xt i xi + λiniii −1 qi - qi 2 = pq - q 2, (11) donde q =[Q1, Q2, · · ·, Qn] t, y el P es una matriz N × N de la siguiente manera. Sea αi = xt i xi xt i xi + λiniii −1, entonces pij = αi j, si xj ∈ Ni 0, de lo contrario, (12) donde pij es la entrada (i, j) -th de p, y αi j representaLa entrada j-th de αi. Hasta ahora podemos escribir el criterio de agrupación combinando predictores de etiquetas lineales regularizadas localmente JL en una forma matemática explícita, y podemos minimizarlo directamente utilizando algunas técnicas de optimización estándar. Sin embargo, los resultados pueden no ser lo suficientemente buenos ya que solo explotamos las informaciones locales del conjunto de datos. En la próxima subsección, introduciremos un criterio global de regularización y lo combinaremos con JL, que tiene como objetivo encontrar un buen resultado de agrupación en una forma global local.2.3 Regularización global en la agrupación de datos, generalmente requerimos que las asignaciones de clúster de los puntos de datos sean suficientemente suaves con respecto al colector de datos subyacente, lo que implica (1) los puntos cercanos tienden a tener las mismas asignaciones de clúster;(2) Los puntos en la misma estructura (por ejemplo, submanifold o clúster) tienden a tener las mismas asignaciones de clúster [31]. Sin la pérdida de generalidad, suponemos que los puntos de datos residen (aproximadamente) en un colector de baja dimensión M2, y Q es la función de asignación de clúster definida en M, es decir, 2 creemos que los datos de texto también se muestrean a partir de algunas dimensiones de baja dimensiónManifold, ya que es imposible para ellos para ∀x ∈ M, Q (x) Devuelve la membresía del clúster de x. La suavidad de Q sobre M se puede calcular mediante la siguiente integral de Dirichlet [2] d [Q] = 1 2 m Q (x) 2 dm, (13) donde el gradiente Q es un vector en el espacio tangente t mx, yLa integral se toma con respecto a la medida estándar en M. Si restringimos la escala de Q por Q, Q M = 1 (donde ·, · m es el producto interno inducido en M), entonces resulta que encontrar el más suaveLa función minimización de D [Q] se reduce a encontrar las funciones propias del operador de Laplace Beltrami L, que se define como LQ -Div Q, (14) donde DIV es la divergencia de un campo vectorial. En general, el gráfico puede verse como la forma discretizada de colector. Podemos modelar el conjunto de datos como un gráfico no dirigido ponderado como en la agrupación espectral [22], donde los nodos gráficos son solo los puntos de datos, y los pesos en los bordes representan las similitudes entre los puntos de pareja. Entonces se puede demostrar que minimizar la ecuación.(13) corresponde a minimizar jg = qt lq = n i = 1 (qi - qj) 2 wij, (15) donde q = [q1, q2, · · ·, qn] t con qi = q (xi), l esEl gráfico la laplaciano con su (i, j) -th Entry lij =    di-wii, si i = j −wij, si xi y xj son adyacentes 0, de lo contrario, (16) donde di = j wij es el títulode Xi, WIJ es la similitud entre XI y XJ. Si Xi y Xj son adyacentes3, WIJ generalmente se calcula de la siguiente manera wij = e - xi - xj 2 2σ2, (17) donde σ es un parámetro dependiente del conjunto de datos. Se demuestra que bajo ciertas condiciones, una forma de WIJ para determinar los pesos en los bordes de los gráficos conduce a la convergencia del gráfico Laplacian al operador de Laplace Beltrami [3] [18]. En resumen, usando la ecuación.(15) con pesos exponenciales puede medir efectivamente la suavidad de las asignaciones de datos con respecto al colector de datos intrínsecos. Por lo tanto, lo adoptamos como un regularizador global para castigar la suavidad de las asignaciones de datos predichas.2.4 Agrupación con regularización local y global que combina los contenidos que hemos introducido en la Sección 2.2 y la Sección 2.3 Podemos derivar el criterio de agrupación es minQ J = JL + λJG = PQ - Q 2 + λQT LQ S.T.qi ∈ {−1, +1}, (18) donde p se define como en la ecuación.(12), y λ es un parámetro de regularización para intercambiar JL y JG. Sin embargo, el relleno discreto en todo el espacio de muestra de alta dimensión. Y se ha demostrado que los métodos basados en el colector pueden lograr buenos resultados en las tareas de clasificación de texto [31].3 En este documento, definimos Xi y XJ como adyacentes si xi ∈ N (xj) o xj ∈ N (xi).La restricción de PI hace que el problema sea un problema de programación de enteros de NP. Una forma natural de hacer el problema solucionable es eliminar la restricción y relajar el Qi para que sea continuo, entonces el objetivo de que nuestro objetivo es minimizar se convierte en j = pq - q 2 + λqt lq = qt (p - i) t (p - i) Q + λqt lq = qt (p - i) t (p - i) + λl q, (19) y agregamos una restricción qt q = 1 para restringir la escala de q. Entonces nuestro objetivo se convierte en minq j = qt (p - i) t (p - i) + λl q s.t.Qt Q = 1 (20) Usando el método lagrangiano, podemos derivar que la solución óptima Q corresponde al vector propio más pequeño de la matriz m = (p - i) t (p - i) + λl, y la asignación del clúster de xipuede determinarse mediante el signo de Qi, es decir, XI se clasificará como clase uno si Qi> 0, de lo contrario, se clasificará como Clase 2. 2.5 CLGR de clase múltiple En lo anterior Hemos introducido el marco básico de la agrupación con Local yRegularización global (CLGR) para el problema de agrupación de dos clases, y la extenderemos a la agrupación de múltiples clases en esta subsección. Primero suponemos que todos los documentos pertenecen a clases C indexadas por l = {1, 2, · · ·, c}.QC es la función de clasificación para la Clase C (1 C C), de modo que QC (XI) devuelve la confianza de que Xi pertenece a la Clase C.Nuestro objetivo es obtener el valor de QC (xi) (1 c c, 1 i n), y la asignación del clúster de Xi puede determinarse por {Qc (xi)} C C = 1 utilizando algunos métodos de discretización adecuados que introduciremosmás tarde. Por lo tanto, en este caso de múltiples clases, para cada documento xi (1 i n), construiremos C predictores de etiqueta regularizados localmente lineales cuyos vectores normales son wc ∗ i = xi xt i xi + λiniii −1 Qc I (1 c c), (21) donde xi = [xi1, xi2, · · · ·, xini] con xik siendo el vecino k-th de xi, y qc i = [qc i1, qc i2, · · ·, qc ini] t con qcik = QC (xik). Entonces (wc ∗ i) t xi devuelve la confianza prevista de xi perteneciente a la clase c.Por lo tanto, el error de predicción local para la clase C se puede definir como j c l = n i = 1 (wc ∗ i) t xi - qc i 2, (22) y el error de predicción local total se convierte en jl = c c = 1 j c l = cc = 1 n i = 1 (wc ∗ i) t xi - qc i 2.(23) Como en la ecuación.(11), podemos definir una matriz N × N P (ver ecuación (12)) y reescribir JL como JL = C C = 1 J C L = C C = 1 PQC - QC 2.(24) Del mismo modo podemos definir el regularizador de suavidad global en el caso de la clase múltiple como jg = c c = 1 n i = 1 (Qc I-Qc J) 2 wij = c c = 1 (qc) t lqc.(25) Luego, el criterio que se minimizará para CLGR en el caso de clase múltiple se convierte en j = jl + λjg = c c = 1 pqc-qc 2 + λ (qc) t lqc = c c = 1 (qc) t (p −I) t (p - i) + λl qc = traza qt (p - i) t (p - i) + λl q, (26) donde q = [q1, q2, · · ·, qc] es un n ×C Matriz, y Trace (·) devuelve el rastro de una matriz. Lo mismo que en la ecuación.(20), también agregamos la restricción de que qt q = i para restringir la escala de Q. Entonces nuestro problema de optimización se convierte en minq j = traza qt (p - i) t (p - i) + λl q s.t. Qt Q = I, (27) Del teorema del ventilador de Ky [28], sabemos que la solución óptima del problema anterior es q ∗ = [q ∗ 1, q ∗ 2, · · ·, q ∗ c] r, ((28) donde q ∗ k (1 k c) es el vector propio corresponde al valor propio más pequeño de K -th de la matriz (p-i) t (p-i) + λl, y r es una matriz C × C arbitraria. Dado que los valores de las entradas en Q ∗ son continuos, necesitamos discretizar aún más Q ∗ para obtener las asignaciones de clúster de todos los puntos de datos. Hay principalmente dos enfoques para lograr este objetivo: 1. Como en [20], podemos tratar la primera fila de Q como la incrustación de Xi en un espacio c-dimensional, y aplicar algunos métodos de agrupación tradicionales como Kmeans para agrupar estos incrustaciones en grupos C.2. Dado que la Q ∗ óptima no es única (debido a la existencia de una matriz arbitraria R), podemos perseguir una R óptima que girará Q ∗ a una matriz de indicación4. El algoritmo detallado se puede referir [26]. El procedimiento de algoritmo detallado para CLGR se resume en la Tabla 1. 3. Experimentos En esta sección, los experimentos se realizan para comparar empíricamente los resultados de agrupación de CLGR con otros 8 algoritmos de agrupación de documentos representativos en 5 conjuntos de datos. Primero presentaremos la información básica de esos conjuntos de datos.3.1 conjuntos de datos utilizamos una variedad de conjuntos de datos, la mayoría de los cuales se usan con frecuencia en la investigación de recuperación de información. La Tabla 2 resume las características de los conjuntos de datos.4 Aquí una matriz de indicación T es una matriz N × C con su (i, j) th entrada tij ∈ {0, 1} tal que para cada fila de q ∗ solo hay una 1. Entonces el Xi se puede asignar al clúster J-th de tal manera que j = argjq ∗ ij = 1. Tabla 1: Agrupación con entrada de regularización local y global (CLGR) Entrada: 1. Conjunto de datos x = {xi} n i = 1;2. Número de grupos c;3. Tamaño del vecindario k;4. Parámetros de regularización local {λi} n i = 1;5. Parámetro de regularización global λ;Salida: la membresía del clúster de cada punto de datos. Procedimiento: 1. Construya los barrios K más cercanos para cada punto de datos;2. Construya la matriz P usando la ecuación.(12);3. Construya la matriz de laplacia L usando la ecuación.(dieciséis);4. Construir la matriz m = (p - i) t (p - i) + λl;5. Haga la descomposición del valor propio en M y construya la matriz Q ∗ según la ecuación.(28);6. ENCONTRA Las asignaciones de clúster de cada punto de datos discretizan correctamente Q ∗. Tabla 2: Descripciones de los conjuntos de datos de documentos Número de documentos Número de clases CSTR 476 4 WebKB4 4199 4 Reuters 2900 10 Webace 2340 20 Newsgroup4 3970 4 CSTR. Este es el conjunto de datos de los resúmenes de los informes técnicos publicados en el Departamento de Ciencias de la Computación de una universidad. El conjunto de datos contenía 476 resúmenes, que se dividieron en cuatro áreas de investigación: procesamiento del lenguaje natural (PNL), robótica/visión, sistemas y teoría. Webkb. El conjunto de datos WebKB contiene páginas web reunidas de los departamentos de ciencias de la computación de la universidad. Hay alrededor de 8280 documentos y se dividen en 7 categorías: estudiante, facultad, personal, curso, proyecto, departamento y otros. El texto en bruto es de aproximadamente 27 MB. Entre estas 7 categorías, el estudiante, el profesorado, el curso y el proyecto se encuentran cuatro categorías de representación de entidades más pobladas. El subconjunto asociado generalmente se llama WebKB4. Reuters. La colección de pruebas de categorización de texto Reuters-21578 contiene documentos recopilados de Reuters Newswire en 1987. Es un punto de referencia de categorización de texto estándar y contiene 135 categorías. En nuestros experimentos, utilizamos un subconjunto de la recopilación de datos que incluye las 10 categorías más frecuentes entre los 135 temas y lo llamamos Reuters-Top 10. Webace. El conjunto de datos de Webace fue del proyecto Webace y se ha utilizado para la agrupación de documentos [17] [5]. El conjunto de datos de WebACE contiene 2340 documentos que consisten en artículos de noticias de Reuters New Service a través de la Web en octubre de 1997. Estos documentos se dividen en 20 clases. News4. El conjunto de datos News4 utilizado en nuestros experimentos se seleccionan del famoso conjunto de datos de 20 newsgroups5. El tema Rec que contiene autos, motocicletas, béisbol y hockey se seleccionó de la versión 20News-18828. El conjunto de datos News4 contiene 3970 vectores de documentos.5 http://people.csail.mit.edu/jrennie/20newsgroups/ Para preprocesar los conjuntos de datos, eliminamos las palabras de parada utilizando una lista de parada estándar, todas las etiquetas HTML se omiten y todos los campos de encabezado, excepto el sujeto y la organización de la organización de la organización de la organización delSe ignoran los artículos publicados. En todos nuestros experimentos, primero seleccionamos las 1000 palabras principales mediante información mutua con etiquetas de clase.3.2 Métricas de evaluación En los experimentos, establecemos el número de grupos igual al verdadero número de clases C para todos los algoritmos de agrupación. Para evaluar su rendimiento, comparamos los grupos generados por estos algoritmos con las clases verdaderas calculando las siguientes dos medidas de rendimiento. Precisión de agrupación (ACC). La primera medida de rendimiento es la precisión de agrupación, que descubre la relación individual entre grupos y clases y mide hasta qué punto cada clúster contenía puntos de datos de la clase correspondiente. Resume todo el grado de coincidencia entre todos los grupos de clase de par. La precisión de la agrupación se puede calcular como: ACC = 1 N MAX    CK, LM T (CK, LM)   , (29) donde CK denota el clúster K-Th en los resultados finales, y LM es el verdadero M-th.clase. T (CK, LM) es el número de entidades que pertenecen a la clase M se asignan al clúster k.La precisión calcula la suma máxima de T (CK, LM) para todos los pares de grupos y clases, y estos pares no tienen superposiciones. La mayor precisión de agrupación significa el mejor rendimiento de agrupación. Información mutua normalizada (NMI). Otra métrica de evaluación que adoptamos aquí es la información mutua normalizada NMI [23], que se usa ampliamente para determinar la calidad de los grupos. Para dos variables aleatorias x e y, el NMI se define como: nmi (x, y) = i (x, y) h (x) h (y), (30) donde yo (x, y) es la información mutuaEntre x e y, mientras que H (x) y H (y) son las entropías de x e y respectivamente. Se puede ver que NMI (x, x) = 1, que es el valor máximo posible de NMI. Dado un resultado de agrupación, el NMI en la ecuación.(30) se estima como nmi = c k = 1 c m = 1 nk, mlog n · nk, m nk ˆnm c k = 1 nklog nk n c m = 1 ˆnmlog ˆnm n, (31) donde nk denota el número deLos datos contenidos en el clúster CK (1 k c), ˆnm son el número de datos que pertenecen a la clase m-ta (1 m c) y nk, m denota el número de datos que están en la intersección entre el clúster ck yLa clase M-Th. El valor calculado en la ecuación.(31) se usa como medida de rendimiento para el resultado de agrupación dado. Cuanto mayor sea este valor, mejor será el rendimiento de la agrupación.3.3 Comparaciones Hemos realizado evaluaciones de rendimiento integrales al probar nuestro método y compararlo con otros 8 métodos de agrupación de datos representativos utilizando los mismos corpus de datos. Los algoritmos que evaluamos se enumeran a continuación.1. K-means tradicional (km).2. M-MEANS SPHERICAL (SKM). La implementación se basa en [9].3. Modelo de mezcla gaussiana (GMM). La implementación se basa en [16].4. Agrupación espectral con cortes normalizados (NCUT). La implementación se basa en [26], y la varianza de la similitud gaussiana está determinada por la escala local [30]. Tenga en cuenta que el criterio que NCUT tiene como objetivo minimizar es solo el regularizador global en nuestro algoritmo CLGR, excepto que NCUT usó el laplaciano normalizado.5. Agrupación utilizando regularización local pura (CPLR). En este método, simplemente minimizamos JL (definido en la ecuación (24)), y los resultados de la agrupación se pueden obtener haciendo la descomposición del valor propio en la matriz (i - p) t (i - p) con algunos métodos de discretización adecuados.6. Iteración del subespacio adaptativo (ASI). La implementación se basa en [14].7. Factorización de matriz no negativa (NMF). La implementación se basa en [27].8. TRI-Factorización de factorización de matriz no negativa (TNMF) [12]. La implementación se basa en [15]. Para la eficiencia computacional, en la implementación de CPLR y nuestro algoritmo CLGR, hemos establecido todos los parámetros de regularización locales {λi} n i = 1 para ser idénticos, que se establece mediante búsqueda de cuadrícula de {0.1, 1, 10}. El tamaño de los vecindarios K-Nears se establece mediante búsqueda de cuadrícula de {20, 40, 80}. Para el método CLGR, su parámetro de regularización global se establece mediante búsqueda de cuadrícula de {0.1, 1, 10}. Al construir el regularizador global, hemos adoptado el método de escala local [30] para construir la matriz de laplacia. El método de discretización final adoptado en estos dos métodos es el mismo que en [26], ya que nuestros experimentos muestran que el uso de dicho método puede lograr mejores resultados que usar métodos basados en Kmeans como en [20].3.4 Resultados experimentales Los resultados de comparación de precisiones de agrupación se muestran en la Tabla 3, y los resultados de comparación de información mutua normalizadas se resumen en la Tabla 4. De las dos tablas observamos principalmente que: 1. Nuestro método CLGR supera a todos los demás métodos de agrupación de documentos en la mayoría de los conjuntos de datos;2. Para la agrupación de documentos, el método esférico K-Means generalmente supera el método tradicional de agrupación de K-means, y el método GMM puede lograr resultados competitivos en comparación con el método esférico de K-means;3. Los resultados logrados de los algoritmos de tipo K-means y GMM suelen ser peores que los resultados logrados por la agrupación espectral. Dado que la agrupación espectral se puede ver como una versión ponderada de Kernel K-Means, puede obtener buenos resultados, los grupos de datos tienen una forma arbitraria. Esto corrobora que los vectores de documentos no se distribuyen regularmente (esférico o elíptico).4. Las comparaciones experimentales verifican empíricamente la equivalencia entre NMF y la agrupación espectral, cuya Tabla 3: Accuraciones de agrupación de los diversos métodos CSTR Webkb4 Reuters Webace News4 Km 0.4256 0.3888 0.4448 0.4001 0.3527 Skm 0.4690 0.44318 0.5025 0.44458 0.3912 Gmm 0.4447. 4521 0.3844 NMF 0.5713 0.4418 0.49470.4761 0.4213 NCUT 0.5435 0.4521 0.4896 0.4513 0.4189 ASI 0.5621 0.4752 0.5235 0.4823 0.4335 TNMF 0.6040 0.4832 0.5541 0.5102 0.4613 CPLR 5 0.5228 0.5341 0.5376 0.5102 Tabla 4: Resultados de información mutua normalizada de los diversos métodos CSTR WebKB4 Reuters Webace News4 Km 0.36750.3023 0.4012 0.3864 0.3318 SKM 0.4027 0.4155 0.4587 0.4003 0.4085 gmm 0.4034 0.4093 0.4356 0.4209 0.3994 NMF 0.5235 0.4517 0.4402 0.4359 0.4130 ncut 0.4833 0.44920 0.42892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892892N 1 ASI 0.5008 0.4833 0.4769 0.4817 0.4503 TNMF 0.5724 0.5011 0.5132 0.5328 0.4749 CPLR 0.5695 0.5231 0.4402 0.5543 0.4690 CLGR 0.6012 0.5434 0.49350.5390 0.4908 se ha demostrado teóricamente en [10]. Se puede observar desde las tablas que NMF y la agrupación espectral generalmente conducen a resultados de agrupación similares.5. Los métodos basados en co-agrupación (TNMF y ASI) generalmente pueden lograr mejores resultados que los métodos tradicionales basados en vectores puramente documentados. Dado que estos métodos realizan una selección de características implícitas en cada iteración, proporcionan una métrica adaptativa para medir el vecindario y, por lo tanto, tienden a obtener mejores resultados de agrupación.6. Los resultados logrados de CPLR suelen ser mejores que los resultados logrados por la agrupación espectral, que admite la teoría de Vapniks [24] que a veces los algoritmos de aprendizaje locales pueden obtener mejores resultados que los algoritmos de aprendizaje global. Además de los experimentos de comparación anteriores, también probamos la sensibilidad del parámetro de nuestro método. Hay principalmente dos conjuntos de parámetros en nuestro algoritmo CLGR, los parámetros de regularización local y global ({λi} n i = 1 y λ, como hemos dicho en la Sección 3.3, hemos establecido que todos los λis son idénticos a λ ∗ en nuestros experimentos), y el tamaño de los vecindarios. Por lo tanto, también hemos realizado dos conjuntos de experimentos: 1. Fijar el tamaño de los vecindarios y probar el rendimiento de la agrupación con λ λ ∗ y λ. En este conjunto de experimentos, encontramos que nuestro algoritmo CLGR puede lograr buenos resultados cuando los dos parámetros de regularización no son ni demasiado grandes ni demasiado pequeños. Por lo general, nuestro método puede lograr buenos resultados cuando λ ∗ y λ son alrededor de 0.1. La Figura 1 nos muestra tal ejemplo de prueba en el conjunto de datos de Webace.2. Fijación de los parámetros de regularización locales y globales, y probando el rendimiento de agrupación con diferentes −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 Regularización local PARA (Valor de log 2) Regularización global) Global Regularización GlobalPARA (valor de log 2) ClusteringCuracy Figura 1: Resultados de pruebas de sensibilidad de parámetros en el conjunto de datos de Webace con el tamaño del vecindario fijado a 20, y el eje x y el eje y representan el valor log2 de λ ∗ y λ.Tamaños de barrios. En este conjunto de experimentos, encontramos que el vecindario con un tamaño demasiado grande o demasiado pequeño deteriorará los resultados finales de agrupación. Esto se puede entender fácilmente ya que cuando el tamaño del vecindario es muy pequeño, los puntos de datos utilizados para entrenar a los clasificadores locales pueden no ser suficientes;Cuando el tamaño del vecindario es muy grande, los clasificadores entrenados tenderán a ser globales y no pueden capturar las características locales típicas. La Figura 2 nos muestra un ejemplo de prueba en el conjunto de datos de Webace. Por lo tanto, podemos ver que nuestro algoritmo CLGR (1) puede lograr resultados satisfactorios y (2) no es muy sensible a la elección de los parámetros, lo que lo hace práctico en aplicaciones del mundo real.4. Conclusiones y trabajos futuros En este documento, derivamos un nuevo algoritmo de agrupación llamado clúster con regularización local y global. Nuestro método conserva el mérito de los algoritmos de aprendizaje locales y la agrupación espectral. Nuestros experimentos muestran que el algoritmo propuesto supera a la mayoría de los algoritmos de última generación en muchos conjuntos de datos de referencia. En el futuro, nos centraremos en los problemas de selección y aceleración de parámetros del algoritmo CLGR.5. Referencias [1] L. Baker y A. McCallum. Agrupación de distribución de palabras para la clasificación de texto. En Actas de la Conferencia Internacional de ACM Sigir sobre Investigación y Desarrollo en Recuperación de Información, 1998. [2] M. Belkin y P. Niyogi. Los mapas propios de la laplidad para la reducción de la dimensionalidad y la representación de datos. Computación neural, 15 (6): 1373-1396. Junio de 2003. [3] M. Belkin y P. Niyogi. Hacia una base teórica para los métodos múltiples basados en laplacianos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (Colt).2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 Tamaño de la Agrupación del Vecindario ACCONO100. [4] M. Belkin, P. Niyogi y V. Sindhwani. Manifold regularización: un marco geométrico para aprender de ejemplos. Journal of Machine Learning Research 7, 1-48, 2006. [5] D. Boley. DIRECCIÓN PRINCIPAL PARTICIÓN DIVISIVA. Data Mining and Knowledge Discovery, 2: 325-344, 1998. [6] L. Botou y V. Vapnik. Algoritmos de aprendizaje local. Neural Computation, 4: 888-900, 1992. [7] P. K. Chan, D. F. Schlag y J. Y. Zien. Partición y agrupación espectral de relación K-Way. IEEE Trans. Diseño asistido por computadora, 13: 1088-1096, septiembre de 1994. [8] D. R. Cutting, D. R. Karger, J. O. Pederson y J. W. Tukey. Dispersión/recopilación: un enfoque basado en clúster para navegar en grandes colecciones de documentos. En Actas de la Conferencia Internacional de ACM Sigir sobre Investigación y Desarrollo en Recuperación de Información, 1992. [9] I. S. Dhillon y D. S. Modha. Concepto descomposición para datos de texto disfreños grandes utilizando la agrupación. Aprendizaje automático, vol.42 (1), páginas 143-175, enero de 2001. [10] C. Ding, X. Él y H. Simon. Sobre la equivalencia de la factorización de la matriz no negativa y la agrupación espectral. En Actas de la Conferencia de Minería de Datos de Siam, 2005. [11] C. Ding, X. Él, H. Zha, M. Gu y H. D. Simon. Un algoritmo de corte min-max para partición gráfica y agrupación de datos. En Proc.de la primera conferencia internacional sobre minería de datos (ICDM), páginas 107-114, 2001. [12] C. Ding, T. Li, W. Peng y H. Park. Matriz ortogonal no negativa Tri-factorizaciones para la agrupación. En Actas de la duodécima Conferencia Internacional de ACM Sigkdd sobre Discovery y Mining de datos, 2006. [13] R. O. Duda, P. E. Hart y D. G. Stork. Clasificación de patrones. John Wiley & Sons, Inc., 2001. [14] T. Li, S. Ma y M. Ogihara. Agrupación de documentos a través de la iteración de subespacio adaptativo. En Actas de la Conferencia Internacional de ACM Sigir sobre Investigación y Desarrollo en Recuperación de Información, 2004. [15] T. Li y C. Ding. Las relaciones entre varios métodos de factorización de matriz no negativos para la agrupación. En Actas de la 6ta Conferencia Internacional sobre Minería de Datos (ICDM).2006. [16] X. Liu e Y. Gong. Agrupación de documentos con capacidades de refinamiento de clúster y selección de modelos. En Proc.de la Conferencia Internacional de ACM Sigir sobre Investigación y Desarrollo en Recuperación de Información, 2002. [17] E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobashery J. Moore. Webace: un agente web para la categorización y exploración de documentos. En Actas de la 2da Conferencia Internacional sobre Agentes Autónomos (Agentes98). ACM Press, 1998. [18] M. Hein, J. Y. Audibert y U. Von Luxburg. Desde gráficos hasta colectores: consistencia puntual débil y fuerte de los laplacianos de gráficos. En Actas de la 18ª Conferencia sobre Teoría del Aprendizaje (Colt), 470-485.2005. [19] J. Él, M. Lan, C.-L.Tan, S.-Y. Sung y H.-B. Bajo. Inicialización de los algoritmos de refinamiento de clúster: una revisión y estudio comparativo. En Proc.de Inter. Conferencia conjunta sobre redes neuronales, 2004. [20] A. Y. Ng, M. I. Jordan, Y. Weiss. En la agrupación espectral: análisis y un algoritmo. En Avances en Sistemas de Procesamiento de Información Neural 14. 2002. [21] B. Sch¨olkopf y A. Smola. Aprendiendo con núcleos. El MIT prensa. Cambridge, Massachusetts.2002. [22] J. Shi y J. Malik. Cortes normalizados y segmentación de imagen. IEEE Trans.en análisis de patrones e inteligencia artificial, 22 (8): 888-905, 2000. [23] A. Strehl y J. Ghosh. Conjuntos de clúster: un marco de reutilización de conocimiento para combinar múltiples particiones. Journal of Machine Learning Research, 3: 583-617, 2002. [24] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Berlín: Springer-Verlag, 1995. [25] Wu, M. y Sch¨olkopf, B. Un enfoque de aprendizaje local para la agrupación. En Avances en Sistemas de Procesamiento de Información Neural 18. 2006. [26] S. X. Yu, J. Shi. Clustering espectral multiclase. En Actas de la Conferencia Internacional sobre Visión Computadora, 2003. [27] W. Xu, X. Liu e Y. Gong. La agrupación de documentos basada en la factorización de la matriz no negativa. En Actas de la Conferencia Internacional de ACM Sigir sobre Investigación y Desarrollo en Recuperación de Información, 2003. [28] H. Zha, X. Él, C. Ding, M. Gu y H. Simon. Relajación espectral para la agrupación de K-means. En NIPS 14. 2001. [29] T. Zhang y F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. Revista de recuperación de información, 4: 5-31, 2001. [30] L. Zelnik-Manor y P. Perona. Agrupación espectral de autoconfiamiento. En NIPS 17. 2005. [31] D. Zhou, O. Bousquet, T. N. Lal, J. Weston y B. Sch¨olkopf. Aprender con consistencia local y global. NIPS 17, 2005.