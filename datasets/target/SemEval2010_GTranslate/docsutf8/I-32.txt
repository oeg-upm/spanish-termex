Un modelo de entorno adversario para agentes racionales limitados en interacciones de suma cero en Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 La Escuela de Ingeniería Bar-Ilan Ciencias de la Computación Ramat-Gan, ISRAEL HEBREO UNIVERSITY HEBREO., Jerusalén, Israel {Zukermi, Sarit, Galk}@cs.biu.ac.il jeff@cs.huji.ac.il Los entornos multiagentes abstractos a menudo no son cooperativos ni colaborativos;En muchos casos, los agentes tienen intereses contradictorios, lo que lleva a interacciones adversas. Este artículo presenta un modelo de entorno adversario formal para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de usar métodos de búsqueda clásicos basados en utilidad pueden generar una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo oponente más matizado y explícito). Definimos un entorno adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas conductuales que están destinados a servir como principios de diseño para construir tales agentes adversos. Exploramos la aplicación de nuestro enfoque analizando archivos de registro de juegos completos de Connect-Four y presentamos un análisis empírico de la idoneidad de los axiomas. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial]: agentes de inteligencia artificial distribuidos, sistemas multiagentes;I.2.4 [Inteligencia artificial]: Formalismos y métodos de representación del conocimiento: lógica moderada Términos generales Diseño, Teoría 1. Introducción Investigación temprana en sistemas multiagentes (MAS) considerados grupos cooperativos de agentes;Debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura de sensor limitada), trabajaron juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera tan eficiente. La investigación de MAS, sin embargo, pronto comenzó a considerar a los agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por diversos intereses, los participantes pueden tener que superar los desacuerdos, las interacciones no cooperativas e incluso los intentos intencionales de dañarse entre sí. Cuando se producen estos tipos de interacciones, los entornos requieren un comportamiento apropiado de los agentes situados en ellas. Llamamos a estos entornos entornos adversos y llamamos a los adversarios de los agentes de enfrentamiento. Los modelos de cooperación y trabajo en equipo se han explorado ampliamente en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de esta investigación trató con dominios adversos y sus implicaciones para el comportamiento de los agentes. Nuestro documento aborda este problema al proporcionar un modelo de estado mental formal y axiomatizado para un subconjunto de dominios adversos, a saber, entornos adversos de suma cero simples. Los encuentros simples de suma cero existen, por supuesto, en varios juegos de doble capa (por ejemplo, ajedrez, damas), pero también existen en los juegos N-jugadores (por ejemplo, riesgo, diplomacia), subastas para un solo bien y en otros lugares. Especialmente en estos últimos entornos, el uso de una búsqueda adversa basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada;La función de pago puede ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en los agentes racionales limitados. Además, los métodos de búsqueda tradicionales (como Min-Max) no utilizan un modelo del oponente, lo que ha demostrado ser una valiosa adición a la planificación adversaria [9, 3, 11]. En este documento, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que están situados en un entorno adversario de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales bases son el modelo Sharedplans [4] para el comportamiento colaborativo. Exploramos las propiedades del medio ambiente y los estados mentales de los agentes para derivar axiomas conductuales;Estos axiomas conductuales constituyen un modelo formal que sirve como guía de especificación y diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Connect-Four. Mostramos que este juego se ajusta a nuestra definición de entorno y analizamos el comportamiento de los jugadores utilizando un gran conjunto de Match Log 550 978-81-904262-7-7 (RPS) C 2007 IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio congresivo ConnectFour. El documento procede de la siguiente manera. La Sección 2 presenta la formalización de los modelos. La Sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4 y concluyamos y presentamos direcciones futuras en la Sección 5. 2. Entornos adversos El modelo de entorno adversario (denotado como AE) está destinado a guiar el diseño de agentes proporcionando una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos centramos aquí en tipos específicos de entornos adversos, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman a cero;2. AES simples: todos los agentes en el medio ambiente son agentes adversos;3. AES bilaterales: AES con exactamente dos agentes;4. AES multilaterales: EA de tres o más agentes. Trabajaremos en instancias bilaterales y multilaterales de entornos simples y de suma cero. En particular, nuestro modelo de entorno adversario se ocupará de las interacciones que consisten en agentes N (n ≥ 2), donde todos los agentes son adversarios, y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, ajedrez, conexión y diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N-bidder en un solo bien).2.1 Descripción general del modelo Nuestro enfoque es formalizar las actitudes y comportamientos mentales de un solo agente adversario;Consideramos cómo un solo agente percibe el AE. La siguiente lista especifica las condiciones y los estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene una intención individual de que su propio objetivo se completará;2. El agente tiene una creencia individual de que él y sus adversarios buscan objetivos conflictivos completos (definidos a continuación) puede haber solo un ganador;3. El agente tiene una creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo;4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. Se requiere el ítem 3, ya que podría ser el caso de que algún agente tenga un objetivo conflictivo completo, y actualmente está considerando adoptar la intención de completarlo, pero hasta el momento, no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha deliberado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que está teniendo actualmente. En tales casos, podría no considerarse ni siquiera estar en un entorno adversario. El ítem 4 establece que el agente debe mantener una creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Se puede administrar explícitamente o se puede aprender de las observaciones de encuentros pasados.2.2 Definiciones del modelo para estados mentales Usamos definiciones de Grosz y Krauss de los operadores modales, predicados y metacredicados, como se define en su formalización del plan compartido [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: int.to (ai, α, tn, tα, c) representan las intenciones de AIS en el momento TN para hacer una acción α en el tiempo Tα en el contexto de C.Int.th (ai, prop, tn, tprop, c) representa las intenciones de AIS en el momento tn que una cierta proposición se mantiene en el tiempo tprop en el contexto de C. los operadores de intención potenciales, pot.int.to (...)y pot.int.th (...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel (AI, F, TF) representa al agente AI que cree en la declaración expresada en la Fórmula F, en el tiempo TF. MB (A, F, TF) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema considera que nuestro entorno está en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier lai ∈ L de posibles estados locales. En cualquier paso de tiempo, el sistema estará en algún mundo W del conjunto de todos los mundos posibles w ∈ W, donde w = E × LA1 × LA2 × ... LAN, y N es el número de adversarios. Por ejemplo, en un juego de póker de Texas Holdem, un estado local de los agentes podría ser su propio conjunto de cartas (que es desconocido para su adversario), mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un posible mundo w ∈ W a un elemento en, lo que expresa la conveniencia del mundo, desde una perspectiva de un solo agente. Por lo general, normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas junto con las definiciones originales para la formalización del entorno adversario: 1. φ es una acción nula (el agente no hace nada).2. GAI es el conjunto de objetivos de agente AIS. Cada objetivo es un conjunto de predicados cuya satisfacción complete el objetivo (usamos g ∗ ai ∈ Gai para representar un objetivo arbitrario del agente ai).3. GAI es el conjunto de subconsportes de agentes AIS. Los subggoal son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo.GG ∗ ai ⊆ gai es el conjunto de subggoals que son importantes para la finalización de la meta G ∗ ai (usaremos g ∗ g ∗ ai ∈ Gg ∗ ai para representar un subggoal arbitrario).4. P AJ AI es el perfil de Object Agent Ai sobre el agente AJ.5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las limitaciones de los entornos. Cai ⊆ Ca es el conjunto de acciones posibles de agentes.6. DO (AI, α, Tα, W) se mantiene cuando AI realiza la acción α en el intervalo de tiempo Tα en el mundo w.7. Lograr (g ∗ ai, α, w) es cierto cuando el objetivo g ∗ ai se logra después de la finalización de la acción α en el mundo w ∈ W, donde α ∈ Cai.8. El perfil (ai, pai ai) es cierto cuando el agente ai contiene un perfil de objeto para el agente AJ. Definición 1. El conflicto completo (FULCONF) describe una interacción Zerosum donde solo se puede completar un objetivo de los objetivos en el conflicto. Fulconf (g ∗ ai, g ∗ aj) ⇒ (∃α ∈ Cai, ∀W, β ∈ Caj) (lograr (g ∗ ai, α, w) ⇒ ¬aChieve (g ∗ aj, β, w)) ∨ ((∃β ∈ Caj, ∀W, α ∈ Cai) (lograr (G ∗ AJ, β, W) ⇒ ¬Acie (G ∗ Ai, α, W)) Definición 2. El conocimiento adversario (ADVKEK) es una función que devuelve un valor que representa la cantidad de la Sexta INTL. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 551 El agente de conocimiento AI tiene en el perfil de Agente AJ, en el momento TN. Cuanto mayor sea el valor, más conocimientos agente de IA. Advnow: P AJ AI × TN → Definición 3. Eval: esta función de evaluación devuelve un valor de utilidad esperado estimado para un agente en A, después de completar una acción de CA en algún estado mundial w.Eval: A × CA × W → Definición 4. TRH - (umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (eval). Una acción que produce una evaluación de utilidad estimada por encima de la TRH se considera una acción altamente beneficiosa. El valor de evaluación es una estimación y no la función de utilidad real, que generalmente se desconoce. El uso del valor de utilidad real para un agente racional daría fácilmente el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino una estimación heurística de ellas. Hay dos propiedades importantes que deben mantener para la función de evaluación: Propiedad 1. La función de evaluación debe indicar que el estado mundial más deseable es uno en el que se logra el objetivo. Por lo tanto, después de que el objetivo se haya cumplido, no puede haber acciones futuras que puedan colocar al agente en un estado mundial con un mayor valor de evaluación.(∀ai, g ∗ ai, α, β ∈ Cai, w ∈ W) lograr (g ∗ ai, α, w) ⇒ eval (ai, α, w) ≥ eval (ai, β, w) propiedad 2. La función de evaluación debe proyectar una acción que cause una finalización de un objetivo o un subggoal a un valor que sea mayor que TRH (una acción altamente beneficiosa).(∀ai, g ∗ ai ∈ Gai, α ∈ Cai, w ∈ W, g ∗ gai ∈ Ggai) alcanzar (g ∗ ai, α, w) ∨ alcanzar (g ∗ gai, α, w) ⇒ eval (ai,α, W) ≥ TRH. Definición 5. SetAction Definimos una acción establecida (setAction) como un conjunto de operaciones de acción (acciones complejas o básicas) de algunas acciones Conjuntos CAI y CAJ que, según la creencia del agente, están unidos por una relación temporal y consecuente, formando una cadenade eventos (acción y su siguiente acción consecuente).(∀α1, ..., αu ∈ Cai, β1, ..., βV ∈ Caj, w ∈ W) setAction (α1, ..., αu, β1, ..., βV, w) ⇒ ((do(Ai, α1, tα1, w) ⇒ do (AJ, β1, Tβ1, W)) ⇒ do (ai, α2, tα2, w) ⇒ ... ⇒ do (ai, αu, tαu, w)) lo consecuencialLa relación puede existir debido a diversas restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. Como conocimiento que tenemos sobre nuestros aumentos adversarios, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones que a su vez crean nuevas acciones establecidas. Formalmente, si nuestro ADVKEK en el momento TN+1 es mayor que Advnow en el momento TN, entonces cada setaction conocida en el momento TN también se conoce en el tiempo TN+1. Advknow (p aj ai, tn+1)> advknow (p aj ai, tn) ⇒ (∀α1, ..., αu ∈ Cai, β1, ..., βv ∈ Caj) bel (AAG, setAction (α1,.., αu, β1, ..., βV), tn) ⇒ bel (aag, setAction (α1, ..., αu, β1, ..., βV), tn+1) 2.3 la formulación del medio ambiente la formulación del medio ambienteLos siguientes axiomas proporcionan la definición formal para un entorno adversario (AE) simple de suma cero. La satisfacción de estos axiomas significa que el agente está situado en dicho entorno. Proporciona especificaciones para que el agente AAG interactúe con su conjunto de adversarios a con respecto a los objetivos g ∗ aag y g ∗ a en el tiempo tco en algún estado mundial w.Ae (aag, a, g ∗ aag, a1, ..., ak, g ∗ a1, ..., g ∗ ak, tn, w) 1. AAG tiene un int.. AAG cree que y cada uno de sus adversarios está buscando objetivos conflictivos completos: (∀ao ∈ {a1, ..., ak}) bel (aag, fulconf (g ∗ aag, g ∗ ao), tn) 3. AAG cree que cada uno de sus adversarios en AO tiene el int.th su objetivo conflictivo g ∗ aoi se completará: (∀ao ∈ {a1, ..., ak}) (∃β ∈ Cao, tβ) bel (aag,Int.th (ao, alcanzar (g ∗ ao, β), tco, tβ, ae), tn) 4. AAG tiene creencias sobre los perfiles (parciales) de sus adversarios (∀ao ∈ {a1, ..., ak}) (∃pao aag ∈ Paag) bel (aag, perfil (ao, pao aag), tn) para construir unAgente que podrá operar con éxito dentro de dicha AE, debemos especificar las pautas de comportamiento para sus interacciones. El uso de una estrategia de maximización de evaluación ingenua a una determinada profundidad de búsqueda no siempre producirá resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar una profundidad fija;(2) la fuerte suposición de un adversario de recursos óptimamente racional e ilimitado;(3) Uso de una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que pueden usarse para diferenciar entre agentes exitosos y menos exitosos en el entorno adversario anterior. Esos axiomas deben usarse como principios de especificación al diseñar e implementar agentes que deberían poder funcionar bien en tales entornos adversos. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.int.to (...)) realizar una acción, que generalmente requerirá un razonamiento de extremo de medios para seleccionar un posible curso de acción. Este razonamiento conducirá a la adopción de un int.to (...) (ver [4]). A1. Objetivo para lograr el axioma. El primer axioma es el caso más simple;Cuando el agente AAG cree que se trata de una acción (α) de lograr su objetivo conflictivo G ∗ AAG, debe adoptar la intención potencial de hacer α y completar su objetivo.(∀AAG, α ∈ Caag, TN, Tα, W ∈ W) (Bel (AAG, DO (AAG, α, Tα, W) ⇒ Logro (G ∗ AAG, α, W)) ⇒ Pot.int.to ((AAG, α, TN, Tα, W) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción lejos de completar el objetivo, debe completar la acción. Cualquier función de evaluación justa clasificaría naturalmente α como la acción del valor máximo (propiedad 1). Sin embargo, sin la axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decidirá tomar otra acción por varias razones, debido a sus recursos de decisión limitados. A2. Axioma de acto preventivo. Al estar en una situación adversa, el Agente AAG podría decidir tomar medidas que dañen uno de sus planes de adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a AAG hacia su objetivo conflictivo G ∗ AAG. Dicha acción preventiva tendrá lugar cuando el Agente AAG cree sobre la posibilidad de que su adversario AO haga una acción β que le dará un alto 552 el Sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Valor de evaluación de utilidad (> TRH). Creyendo que tomar medidas α evitará que el oponente haga su β, adoptará una intención potencial de hacer α.(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, TN, Tβ, W ∈ W) (BEL (AAG, DO (AO, β, Tβ, W) ∧ Eval (AO, β, W)> TRH, Tn) ∧ bel (aag, do (aag, α, tα, w) ⇒ ¬do (ao, β, tβ, w), tn) ⇒ pot.int.to (aag, α, tn, tα, w)Este axioma es un componente básico de cualquier entorno adversario. Por ejemplo, mirando un juego de mesa de ajedrez, un jugador podría darse cuenta de que está a punto de ser revisado por su oponente, haciendo un movimiento preventivo. Otro ejemplo es un juego Connect Four: cuando un jugador tiene una fila de tres chips, su oponente debe bloquearlo o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y el agente debe tomar medidas preventivas inmediatas. Formalmente, tenemos las mismas creencias que se indican anteriormente, con una creencia cambiada de que hacer una acción β hará que el agente AO alcance su objetivo. Proposición 1: prevenir o perder el caso.(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, G ∗ Ao, TN, Tα, Tβ, W ∈ W) Bel (AAG, DO (AO, β, Tβ, W) ⇒ Logro (G ∗ AO, β, w), tn) ∧ bel (aag, do (aag, α, tα, w) ⇒ ¬do (ao, β, tβ, w)) ⇒ pot.int.to (aag, α, tn, tα, w) Boceto de prueba: la Proposición 1 puede derivarse fácilmente del Axiom A1 y la Propiedad 2 de la función EVEV, que establece que cualquier acción que cause una finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función EVAL sea igual a la función de utilidad del mundo real. Sin embargo, siendo agentes racionales limitados y lidiar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente en la función de evaluación. A3. Axioma de movimiento táctico subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decidirá no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor máximo de evaluación de servicios públicos), ya que cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría producir(Dependiendo de la respuesta de los adversarios) Una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función EVAL sea inexacta y difiere en gran medida de la función de utilidad. Dicho formalmente, el Agente AAG cree en una cierta medida que evolucionará de acuerdo con su acción inicial y producirá un alto valor beneficioso (> TRH) únicamente para ello.(∀aag, ao ∈ A, tn, w ∈ W) (∃α1, ..., αu ∈ Cai, β1, ..., βv ∈ Caj, tα1) bel (AAG, setAction (α1, ...,.,,,,,,,,,.αu, β1, ..., βV), tn) ∧ bel (aag, eval (ao, βv, w) <trh <eval (aag, αu, w), tn) ⇒ pot.int.to (AAG, α1, TN, Tα1, W) Un agente podría creer que una cadena de eventos ocurrirá por varias razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: un movimiento provoca una posición de verificación, lo que a su vez limita que los oponentes se mueven para evitar el cheque, al que el primer jugador podría reaccionar con otro control, y así sucesivamente. El agente también podría creer en una cadena de eventos basados en su conocimiento de su perfil de adversario, lo que le permite prever los movimientos de adversarios con alta precisión. A4. Axioma de detección de perfil. El agente puede ajustar sus perfiles de adversario mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele el conocimiento del perfil al respecto. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (<trh), el agente puede hacer una acción α en el tiempo Tα si cree que dará como resultado una acción no altamente beneficiosa β de su adversario,Lo que a su vez le enseña sobre el perfil de los adversarios, es decir, ofrece un advk más alto (p aj ai, tβ).(∀Aag, ao ∈ A, α ∈ Caag, β ∈ Cao, TN, Tα, Tβ, W ∈ W) Bel (AAG, (∀γ ∈ Caag) Eval (AAG, γ, W) <TRH, TN) ∧BEL (AAG, DO (AAG, α, Tα, W) ⇒ Do (AO, β, Tβ, W), TN) ∧ Bel (AAG, Eval (AO, β, W) <TRH) ∧ Bel (AAG, ADVWOK(P AJ AI, Tβ)> advknow (P AJ AI, TN), TN) ⇒ Pot.int.to (AAG, α, TN, Tα, W) Por ejemplo, volver al escenario del juego de tablero, considerar comenzarUn juego versus un oponente sobre el que no sabemos nada, ni siquiera si es un oponente humano o computarizado. Podríamos comenzar a jugar una estrategia que será adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo con su nivel de juego. A5. Axioma de formación de la alianza El siguiente axioma de comportamiento es relevante solo en una instanciación multilateral del entorno adversario (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que lo mejor para su mejor interés formará una alianza temporal. Tal alianza es un acuerdo que limita el comportamiento de sus miembros, pero sus miembros cree que les permitan lograr un valor de utilidad más alto que el que se puede lograr fuera de la alianza. Como ejemplo, podemos ver el juego de mesa de riesgo clásico, donde cada jugador tiene un objetivo individual de ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unir fuerzas y atacar a un oponente que es más fuerte que el resto. Los términos de alianzas define la forma en que deben actuar sus miembros. Es un conjunto de predicados, denotados como términos, que los miembros de la alianza acuerdan, y debe seguir siendo cierto durante la duración de la alianza. Por ejemplo, los términos establecidos en el escenario de riesgo podrían contener los siguientes predicados: 1. Los miembros de la alianza no se atacan entre sí en los territorios X, Y y Z;2. Los miembros de la alianza contribuirán con unidades C por turno para atacar el adversario AO;3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo TK o hasta que el ejército de adversario sea más pequeño que Q. Los términos establecidos especifican restricciones entre grupos en cada uno de los miembros de la alianza (∀aal i ∈ Aal ⊆ a) conjunto de acciones cal i ⊆ C. Definición 6. Al Val: el valor de evaluación total que el Agente AI logrará mientras es parte de AAL es la suma de Evali (Eval para AI) de cada uno de los valores de evaluación AAL J después de tomar sus propias acciones α (a través del agente (α) predicado):Al Val (AI, Cal, AAL, W) = α∈Cal Evali (AAL J, Agente (α), W) Definición 7. AL TRH - es un número que representa un Al Val el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) umbral 553;Por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TRH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de que se forma una alianza, sus miembros ahora están trabajando en su entorno de confrontación normal, así como de acuerdo con los estados mentales y los axiomas necesarios para sus interacciones como parte de la alianza. El siguiente Modelo de Alianza (AL) especifica las condiciones bajo las cuales se puede decir que el grupo AAL está en una alianza y trabajando con un conjunto nuevo y limitado de acciones Cal, en el momento TN. Al (Aal, Cal, W, Tn) 1. AAL tiene un MB de que todos los miembros son parte de AAL: MB (AAL, (∀AAL I ∈ AAL) Miembro (AAL I, AAL), TN) 2. AAL tiene un MB de que se mantenga el grupo: MB (AAL, (∀aal i ∈ Aal) int.th (ai, miembro (ai, aal), tn, tn+1, co), tn) 3. AAL tiene un MB que ser miembros les da un alto valor de utilidad: MB (AAL, (∀aal i ∈ Aal) Al Val (AAL I, Cal, AAL, W) ≥ Al Trh, TN) Los perfiles de los miembros son una parte crucial de exitososalianzas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Dichos agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (ítem 2 en el modelo anterior) y tomar contrarrestaciones (cuando el ítem 3 se falsifique). La robustez de la alianza es en parte una función de la medida de confianza de sus miembros, la estimación de posición objetiva y otras propiedades del perfil. Debemos tener en cuenta que un agente puede ser parte de más de una alianza. Tal alianza temporal, donde los miembros del grupo no tienen un objetivo conjunto, sino que actúan en colaboración por el interés de sus propios objetivos individuales, se clasifica como un grupo de tratamiento por psicólogos modernos [12] (en contraste con un grupo de tareas, donde sus miembrostener un objetivo conjunto). El modelo de actividad compartida como se presenta en [5] el comportamiento del grupo de tratamiento modelado utilizando la misma formalización de planos compartidos. Al comparar ambas definiciones de una alianza y un grupo de tratamiento, encontramos una semejanza no sorprendente entre ambos modelos: las definiciones de los modelos ambientales son casi idénticos (ver definiciones SAS en [5]), y sus axiomas de acto egoístas y cooperativos se ajustan a nuestras adversarios adversoscomportamiento de los agentes. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la actividad compartida que no puede ser parte de la nuestra. Este axioma establece que un agente considerará tomar medidas que reducirán su valor de evaluación (para cierto límite inferior), si cree que un socio grupal obtendrá un beneficio significativo. Tal comportamiento no puede ocurrir en un entorno adversario puro (como es un juego de suma cero), donde los miembros de la alianza están constantemente de vigilancia para manipular su alianza para su propio beneficio. A6. Axioma de maximización de evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximiza el valor heurístico según lo calculado en la función EV.(∀Aag, Ao ∈ A, α ∈ CAG, TN, W ∈ W) Bel (AAG, (∀γ ∈ CAG) Eval (AAG, α, W) ≥ Ev (AAG, γ, W), TN) ⇒ Pot.Int.to (AAG, α, TN, Tα, W) T1. Optimización en eval = utilidad El modelo axiomático anterior maneja situaciones donde se desconoce la utilidad y los agentes son agentes racionales limitados. El siguiente teorema muestra que en las interacciones bilaterales, donde los agentes tienen la función de utilidad real (es decir, eval = utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversaria clásica (por ejemplo, min-max). Teorema 1. Deje que AE AG sea un agente AE racional ilimitado que use la función de evaluación heurística Eval, AU AG sea el mismo agente que usa la función de utilidad verdadera y AO es un único adversario racional basado en utilidades no resistidos. Dado que eval = utilidad: (∀α ∈ Cau AG, α ∈ Cae AG, TN, W ∈ W) Pot.int.to (Au Ag, α, TN, Tα, W) → Pot.int.to (AE AG, α, Tn, Tα, W) ∧ ((α = α) ∨ (utilidad (Au Ag, α, W) = Eval (Ae AG, α, W))) Boceto de prueba, dado que Au AG tiene el realFunción de utilidad y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MINMAX óptimo para elegir la acción de valor de utilidad más alto, que denotamos, α. La prueba mostrará que AE AG, utilizando los axiomas AE, seleccionará la misma utilidad α α (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utility.(A1) Objetivo que logra el axioma: suponga que hay un α de tal manera que su finalización alcanzará el objetivo de AU AGS. Obtendrá la utilidad más alta por Min-Max para Au AG. El agente AE AG seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si dicho α no existe, AE AG no puede aplicar este axioma y procede a A2.(A2) Axioma de acto preventivo - (1) Mirando el caso básico (ver PROP1), si hay un β que lleva a AO a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para AU AG. AU AG lo elegirá a través de la utilidad, mientras que AE AG lo elegirá a través de A2.(2) En el caso general, β es una acción altamente beneficiosa para AO, por lo tanto, produce baja utilidad para AU AG, que lo guiará para seleccionar un α que evitará β, mientras que AE AG lo elegirá a través de A2.1β no existe para AO, entonces A2 no es aplicable y AE AG puede proceder a A3.(A3) Axioma de movimiento táctico subóptimo: cuando se usa una función de evaluación heurística, AE AG tiene una creencia parcial en el perfil de su adversario (ítem 4 en el modelo AE), lo que puede hacer que crea en las setacciones (PROP1). En nuestro caso, AE AG está manteniendo un perfil completo en su adversario óptimo y sabe que AO se comportará de manera óptima de acuerdo con los valores de utilidad reales en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre la setaction subóptima no puede existir, produciendo este axioma inaplicable. AE AG procederá a A4.(A4) Axioma de detección de perfil: dado que AE AG tiene el perfil completo de AO, ninguna de las acciones de AE AGS puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente continuará con A6 (A5 se ignorará porque la interacción es bilateral).(A6) Axioma de maximización de la evaluación: este axioma seleccionará la evaluación máxima para AE AG. Dado que eval = utilidad, se seleccionará el mismo α que fue seleccionado por Au AG.3. Evaluación El objetivo principal de nuestro análisis experimental es evaluar el comportamiento y el rendimiento de los modelos en un entorno adversario real. Esta sección investiga si se limita 1 un caso en el que después de la finalización de β existe una γ que proporciona una alta utilidad para el agente AU AG, no puede ocurrir porque AO usa la misma utilidad, y la existencia de γs hará que clasifique β como una acción de baja utilidad.554 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07), los agentes racionales situados en tales entornos adversos serán mejor aplicando nuestros axiomas de comportamiento sugeridos.3.1 El dominio para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos usar el juego Connect-Four como nuestro entorno adversario. Connect-Four es un juego de 2 jugadores, Zerosum que se juega con un tablero de 6x7 tipo matriz. Cada turno, un jugador deja caer un disco en una de las 7 columnas (el conjunto de 21 discos generalmente es de color amarillo para el jugador 1 y el rojo para el jugador 2; usaremos blanco y negro respectivamente para evitar confusión). El ganador es el primer jugador en completar un conjunto horizontal, vertical o diagonal de cuatro discos con su color. En ocasiones muy raras, el juego podría terminar en un empate si se llenan todas las cuadrículas vacías, pero ningún jugador logró crear un conjunto de 4 discos. El juego Connect-Four se resolvió en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna central (columna 4) y jugando de manera óptima, la estrategia óptima esMuy complejo y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos continuar con el comportamiento del agente, primero debemos verificar que el dominio se ajuste a la definición de entornos adversos como se dio anteriormente (en los que se basan los axiomas conductuales). Primero, cuando juega un juego Connect-Four, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Connect-Four solo puede haber un ganador (o ningún ganador en absoluto en la rara ocurrencia de un empate). Además, nuestro agente cree que su oponente al juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar de la nada, a través de hechos simples como la edad., a estrategias y debilidades). Por supuesto, no todos los encuentros de conexión de cuatro son adversos. Por ejemplo, cuando un padre está jugando con su hijo, la siguiente situación puede ocurrir: el niño, que tiene un fuerte incentivo para ganar, trata el medio ambiente como adversario (tiene la intención de ganar, entiende que solo puede haber un ganador,y cree que su padre está tratando de vencerlo). Sin embargo, el punto de vista de los padres podría ver el entorno como educativo, donde su objetivo no es ganar el juego, sino causar disfrute o practicar razonamiento estratégico. En un entorno educativo así, un nuevo conjunto de axiomas conductuales podría ser más beneficioso para los objetivos de los padres que nuestros axiomas conductuales adversos sugeridos.3.2 Análisis de axiomas Después de mostrar que el juego Connect-Four es de hecho un entorno adversario bilateral de suma cero, el siguiente paso es observar los comportamientos de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de los juegos completos de Connect-Four que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados vinieron de los sitios de juego por correo electrónico (PBEM). Estos son sitios web que alojan juegos de correo electrónico, donde cada movimiento es realizado por un intercambio de correo electrónico entre el servidor y los reproductores. Muchos de estos archivos de sitios contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Connect-Four tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (que llamamos el jugador blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (para ser llamados negros). El jugador blanco, siendo el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un lugar vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que se puede realizar en el oponente el próximo movimiento. Para que el jugador negro gane, de alguna manera debe girar el rumbo, aprovechar la ventaja y comenzar a presentar amenazas al jugador blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, creamos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos;(2) Detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará a una profundidad dada, D y para cada movimiento α generará el valor heurístico para la siguiente acción tomada por el jugador como está escrito en el archivo de registro, H (α), junto con el valor heurístico máximo, Maxh(α), eso podría lograrse antes de tomar el movimiento (obviamente, si h (α) = maxH (α), entonces el jugador no hizo el movimiento óptimo heurísticamente). El trabajo de los detectores de amenazas es notificar si se tomaron algunas medidas para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento de los oponentes). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable para los oponentes humanos: la definición 8. Deje que el grupo sea un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. Groupn B (Groupn W) sea un grupo con n piezas del color negro (blanco) y cuadrados vacíos de 4 -n.h = ((grupo1 b ∗ α)+(grupo2 b ∗ β)+(grupo3 b ∗ γ)+(grupo4 b ∗ ∞ ∞)))∗ γ)+(grupo4 w ∗ ∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada;Sin embargo, es importante valorarlos con el orden de α <β <δ (usamos 1, 4 y 8 como valores respectivos). Grupos de 4 discos del mismo color significa victoria, por lo tanto, el descubrimiento de dicho grupo dará como resultado ∞ para garantizar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores negros durante la interacción conecta y cuatro adversas. Cada juego del archivo de registro se ingresó en la aplicación, que procesó y emitió un archivo de registro reformateado que contiene el valor H del movimiento actual, el valor MAXH que podría lograrse y una notificación si se detectó una amenaza abierta. Se analizaron un total de 123 juegos (57 con blanco ganador y 66 con negro ganador). Algunos juegos adicionales se ignoraron manualmente en el experimento, debido a estos problemas: un jugador que abandona el juego mientras que el resultado no es final, o un movimiento irracional contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo ganador obvio enla primera apertura se mueve). Además, también se eliminó un solo juego de empate. El simulador se ejecutó a una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 555 Tabla 1: Análisis promedio de diferencia heurística Pérdidas negras Negras Won AVG MinH -17.62 -12.02 AVG 3 Movimientos H más bajos (Min3 H) -13.20 -8.70 3.2.1 Afirmación de la táctica suboptimalMover axioma La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las ideas del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda se seleccionó para que los resultados fueran comparables a [9], ver Sección 3.2.3). Los datos heurísticos de las tablas son la diferencia entre el valor heurístico máximo actual y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cerca esté el número de 0, más cerca estaba la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tenían el valor de diferencia máxima entre todas las acciones de los jugadores negros en un juego dado, como se promedió sobre todos los negros ganadores y perdedores (ver columnas respectivas). En los juegos en los que el jugador negro pierde, su valor de diferencia promedio fue -17.62, mientras que en los juegos en los que ganó el jugador negro, su promedio fue -12.02. La segunda fila expande el análisis considerando las 3 acciones de diferencia heurística más altas y promediándolas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos que el jugador negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron tomar una suposición educada en un número umbral de 11.5, como el valor de la constante TRH, que diferencia entre acciones normales y las altamente beneficiosas. Después de encontrar una constante TRH aproximada, podemos continuar con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de los negros fue de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio Min3 H diferente de los 3 rangos más grandes y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones heurísticamente de diferencia heurísticamente (min3 h) fue más pequeña que el umbral sugerido, TRH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando Min3 H> −4 el jugador negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estaban más cerca de los valores máximos darán como resultado más juegos ganadores para Black. Sin embargo, parece que en el dominio Connect-Four, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las ideas principales del análisis;La mayoría de los negros victorias (83%) llegaron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección minuciosa de esos juegos ganadores negros muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, Black de repente deja un disco en una columna aislada, lo que parece un desperdicio de un movimiento. White continúa construyendo sus amenazas, mientras que generalmente ignora el último movimiento de los negros, que a su vez usa el disco aislado como un ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para la Tabla 2 de los jugadores negros: porcentajes de ganancias de los negros% de los juegos min3 H <−11.5 12% min3 h> −4 5% −11.5 ≤ min3 h ≤ −4 83% para tomar acciones subóptimas y noDé el valor heurístico más alto posible actual, pero no será demasiado dañino para su posición (es decir, no le dará un alto valor beneficioso a su adversario). Al final resultó que, aprender el umbral es un aspecto importante del éxito: tomar movimientos muy riesgosos (Min3 H <-11.5) o tratar de evitarlos (Min3 H> −4) reduce a los jugadores negros ganando oportunidades por un gran margen.3.2.2 Afirmando el axioma de monitoreo de perfil en la tarea de mostrar la importancia de monitorear los perfiles adversarios, nuestros archivos de registro no pudieron usarse porque no contenían interacciones repetidas entre los jugadores, que son necesarios para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para lograr ventajas tácticas ya se estudió en varios dominios ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en las interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio de adversario Connect-Four, que ahora se puede utilizar para comprender la importancia de monitorear el perfil de adversario. Después de la presentación de su modelo teórico, describen un estudio empírico extenso y verifican el rendimiento de los agentes después de aprender el modelo de debilidad con ejemplos pasados. Uno de los dominios utilizados como un entorno competitivo fue el mismo juego de Connect-Four (Checkers fue el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí en sus valores de coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio Connect-Four muestra una mejora de una tasa ganadora de 0.556 antes de modelar a un 0.69 después del modelado (página 22). Sus conclusiones, que muestran un rendimiento mejorado al mantener y usar el modelo de adversario, justifican el esfuerzo para monitorear el perfil adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se ha aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico con el que eligieron trabajar, todos los métodos de integración pueden hacer que el agente tome decisiones subóptimas;Puede hacer que el agente prefiera acciones que sean subóptimas en la unión de decisión actual, pero que puede hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo que a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demostró en [9], confirma y fortalece aún más nuestro axioma táctico subóptimo como se discutió en la sección anterior.556 El sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 3.2.3 Contensores adicionales La necesidad de lograr el objetivo, el acto preventivo y los axiomas de maximización de la evaluación son obvios y no necesitan más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registro. Los axiomas de acto preventivo y logros preventivos, aunque teóricamente triviales, parecen proporcionar algún desafío a un jugador humano. En la inspección inicial de los registros, encontramos pocos juegos2 donde un jugador, por razones inexplicables, no bloqueó al otro para ganar o no pudo ejecutar su propio movimiento ganador. Podemos culpar a esas fallas a la falta de atención de los humanos, o un error de escritura en su respuesta de movimiento;Sin embargo, esos errores pueden ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Connect Four gira en torno a generar amenazas y bloquearlas. En nuestro análisis, buscamos acciones preventivas explícitas, es decir, se mueven que bloquean un grupo de 3 discos, o que eliminan una amenaza futura (en nuestro horizonte de búsqueda limitado). Descubrimos que en el 83% del total de juegos había al menos una acción preventiva tomada por el jugador negro. También se descubrió que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras promedió 1.5 acciones preventivas por juego al ganar. Parece que el negro requiere 1 o 2 acciones preventivas para construir su posición inicial, antes de comenzar a presentar amenazas. Si no logró ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir al blanco.4. Trabajo relacionado Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de las personas: algunos modelos usan conocimiento y creencias [10], otros tienen modelos de objetivos e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan con el trabajo en equipo del agente y la cooperación. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversos explícitos y comportamiento de los agentes en él. El algoritmo de búsqueda adversario de Min-Max clásico fue el primer intento de integrar al oponente en el espacio de búsqueda con una suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha realizado mucho esfuerzo para integrar el modelo de oponente en el procedimiento de decisión para predecir el comportamiento futuro. El algoritmo M ∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos oponentes en la búsqueda adversaria, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes fueron modeladoscomo autómatas finitos. Willmott et al. Realizaron un trabajo adicional de planificación adversaria.[13], que proporcionó un enfoque de planificación adversa al juego de GO. La investigación mencionada anteriormente trató la búsqueda adversaria y la integración de los modelos oponentes en métodos de búsqueda basados en servicios públicos clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo a una ventaja de los agentes. Sin embargo, las limitaciones básicas de esos métodos de búsqueda aún se aplican;Nuestro modelo trata de superar esas limitaciones presentando un modelo formal para una nueva especificación adversaria basada en el estado mental.5. Conclusiones Presentamos un modelo de entorno adversario para un 2 Estas luego se eliminaron del análisis final.Agente racional limitado que está situado en un entorno N-Relayer, Zerosum. Utilizamos la formalización de Plans Shared para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a usarse como una guía para diseñar agentes que necesitan operar en tales entornos adversos. Presentamos resultados empíricos, basados en el análisis de archivos de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que cubrirá todos los tipos de entornos adversos, por ejemplo, entornos que no son de cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más se tratarán en futuras investigaciones.6. Reconocimiento Esta investigación fue apoyada en parte por Israel Science Foundation Otorges #1211/04 y #898/05.7. Referencias [1] L. V. Allis. Un enfoque basado en el conocimiento de Connect-Four: el juego está resuelto: White Wins. Tesis de Masters, Free University, Amsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos oponentes en la búsqueda adversaria. En Actas de la Decimotercera Conferencia Nacional sobre Inteligencia Artificial, páginas 120-125, Portland, OR, 1996. [3] D. Carmel y S. Markovitch. Modelado del oponente en sistemas de múltiples agentes. En G. Weiß y S. Sen, editores, adaptación y aprendizaje en sistemas de múltiples agentes, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes de colaboración para una acción grupal compleja. Inteligencia Artificial, 86 (2): 269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyo a la actividad colaborativa. En Proc.de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/ightpbmserv/.[7] S. Kraus y D. Lehmann. Diseño y construcción de un agente automatizado de negociación. Computational Intelligence, 11: 132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Al actuar juntos. En Proc.de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y explotar las debilidades relativas de los agentes oponentes. Agentes autónomos y sistemas de múltiples agentes, 10 (2): 103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard. Resolución de problemas adversos: modelar un oponente usando coherencia explicativa. Cognitive Science, 16 (1): 123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica laboral grupal. Prentice Hall, Englewood Cliffs, NJ, 2nd Edition Edition, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversa para ir. Notas de conferencia en informática, 1558: 93-112, 1999. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 557