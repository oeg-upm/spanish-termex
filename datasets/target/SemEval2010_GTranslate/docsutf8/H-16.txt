El impacto del almacenamiento en caché en los motores de búsqueda ricardo baza-yates1 rbaeza@acm.org aristides gionis1 gionis@yahoo-inc.com flavio Junqueira1 fpj@yahoo-inc.com.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! Investigación Barcelona 2 ISTI - CNR Barcelona, España Pisa, Italia Resumen En este documento Estudiamos las compensaciones en el diseño de sistemas de almacenamiento de almacenamiento eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático frente a la dinámica y los resultados de la consulta de almacenamiento en caché frente a las listas de publicación de almacenamiento en caché. Utilizando un registro de consultas que abarca todo un año, exploramos las limitaciones del almacenamiento en caché y demostramos que las listas de publicación de almacenamiento en caché pueden lograr tasas de éxito más altas que las respuestas de la consulta en caché. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de las listas de publicación, lo que supera a los métodos anteriores. También estudiamos el problema de encontrar la forma óptima de dividir el caché estático entre respuestas y listas de publicación. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dada nuestra observación de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso de datos, por ejemplo, para una capa de memoria/disco o una capa de corredor/servidor remoto. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información - Proceso de búsqueda;H.3.4 [Algoritmos de términos generales de almacenamiento y recuperación de la información]: Sistemas y software - Sistemas distribuidos, Evaluación del desempeño (eficiencia y efectividad), Experimentación 1. Introducción Millones de consultas se envían diariamente a los motores de búsqueda web, y los usuarios tienen altas expectativas de la calidad y velocidad de las respuestas. A medida que la red de búsqueda se vuelve cada vez más grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En tal entorno, para lograr un tiempo de respuesta rápido y para aumentar el rendimiento de la consulta, usar un caché es crucial. El uso principal de una memoria de caché es acelerar el cálculo explotando datos utilizados con frecuencia o recientemente, aunque reducir la carga de trabajo a los servidores de fondo también es un objetivo importante. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red de área local o amplia. La decisión de qué almacenar en caché es fuera de línea (estático) o en línea (dinámico). Un caché estático se basa en información histórica y se actualiza periódicamente. Un caché dinámico reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si desalojar una entrada del caché en el caso de una falla de caché. Dichas decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de usar una memoria de caché: respuestas de almacenamiento en caché: a medida que el motor devuelve las respuestas a una consulta en particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: a medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicación de los términos de consulta involucrados. A menudo, todo el conjunto de listas de publicación no encaja en la memoria y, en consecuencia, el motor tiene que seleccionar un pequeño conjunto para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en el caché es más eficiente que calcular la respuesta utilizando listas de publicación en caché. Por otro lado, las consultas previamente invisibles ocurren con más frecuencia que los términos no vistos anteriormente, lo que implica una tasa de fallas más alta para las respuestas almacenadas en caché. El almacenamiento en caché de las listas de publicación tiene desafíos adicionales. Como las listas de publicación tienen un tamaño variable, almacenarlos en caché dinámicamente no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y la distribución sesgada de la corriente de consulta, como se muestra más adelante. El almacenamiento en caché estático de las listas de publicación plantea aún más desafíos: al decidir qué términos almacenados en caché se enfrenta la compensación entre los términos y términos consultados frecuentemente con pequeñas listas de publicación que son eficientes en el espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estático, el flujo de consulta debe analizarse para verificar que sus características no cambien rápidamente con el tiempo. Broker Caching Static en caché Listas de las respuestas de caché dinámico/estático Procesador de consultas locales Disco Nivel de almacenamiento en caché Acceso a la red remota Acceso a la red remota Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos las compensaciones en el diseño de cada nivel de caché, lo que demuestra que el problema es el mismo y solo cambian unos pocos parámetros. En general, suponemos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de las respuestas de consulta de almacenamiento dinámico en caché o publicar listas para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Las respuestas de consulta de almacenamiento en caché dan como resultado relaciones más bajas en comparación con el almacenamiento en caché de las listas de publicación para los términos de la consulta, pero es más rápido porque no hay necesidad de evaluación de consultas. Proporcionamos un marco para el análisis de la compensación entre el almacenamiento en caché estático de las respuestas de consulta y las listas de publicación;• El almacenamiento en caché estático de los términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicación para poner en un caché estático, y mostramos mejoras sobre el trabajo anterior, logrando una relación de éxito durante el 90%;• Los cambios de la distribución de la consulta a lo largo del tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La Sección 4 analiza las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 introducen algoritmos para el almacenamiento en caché de listas de publicación y un marco teórico para el análisis del almacenamiento en caché estático, respectivamente. La Sección 7 analiza el impacto de los cambios en la distribución de la consulta en el almacenamiento en caché estático, y la Sección 8 proporciona comentarios finales.2. Trabajo relacionado Hay un gran trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término a tiempo para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los principales documentos K para una consulta se pueden devolver sin la necesidad de evaluar el conjunto completo de listas de publicación [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual, ya que no consideran el almacenamiento en caché. Pueden considerarse separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros documentos sobre la explotación del historial de consultas de usuarios, proponen usar una base de consulta, basado en un conjunto de consultas óptimas persistentes presentadas en el pasado, para mejorar la efectividad de la recuperación para consultas futuras similares. Markatos [10] muestra la existencia de localidad temporal en consultas y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basado en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada almacenamiento en caché impulsado probabilístico, al intentar estimar la distribución de probabilidad de todas las consultas posibles presentadas a un motor de búsqueda [8]. Fagni et al.Siga el trabajo de Markatos mostrando que combinar políticas de almacenamiento en caché estático y dinámico junto con una política de captación previa adaptativa logra una alta relación HIT [7]. A diferencia de nuestro trabajo, consideran almacenamiento en caché y captación previa de páginas de resultados. Como los sistemas a menudo son jerárquicos, también ha habido un esfuerzo en arquitecturas de varios niveles. Saraiva et al.Proponga una nueva arquitectura para los motores de búsqueda web utilizando un sistema de almacenamiento de almacenamiento dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta para los motores jerárquicos. En su arquitectura, ambos niveles usan una política de desalojo de LRU. Encuentran que el caché de segundo nivel puede reducir efectivamente el tráfico de disco, lo que aumenta el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos frecuentes y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos documentos están relacionados con los nuestros, ya que explotan diferentes estrategias de almacenamiento en caché en diferentes niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la frecuencia/tamaño de relación para evaluar la bondad de un elemento en caché. Se han utilizado ideas similares en el contexto del almacenamiento en caché de archivos [17], el almacenamiento en caché web [5] e incluso el almacenamiento en caché de las listas de publicación [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de las listas de publicación.3. Caracterización de datos Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido, y los registros de consulta de un año de consultas enviadas a http://www.yahoo.co.uk de noviembre de 2005 a noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas es único. La longitud promedio de la consulta es de 2.5 términos, con la consulta más larga que tiene 731 términos.1E-07 1E-06 1E-05 1E-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizado) Rango de frecuencia (normalizado) Figura 2: La distribución de la distribución de la distribución de la distribución deconsultas (curva inferior) y términos de consulta (curva media) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido-2006 (curva superior). La Figura 2 muestra las distribuciones de consultas (curva inferior) y términos de consulta (curva media). El eje x representa el rango de frecuencia normalizado de la consulta o término.(La consulta más frecuente aparece más cerca del eje y). El eje Y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. UK-2006 Estadísticas de muestra # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 La frecuencia normalizada para una consulta dada (o término). Como se esperaba, la distribución de frecuencias de consulta y frecuencias de términos de consulta sigue las distribuciones de ley de energía, con pendiente de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon cuando aparecen en los registros sin normalización para el caso o el espacio en blanco. Los términos de consulta (curva media) se han normalizado para el caso, al igual que los términos en la recopilación de documentos. La recopilación de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido que se arrastró en mayo de 2006.1 Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo de amplitud, que comprende 15 GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de la ley de potencia con la pendiente 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término particular en el registro de consultas para ser 0.424. En la Figura 3 se muestra una gráfica de dispersión para una muestra aleatoria de términos. En este experimento, los términos se han convertido en minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables.1E-07 1E-06 1E-05 1E-04 0.001 0.01 0.1 1 1e-06 1E-05 1E-04 0.001 0.01 0.1 1 Frecuencia de documento de consecuencia Figura 3: Gráfico de dispersión normalizado de las frecuencias de documento a término frente a las frecuencias de término de consultas.4. El almacenamiento en caché de consultas y términos almacenado en caché se basa en la suposición de que hay localidad en el flujo de solicitudes. Es decir, debe haber suficiente repetición en el flujo de solicitudes y dentro de los intervalos de tiempo que permiten que una memoria de caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton fuera de todo el volumen. Por lo tanto, de todas las consultas en la corriente que componen el registro de consultas, el umbral superior en la relación HIT es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Sin embargo, es importante observar que no todas las consultas en este 56% pueden ser éxitos de caché debido a las fallas obligatorias. Una misión obligatoria 1 La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperada 05/2007.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de contenedores Número de contenedores Términos Términos Diff Consultas totales de consultas únicas Términos únicos Consulta Diferencia Figura 4: Llegada para términos y consultas.sucede cuando el caché recibe una consulta por primera vez. Esto es diferente de las fallas de capacidad, que ocurren debido a las limitaciones de espacio en la cantidad de memoria que usa el caché. Si consideramos un caché con memoria infinita, entonces la relación HIT es del 50%. Tenga en cuenta que para un caché infinito no hay capacidad de capacidad. Como mencionamos antes, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto da más libertad en la utilización del contenido de caché para responder a las consultas porque los términos almacenados en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de los términos singleton en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de los términos de consulta. Mostramos en la Sección 5 que el almacenamiento en caché de una pequeña fracción de términos, mientras que contabilizar los términos que aparecen en muchos documentos, es potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, trazamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado durante todo el período del registro de consulta. Las consultas totales y los términos totales corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, la diferencia de consulta y los términos diff corresponden a la diferencia entre las curvas para total y único. En la Figura 4, como se esperaba, el volumen de términos es mucho más alto que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la relación de un solo volumen es mayor para los términos y consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como una medida de cuánto trabajo impone una consulta a un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada para los términos que se muestran en la Figura 4. Para demostrar el efecto de un caché dinámico en la distribución de frecuencia de consulta de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de las consultas Figura 5: Gráfico de frecuencia después del caché LRU.Después de pasar por un caché LRU. En una falla de caché, un caché de LRU decide una entrada para desalojar el uso de la información sobre la recuperación de consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes del caché. Es posible que las consultas que sean más frecuentes después del caché tengan características diferentes, y ajustar el motor de búsqueda a consultas frecuentes antes de que el caché pueda degradar el rendimiento de las consultas no consultadas. La frecuencia máxima después del almacenamiento en caché es inferior al 1% de la frecuencia máxima antes del caché, lo que demuestra que el caché es muy efectivo para reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas de acuerdo con la frecuencia posterior a la atención, la distribución sigue siendo una ley de potencia, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallas de caché. Para analizar la tasa de fallas de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, informalmente, es el conjunto de referencias con la que una aplicación o un sistema operativo está trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo luego consiste en mantener en la memoria solo los elementos a los que se hace referencia en los pasos θ anteriores de la secuencia de entrada, donde θ es un parámetro configurable correspondiente al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de la página de los sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. En segundo lugar, permite un análisis de la tasa de fallas esperado dadas diferentes restricciones de memoria. Tercero, los establecimientos de trabajo capturan aspectos de algoritmos de almacenamiento de almacenamiento eficientes como LRU. LRU supone que las referencias más lejos en el pasado tienen menos probabilidades de ser referenciadas en el presente, lo que está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 traza la tasa de fallas para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo de consultas y términos. Los tamaños del conjunto de trabajo se normalizan contra el número total de consultas en el registro de consultas. En el gráfico para consultas, hay una disminución aguda hasta aproximadamente 0.01, y la velocidad a la que disminuye la tasa de fallas disminuye a medida que aumentamos el tamaño del conjunto de trabajo en más de 0.01. Finalmente, el valor mínimo que alcanza es la tasa de fallas del 50%, no se muestra en la cifra, ya que hemos cortado la cola de la curva para fines de presentación.0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Missro Normalización de trabajo de trabajo de trabajo Términos Figura 6: Tasa de fallas en función del tamaño del conjunto de trabajo.1 10 100 1000 10000 100000 1E+06 Distancia de frecuencia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa de fallas mínima para los términos es sustancialmente menor. La tasa de fallas también cae bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. Sin embargo, el valor mínimo es ligeramente superior al 10%, que es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con dicha política es posible alcanzar más del 80% de la tasa de aciertos, si consideramos el almacenamiento en caché de las listas de publicación dinámica para términos en lugar de respuestas de almacenamiento en caché para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria de caché, o la cantidad de tiempo que lleva armar una respuesta a una consulta de usuario. Analizamos estos problemas con más cuidado más adelante en este documento. También es interesante observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de la tasa de fallecimiento. Reporta la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en la gráfica se mide en el número de consultas distintas que separan una consulta y su repetición, y considera solo consultas que aparecen al menos 10 veces. De las Figuras 6 y 7, concluimos que incluso si establecemos el tamaño de la consulta responde al caché a un número relativamente grande de entradas, la tasa de fallas es alta. Por lo tanto, el almacenamiento en caché de las listas de publicaciones de términos tiene el potencial de mejorar la relación HIT. Esto es lo que exploramos a continuación.5. Listas de publicación de almacenamiento en caché La sección anterior muestra que las listas de publicación de almacenamiento en caché pueden obtener una tasa de aciertos más alta en comparación con las respuestas de consulta en caché. En esta sección, estudiamos el problema de cómo seleccionar listas de publicación para colocar en una cierta cantidad de memoria disponible, suponiendo que todo el índice sea mayor que la cantidad de memoria disponible. Las listas de publicación tienen un tamaño variable (de hecho, su distribución de tamaño sigue una ley de energía), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicación. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, usamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicación. Antes de discutir las estrategias de almacenamiento en caché estático, presentamos cierta notación. Utilizamos FQ (t) para denotar la frecuencia a término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consulta y FD (t) para denotar la frecuencia del documento de t, es decir, el númerode documentos en la colección en la que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicación de los términos con las frecuencias de consulta más altas FQ (t). Llamamos a este algoritmo QTF. Observamos que hay una compensación entre FQ (t) y FD (t). Los términos con alto FQ (t) son útiles para mantener en el caché porque se consultan a menudo. Por otro lado, los términos con alto FD (t) no son buenos candidatos porque corresponden a largas listas de publicación y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicación para el caché estático corresponde al problema estándar de la mochila: dada una mochila de capacidad fija y un conjunto de n elementos, como el elemento I-Th tiene valor CI y tamaño SI,Seleccione el conjunto de elementos que caben en la mochila y maximicen el valor general. En nuestro caso, el valor corresponde a FQ (t) y el tamaño corresponde a FD (t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que es seleccionar las listas de publicación de los términos con los valores más altos de la relación fq (t) fd (t). Llamamos a este algoritmo QTFDF. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de término, pero la ganancia fue mínima en comparación con la complejidad agregada. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: un algoritmo LRU estándar, pero es posible que deba desalojarse muchas listas de publicación (en orden de uso menos reciente) hasta que haya suficiente espacio en el espacio en el espacio en elmemoria para colocar la lista de publicaciones de acceso actualmente;• LFU: un algoritmo estándar de LFU (desalojo de los menos utilizados), con la misma modificación que la LRU;• Dyn-Qtfdf: una versión dinámica del algoritmo QTFDF;Evice de la memoria caché el término (s) con la relación FQ (t) FD (t) más baja. El rendimiento de todos los algoritmos anteriores durante 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño del caché se mide como una fracción del espacio total requerido para almacenar las listas de publicación de todos los términos. Para los algoritmos dinámicos, cargamos el caché con los términos en orden de FQ (t) y dejamos que el caché se calienta por 1 millón de consultas. Para los algoritmos estáticos, asumimos el conocimiento completo de las frecuencias fq (t), es decir, estimamos FQ (t) de toda la corriente de consulta. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias a plazo de consulta utilizando las primeras 3 o 4 semanas del registro de la consulta y medimos la tasa de aciertos en el resto.0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 HITRATE Tamaño de caché Publicación de caché de la publicación Listas de Publicación estática QTF/DF LRU LFU DYN-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para las listas de publicaciones en caché. La observación más importante de nuestros experimentos es que el algoritmo estático QTFDF tiene una mejor tasa de éxito que todos los algoritmos dinámicos. Un beneficio importante que un caché estático es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar las consultas. Sin embargo, si las características del tráfico de consulta cambian con frecuencia con el tiempo, entonces requiere re-populando el caché a menudo o habrá un impacto significativo en la tasa de aciertos.6. Análisis del almacenamiento en caché estático En esta sección, proporcionamos un análisis detallado para el problema de decidir si es preferible a las respuestas de consulta de caché o listas de publicación de caché. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso de datos. Se puede aplicar en la capa de memoria/disco o en una capa de servidor/servidor remoto como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema en particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente usamos para decidir la compensación óptima entre las respuestas de consulta en caché y las listas de publicación de almacenamiento en caché.6.1 Modelo analítico Sea m del tamaño del caché medido en las unidades de respuestas (el caché puede almacenar m respuestas de consulta). Suponga que todas las listas de publicación son de la misma longitud L, medidas en unidades de respuesta. Consideramos los siguientes dos casos: (a) un caché que almacena solo respuestas precomputadas, y (b) un caché que almacena solo las listas de publicación. En el primer caso, NC = M respuestas se ajustan en el caché, mientras que en el segundo caso np = m/l de publicaciones de publicaciones se ajustan en el caché. Así, np = nc/l. Tenga en cuenta que aunque las listas de publicación requieren más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (a), suponga que una respuesta de consulta en el caché puede evaluarse en 1 unidad de tiempo. Para el caso (b), suponga que si las listas de publicación de los términos de una consulta están en la memoria caché, los resultados se pueden calcular en unidades de tiempo TR1, mientras que si las listas de publicación no están en el caché, los resultados se pueden calcular enUnidades de tiempo TR2. Por supuesto TR2> TR1. Ahora queremos comparar el tiempo para responder un flujo de consultas Q en ambos casos. Deje que VC (NC) sea el volumen de las consultas NC más frecuentes. Luego, para el caso (A), tenemos un tiempo general TCA = VC (NC) + TR2 (Q - VC (NC)). Del mismo modo, para el caso (b), deje que VP (NP) sea el número de consultas computables. Luego tenemos tiempo general TP L = TR1VP (NP) + TR2 (Q - VP (NP)). Queremos verificar en qué condiciones tenemos tp l <tca. Tenemos TP L - TCA = (TR2 - 1) VC (NC) - (TR2 - TR1) VP (NP)> 0. La Figura 9 muestra los valores de VP y VC para nuestros datos. Podemos ver que las respuestas de almacenamiento en caché se saturan más rápido y para estos datos particulares no hay un beneficio adicional al usar más del 10% del espacio de índice para el almacenamiento en caché. Como la distribución de la consulta es una ley de potencia con el parámetro α> 1, la consulta más frecuente de I-th aparece con probabilidad proporcional a 1 Iα. Por lo tanto, el volumen VC (n), que es el número total de las N consultas más frecuentes, es VC (N) = V0 N I = 1 Q Iα = γNQ (0 <γn <1). Sabemos que VP (N) crece más rápido que VC (N) y suponemos, basado en resultados experimentales, que la relación es de la forma VP (N) = K VC (N) β. En el peor de los casos, para un caché grande, β → 1. Es decir, ambas técnicas almacenan en caché una fracción constante del volumen general de la consulta. Entonces las listas de publicación de almacenamiento en caché tienen sentido solo si L (TR2 - 1) K (TR2 - TR1)> 1. Si usamos compresión, tenemos L <L y TR1> TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para un caché pequeño, estamos interesados en el comportamiento transitorio y luego β> 1, según lo calculado a partir de nuestros datos. En este caso, siempre habrá un punto en el que TP L> TCA para una gran cantidad de consultas. En realidad, en lugar de llenar el caché solo con respuestas o solo con listas de publicación, una mejor estrategia será dividir el espacio total de caché en caché para respuestas y caché para publicar listas. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes del caché. Como el caché de respuesta es más rápido, será la primera opción para responder esas consultas. Deje que QNC y QNP sean el conjunto de consultas que pueden responder las respuestas en caché y las listas de publicación en caché, respectivamente. Luego, el tiempo general es t = VC (NC)+TR1V (QNP −QNC)+TR2 (Q - V (QNP ∪QNC)), donde np = (m - nc)/l. Encontrar la división óptima del caché para minimizar el tiempo de recuperación general es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para obtener compensaciones óptimas de caché para ejemplos de implementación particulares.6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] tanto para los documentos de indexación como para las consultas de procesamiento, en una sola máquina con un Pentium 4 a 2GHz y 1 GB de RAM. Indexamos los documentos del conjunto de datos del Reino Unido-2006, sin eliminar las palabras de detención o la aplicación de Stemming. Las listas de publicación en el archivo invertido consisten en pares de identificador de documentos y frecuencia de término. Comprimimos las brechas del identificador del documento utilizando la codificación de Elias gamma, y el 0.1 0.2 0.3 0.3 0.4 0.5 0.6 0.7 0.8 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Espacio de consulta Respuestas precomputadas Publicado Listas Figura 9: Cache como función de tamaño de tamaño. Tabla 2: Relaciones entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W W1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias a término en documentos que utilizan una codificación unaria [16]. El tamaño del archivo invertido es de 1.189 MB. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir toma 8 bytes. De la Tabla 1, obtenemos l = (8 ·# de publicaciones) 1264 ·# de términos = 0.75 y l = tamaño de archivo invertido 1264 ·# de términos = 0.26. Estimamos la relación tr = t/tc entre el tiempo promedio T Se necesita para evaluar una consulta y el tiempo promedio TC que se necesita para devolver una respuesta almacenada para la misma consulta, de la siguiente manera. TC se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo las consultas desde la memoria. El tiempo promedio es TC = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si hay al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia más alta que el número de documentos en el índice. Utilizamos un enfoque de documento para recuperar documentos que contienen todos los términos de consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de publicación comprimidas del archivo invertido. Realizamos una evaluación completa y parcial de respuestas, porque es probable que algunas consultas recuperen una gran cantidad de documentos, y solo los usuarios verán solo una fracción de los documentos recuperados. En la evaluación parcial de consultas, terminamos el procesamiento después de igualar 10,000 documentos. Las relaciones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación de consultas parcial y listas de publicación comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈Q fq · fd (t). Observe que el número total de publicaciones de la consulta-Termos no necesariamente proporciona una estimación precisa de la carga de trabajo impuesta al sistema mediante una consulta (que es el caso de la evaluación completa y las listas sin comprimir).0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total Postingstoprocessquery (normalizado) Tiempo total para procesar la consulta (normalizada) Procesamiento parcial de las publicaciones comprimidas Consulta LEN = 1 Consulta LEN en [2,3] Consulta LEN en [4,8] consultaLEN> 8 Figura 10: Carga de trabajo para evaluación de consultas parcial con listas de publicación comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuido en uno o múltiples sitios. Suponga que un sistema distribuido dividido en documento se ejecuta en un clúster de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consulta, que responden las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (suponemos que el tiempo dedicado a fusionar resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición del documento es la comunicación adicional entre el corredor y los procesadores de consultas. Usando pings ICMP en una LAN de 100 Mbps, hemos medido que enviar la consulta del corredor a los procesadores de consulta que envían una respuesta de 4.000 bytes al corredor lleva en promedio 0.615 ms. Por lo tanto, TRL = TR + 0.615MS/0.069MS = TR + 9. En el caso de que el corredor y los procesadores de consultas se encuentren en diferentes sitios conectados con una red de área amplia (WAN), estimamos que transmitir la consulta del corredor a los procesadores de consulta y recuperar una respuesta de 4.000 bytes toma en promedio 329 ms. Por lo tanto, TRW = TR + 329MS/0.069MS = TR + 4768. 6.3 Resultados de simulación Ahora abordamos el problema de encontrar la compensación óptima entre las respuestas de la consulta de almacenamiento en caché y las listas de publicación de almacenamiento en caché. Para hacer el problema concreto, asumimos un presupuesto fijo en la memoria disponible, de la cual se utilizan X unidades para almacenar en caché las respuestas de consulta y M - x para las listas de publicaciones en caché. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Utilizando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la memoria caché las respuestas a las consultas más frecuentes que se ajustan en el espacio X, y luego usamos el resto de la memoria para almacenar en caché las listas de publicación. Para seleccionar listas de publicación, utilizamos el algoritmo QTFDF, aplicado al registro de consultas de capacitación pero excluyendo las consultas que ya se han almacenado en caché. En la Figura 11, trazamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación de consultas parcial con listas de publicación comprimidas, el tiempo de respuesta más bajo se logra cuando 0.15 GB de los 0.5 GB se asignan para almacenar respuestas para consultas. Obtuvimos tendencias similares en los resultados para la configuración LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el corredor, que contiene el almacenamiento de trabajo simulado 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 1 1G Parcial / UNCOMPR / 1 g completo / Compr / 0.5 g Parcial / Compr / 0.5 g Figura 11: División óptima del caché en un servidor.3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 AveragerSponsetEment Space (GB) Carga de trabajo simulada - WAN Full / Uncomppr / 1 G Parcial / UncompR / 1 G Full / CompR / 0.5 G Parcial / Compr / 0.5 g Figura 12:División óptima del caché Cuando el siguiente nivel requiere acceso WAN.Respuestas de consultas y los procesadores de consulta, que contienen el caché de las listas de publicación. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque la sobrecarga de comunicación de red aumenta el tiempo de respuesta sustancialmente. Cuando se usa listas de publicación sin comprimir, la asignación óptima de la memoria corresponde a usar aproximadamente el 70% de la memoria para el almacenamiento en caché de las respuestas de consultas. Esto se explica por el hecho de que no hay necesidad de comunicación de red cuando la consulta puede ser respondida por el caché en el corredor.7. Efecto de la dinámica de la consulta para nuestro registro de consultas, la distribución de consultas y la distribución de consultas cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo los temas cambian la comparación de la distribución de consultas desde la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no apareció en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de consultas son nuevas consultas. La mayoría de las consultas que aparecen en una semana dada se repiten en las siguientes semanas durante los próximos seis meses. Luego calculamos la tasa de aciertos de un caché estático de 128, 000 respuestas entrenadas durante un período de dos semanas (Figura 13). Reportamos la tasa de aciertos por hora durante 7 días, a partir de las 5 p.m. Observamos que la tasa de golpes alcanza su valor más alto durante la noche (alrededor de la medianoche), mientras que alrededor de las 2-3 p.m. alcanza su mínimo. Después de una pequeña descomposición en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que el caché estático es efectivo durante una semana completa después del período de entrenamiento.0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 40 60 80 100 120 140 160 Hits de tiempo de golpe de golpe en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para un caché estático que contiene 128,000 respuestas durante el período de un Asemana. El caché estático de las listas de publicación se puede recomputar periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recomputar las listas de publicación en el caché estático, debemos considerar una compensación de eficiencia/calidad: usar un intervalo de tiempo demasiado corto podría ser prohibitivamente costoso, mientras que recomputar el caché con demasiada frecuencia podría conducir aTener un caché obsoleto que no corresponde a las características estadísticas de la corriente de consulta actual. Medimos el efecto sobre el algoritmo QTFDF de los cambios en una corriente de consulta de 15 semanas (Figura 14). Calculamos las frecuencias del término de consulta en toda la secuencia, seleccionamos qué términos almacenan en caché y luego calculamos la tasa de aciertos en todo el flujo de consulta. Esta tasa de éxito es como un límite superior, y asume un conocimiento perfecto de las frecuencias del término de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas de la corriente de consulta para calcular las frecuencias de los términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de golpes disminuye en menos del 2%. La alta correlación entre las frecuencias del término de consulta durante diferentes períodos de tiempo explica la elegante adaptación de los algoritmos de almacenamiento en caché estático a la corriente de consulta futura. De hecho, la correlación por pares entre todos los períodos posibles de 3 semanas de la corriente de consulta de 15 semanas es superior al 99.5%.8. Conclusiones El almacenamiento en caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados en el almacenamiento en caché dinámico y estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallas obligatorias causadas por el número de consultas únicas o infrecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de fallas es del 50% utilizando una estrategia de conjunto de trabajo. Los términos de almacenamiento en caché son más efectivos con respecto a la tasa de fallas, logrando valores tan bajos como 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de las listas de publicación que superan a los algoritmos de almacenamiento estático previos, así como algoritmos dinámicos como LRU y LFU, la obtención de valores de tasa de éxitos que son más de un 10% más altos compararon estas estrategias. Presentamos un marco para el análisis de la compensación entre los resultados de las consultas de almacenamiento en caché y las listas de publicación de almacenamiento en caché, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para los entornos centralizados y LAN, existe una asignación óptima de los resultados de la consulta de almacenamiento en caché y el almacenamiento en caché de las listas de publicación, mientras que para los escenarios WAN en los que prevalece el tiempo de la red, es más importante para cachear los resultados de la consulta.0.45 0.5 0.5 0.5 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.5 0.6 0.7 Dinámica de tamaño de caché HITRate Dinámica de la caché de caché estático/DF Conocimiento perfecto de 6 semanas Capacitación de 3 semanas Figura 14: Impacto de la distribución Cambios de distribución en el caché estáticode publicar listas.9. Referencias [1] V. N. Anh y A. Moffat. Evaluación de consultas podadas utilizando impactos precomputados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En Spire, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas vectoriales invertidas. En ACM Sigir, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índice estático en los sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché del proxy WWW consciente de WWW. En Usits, 1997. [6] P. Denning. Trabajar conjuntos pasados y presentes. IEEE Trans.en Ingeniería de Software, SE-6 (1): 64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Aumentando el rendimiento de los motores de búsqueda web: almacenamiento en caché y captación de los resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. Inf. Syst., 24 (1): 51-78, 2006. [8] R. Lempel y S. Moran. El almacenamiento en caché predictivo y la captación previa de la consulta resulta en motores de búsqueda. En www, 2003. [9] X. Long y T. Suel. El almacenamiento en caché de tres niveles para un procesamiento de consultas eficiente en grandes motores de búsqueda web. En www, 2005. [10] E. P. Markatos. En los resultados de la consulta del motor de búsqueda en caché. Computer Communications, 24 (2): 137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. MacDonald y C. Lioma. Terrier: una plataforma de recuperación de información de alto rendimiento y escalable. En el taller Sigir sobre la recuperación de información de código abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM Sigir, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. El almacenamiento en caché de dos niveles para preservar el rango para motores de búsqueda escalables. En ACM Sigir, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño de conjunto de trabajo promedio. Comunicaciones de la ACM, 17 (10): 563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM Sigir, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestión de gigabytes: compresión e indexación de documentos e imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Almacenamiento en caché de archivos en línea. Algorithmica, 33 (3): 371-383, 2002.