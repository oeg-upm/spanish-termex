Aprendizaje y deliberación conjunta a través de la argumentación en sistemas de múltiples agentes Santi Ontañón CCL, Lab de computación cognitiva Georgia Institute of Technology Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación de Inteligencia Artificial CSIC, Consejo Español para la Investigación Científica para la Investigación CientíficaCampus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es Resumen En este documento presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado para dos fines: (1) para la deliberación conjunta y (2) paraAprendiendo de la comunicación. El marco AMAL se basa completamente en el aprendizaje de los ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de acuartiones son técnicas basadas en casos. Para unirse a la deliberación, los agentes de aprendizaje comparten su experiencia al formar un comité para decidir sobre alguna decisión conjunta. Mostramos experimentalmente que la argumentación entre los comités de los agentes mejora el desempeño individual y conjunto. Para aprender de la comunicación, un agente se dedica a discutir con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos;El proceso de argumentación mejora su alcance de aprendizaje y su desempeño individual. Categorías y descriptores de sujetos I.2.6 [Inteligencia artificial]: aprendizaje;I.2.11 [Inteligencia artificial]: Sistemas distribuidos de inteligencia artificial-multiagente, agentes inteligentes 1. Introducción Los marcos de argumentación para sistemas de múltiples agentes se pueden utilizar para diferentes fines como deliberación conjunta, persuasión, negociación y resolución de conflictos. En este artículo presentaremos un marco de argumentación para los agentes de aprendizaje y demostraremos que puede usarse para dos fines: (1) deliberación conjunta y (2) aprendizaje de la comunicación. La deliberación conjunta basada en la argumentación implica la discusión sobre el resultado de una situación particular o el curso de acción apropiado para una situación particular. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que los ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado de la situación en cuestión. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también es limitada. Por lo tanto, los agentes de aprendizaje que son capaces de discutir sus predicciones individuales con otros agentes pueden alcanzar una mejor precisión de predicción después de tal proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas de múltiples agentes se basan en la lógica deductiva o en algún otro formalismo lógico deductivo diseñado específicamente para apoyar la argumentación, como la lógica predeterminada [3]). Por lo general, un argumento se considera una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento [4, 13];Los agentes usan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en la lógica asumen agentes con conocimiento precargado y relación de preferencia. En este documento, nos centramos en un marco de aprendizaje múltiple (AMAL) basado en argumentación donde se aprende tanto el conocimiento como la relación de preferencia de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) funcionan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender de ejemplos y (3) comunicarse utilizando un marco argumentativo. Tener capacidades de aprendizaje permite que los agentes usen efectivamente una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos cuestiones: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos contradictorios que han sido inducidos por ejemplos. Este documento presenta un enfoque basado en casos para abordar ambos problemas. Los agentes usan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) para predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco AMAL en los agentes de los apoyos para alcanzar una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre los agentes de aprendizaje puede producir una predicción conjunta que mejore sobre el rendimiento del aprendizaje individual y (2) si el aprendizaje de los contraexperios transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se usan mientras discuten entrea ellos. El papel está estructurado de la siguiente manera. La Sección 2 discute la relación entre argumentación, colaboración y aprendizaje. Luego, la Sección 3 introduce nuestro marco CBR (MAC) de múltiples agentes y la noción de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y políticas de generación de argumentos respectivamente. Más tarde, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El documento se cierra con secciones de trabajo y conclusiones relacionadas.2. La argumentación, la colaboración y el aprendizaje tanto el aprendizaje como la colaboración son formas en que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en los sistemas de múltiples agentes, ya que ambas son formas en que los agentes pueden lidiar con sus deficiencias. Demostremos cuáles son las principales motivaciones que un agente puede tener que aprender o colaborar.• Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Aumentar el rango de problemas solucionables.• Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Aumentar el rango de problemas solucionables, aumentar el rango de recursos accesibles. Mirando las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas de múltiples agentes. De hecho, con la excepción del último elemento en las motivaciones para colaborar la lista, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, colaborando o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo, propondremos Amal, un marco de argumentación para los agentes de aprendizaje, y también mostraremos cómo se puede usar Amal tanto para aprender de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa a través de la participación deUn proceso de argumentación sobre la predicción de la situación en cuestión. Usando esta colaboración, la predicción se puede hacer de una manera más informada, ya que la información conocida por varios agentes se ha tenido en cuenta.• Los agentes también pueden aprender de la comunicación con otros agentes involucrando un proceso de argumentación. Los agentes que participan en tales procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y usar esta información para predecir los resultados de situaciones futuras. En el resto de este documento, propondremos un marco de argumentación y mostraremos cómo se puede usar tanto para aprender como para resolver problemas de manera colaborativa.3. Sistemas CBR de múltiples agentes Un sistema de razonamiento basado en casos de múltiples agentes (Mac) M = {(a1, c1), ..., (an, cn)} es un sistema de múltiples agentes compuesto de a = {ai, .. ..., An}, un conjunto de agentes CBR, donde cada agente ai ∈ A posee una base de casos individual CI. Cada agente individual IA en una Mac es completamente autónoma y cada IA de agente solo tiene acceso a su CI de base individual y privada. Una base base CI = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver individualmente problemas, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos restringiremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. A continuación, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso C = P, S es una tupla que contiene una descripción de caso P y una clase de solución S ∈ S. A continuación, usaremos el problema de los términos y la descripción del caso de manera indistintiva. Además, utilizaremos la notación DOT para referirnos a elementos dentro de una tupla;por ejemplo, para referirnos a la clase de solución de un caso C, escribiremos C.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta, cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que los otros agentes pueden examinar y criticar). La siguiente sección aborda este problema.3.1 Predicciones justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] a cargo de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, lo que aumenta la confiabilidad del sistema. La mayor parte del trabajo existente en la generación de explicaciones se centra en generar explicaciones que se proporcionarán al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y la coordinación entre los agentes. Estamos interesados en las justificaciones, ya que pueden usarse como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación creada por un método CBR después de determinar que la solución de un problema particular P fue SK es una descripción que contiene la información relevante del problema P que el método CBR ha considerado predecir SK como la solución de P. En particular, CBRLos métodos funcionan recuperando casos similares al problema en cuestión y luego reutilizando sus soluciones para el problema actual, esperando que, dado que el problema y los casos sean similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., CN para resolver un problema particular P La justificación creada contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, es decirContendrá la información relevante que P y C1, ..., CN tienen en común. Por ejemplo, la Figura 1 muestra una construcción de justificación por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Traffic_light y Cars_Passing), el mecanismo de recuperación del sistema CBR se da cuenta de que al considerar solo el atributo Traffic_light, puede recuperar dos casos que predicen la misma solución: espera. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de los atributos son irrelevantes, ya que sea cual sea su valor, la clase de solución habría sido el mismo.976 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (Aamas 07) Problema Traffic_light: Red Cars_Passing: No Case 1 Traffic_light: Red Cars_Passing: No Solución: Wait Case 3 Traffic_light: Red Cars_Passing: Sí Solución: Caso de espera 4 Traffic_light: Green Cars_Passing: Sí.: Wait Case 2 Traffic_light: Green Cars_Passing: No Solución: Casos de Cruz Recuperados Solución: Justificación de espera Traffic_light: Rojo Figura 1: Un ejemplo de generación de justificación en un sistema CBR. Observe que, dado que la única característica relevante para decidir es TRATFER_LIGHT (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría de) los casos en la base de casos de un agente que satisface la justificación (es decir, todos los casos que se subsumen por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsunción. En nuestro trabajo, usamos Lid [2], un método CBR capaz de construir justificaciones simbólicas, como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: Definición 3.1. Una predicción justificada es una tupla j = a, p, s, d donde el agente A considera la solución correcta para el problema P, y esa predicción está justificada una descripción simbólica d de tal manera que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas CBR [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, para permitir que los agentes de aprendizaje participen en procesos de argumentación.4. Argumentos y contraargumentos para nuestros propósitos Un argumento α generado por un agente A está compuesto por una declaración sy alguna evidencia d que respalda S como correcta. En el resto de esta sección, veremos cómo esta definición general de argumento puede instanciarse en un tipo de argumentos específicos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre las predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S y B) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente AI para argumentar que AI cree que la solución correcta para un problema P es α.s, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente AI puede generar el argumento α = ai, p, espera, (tráfico_light = rojo), lo que significa que el agente AI cree que la solución correcta para P es esperar porque el atributo tráfico_light es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco, un contraargumento consiste en una predicción justificada AJ, P, S, D generada por un agente AJ con la intención de refutar un argumento α generado por otro agente AI, que respalda una solución de solución diferente de la de α.spara el problema en cuestión y justifica esto con una justificación d. En el ejemplo de la Figura 1, si un agente genera el argumento α = ai, p, walk, (cars_passing = no), un agente que piensa que la solución correcta es la espera podría responder con el contraargumento β = AJ, P, Wait,(Cars_passing = no ∧ tráfico_light = rojo), lo que significa que, aunque no hay automóviles que pasen, el semáforo es rojo y la calle no se puede cruzar. Un contraejemplo C es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que establece que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c.Específicamente, para que un caso C sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D C y α.s = C.S, es decir, el caso debe satisfacer la justificación α.d y la solución de Cser diferente al predicho por α. Al intercambiar argumentos y contraargumentos (incluidos los contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden involucrar un proceso de deliberación conjunta. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre los argumentos contradictorios y una política de decisión para generar argumentos contra los contratiempos (incluidas las contraexampres). En las siguientes secciones presentaremos estos elementos.5. Relación de preferencia Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre las predicciones justificadas basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tiene en cuenta los casos propiedad de cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son contraejemplos. Cuanto más sean los casos de respaldo, mayor es la confianza;Y cuanto más sean las contraejemplos, menor será la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individual que subsume por α.d. Con ellos, un agente ai obtiene los valores Y (aye) y n (no): • y ai α = | {c ∈ Ci |α.D C.P ∧ α.S = C.S} |es el número de casos en la base del caso de los agentes subsumidos por la justificación α.d que pertenecen a la clase de solución α.s, • nai α = | {c ∈ Ci |α.D C.P ∧ α.S = C.S} |es el número de casos en la base del caso de los agentes subsumidos por justificación α.d que no pertenecen a esa clase de solución. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 977 + + + + + + - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: Cai (α) = y ai α 1 + y ai α + nai α, es decir, la confianza en una predicción justificada es el número de casos de respaldo divididos por el número de casos de respaldo más contraexplatos. Tenga en cuenta que agregamos 1 al denominador, esto es para evitar dar confidencias excesivamente altas a las predicciones justificadas cuya confianza se ha calculado utilizando un pequeño número de casos. Observe que esta corrección sigue la misma idea que la corrección de Laplace para estimar las probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, tres casos de respaldo y un contraejemplo se encuentran en la base de casos de IA de los agentes, dando una confianza estimada de 0.6 Además, también podemos definir la confianza conjunta de un argumentoα a medida que la confianza se calculó utilizando los casos presentes en las bases de casos de todos los agentes del grupo: c (α) = i y ai α 1 + i y ai α + nai α notifica que, calcular colaborativamente la confianza articular, laLos agentes solo tienen que hacer públicos los valores de AYE y no calculados localmente para un argumento determinado. En nuestro marco, los agentes usan esta confianza articular como relación de preferencia: se prefiere una predicción justificada α sobre otro β si c (α) ≥ c (β).6. Generación de argumentos En nuestro marco, los agentes generan argumentos de los casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede usarse para generar argumentos. Por ejemplo, los árboles de decisión y la tapa [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos informados en este documento, los agentes usan la tapa. Por lo tanto, cuando un agente quiere generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por la tapa después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, una nueva esponja para determinar su orden), el agente usa la tapa para generar un argumento (que consiste en una predicción justificada) utilizando los casos en elBase de casos del agente. La justificación que se muestra en la Figura 3 se puede interpretar diciendo que la solución predicha es Hadromerida porque la forma suave de los megascleros del esqueleto espiculado de la esponja es de tipo tylostyle, el esqueleto espikulado de la esponja no tiene una longitud uniforme, y no hayGemmules en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, Hadromerida, D1.6.1 Generación de contraargumentos Como se indicó anteriormente, los agentes pueden tratar de refutar los argumentos generando contraargumentos o encontrando contraejemplos. Expliquemos cómo se pueden generar. Un agente AI quiere generar un contraargumento β para refutar un argumento α cuando α está en contradicción con la base de casos local de IA. Además, mientras se genera tal contraargumento β, la IA espera que se prefieran β sobre α. Para ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se usa ampliamente en marcos deductivos para la argumentación, y afirma que entre dos argumentos conflictivos, los más específicos deben preferirse, ya que está, en principio, más informado. Por lo tanto, se espera que los argumentos generados generados en función del criterio de especificidad sean preferibles (ya que están más informados) a los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales argumentos contra los contraar ganan, ya que, como hemos dicho en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno puede pensar que sería mejor que los agentes generen contraargumentos basados en la relación de preferencia de confianza conjunta;Sin embargo, no es obvio cómo generar argumentos en contra de la confianza conjunta de una manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar acuartiones (actualmente una de nuestras futuras líneas de investigación). Por lo tanto, en nuestro marco, cuando un agente quiere generar un contraargumento β a un argumento α, β debe ser más específico que α (es decir, α.D <β.D). La generación de argumentos contra los que utilizan el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 se pueden adaptar fácilmente a esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción que comienza desde cero y agregue heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se hace más específica que en el paso anterior, y el número de casos que se subsume por esa descripción se reduce. Cuando la descripción cubre solo (o casi) casos de una sola clase de solución termina y predice esa clase de solución. Para generar un argumento contra un argumento, la tapa α solo tiene que usar como punto de partida, la descripción α.d en lugar de comenzar desde cero. De esta manera, la justificación proporcionada por la tapa siempre será subsumida por α.D y, por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, observe que la tapa a veces puede no ser capaz de generar contraargumentos, ya que la tapa puede no especializar la descripción α.d más o porque la IA del agente no tiene un caso inci que sea subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento que se muestra en la Figura 3, genera un contraargumento usando la tapa. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 El sexto intl. Conf.Sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) Solución: Justificación de Hadromerida: D1 Sponge Spikule Skeleton Características externas Características externas Gemmules: No Spikule Skeleton Megascleros Longitud uniforme: No Megascleros Forma lisa: Base de caso de estilo tylostyle de A1 NUEVO esponja P Figura 3 Figura 3: Ejemplo de una justificación real generada por la tapa en el conjunto de datos de las esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente AI quiere refutar un argumento α, utiliza la siguiente política: 1. Agent AI usa la tapa para tratar de encontrar un contraargumento β más específico que α;Si se encuentra, β se envía al otro agente como un contraargumento de α.2. Si no se encuentra, entonces AI busca un contraejemplo C ∈ Ci de α. Si se encuentra un caso C, entonces C se envía al otro agente como un contraejemplo de α.3. Si no se encuentran contraejemplos, entonces AI no puede refutar el argumento α.7. Aprendizaje múltiple basado en argumentación El protocolo de interacción de AMAL permite que un grupo de agentes A1 ..., y deliberaran sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta finaliza;De lo contrario, se utiliza un voto ponderado para determinar la solución articular. Además, Amal también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo Amal consiste en una serie de rondas. En la ronda inicial, cada agente establece que es su predicción individual para P. Entonces, en cada ronda, un agente puede tratar de refutar la predicción realizada por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de aprobación de tokens para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción realizada por cualquier otro agente. Específicamente, cada agente puede enviar un contraargumento o un contraejemplo cada vez que obtiene el token (observe que esta restricción es solo para simplificar el protocolo, y que no restringe el número de contraargumentos que un agente puede enviar, ya que puede retrasarsepara rondas posteriores). Cuando un agente recibe un contraargumento o un contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los argumentos contra los años cuando reciben el token, tratando de generar un acoplamiento contra el contraargumento. Cuando todos los agentes han tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento en el protocolo, todos los agentes están de acuerdo o durante las últimas n rondas, ningún agente ha generado ningún argumento contra la contraer, el protocolo termina. Además, si al final de la argumentación, los agentes no han llegado a un acuerdo, entonces un mecanismo de votación que usa la confianza de cada predicción como pesas se usa para decidir la solución final (por lo tanto, Amal sigue el mismo mecanismo que los comités humanos, primeroCada miembro individual de un comité expone sus argumentos y discuso los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • Afirmar (α): la predicción justificada mantenida durante la próxima ronda será α. Un agente solo puede contener una sola predicción en cada ronda, por lo que se envían múltiples afirmaciones, solo la última se considera la predicción actualmente sostenida.• RECUTAR (β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos HT = αT 1, ..., αT N como las predicciones que cada uno de los n agentes contiene en una ronda t.Además, también definiremos contradecir (αT i) = {α ∈ Ht | α.S = αT i.s} como el conjunto de argumentos contradictorios para un agente ai en una ronda t, es decir, el conjunto de argumentos en la redonclase de solución diferente que αT i. El protocolo se inicia porque uno de los agentes recibe un problema P para resolverse. Después de eso, el agente informa a todos los demás agentes sobre el problema P para resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método CBR. Luego, cada agente AI envía el afirmación performativo (α0 I) a los otros agentes. Por lo tanto, los agentes saben H0 = α0 I, ..., α0 n. Una vez que se han enviado todas las predicciones, el token se entrega al primer agente A1.2. En cada ronda T (que no sea 0), los agentes verifican si sus argumentos en HT están de acuerdo. Si lo hacen, el protocolo se mueve al paso 5. Además, si durante las últimas n rondas, ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también se mueve al paso 5. De lo contrario, el propietario del agente AI del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradicto (αT I) ⊆ HT (ver Sección 6.1). Luego, el contraargumento βT I contra la predicción αT J con la confianza más baja C (αT J) se selecciona (ya que αT J es más probable que la predicción se refutara con éxito).• Si βT I es un contraargumento, entonces, AI compara localmente αT I con βT I evaluando su confianza contra su CI de la base de casos individual (ver Sección 5) (observe que la IA está comparando su argumento anterior con el contraargumento de que la IA tiene sologenerado y eso es aproximadamente el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 979 SPONGE SPIKULE SKELETA CARACTERÍSTICAS EXTERNAS CARACTERÍSTICAS EXTERNAS: D2 Figura 4: Generación de un contraargumento usando la tapa en el conjunto de datos de Sponges.para enviar a AJ). Si Cai (βt i)> Cai (αT I), entonces AI considera que βT I es más fuerte que su argumento anterior, cambia su argumento a βT I enviando afirmación (βt I) al resto de los agentes (la intuición detrás de estoes que, dado que un contraargumento también es un argumento, la IA verifica si el recién contraargumento es un mejor argumento que el que estaba sosteniendo anteriormente) y refutar (βT I, αT J) a AJ. De lo contrario (es decir, CAI (βT I) ≤ Cai (αT I)), AI solo enviará refut (βt I, αT J) a AJ. En cualquiera de las dos situaciones, el protocolo se mueve al paso 3. • Si βT I es un contraejemplo C, entonces AI envía refut (C, αT J) a AJ. El protocolo se mueve al paso 4. • Si la IA no puede generar ningún argumento contra la contraarrato o el contraejemplo, el token se envía al siguiente agente, una nueva ronda T + 1 comienza y el protocolo se mueve al estado 2. 3. El agente AJ que ha recibido el contraargumento βT I, lo compara localmente con su propio argumento, αT J, evaluando localmente su confianza. Si Caj (βT I)> Caj (αT J), entonces AJ aceptará el contraargumento como más fuerte que su propio argumento, y enviará afirmación (βT I) a los otros agentes. De lo contrario (es decir, CAJ (βT I) ≤ Caj (αT J)), AJ no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones comienza una nueva ronda T + 1, AI envía el token al siguiente agente, y el protocolo regresa al estado 2. 4. El agente AJ que ha recibido el contraejemplo C lo retiene en su base de casos y genera un nuevo argumento αT+1 J que tiene en cuenta C, e informa al resto de los agentes enviando Assert (αT+1 J) a todos ellos. Luego, AI envía el token al siguiente agente, una nueva ronda T + 1 comienza y el protocolo regresa al paso 2. 5. El protocolo termina produciendo una predicción conjunta, de la siguiente manera: si los argumentos en HT están de acuerdo, entonces su predicción es la predicción conjunta, de lo contrario se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: s = arg max sk∈S αi∈Ht | αi.s = SK c (αi) Además, para evitar iteraciones infinitas, si un agente envía dos vecesEl mismo argumento o contraargumento al mismo agente, el mensaje no se considera.8. Ejemplificación consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1 recibe un problema P para resolver, y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, Hadromérida, D1 • α0 2 = A2, P, Astrophorida, D2 • α0 3 = A3, P, Axinellida, D3 A1 comienza a ser propietario del token e intenta generar los contraargumentos.Para α0 2 y α0 3, pero no tiene éxito, sin embargo, tiene un contraecomprador C13 para α0 3. Por lo tanto, A1 envía la refut del mensaje (C13, α0 3) a A3. A3 incorpora C13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta C13. A3 se le ocurre la predicción justificada α1 3 = A3, P, Hadromerida, D4, y la transmite al resto de los agentes con la afirmación del mensaje (α1 3). Por lo tanto, todos saben el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene la ficha. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo tiene éxito para generar un contraargumento β1 2 = A2, P, Astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con la refutación del mensaje (β1 2, α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento y, por lo tanto, H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 obtiene la ficha. A3 genera un contraargumento β2 3 = A3, P, Hadromerida, D6 para α0 2 y lo envía a A2 con la refutación del mensaje (β2 3, α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con la afirmación del mensaje (β2 3). Después de eso, H3 = α0 1, β2 3, α1 3. En la ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen Hadromérida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera a Hadromérida como la solución conjunta para el problema P. 9. Evaluación experimental 980 El sexto intl. Conf.sobre agentes autónomos y sistemas de múltiples agentes (AAMAS 07) esponja 75 77 79 81 83 85 87 89 91 2 3 4 5 Amal Votación de soja individual 55 60 65 70 75 80 85 90 2 3 4 5 Votación Amal Figura 5: individual e individual e individual e individual e individual e individual e individual e individual e individualPrecisión conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacionales). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de la esponja tiene 280 ejemplos y 3 clases de soluciones. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de pruebas. Los ejemplos del conjunto de capacitación se distribuyen entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de prueba, los problemas en el conjunto de pruebas llegan al azar a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación;y (H2) que aprender de la comunicación mejora el desempeño individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a partir de la argumentación aumente a medida que aumenta el número de agentes que participan en la argumentación (ya que se tendrá en cuenta más información). Con respecto a H1 (la argumentación es un marco útil para la deliberación conjunta), ejecutamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que la capacitación siempre se distribuyeentre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponja y soja. La precisión de la clasificación se traza en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, votación y amal. La barra individual muestra la precisión promedio de las predicciones de agentes individuales;La barra de votación muestra la precisión promedio de la predicción conjunta lograda por votación pero sin ninguna argumentación;y finalmente la barra de Amal muestra la precisión promedio de la predicción conjunta utilizando la argumentación. Los resultados mostrados son el promedio de 5 corridas de validación cruzada de 10 veces. La Figura 5 muestra que la colaboración (votación y amal) supera a la resolución de problemas individuales. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera a la votación estándar, lo que demuestra que las decisiones conjuntas se basan en una mejor información según lo previsto por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de la esponja es de 87.57% para AMAL y 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por Amal sobre la votación es aún mayor en el conjunto de datos de la soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que Amal explota efectivamente la oportunidad de mejorar: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la votación). Con respecto a H2 (aprender de la comunicación en los procesos de argumentación mejora la predicción individual), ejecutamos el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de capacitación entre los cinco agentes;Después de eso, el resto de los casos en el conjunto de capacitación se envían a los agentes uno por uno;Cuando un agente recibe un nuevo caso de capacitación, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo o el agente puede usarlo para involucrar un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres parcelas, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje en absoluto;L (aprendizaje), muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden al retener los casos de capacitación que reciben individualmente (observe que cuando todos los casos de capacitación se han retenido al 100%, la precisión debe ser igual a la de la Figura 5 paraagentes individuales);Y finalmente, LFC (aprender de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden al retener los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de los ejemplos de capacitación como de los contraegumas). La Figura 6 muestra que si un agente AI aprende también de la comunicación, la IA puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (aquellos seleccionados como contraejemplos relevantes para la IA durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación versus una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de Sponges, los agentes han conservado solo muy pocos casos adicionales y mejoraron significativamente la precisión individual;a saber, conservan 59.96 casos en promedio (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja, se aprenden más contraejemplos para mejorar significativamente la precisión individual, a saber, conservan 87.16 casos en promedio (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para aprender de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más que los necesarios), y tienen el efecto previsto.10. Trabajo relacionado con respecto a CBR en un entorno de múltiples agentes, la primera investigación fue sobre la recuperación de casos negociados [11] entre los grupos de agentes. Nuestro trabajo sobre aprendizaje basado en casos de múltiples agentes comenzó en 1999 [6];Más tarde, MC Ginty y Smyth [7] presentaron un enfoque CBR colaborativo de múltiples agentes (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de bases múltiples (MCBR) [5], que se ocupa del sexto INTL. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 981 SPONGE 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL Soybean 20 30 40 50 60 70 90 25% 40% 55% 55%70% 85% 100% LFC L NL Figura 6: Aprendizaje de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes.Sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación base transversal. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de reutilización de CBR (usando un sistema de votación) mientras cada agente realiza la recuperación individualmente;Los otros enfoques de CBR multiagente, sin embargo, se centran en distribuir el proceso de recuperación. La investigación sobre la argumentación de MAS se centra en varios temas como A) lógicas, protocolos e idiomas que apoyan la argumentación, b) selección de argumentos e interpretación de argumentos. Los enfoques para la lógica y los idiomas que respaldan la argumentación incluyen lógica defensible [4] y modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre los argumentos. En nuestro marco hemos abordado las relaciones de selección de argumentos y preferencias utilizando un enfoque basado en casos.11. Conclusiones y trabajo futuro En este documento hemos presentado un marco basado en la argumentación para el aprendizaje de múltiples agentes. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje se pueden usar para generar argumentos y contraargumentos. La evaluación experimental muestra que la mayor cantidad de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, y especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para los agentes de aprendizaje;b) una relación de preferencia basada en casos sobre los argumentos, basada en calcular una estimación general de la confianza de los argumentos;c) una política basada en casos para generar argumentos contra la contraerrota y seleccionar contraejemplos;y d) un enfoque basado en la argumentación para aprender de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje conservaría todos los contraejemplos presentados por el otro agente;Sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales podrían mejorar significativamente usando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y las obras futuras tienen como objetivo incorporar un aprendizaje inductivo ansioso dentro del marco argumentativo para aprender de la comunicación.12. Referencias [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: problemas fundamentales, variaciones metodológicas y enfoques del sistema. Comunicaciones de inteligencia artificial, 7 (1): 39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje relacional basado en casos. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentos dinámicos: un modelo formal de procesos de argumentación basados en el cálculo de la situación. Journal of Logic and Computation, 11 (2): 257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación defensora utilizando sistemas deductivos etiquetados. Journal of Computer Science & Technology, 1 (4): 18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando automáticamente estrategias para el razonamiento de bases múltiples. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag.[6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. El conocimiento y la reutilización de la experiencia a través de las comunicaciones entre agentes competentes (pares). International Journal of Software Engineering and Knowledge Engineering, 9 (3): 319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento colaborativo basado en casos: aplicaciones en planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAi, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificación. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Artificial Intelligence Review, 24 (2): 145-161, 2005. [10] David Poole. Sobre la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M v Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidos. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzar acuerdos a través de la argumentación: un modelo lógico e implementación. Artificial Intelligence Journal, 104: 1-69, 1998. [13] N. R. Jennings S. Parsons, C. Sierra. Agentes que razonan y negocian discutiendo. Journal of Logic and Computation, 8: 261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de los sistemas de software. ACM Crossroads, 5.1, 1998. 982 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07)