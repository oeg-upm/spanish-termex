Recuperación de información sensible al contexto utilizando retroalimentación implícita Xuehua Shen Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign Bin Tan Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign ChengXiang Zhai Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign RESUMEN Una limitación importante de la mayoría de los modelos y sistemas de recuperación existentes es que la decisión de recuperación se toma únicamente en función de la consulta y la colección de documentos; la información sobre el usuario real y el contexto de búsqueda se ignora en gran medida. En este artículo, estudiamos cómo aprovechar la información de retroalimentación implícita, incluidas las consultas anteriores y la información de clics, para mejorar la precisión de recuperación en un entorno de recuperación de información interactiva. Proponemos varios algoritmos de recuperación sensibles al contexto basados en modelos de lenguaje estadístico para combinar las consultas anteriores y los resúmenes de documentos clicados con la consulta actual para una mejor clasificación de documentos. Utilizamos los datos de TREC AP para crear una colección de pruebas con información de contexto de búsqueda, y evaluamos cuantitativamente nuestros modelos utilizando este conjunto de pruebas. Los resultados del experimento muestran que el uso de retroalimentación implícita, especialmente los resúmenes de documentos clicados, puede mejorar significativamente el rendimiento de recuperación. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Modelos de recuperación Términos generales Algoritmos 1. En la mayoría de los modelos de recuperación de información existentes, el problema de recuperación se trata como si involucrara una sola consulta y un conjunto de documentos. A partir de una sola consulta, sin embargo, el sistema de recuperación solo puede tener una pista muy limitada sobre la necesidad de información del usuario. Un sistema de recuperación óptimo debería intentar aprovechar la mayor cantidad de información de contexto adicional posible para mejorar la precisión de la recuperación, siempre que esté disponible. De hecho, la recuperación sensible al contexto ha sido identificada como un desafío importante en la investigación de la recuperación de información[2]. Hay muchos tipos de contexto que podemos aprovechar. La retroalimentación de relevancia [14] puede considerarse como una forma para que un usuario proporcione más contexto de búsqueda y se sabe que es efectiva para mejorar la precisión de recuperación. Sin embargo, la retroalimentación de relevancia requiere que un usuario proporcione explícitamente información de retroalimentación, como especificar la categoría de la necesidad de información o marcar un subconjunto de documentos recuperados como documentos relevantes. Dado que obliga al usuario a participar en actividades adicionales mientras los beneficios no siempre son evidentes para el usuario, a menudo el usuario se muestra reacio a proporcionar esa información de retroalimentación. Por lo tanto, la efectividad de la retroalimentación de relevancia puede estar limitada en aplicaciones reales. Por esta razón, el feedback implícito ha atraído mucha atención recientemente [11, 13, 18, 17, 12]. En general, los resultados de recuperación utilizando la consulta inicial del usuario pueden no ser satisfactorios; a menudo, el usuario tendría que revisar la consulta para mejorar la precisión de la recuperación/clasificación [8]. Para una necesidad de información compleja o difícil, el usuario puede necesitar modificar su consulta y ver documentos clasificados con muchas iteraciones antes de que la necesidad de información esté completamente satisfecha. En un escenario de recuperación interactiva, la información naturalmente disponible para el sistema de recuperación es más que solo la consulta actual del usuario y la colección de documentos; en general, todo el historial de interacción puede estar disponible para el sistema de recuperación, incluidas las consultas anteriores, información sobre qué documentos ha elegido ver el usuario e incluso cómo ha leído un documento el usuario (por ejemplo, en qué parte de un documento pasa mucho tiempo leyendo). Definimos la retroalimentación implícita de manera amplia como la explotación de todo el historial de interacción naturalmente disponible para mejorar los resultados de recuperación. Una ventaja importante del feedback implícito es que podemos mejorar la precisión de recuperación sin necesidad de requerir esfuerzo por parte del usuario. Por ejemplo, si la consulta actual es java, sin conocer ninguna información adicional, sería imposible saber si se refiere al lenguaje de programación Java o a la isla de Java en Indonesia. Como resultado, es probable que los documentos recuperados tengan ambos tipos de documentos: algunos pueden ser sobre el lenguaje de programación y otros sobre la isla. Sin embargo, es poco probable que algún usuario en particular esté buscando ambos tipos de documentos. Una ambigüedad como esta puede resolverse explotando la información histórica. Por ejemplo, si sabemos que la consulta anterior del usuario es programación cgi, sugeriría fuertemente que es el lenguaje de programación que el usuario está buscando. El feedback implícito fue estudiado en varios trabajos previos. En [11], Joachims exploró cómo capturar y explotar la información de clics y demostró que dicha información de retroalimentación implícita puede mejorar la precisión de la búsqueda para un grupo de personas. En [18], se realizó un estudio de simulación sobre la efectividad de diferentes algoritmos de retroalimentación implícita, y se propusieron y evaluaron varios modelos de recuperación diseñados para aprovechar la información de clics. En [17], algunos algoritmos de recuperación existentes se adaptan para mejorar los resultados de búsqueda basados en el historial de navegación de un usuario. Otros trabajos relacionados sobre el uso del contexto incluyen la búsqueda personalizada [1, 3, 4, 7, 10], análisis de registros de consultas [5], factores de contexto [12] y consultas implícitas [6]. Si bien el trabajo anterior se ha centrado principalmente en el uso de la información de clics, en este artículo utilizamos tanto la información de clics como las consultas anteriores, y nos enfocamos en desarrollar nuevos modelos de lenguaje sensibles al contexto para la recuperación. Específicamente, desarrollamos modelos para utilizar información de retroalimentación implícita, como la consulta y el historial de clics de la sesión de búsqueda actual, con el fin de mejorar la precisión de recuperación. Utilizamos el modelo de recuperación de divergencia KL [19] como base y proponemos tratar la recuperación sensible al contexto como la estimación de un modelo de lenguaje de consulta basado en la consulta actual y cualquier información de contexto de búsqueda. Proponemos varios modelos estadísticos del lenguaje para incorporar el historial de consultas y clics en el modelo de divergencia KL. Un desafío al estudiar modelos de retroalimentación implícita es que no existe ninguna colección de pruebas adecuada para la evaluación. Por lo tanto, utilizamos los datos de TREC AP para crear una colección de pruebas con información de retroalimentación implícita, la cual puede ser utilizada para evaluar cuantitativamente modelos de retroalimentación implícita. Hasta donde sabemos, este es el primer conjunto de pruebas para retroalimentación implícita. Evaluamos los modelos propuestos utilizando este conjunto de datos. Los resultados experimentales muestran que el uso de información de retroalimentación implícita, especialmente los datos de clics, puede mejorar sustancialmente el rendimiento de recuperación sin requerir esfuerzo adicional por parte del usuario. Las secciones restantes están organizadas de la siguiente manera. En la Sección 2, intentamos definir el problema de la retroalimentación implícita e introducir algunos términos que utilizaremos más adelante. En la Sección 3, proponemos varios modelos de retroalimentación implícita basados en modelos estadísticos de lenguaje. En la Sección 4, describimos cómo creamos el conjunto de datos para experimentos de retroalimentación implícita. En la Sección 5, evaluamos diferentes modelos de retroalimentación implícita en el conjunto de datos creado. La sección 6 es nuestras conclusiones y trabajo futuro. 2. DEFINICIÓN DEL PROBLEMA Hay dos tipos de información de contexto que podemos utilizar para obtener retroalimentación implícita. Uno es el contexto a corto plazo, que es la información inmediata que rodea y arroja luz sobre la necesidad de información actual de un usuario en una sola sesión. Una sesión puede ser considerada como un período que consiste en todas las interacciones para la misma necesidad de información. La categoría de la necesidad de información de un usuario (por ejemplo, niños o deportes), consultas anteriores y documentos recientemente vistos son todos ejemplos de contexto a corto plazo. Dicha información está directamente relacionada con la necesidad actual de información del usuario y, por lo tanto, se espera que sea la más útil para mejorar la búsqueda actual. En general, el contexto a corto plazo es más útil para mejorar la búsqueda en la sesión actual, pero puede que no sea tan útil para las actividades de búsqueda en una sesión diferente. El otro tipo de contexto es el contexto a largo plazo, que se refiere a información como el nivel educativo de los usuarios y sus intereses generales, el historial de consultas de los usuarios acumulado y la información de clics anteriores de los usuarios; esta información suele ser estable durante mucho tiempo y se acumula con el tiempo. El contexto a largo plazo puede ser aplicable a todas las sesiones, pero puede no ser tan efectivo como el contexto a corto plazo para mejorar la precisión de búsqueda en una sesión particular. En este artículo, nos enfocamos en el contexto a corto plazo, aunque algunos de nuestros métodos también pueden ser utilizados para incorporar de manera natural cierto contexto a largo plazo. En una sola sesión de búsqueda, un usuario puede interactuar con el sistema de búsqueda varias veces. Durante las interacciones, el usuario modificaría continuamente la consulta. Por lo tanto, para la consulta actual Qk (excepto por la primera consulta de una sesión de búsqueda), existe un historial de consultas, HQ = (Q1, ..., Qk−1) asociado con ella, que consiste en las consultas anteriores realizadas por el mismo usuario en la sesión actual. Se debe tener en cuenta que en este artículo asumimos que los límites de las sesiones son conocidos. En la práctica, necesitamos técnicas para descubrir automáticamente los límites de las sesiones, los cuales han sido estudiados en [9, 16]. Tradicionalmente, el sistema de recuperación solo utiliza la consulta actual Qk para realizar la recuperación. Pero el historial de consultas a corto plazo claramente puede proporcionar pistas útiles sobre la necesidad de información actual del usuario, como se ve en el ejemplo de Java dado en la sección anterior. De hecho, nuestro trabajo previo [15] ha demostrado que el historial de consultas a corto plazo es útil para mejorar la precisión de recuperación. Además del historial de consultas, puede haber otra información de contexto a corto plazo disponible. Por ejemplo, un usuario presumiblemente haría clic frecuentemente en algunos documentos para verlos. Nos referimos a los datos asociados con estas acciones como historial de clics. Los datos de clics pueden incluir el título, resumen y posiblemente también el contenido y la ubicación (por ejemplo, la URL) del documento al que se hizo clic. Aunque no está claro si un documento visualizado es realmente relevante para la necesidad de información del usuario, podemos asumir con seguridad que el resumen/título mostrado sobre el documento es atractivo para el usuario, por lo tanto transmite información sobre la necesidad de información del usuario. Supongamos que concatenamos toda la información de texto mostrada sobre un documento (generalmente título y resumen) juntas, también tendremos un resumen clicado Ci en cada ronda de recuperación. En general, podemos tener un historial de resúmenes clicados C1, ..., Ck−1. También aprovecharemos el historial de clics HC = (C1, ..., Ck−1) para mejorar la precisión de nuestra búsqueda para la consulta actual Qk. Trabajos anteriores también han mostrado resultados positivos utilizando información de clics similar [11, 17]. Tanto el historial de consultas como el historial de clics son información de retroalimentación implícita, que existe naturalmente en la recuperación de información interactiva, por lo que no se necesita esfuerzo adicional por parte del usuario para recopilarlos. En este artículo, estudiamos cómo explotar dicha información (HQ y HC), desarrollar modelos para incorporar el historial de consultas y el historial de clics en una función de clasificación de recuperación, y evaluar cuantitativamente estos modelos. Los modelos de lenguaje para la recuperación de información contextualmente sensible. Intuitivamente, el historial de consultas HQ y el historial de clics HC son útiles para mejorar la precisión de la búsqueda para la consulta actual Qk. Una pregunta de investigación importante es cómo podemos explotar esa información de manera efectiva. Proponemos utilizar modelos de lenguaje estadístico para modelar la necesidad de información de los usuarios y desarrollar cuatro modelos de lenguaje específicos sensibles al contexto para incorporar información de contexto en un modelo básico de recuperación. 3.1 Modelo básico de recuperación Utilizamos el método de divergencia de Kullback-Leibler (KL) [19] como nuestro método básico de recuperación. Según este modelo, la tarea de recuperación implica calcular un modelo de lenguaje de consulta θQ para una consulta dada y un modelo de lenguaje de documento θD para un documento y luego calcular su divergencia KL D(θQ||θD), que sirve como la puntuación del documento. Una ventaja de este enfoque es que podemos incorporar de forma natural el contexto de búsqueda como evidencia adicional para mejorar nuestra estimación del modelo de lenguaje de la consulta. Formalmente, sea HQ = (Q1, ..., Qk−1) el historial de consultas y la consulta actual sea Qk. Sea HC = (C1, ..., Ck−1) el historial de clics. Ten en cuenta que Ci es la concatenación de todos los resúmenes de documentos clicados en la i-ésima ronda de recuperación, ya que podemos tratar razonablemente todos estos resúmenes de manera igual. Nuestra tarea es estimar un modelo de consulta de contexto, que denotamos como p(w|θk), basado en la consulta actual Qk, así como en el historial de consultas HQ y el historial de clics HC. Ahora describimos varios modelos de lenguaje diferentes para explotar HQ y HC para estimar p(w|θk). Usaremos c(w, X) para denotar el conteo de la palabra w en el texto X, que podría ser una consulta, un resumen de documentos clicados u otro texto. Usaremos |X| para denotar la longitud del texto X o el número total de palabras en X. 3.2 Interpolación de Coeficiente Fijo (FixInt) Nuestra primera idea es resumir el historial de consultas HQ con un modelo de lenguaje unigrama p(w|HQ) y el historial de clics HC con otro modelo de lenguaje unigrama p(w|HC). Luego interpolamos linealmente estos dos modelos históricos para obtener el modelo histórico p(w|H). Finalmente, interpolamos el modelo de historia p(w|H) con el modelo de consulta actual p(w|Qk). Estos modelos se definen de la siguiente manera. p(w|Qi) = c(w, Qi) |Qi| p(w|HQ) = 1 k − 1 i=k−1 i=1 p(w|Qi) p(w|Ci) = c(w, Ci) |Ci| p(w|HC ) = 1 k − 1 i=k−1 i=1 p(w|Ci) p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ) p(w|θk) = αp(w|Qk) + (1 − α)p(w|H) donde β ∈ [0, 1] es un parámetro para controlar el peso en cada modelo de historial, y donde α ∈ [0, 1] es un parámetro para controlar el peso en la consulta actual y la información histórica. Si combinamos estas ecuaciones, vemos que p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)]. Es decir, el modelo de consulta de contexto estimado es simplemente una interpolación de coeficiente fijo de tres modelos p(w|Qk), p(w|HQ) y p(w|HC ). 3.3 Interpolación Bayesiana (BayesInt) Un posible problema con el enfoque FixInt es que los coeficientes, especialmente α, son fijos en todas las consultas. Pero intuitivamente, si nuestra consulta actual Qk es muy larga, deberíamos confiar más en la consulta actual, mientras que si Qk tiene solo una palabra, puede ser beneficioso poner más peso en el historial. Para capturar esta intuición, tratamos p(w|HQ) y p(w|HC) como priors de Dirichlet y Qk como los datos observados para estimar un modelo de consulta de contexto utilizando un estimador bayesiano. El modelo estimado está dado por p(w|θk) = c(w, Qk) + µp(w|HQ) + νp(w|HC ) |Qk| + µ + ν = |Qk| |Qk| + µ + ν p(w|Qk)+ µ + ν |Qk| + µ + ν [ µ µ + ν p(w|HQ)+ ν µ + ν p(w|HC )] donde µ es el tamaño de la muestra previa para p(w|HQ) y ν es el tamaño de la muestra previa para p(w|HC ). Vemos que la única diferencia entre BayesInt y FixInt es que los coeficientes de interpolación ahora son adaptables a la longitud de la consulta. De hecho, al ver BayesInt como FixInt, vemos que α = |Qk| |Qk|+µ+ν , β = ν ν+µ , por lo tanto, con µ y ν fijos, tendremos un α dependiente de la consulta. Más adelante demostraremos que un α adaptativo empíricamente funciona mejor que un α fijo. 3.4 Actualización Bayesiana en Línea (OnlineUp) Tanto FixInt como BayesInt resumen la información histórica promediando los modelos de lenguaje de unigrama estimados en base a consultas anteriores o resúmenes clicados. Esto significa que todas las consultas anteriores se tratan por igual, al igual que todos los resúmenes clicados. Sin embargo, a medida que el usuario interactúa con el sistema y adquiere más conocimiento sobre la información en la colección, presumiblemente, las consultas reformuladas mejorarán cada vez más. Por lo tanto, asignar pesos decrecientes a las consultas anteriores para confiar más en una consulta reciente que en una consulta anterior parece ser razonable. Interesantemente, si actualizamos incrementalmente nuestra creencia sobre la necesidad de información de los usuarios después de ver cada consulta, podríamos obtener naturalmente pesos decrecientes en las consultas anteriores. Dado que una estrategia de actualización en línea incremental puede ser utilizada para aprovechar cualquier evidencia en un sistema de recuperación interactivo, la presentamos de una manera más general. En un sistema de recuperación típico, el sistema de recuperación responde a cada nueva consulta ingresada por el usuario presentando una lista clasificada de documentos. Para clasificar documentos, el sistema debe contar con un modelo de la necesidad de información de los usuarios. En el modelo de recuperación de divergencia de KL, esto significa que el sistema debe calcular un modelo de consulta cada vez que un usuario ingresa una consulta (nueva). Una forma fundamentada de actualizar el modelo de consulta es utilizar la estimación bayesiana, la cual discutimos a continuación. 3.4.1 Actualización bayesiana Primero discutimos cómo aplicamos la estimación bayesiana para actualizar un modelo de consulta en general. Sea p(w|φ) nuestro modelo de consulta actual y T sea una nueva pieza de evidencia de texto observada (por ejemplo, T puede ser una consulta o un resumen clicado). Para actualizar el modelo de consulta basado en T, usamos φ para definir un parámetro previo de Dirichlet parametrizado como Dir(µT p(w1|φ), ..., µT p(wN |φ)) donde µT es el tamaño de muestra equivalente del prior. Utilizamos la prior de Dirichlet porque es una prior conjugada para distribuciones multinomiales. Con un prior conjugado como este, la distribución predictiva de φ (o equivalentemente, la media de la distribución posterior de φ) está dada por p(w|φ) = c(w, T) + µT p(w|φ) |T| + µT (1) donde c(w, T) es la cantidad de veces que w aparece en T y |T| es la longitud de T. El parámetro µT indica nuestra confianza en el prior expresada en términos de una muestra de texto equivalente a T. Por ejemplo, µT = 1 indica que la influencia del prior es equivalente a agregar una palabra extra a T. Actualización del modelo de consulta secuencial Ahora discutimos cómo podemos actualizar nuestro modelo de consulta con el tiempo durante un proceso interactivo de recuperación utilizando estimación bayesiana. En general, asumimos que el sistema de recuperación mantiene un modelo de consulta actual φi en todo momento. Tan pronto como obtengamos alguna evidencia de retroalimentación implícita en forma de un fragmento de texto Ti, actualizaremos el modelo de consulta. Inicialmente, antes de ver cualquier consulta de usuario, es posible que ya tengamos cierta información sobre el usuario. Por ejemplo, podemos tener información sobre qué documentos ha visto el usuario en el pasado. Utilizamos dicha información para definir una distribución a priori en el modelo de consulta, que se denota como φ0. Después de observar la primera consulta Q1, podemos actualizar el modelo de consulta basado en los nuevos datos observados de Q1. El modelo de consulta actualizado φ1 puede ser utilizado para clasificar documentos en respuesta a Q1. A medida que el usuario visualiza algunos documentos, el texto resumido mostrado para dichos documentos C1 (es decir, resúmenes seleccionados) puede servir como nuevos datos para que actualicemos aún más el modelo de consulta y obtener φ1. Al obtener la segunda consulta Q2 del usuario, podemos actualizar φ1 para obtener un nuevo modelo φ2. En general, podemos repetir este proceso de actualización para actualizar de forma iterativa el modelo de consulta. Claramente, vemos dos tipos de actualización: (1) actualización basada en una nueva consulta Qi; (2) actualización basada en un nuevo resumen clicado Ci. En ambos casos, podemos tratar el modelo actual como una prioridad del modelo de consulta de contexto y tratar la nueva consulta observada o el resumen clicado como datos observados. Así tenemos las siguientes ecuaciones de actualización: p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|φi) = c(w, Ci) + νip(w|φi) |Ci| + νi donde µi es el tamaño de muestra equivalente para la prior al actualizar el modelo basado en una consulta, mientras que νi es el tamaño de muestra equivalente para la prior al actualizar el modelo basado en un resumen clicado. Si establecemos µi = 0 (o νi = 0) esencialmente ignoramos el modelo previo, por lo tanto comenzaríamos un modelo de consulta completamente nuevo basado en la consulta Qi (o el resumen clicado Ci). Por otro lado, si establecemos µi = +∞ (o νi = +∞) esencialmente ignoramos la consulta observada (o el resumen clicado) y no actualizamos nuestro modelo. Por lo tanto, el modelo permanece igual como si no observáramos ninguna nueva evidencia textual. En general, los parámetros µi y νi pueden tener valores diferentes para diferentes i. Por ejemplo, al principio, es posible que tengamos un historial de consultas muy escaso, por lo tanto podríamos usar un µi más pequeño, pero más tarde, a medida que el historial de consultas sea más amplio, podríamos considerar usar un µi más grande. Pero en nuestros experimentos, a menos que se indique lo contrario, los establecemos en las mismas constantes, es decir, ∀i, j, µi = µj, νi = νj. Ten en cuenta que podemos tomar tanto p(w|φi) como p(w|φi) como nuestro modelo de consulta de contexto para clasificar documentos. Esto sugiere que no tenemos que esperar a que un usuario ingrese una nueva consulta para iniciar una nueva ronda de recuperación; en su lugar, tan pronto como recolectemos el resumen clickeado Ci, podemos actualizar el modelo de consulta y usar p(w|φi) para volver a clasificar inmediatamente cualquier documento que un usuario aún no haya visto. Para puntuar documentos después de ver la consulta Qk, usamos p(w|φk), es decir, p(w|θk) = p(w|φk) 3.5 Actualización bayesiana por lotes (BatchUp). Si fijamos los parámetros de tamaño de muestra equivalente a una constante fija, el algoritmo OnlineUp introduciría un factor de decaimiento: la interpolación repetida haría que los datos iniciales tuvieran un peso bajo. Esto puede ser apropiado para el historial de consultas, ya que es razonable creer que el usuario mejora cada vez más en la formulación de consultas a medida que pasa el tiempo, pero no es necesariamente apropiado para la información de clics, especialmente porque utilizamos el resumen mostrado en lugar del contenido real de un documento al que se hizo clic. Una forma de evitar aplicar una interpolación en descomposición a los datos de clics es hacer OnlineUp solo para el historial de consultas Q = (Q1, ..., Qi−1), pero no para los datos de clics C. Primero almacenamos en búfer todos los datos de clics juntos y utilizamos el conjunto completo de datos de clics para actualizar el modelo generado mediante la ejecución de OnlineUp en consultas anteriores. Las ecuaciones de actualización son las siguientes. p(w|φi) = c(w, Qi) + µip(w|φi−1) |Qi| + µi p(w|ψi) = i−1 j=1 c(w, Cj) + νip(w|φi) i−1 j=1 |Cj| + νi donde µi tiene la misma interpretación que en OnlineUp, pero νi ahora indica en qué medida queremos confiar en los resúmenes clicados. En OnlineUp, establecemos todos los µis y los νis en el mismo valor. Y para clasificar los documentos después de ver la consulta actual Qk, usamos p(w|θk) = p(w|ψk) 4. RECOLECCIÓN DE DATOS Para evaluar cuantitativamente nuestros modelos, necesitamos un conjunto de datos que incluya no solo una base de datos de texto y temas de prueba, sino también el historial de consultas y el historial de clics para cada tema. Dado que no contamos con un conjunto de datos disponible, debemos crear uno. Hay dos opciones. Uno debe extraer temas y cualquier historial de consultas asociado, así como el historial de clics para cada tema del registro de un sistema de recuperación (por ejemplo, un motor de búsqueda). Pero el problema es que no tenemos juicios de relevancia sobre esos datos. La otra opción es utilizar un conjunto de datos TREC, que cuenta con una base de datos de texto, una descripción del tema y un archivo de juicio de relevancia. Lamentablemente, no hay datos de historial de consultas e historial de clics. Decidimos ampliar un conjunto de datos de TREC recopilando datos de historial de consultas e historial de clics. Seleccionamos los datos de TREC AP88, AP89 y AP90 como nuestra base de texto, porque los datos de AP han sido utilizados en varias tareas de TREC y cuentan con juicios relativamente completos. Hay en total 242918 artículos de noticias y la longitud promedio de los documentos es de 416 palabras. La mayoría de los artículos tienen títulos. Si no, seleccionamos la primera oración del texto como título. Para el preprocesamiento, solo realizamos la conversión a minúsculas y no eliminamos stopwords ni realizamos stemming. Seleccionamos 30 temas relativamente difíciles de los temas TREC 1-150. Estos 30 temas tienen el peor rendimiento promedio de precisión entre los temas 1-150 de TREC según algunos experimentos de referencia utilizando el modelo de Divergencia de KL con suavizado de priorización bayesiana [20]. La razón por la que seleccionamos temas difíciles es que el usuario tendría que tener varias interacciones con el sistema de recuperación para obtener resultados satisfactorios, de modo que podemos esperar recopilar un historial de consultas y datos de historial de clics relativamente más ricos del usuario. En aplicaciones reales, también podemos esperar que nuestros modelos sean más útiles para temas difíciles, por lo que nuestra estrategia de recolección de datos refleja bien las aplicaciones del mundo real. Indexamos el conjunto de datos TREC AP y configuramos un motor de búsqueda e interfaz web para los artículos de noticias de TREC AP. Utilizamos 3 sujetos para realizar experimentos y recopilar datos de historial de consultas e historial de clics. A cada sujeto se le asignan 10 temas y se le proporcionan las descripciones de los temas proporcionadas por TREC. Para cada tema, la primera consulta es el título del tema dado en la descripción original del tema de TREC. Después de que el sujeto envíe la consulta, el motor de búsqueda realizará la recuperación y devolverá una lista clasificada de resultados de búsqueda al sujeto. El sujeto revisará los resultados y tal vez haga clic en uno o más resultados para leer el texto completo del artículo o artículos. El sujeto también puede modificar la consulta para realizar otra búsqueda. Para cada tema, el sujeto compone al menos 4 consultas. En nuestro experimento, solo se utilizan las primeras 4 consultas para cada tema. El usuario debe seleccionar el número del tema de un menú de selección antes de enviar la consulta al motor de búsqueda para que podamos detectar fácilmente el límite de la sesión, que no es el foco de nuestro estudio. Utilizamos una base de datos relacional para almacenar las interacciones de los usuarios, incluyendo las consultas enviadas y los documentos clicados. Para cada consulta, almacenamos los términos de la consulta y las páginas de resultados asociadas. Y para cada documento clicado, almacenamos el resumen tal como se muestra en la página de resultados de búsqueda. El resumen del artículo es dependiente de la consulta y se calcula en línea utilizando la recuperación de pasajes de longitud fija (modelo de divergencia KL con suavizado de prior bayesiano). De entre 120 consultas (4 para cada uno de los 30 temas) que estudiamos en el experimento, la longitud promedio de las consultas es de 3.71 palabras. En total hay 91 documentos clicados para ver. En promedio, hay alrededor de 3 clics por tema. La longitud promedio del resumen clicado FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ + HC 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 Mejora. 3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ + HC 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 Mejora 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ + HC 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 Mejora 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% Tabla 1: Efecto de utilizar historial de consultas y datos de clics para la clasificación de documentos. es de 34.4 palabras. De los 91 documentos seleccionados, 29 documentos son considerados relevantes según el archivo de juicio de TREC. Este conjunto de datos está disponible públicamente. EXPERIMENTOS 5.1 Diseño del experimento Nuestra hipótesis principal es que el uso del contexto de búsqueda (es decir, el historial de consultas y la información de clics) puede ayudar a mejorar la precisión de la búsqueda. En particular, el contexto de búsqueda puede proporcionar información adicional para ayudarnos a estimar un modelo de consulta mejor que simplemente utilizando la consulta actual. Por lo tanto, la mayoría de nuestros experimentos implican comparar el rendimiento de recuperación utilizando solo la consulta actual (ignorando así cualquier contexto) con el rendimiento utilizando tanto la consulta actual como el contexto de búsqueda. Dado que recopilamos cuatro versiones de consultas para cada tema, realizamos tales comparaciones para cada versión de consultas. Utilizamos dos medidas de rendimiento: (1) Precisión Promedio Media (MAP): Esta es la precisión promedio estándar no interpolada y sirve como una buena medida de la precisión general de clasificación. (2) Precisión en 20 documentos (pr@20docs): Esta medida no se promedia bien, pero es más significativa que MAP y refleja la utilidad para los usuarios que solo leen los primeros 20 documentos. En todos los casos, la cifra reportada es el promedio de los 30 temas. Evaluamos los cuatro modelos para explotar el contexto de búsqueda (es decir, FixInt, BayesInt, OnlineUp y BatchUp). Cada modelo tiene precisamente dos parámetros (α y β para FixInt; µ y ν para los demás). Ten en cuenta que µ y ν pueden necesitar ser interpretados de manera diferente para diferentes métodos. Variamos estos parámetros e identificamos el rendimiento óptimo para cada método. También variamos los parámetros para estudiar la sensibilidad de nuestros algoritmos a la configuración de los parámetros. 5.2 Análisis de resultados 5.2.1 Efecto general del contexto de búsqueda Comparamos el rendimiento óptimo de cuatro modelos con aquellos que utilizan solo la consulta actual en la Tabla 1. Una fila etiquetada con qi es el rendimiento base y una fila etiquetada con qi + HQ + HC es el rendimiento al utilizar el contexto de búsqueda. Podemos hacer varias observaciones a partir de esta tabla: 1. Comparar las actuaciones de referencia indica que, en promedio, las consultas reformuladas son mejores que las consultas anteriores, siendo la actuación de q4 la mejor. Los usuarios generalmente formulan consultas cada vez mejores. El uso del contexto de búsqueda generalmente tiene un efecto positivo, especialmente cuando el contexto es rico. Esto se puede observar en el hecho de que la mejora del 1 http://sifaka.cs.uiuc.edu/ir/ucair/QCHistory.zip para q4 y q3 es generalmente más sustancial en comparación con q2. De hecho, en muchos casos con q2, el uso del contexto puede perjudicar el rendimiento, probablemente porque el historial en ese punto es escaso. Cuando el contexto de búsqueda es amplio, la mejora en el rendimiento puede ser bastante sustancial. Por ejemplo, BatchUp logra una mejora del 92.4% en la precisión media promedio sobre q3 y del 77.2% sobre q4. (Las precisiones generalmente bajas también hacen que la mejora relativa sea engañosamente alta, sin embargo). 3. Entre los cuatro modelos que utilizan contexto de búsqueda, las actuaciones de FixInt y OnlineUp son claramente peores que las de BayesInt y BatchUp. Dado que BayesInt funciona mejor que FixInt y la principal diferencia entre BayesInt y FixInt es que el primero utiliza un coeficiente adaptativo para la interpolación, los resultados sugieren que el uso de un coeficiente adaptativo es bastante beneficioso y que una interpolación de estilo bayesiano tiene sentido. La principal diferencia entre OnlineUp y BatchUp es que OnlineUp utiliza coeficientes decaídos para combinar los múltiples resúmenes clicados, mientras que BatchUp simplemente concatena todos los resúmenes clicados. Por lo tanto, el hecho de que BatchUp sea consistentemente mejor que OnlineUp indica que los pesos para combinar los resúmenes clicados no deberían estar decayendo. Si bien OnlineUp es teóricamente atractivo, su rendimiento es inferior al de BayesInt y BatchUp, probablemente debido al coeficiente de decaimiento. En general, BatchUp parece ser el mejor método cuando variamos la configuración de los parámetros. Tenemos dos tipos diferentes de contexto de búsqueda: historial de consultas y datos de clics. Ahora analizamos la contribución de cada tipo de contexto. 5.2.2 Utilizando solo el historial de consultas En cada uno de los cuatro modelos, podemos desactivar los datos de historial de clics configurando los parámetros adecuadamente. Esto nos permite evaluar el efecto de utilizar solo el historial de consultas. Utilizamos la misma configuración de parámetros para el historial de consultas que en la Tabla 1. Los resultados se muestran en la Tabla 2. Aquí vemos que en general, el beneficio de utilizar el historial de consultas es muy limitado con resultados mixtos. Esto difiere de lo reportado en un estudio previo [15], donde el uso del historial de consultas resultó ser consistentemente útil. Otra observación es que los contextos de ejecución funcionan mal en q2, pero generalmente funcionan (ligeramente) mejor que los puntos de referencia para q3 y q4. Esto se debe nuevamente probablemente a que al principio la consulta inicial, que es el título en la descripción original del tema de TREC, puede no ser una buena consulta; de hecho, en promedio, el rendimiento de estas consultas de primera generación es claramente inferior al de todas las demás consultas formuladas por los usuarios en las generaciones posteriores. Otra observación es que al utilizar solo el historial de consultas, el modelo BayesInt parece ser mejor que otros modelos. Dado que se ignoran los datos de clics, OnlineUp y BatchUp FixInt BayesInt OnlineUp BatchUp Query (α = 0.1, β = 0) (µ = 0.2,ν = 0) (µ = 5.0,ν = +∞) (µ = 2.0, ν = +∞) MAP pr@20docs MAP pr@20docs MAP pr@20docs MAP pr@20docs q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + HQ 0.0097 0.0317 0.0311 0.1200 0.0213 0.0783 0.0287 0.0967 Mejora -68.9% -72.4% -0.3% 4.3% -31.7% -31.9% -8.0% -15.9% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + HQ 0.0261 0.0917 0.0451 0.1517 0.0444 0.1333 0.0455 0.1450 Mejora -38.2% -38.2% 7.1% 2.3% 5.5% -10.1% 8.1% -2.2% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + HQ 0.0428 0.1467 0.0537 0.1917 0.0550 0.1733 0.0552 0.1917 Mejora -20.1% -24.1% 0.2% -0.8% 3.0% -10.3% 3.0% -0.8% Tabla 2: Efecto de utilizar solo el historial de consultas para la clasificación de documentos. µ 0 0.5 1 2 3 4 5 6 7 8 9 q2 + HQ MAP 0.0312 0.0313 0.0308 0.0287 0.0257 0.0231 0.0213 0.0194 0.0183 0.0182 0.0164 q3 + HQ MAP 0.0421 0.0442 0.0441 0.0455 0.0457 0.0458 0.0444 0.0439 0.0430 0.0390 0.0335 q4 + HQ MAP 0.0536 0.0546 0.0547 0.0552 0.0544 0.0548 0.0550 0.0541 0.0534 0.0525 0.0513 Tabla 3: La precisión promedio de BatchUp utilizando solo el historial de consultas es esencialmente el mismo algoritmo. Los resultados mostrados reflejan la variación causada por el parámetro µ. Un ajuste más bajo de 2.0 se ve mejor que un valor más alto de 5.0. Una imagen más completa de la influencia del ajuste de µ se puede ver en la Tabla 3, donde mostramos las cifras de rendimiento para un rango más amplio de valores de µ. El valor de µ se puede interpretar como cuántas palabras consideramos que vale la historia de la consulta. Un valor más grande pone más peso en la historia y se considera que afecta más el rendimiento cuando la información histórica no es abundante. Por lo tanto, aunque para q4 el mejor rendimiento tiende a lograrse para µ ∈ [2, 5], solo cuando µ = 0.5 vemos algún pequeño beneficio para q2. Como esperaríamos, un µ excesivamente grande perjudicaría el rendimiento en general, pero q2 es el más perjudicado y q4 apenas se ve afectado, lo que indica que a medida que acumulamos más información del historial de consultas, podemos poner más peso en esa información histórica. Esto también sugiere que una estrategia mejor probablemente debería ajustar dinámicamente los parámetros según la cantidad de información histórica que tengamos. Los resultados mixtos del historial de consultas sugieren que el efecto positivo de utilizar información de retroalimentación implícita puede haber provenido en gran medida del uso del historial de clics, lo cual es cierto tal como discutimos en la siguiente subsección. 5.2.3 Uso solo del historial de clics. Ahora desactivamos el historial de consultas y solo utilizamos los resúmenes clicados junto con la consulta actual. Los resultados se muestran en la Tabla 4. Vemos que el beneficio de utilizar la información de clics es mucho más significativo que el de utilizar el historial de consultas. Observamos un efecto generalmente positivo, a menudo con una mejora significativa sobre el punto de partida. También es claro que cuanto más ricos sean los datos del contexto, mayor será la mejora que se puede lograr al utilizar resúmenes seleccionados. Aparte de alguna degradación ocasional de la precisión en 20 documentos, la mejora es bastante consistente y a menudo bastante sustancial. Estos resultados muestran que el texto resumido seleccionado es generalmente bastante útil para inferir la necesidad de información de un usuario. Intuitivamente, tiene más sentido utilizar el resumen del texto en lugar del contenido real del documento, ya que es bastante posible que el documento detrás de un resumen aparentemente relevante en realidad no lo sea. 29 de los 91 documentos clicados son relevantes. Actualizar el modelo de consulta basado en dichos resúmenes elevaría la clasificación de estos documentos relevantes, lo que provocaría una mejora en el rendimiento. Sin embargo, tal mejora realmente no es beneficiosa para el usuario, ya que el usuario ya ha visto estos documentos relevantes. Para ver cuánta mejora hemos logrado en la mejora de los rangos de los documentos relevantes no vistos, excluimos estos 29 documentos relevantes de nuestro archivo de juicio y recalculamos el rendimiento de BayesInt y la línea base utilizando el nuevo archivo de juicio. Los resultados se muestran en la Tabla 5. Ten en cuenta que el rendimiento del método base es menor debido a la eliminación de los 29 documentos relevantes, los cuales habrían sido generalmente clasificados en los primeros puestos de los resultados. Desde la Tabla 5, vemos claramente que el uso de resúmenes clicados también ayuda a mejorar significativamente las clasificaciones de documentos relevantes no vistos. La pregunta restante es si los datos de clics siguen siendo útiles si ninguno de los documentos clicados es relevante. Para responder a esta pregunta, extraímos los 29 resúmenes relevantes de nuestros datos de historial de clics HC para obtener un conjunto más pequeño de resúmenes clicados HC, y reevaluamos el rendimiento del método BayesInt utilizando HC con la misma configuración de parámetros que en la Tabla 4. Los resultados se muestran en la Tabla 6. Vemos que aunque la mejora no es tan sustancial como en la Tabla 4, la precisión promedio mejora en todas las generaciones de consultas. Estos resultados deben interpretarse como muy alentadores, ya que se basan solo en 62 clics no relevantes. En realidad, es más probable que un usuario haga clic en algunos resúmenes relevantes, lo que ayudaría a mostrar más documentos relevantes, como hemos visto en la Tabla 4 y la Tabla 5. Tabla 4: Efecto de utilizar solo datos de clics para la clasificación de documentos. El beneficio de la información del historial de consultas y la información de clics es principalmente aditivo, es decir, combinarlos puede lograr un mejor rendimiento que usar cada uno por separado, pero la mayor parte de la mejora claramente proviene de la información de clics. En la Tabla 7, mostramos este efecto para el método BatchUp. Sensibilidad de parámetros. Los cuatro modelos tienen dos parámetros para controlar los pesos relativos de HQ, HC y Qk, aunque la parametrización es diferente de un modelo a otro. En esta subsección, estudiamos la sensibilidad de los parámetros para BatchUp, que parece tener un rendimiento relativamente mejor que otros. BatchUp tiene dos parámetros µ y ν. Primero miramos a µ. Cuando µ se establece en 0, el historial de consultas no se utiliza en absoluto, y esencialmente solo usamos los datos de clics combinados con la consulta actual. Si aumentamos µ, incorporaremos gradualmente más información de las consultas anteriores. En la Tabla 8, mostramos cómo la precisión promedio de BatchUp cambia a medida que variamos µ con ν fijo en 15.0, donde se logra el mejor rendimiento de BatchUp. Observamos que el rendimiento es principalmente insensible al cambio de µ para q3 y q4, pero disminuye a medida que µ aumenta para q2. El patrón también es similar cuando establecemos ν en otros valores. Además del hecho de que q1 suele ser peor que q2, q3 y q4, otra posible razón por la que la sensibilidad es menor para q3 y q4 puede ser que generalmente tengamos más datos de clics disponibles para q3 y q4 que para q2, y la influencia dominante de los datos de clics ha hecho que las pequeñas diferencias causadas por µ sean menos visibles para q3 y q4. El mejor rendimiento generalmente se logra cuando µ está alrededor de 2.0, lo que significa que la información de consultas anteriores es tan útil como aproximadamente 2 palabras en la consulta actual. Excepto por q2, claramente hay un cierto equilibrio entre la consulta actual y las consultas anteriores. Consulta MAP pr@20docs q2 0.0312 0.1150 q2 + HQ 0.0287 0.0967 Mejora -8.0% -15.9% q2 + HC 0.0344 0.1167 Mejora 10.3% 1.5% q2 + HQ + HC 0.0342 0.1100 Mejora 9.6% -4.3% q3 0.0421 0.1483 q3 + HQ 0.0455 0.1450 Mejora 8.1% -2.2% q3 + HC 0.0513 0.1650 Mejora 21.9% 11.3% q3 + HQ + HC 0.0810 0.2067 Mejora 92.4% 39.4% q4 0.0536 0.1930 q4 + HQ 0.0552 0.1917 Mejora 3.0% -0.8% q4 + HC 0.0623 0.2050 Mejora 16.2% 6.1% q4 + HQ + HC 0.0950 0.2250 Mejora 77.2% 16.4% Tabla 7: El beneficio aditivo de la información de contexto y el uso de una combinación equilibrada de ellas logra un mejor rendimiento que usar cada una por separado. Ahora pasamos al otro parámetro ν. Cuando ν se establece en 0, solo usamos los datos de clics; cuando ν se establece en +∞, solo usamos el historial de consultas y la consulta actual. Con µ establecido en 2.0, donde se logra el mejor rendimiento de BatchUp, variamos ν y mostramos los resultados en la Tabla 9. Vemos que el rendimiento tampoco es muy sensible cuando ν ≤ 30, con frecuencia el mejor rendimiento se logra en ν = 15. Esto significa que la información combinada del historial de consultas y la consulta actual es tan útil como aproximadamente 15 palabras en los datos de clics, lo que indica que la información de clics es muy valiosa. En general, estos resultados de sensibilidad muestran que BatchUp no solo tiene un mejor rendimiento que otros métodos, sino que también es bastante robusto. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este artículo, hemos explorado cómo aprovechar la información de retroalimentación implícita, incluida la historia de consultas y la historia de clics dentro de la misma sesión de búsqueda, para mejorar el rendimiento de la recuperación de información. Usando el modelo de recuperación de divergencia KL como base, propusimos y estudiamos cuatro modelos estadísticos de lenguaje para la recuperación de información sensible al contexto, es decir, FixInt, BayesInt, OnlineUp y BatchUp. Utilizamos los datos de TREC AP para crear un conjunto de pruebas µ 0 1 2 3 4 5 6 7 8 9 10 MAP 0.0386 0.0366 0.0342 0.0315 0.0290 0.0267 0.0250 0.0236 0.0229 0.0223 0.0219 q2 + HQ + HC pr@20 0.1333 0.1233 0.1100 0.1033 0.1017 0.0933 0.0833 0.0767 0.0783 0.0767 0.0750 MAP 0.0805 0.0807 0.0811 0.0814 0.0813 0.0808 0.0804 0.0799 0.0795 0.0790 0.0788 q3 + HQ + HC pr@20 0.210 0.2150 0.2067 0.205 0.2067 0.205 0.2067 0.2067 0.2050 0.2017 0.2000 MAP 0.0929 0.0947 0.0950 0.0940 0.0941 0.0940 0.0942 0.0937 0.0936 0.0932 0.0929 q4 + HQ + HC pr@20 0.2183 0.2217 0.2250 0.2217 0.2233 0.2267 0.2283 0.2333 0.2333 0.2350 0.2333 Tabla 8: Sensibilidad de µ en BatchUp ν 0 1 2 5 10 15 30 100 300 500 MAP 0.0278 0.0287 0.0296 0.0315 0.0334 0.0342 0.0328 0.0311 0.0296 0.0290 q2 + HQ + HC pr@20 0.0933 0.0950 0.0950 0.1000 0.1050 0.1100 0.1150 0.0983 0.0967 0.0967 MAP 0.0728 0.0739 0.0751 0.0786 0.0809 0.0811 0.0770 0.0634 0.0511 0.0491 q3 + HQ + HC pr@20 0.1917 0.1933 0.1950 0.2100 0.2000 0.2067 0.2017 0.1783 0.1600 0.1550 MAP 0.0895 0.0903 0.0914 0.0932 0.0944 0.0950 0.0919 0.0761 0.0664 0.0625 q4 + HQ + HC pr@20 0.2267 0.2233 0.2283 0.2317 0.2233 0.2250 0.2283 0.2200 0.2067 0.2033 Tabla 9: Sensibilidad de ν en BatchUp para evaluar modelos de retroalimentación implícita. Los resultados del experimento muestran que el uso de retroalimentación implícita, especialmente el historial de clics, puede mejorar sustancialmente el rendimiento de recuperación sin requerir ningún esfuerzo adicional por parte del usuario. El trabajo actual se puede ampliar de varias maneras: en primer lugar, solo hemos explorado algunos modelos de lenguaje muy simples para incorporar información de retroalimentación implícita. Sería interesante desarrollar modelos más sofisticados para aprovechar mejor el historial de consultas y el historial de clics. Por ejemplo, podemos tratar de manera diferente un resumen seleccionado dependiendo de si la consulta actual es una generalización o refinamiento de la consulta anterior. Segundo, los modelos propuestos pueden ser implementados en cualquier sistema práctico. Actualmente estamos desarrollando un agente de búsqueda personalizado del lado del cliente, que incorporará algunos de los algoritmos propuestos. También realizaremos un estudio de usuario para evaluar la efectividad de estos modelos en la búsqueda web real. Finalmente, deberíamos estudiar más a fondo un marco general de recuperación para la toma de decisiones secuenciales en la recuperación de información interactiva y estudiar cómo optimizar algunos de los parámetros en los modelos de recuperación sensibles al contexto. 7. AGRADECIMIENTOS Este material se basa en parte en trabajos apoyados por la Fundación Nacional de Ciencias bajo los números de premio IIS-0347933 e IIS-0428472. Agradecemos a los revisores anónimos por sus comentarios útiles. 8. REFERENCIAS [1] E. Adar y D. Karger. Haystack: Entornos de información por usuario. En Actas de CIKM 1999, 1999. [2] J. Allan y et al. Desafíos en la recuperación de información y modelado del lenguaje. Taller en la Universidad de Amherst, 2002. [3] K. Bharat. Searchpad: Captura explícita del contexto de búsqueda para apoyar la búsqueda web. En Actas de WWW 2000, 2000. [4] W. B. Croft, S. Cronen-Townsend y V. Larvrenko. Retroalimentación de relevancia y personalización: Una perspectiva de modelado del lenguaje. En Actas del Segundo Taller DELOS: Personalización y Sistemas de Recomendación en Bibliotecas Digitales, 2001. [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. -> Nie, y W.-Y. This is not a complete sentence. Please provide the full sentence you would like me to translate to Spanish. Expansión de consulta probabilística utilizando registros de consulta. En Actas de WWW 2002, 2002. [6] S. T. Dumais, E. Cutrell, R. Sarin y E. Horvitz. Consultas implícitas (IQ) para búsqueda contextualizada (descripción de demostración). En Actas de SIGIR 2004, página 594, 2004. [7] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman y E. Ruppin. Colocando la búsqueda en contexto: El concepto revisado. En Actas de WWW 2002, 2001. [8] C. Huang, L. Chien y Y. Oyang. Sugerencia de términos basada en la sesión de búsqueda interactiva en la web. En Actas de WWW 2001, 2001. [9] X. Huang, F. Peng, A. An, y D. Schuurmans. Identificación dinámica de sesiones de registro web con modelos de lenguaje estadístico. Revista de la Sociedad Americana de Ciencia de la Información y Tecnología, 55(14):1290-1303, 2004. [10] G. Jeh y J. Widom. Escalando la búsqueda web personalizada. En Actas de WWW 2003, 2003. [11] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En Actas de SIGKDD 2002, 2002. [12] D. Kelly y N. J. Belkin. Mostrar el tiempo como retroalimentación implícita: Comprendiendo los efectos de la tarea. En Actas de SIGIR 2004, 2004. [13] D. Kelly y J. Teevan. Retroalimentación implícita para inferir la preferencia del usuario. Foro SIGIR, 32(2), 2003. [14] J. Rocchio. Recuperación de información con retroalimentación de relevancia. En el Sistema de Recuperación Inteligente-Experimentos en Procesamiento Automático de Documentos, páginas 313-323, Kansas City, MO, 1971. Prentice-Hall. [15] X. Shen y C. Zhai. Explotando el historial de consultas para la clasificación de documentos en la recuperación de información interactiva (póster). En Actas de SIGIR 2003, 2003. [16] S. Sriram, X. Shen y C. Zhai. Un motor de búsqueda basado en sesiones (póster). En Actas de SIGIR 2004, 2004. [17] K. Sugiyama, K. Hatano y M. Yoshikawa. Búsqueda web adaptativa basada en el perfil del usuario construido sin ningún esfuerzo por parte de los usuarios. En Actas de WWW 2004, 2004. [18] R. W. White, J. M. Jose, C. J. van Rijsbergen e I. Ruthven. Un estudio simulado de modelos de retroalimentación implícita. En Actas de ECIR 2004, páginas 311-326, 2004. [19] C. Zhai y J. Lafferty. Retroalimentación basada en el modelo en el modelo de recuperación de divergencia de Kullback-Leibler. En Actas de CIKM 2001, 2001. [20] C. Zhai y J. Lafferty. Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. En Actas de SIGIR 2001, 2001.