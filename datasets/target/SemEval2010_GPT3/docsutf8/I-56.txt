Este artículo presenta un marco de negociación BDI para unificar algoritmos de restricción distribuida basados en la satisfacción de restricciones, desarrollado por Bao Chau Le Dinh y Kiam Tian Seow de la Escuela de Ingeniería Informática de la Universidad Tecnológica de Nanyang en Singapur. El Problema de Satisfacción de Restricciones Distribuidas (DCSP) es aquel que implica a varios agentes en la búsqueda de un acuerdo, que es una combinación consistente de acciones que satisface sus restricciones mutuas en un entorno compartido. Al centrar la búsqueda de DCSP en la negociación automatizada, demostramos que varios algoritmos de DCSP conocidos son en realidad mecanismos que pueden llegar a acuerdos a través de un protocolo común de Creencia-Deseo-Intención (BDI), pero utilizando estrategias diferentes. Una de las principales motivaciones de este marco BDI es que no solo proporciona una comprensión conceptual más clara de los algoritmos existentes de DCSP desde una perspectiva de modelo de agente, sino que también abre oportunidades para extender y desarrollar nuevas estrategias para DCSP. Con este fin, se propone una nueva estrategia llamada Consejo Mutuo No Solicitado (UMA). La evaluación de rendimiento muestra que la estrategia UMA puede superar a algunos mecanismos existentes en términos de ciclos computacionales. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes, Sistemas Multiagente Términos Generales Algoritmos, Diseño, Experimentación 1. INTRODUCCIÓN En el núcleo de muchas aplicaciones distribuidas emergentes se encuentra el problema de satisfacción de restricciones distribuidas (DCSP) - uno que implica encontrar una combinación consistente de acciones (abstraídas como valores de dominio) para satisfacer las restricciones entre múltiples agentes en un entorno compartido. Ejemplos importantes de aplicación incluyen la asignación de recursos distribuidos [1] y la programación distribuida [2]. Se han desarrollado muchos algoritmos importantes, como el breakout distribuido (DBO) [3], el retroceso asincrónico (ABT) [4], la superposición parcial asincrónica (APO) [5] y el compromiso débil asincrónico (AWC) [4], para abordar el DCSP y proporcionar la base de solución para sus aplicaciones. En términos generales, estos algoritmos se basan en dos enfoques diferentes, ya sea extendiéndose a partir de algoritmos clásicos de retroceso [6] o introduciendo mediación entre los agentes. Aunque no ha faltado esfuerzo en este prometedor campo de investigación, especialmente en abordar problemas destacados como las restricciones de recursos (por ejemplo, límites de tiempo y comunicación) [7] y los requisitos de privacidad [8], desafortunadamente no existe un tratamiento conceptualmente claro para abrir el funcionamiento modelotécnico de los diversos algoritmos de agentes que se han desarrollado. Como resultado, por ejemplo, no es posible obtener un entendimiento intelectual más profundo sobre por qué un algoritmo es mejor que otro, más allá de cuestiones computacionales. En este artículo, presentamos un marco novedoso y unificado de satisfacción de restricciones distribuidas basado en negociación automatizada [9]. La negociación se considera un proceso en el que varios agentes buscan una solución llamada acuerdo. La búsqueda puede realizarse a través de un mecanismo de negociación (o algoritmo) por el cual los agentes siguen un protocolo de alto nivel que prescribe las reglas de interacción, utilizando un conjunto de estrategias diseñadas para seleccionar sus propias preferencias en cada paso de la negociación. Anclando la búsqueda de DCSP en la negociación automatizada, mostramos en este artículo que varios algoritmos de DCSP bien conocidos [3] son en realidad mecanismos que comparten el mismo protocolo de interacción de Creencia-Deseo-Intención (BDI) para llegar a acuerdos, pero utilizan diferentes estrategias de selección de acciones o valores. El marco propuesto no solo proporciona una comprensión más clara de los algoritmos existentes de DCSP desde una perspectiva unificada de agente BDI, sino que también abre oportunidades para extender y desarrollar nuevas estrategias para DCSP. Con este fin, se propone una nueva estrategia llamada Consejo Mutuo No Solicitado (UMA). Nuestra evaluación de rendimiento muestra que UMA puede superar a ABT y AWC en términos del número promedio de ciclos computacionales tanto para los problemas de coloración dispersa como crítica [6]. El resto de este documento está organizado de la siguiente manera. En la Sección 2, proporcionamos una descripción formal de DCSP. La sección 3 presenta un modelo de negociación BDI mediante el cual un agente DCSP razona. La sección 4 presenta los algoritmos existentes ABT, AWC y DBO como diferentes estrategias formalizadas en un protocolo común. Se propone una nueva estrategia llamada Consejos Mutuos No Solicitados en la Sección 5; nuestros resultados empíricos y discusión intentan resaltar los méritos de la nueva estrategia sobre las existentes. La sección 6 concluye el artículo y señala algunos trabajos futuros. 2. DCSP: FORMALIZACIÓN DEL PROBLEMA El DCSP [4] considera el siguiente entorno. • Hay n agentes con k variables x0, x1, · · · , xk−1, n ≤ k, que tienen valores en dominios D1, D2, · · · , Dk, respectivamente. Definimos una función parcial B sobre el rango de productos {0, 1, . . . , (n−1)}×{0, 1, . . . , (k −1)} de tal manera que la pertenencia de la variable xj al agente i se denota por B(i, j)!. El signo de exclamación ! está definido. • Hay m restricciones c0, c1, · · · cm−1 que deben satisfacerse conjuntamente. De manera similar a como se define para B(i, j), usamos E(l, j)!, (0 ≤ l < m, 0 ≤ j < k), para denotar que xj es relevante para la restricción cl. El DCSP puede ser formalmente expresado de la siguiente manera. Declaración del problema: ∀i, j (0 ≤ i < n)(0 ≤ j < k) donde B(i, j)!, encontrar la asignación xj = dj ∈ Dj tal que ∀l (0 ≤ l < m) donde E(l, j)!, se cumpla cl. Una restricción puede consistir en diferentes variables pertenecientes a diferentes agentes. Un agente no puede cambiar o modificar los valores de asignación de variables de otros agentes. Por lo tanto, al buscar cooperativamente una solución de DCSP, los agentes necesitarían comunicarse entre sí, y ajustar y reajustar sus propias asignaciones variables en el proceso. Modelo de Agente DCSP En general, todos los agentes DCSP deben interactuar cooperativamente y esencialmente realizar la asignación y reasignación de valores de dominio a variables para resolver todas las violaciones de restricciones. Si los agentes tienen éxito en su resolución, se encuentra una solución. Para participar en un comportamiento cooperativo, un agente DCSP necesita cinco parámetros fundamentales, a saber, (i) una variable [4] o un conjunto de variables [10], (ii) dominios, (iii) prioridad, (iv) una lista de vecinos y (v) una lista de restricciones. Cada variable asume un rango de valores llamado dominio. Un valor de dominio, que generalmente abstrae una acción, es una opción posible que un agente puede tomar. Cada agente tiene una prioridad asignada. Estos valores de prioridad ayudan a decidir el orden en el que revisan o modifican sus asignaciones de variables. La prioridad de un agente puede ser fija (estática) o cambiante (dinámica) al buscar una solución. Si un agente tiene más de una variable, cada variable puede ser asignada una prioridad diferente, para ayudar a determinar qué asignación de variable el agente debería modificar primero. Un agente que comparte la misma restricción con otro agente se llama vecino de este último. Cada agente necesita hacer referencia a su lista de vecinos durante el proceso de búsqueda. Esta lista también puede mantenerse sin cambios o actualizarse según sea necesario en tiempo de ejecución. Del mismo modo, cada agente mantiene una lista de restricciones. El agente debe asegurarse de que no haya violación de las restricciones en esta lista. Las restricciones pueden ser añadidas o eliminadas de la lista de restricciones de un agente en tiempo de ejecución. Al igual que con un agente, una restricción también puede estar asociada con un valor de prioridad. Las restricciones con alta prioridad se consideran más importantes que las restricciones con una prioridad más baja. Para distinguirlo de la prioridad de un agente, la prioridad de una restricción se llama su peso. 3. El modelo de negociación BDI se origina en el trabajo de M. Bratman [11]. Según [12, Cap.1], la arquitectura BDI se basa en un modelo filosófico de razonamiento práctico humano, y describe el proceso de razonamiento mediante el cual un agente decide qué acciones realizar en momentos consecutivos al perseguir ciertos objetivos. Al enraizar el alcance en el marco de DCSP, el objetivo común de todos los agentes es encontrar una combinación de valores de dominio para satisfacer un conjunto de restricciones predefinidas. En la negociación automatizada [9], dicha solución se llama un acuerdo entre los agentes. Dentro de este ámbito, descubrimos que pudimos desenterrar el comportamiento genérico de un agente DCSP y formularlo en un protocolo de negociación, prescrito utilizando los poderosos conceptos de BDI. Por lo tanto, se puede decir que nuestro modelo de negociación propuesto combina los conceptos BDI con la negociación automatizada en un marco multiagente, lo que nos permite separar conceptualmente los mecanismos de DCSP en un protocolo de interacción BDI común y las estrategias adoptadas. 3.1 El protocolo genérico La Figura 1 muestra los pasos básicos de razonamiento en una ronda arbitraria de negociación que constituyen el nuevo protocolo. La línea sólida indica el componente común o la transición que siempre existe independientemente de la estrategia utilizada. La línea punteada indica el componente o transición del protocolo de interacción BDI que puede o no aparecer dependiendo de la estrategia adoptada. A través de este protocolo se intercambian dos tipos de mensajes, a saber, el mensaje de información y el mensaje de negociación. Un mensaje de información percibido es un mensaje enviado por otro agente. El mensaje contendrá los valores y prioridades actuales seleccionados de las variables de ese agente emisor. El propósito principal de este mensaje es informar al agente sobre el entorno actual. El mensaje de información se envía al final de una ronda de negociación (también llamada ciclo de negociación) y se recibe al comienzo de la siguiente ronda. Un mensaje de negociación es un mensaje que puede ser enviado dentro de una ronda. Este mensaje es para fines de mediación. El agente puede incluir diferentes contenidos en este tipo de mensaje siempre y cuando esté acordado entre el grupo. El formato del mensaje de negociación y cuándo se enviará están sujetos a la estrategia. Un mensaje de negociación puede ser enviado al final de un paso de razonamiento y recibido al comienzo del siguiente paso. La mediación es un paso del protocolo que depende de si la interacción de los agentes con otros es sincrónica o asincrónica. En el mecanismo sincrónico, la mediación es necesaria en cada ronda de negociación. En un sistema asíncrono, la mediación solo es necesaria en una ronda de negociación cuando el agente recibe un mensaje de negociación. Una visión más detallada de este paso de mediación se proporciona más adelante en esta sección. El protocolo BDI prescribe la estructura esquelética para la negociación de DCSP. Mostraremos en la Sección 4 que varios mecanismos conocidos de DCSP heredan este modelo genérico. Los detalles de los seis pasos principales de razonamiento para el protocolo (ver Figura 1) se describen de la siguiente manera para un agente DCSP. Para una descripción conceptualmente más clara, asumimos que solo hay una variable por agente. • Percepción. En este paso, el agente recibe mensajes de información de sus vecinos en el entorno, y utilizando su función de Percepción, devuelve una imagen P. Esta imagen contiene los valores actuales asignados a las variables de todos los agentes en su lista de vecinos. La imagen P guiará las acciones de los agentes en los pasos siguientes. El agente también actualiza su lista de restricciones C utilizando algunos criterios de la estrategia adoptada. • Creencia. Usando la imagen P y la lista de restricciones C, el agente verificará si hay alguna restricción violada. Si no hay ninguna violación, el agente creerá que está eligiendo la opción correcta y, por lo tanto, no tomará ninguna acción. El agente no hará nada si se encuentra en un estado local estable: una instantánea de las asignaciones de variables del agente y todos sus vecinos mediante las cuales satisfacen sus restricciones compartidas. Cuando todos los agentes se encuentran en sus estados estables locales, se dice que todo el entorno está en un estado estable global y en un acuerdo. Se encontró el documento de la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 525. En caso de que el agente encuentre que su valor entra en conflicto con algunos de sus vecinos, es decir, la combinación de valores asignados a las variables conduce a una violación de restricciones, el agente primero intentará reasignar su propia variable utilizando una estrategia específica. Si encuentra una opción adecuada que cumpla con algunos criterios de la estrategia adoptada, el agente creerá que debería cambiar a la nueva opción. Sin embargo, no siempre sucede que un agente pueda encontrar con éxito esa opción. Si no se puede encontrar ninguna opción, el agente creerá que no tiene ninguna opción, y por lo tanto solicitará a sus vecinos que reconsideren sus asignaciones de variables. En resumen, hay tres tipos de creencias que un agente DCSP puede formar: (i) puede cambiar la asignación de sus variables para mejorar la situación actual, (ii) no puede cambiar la asignación de sus variables y algunas violaciones de restricciones no pueden resolverse y (iii) no es necesario cambiar la asignación de sus variables ya que todas las restricciones se cumplen. Una vez que se forman las creencias, el agente determinará sus deseos, que son las opciones que intentan resolver las violaciones actuales de las restricciones. • Deseo. Si el agente elige la Creencia (i), generará una lista de sus propios valores de dominio adecuados como su conjunto de deseos. Si el agente elige la Creencia (ii), no podrá determinar su conjunto de deseos, pero generará una sublista de agentes de su lista de vecinos, a quienes les pedirá que reconsideren sus asignaciones de variables. Cómo se crea esta sublista depende de la estrategia ideada para el agente. En esta situación, el agente utilizará un conjunto de deseos virtuales que determina en función de su estrategia adoptada. Si el agente adopta la Creencia (iii), no tendrá ningún deseo de revisar su valor de dominio, y por lo tanto no tendrá intención. • Intención. El agente seleccionará un valor de su conjunto de deseos como su intención. Una intención es la mejor opción deseada que el agente asigna a su variable. Los criterios para seleccionar un deseo como la intención de los agentes dependen de la estrategia utilizada. Una vez que se forma la intención, el agente puede proceder a la etapa de ejecución o someterse a mediación. Nuevamente, la decisión de hacerlo está determinada por algunos criterios de la estrategia adoptada. • Mediación. Esta es una función importante del agente. Dado que, si el agente ejecuta su intención sin llevar a cabo la mediación de intenciones con sus vecinos, la violación de restricciones entre los agentes puede no resolverse. Por ejemplo, supongamos que dos agentes tienen variables, x1 y x2, asociadas con el mismo dominio {1, 2}, y su restricción compartida es (x1 + x2 = 3). Entonces, si ambas variables se inicializan con el valor 1, ambas cambiarán concurrentemente entre los valores 2 y 1 en ausencia de mediación entre ellas. Hay dos tipos de mediación: mediación local y mediación grupal. En el primero, los agentes intercambian sus intenciones. Cuando un agente recibe la intención de otro que entra en conflicto con la suya, el agente debe mediar entre las intenciones, ya sea cambiando su propia intención o informando al otro agente para que cambie su intención. En este último, hay un agente que actúa como mediador del grupo. Este mediador recopilará las intenciones del grupo - una unión del agente y sus vecinos - y determinará cuál intención se ejecutará. El resultado de esta mediación se devuelve a los agentes del grupo. Tras la mediación, el agente puede proceder al siguiente paso de razonamiento para ejecutar su intención o comenzar una nueva ronda de negociación. • Ejecución. Este es el último paso de una ronda de negociación. El agente ejecutará actualizando su asignación de variables si la intención obtenida en este paso es la suya propia. Tras la ejecución, el agente informará a sus vecinos sobre su nueva asignación de variables y prioridad actualizada. Para hacerlo, el agente enviará un mensaje informativo. 3.2 La estrategia Una estrategia juega un papel importante en el proceso de negociación. Dentro del protocolo, a menudo se determinará la eficiencia del proceso de búsqueda de estrategias de retroceso asíncrono en términos de ciclos computacionales y costos de comunicación de mensajes del protocolo Percept Belief Desire Intention Mediation Execution P B D I Info Message Info Message Negotiation Message Negotiation Message Negotiation Message Figura 2. El espacio de diseño al idear una estrategia está influenciado por las siguientes dimensiones: (i) asíncrono o síncrono, (ii) prioridad dinámica o estática, (iii) peso de la restricción dinámica o estática, (iv) número de mensajes de negociación a comunicar, (v) el formato del mensaje de negociación y (vi) la propiedad de completitud. En otras palabras, estas dimensiones proporcionan consideraciones técnicas para el diseño de una estrategia. 4. ALGORITMOS DCSP: PROTOCOLO BDI + ESTRATEGIAS En esta sección, aplicamos el modelo de negociación BDI propuesto presentado en la Sección 3 para exponer el protocolo BDI y las diferentes estrategias utilizadas para tres algoritmos bien conocidos, ABT, AWC y DBO. Todos estos algoritmos asumen que solo hay una variable por agente. Bajo nuestro marco de trabajo, llamamos a las estrategias aplicadas las estrategias ABT, AWC y DBO, respectivamente. Para describir formalmente cada estrategia, se utilizan las siguientes notaciones matemáticas: • n es el número de agentes, m es el número de restricciones; • xi denota la variable mantenida por el agente i, (0 ≤ i < n); • Di denota el dominio de la variable xi; Fi denota la lista de vecinos del agente i; Ci denota su lista de restricciones; • pi denota la prioridad del agente i; y Pi = {(xj = vj, pj = k) | agente j ∈ Fi, vj ∈ Dj es el valor actual asignado a xj y el valor de prioridad k es un entero positivo} es la percepción del agente i; • wl denota el peso de la restricción l, (0 ≤ l < m); • Si(v) es el peso total de las restricciones violadas en Ci cuando su variable tiene el valor v ∈ Di. 4.1 Retroceso Asíncrono La Figura 2 presenta el modelo de negociación BDI que incorpora la estrategia de Retroceso Asíncrono (ABT). Como se menciona en la Sección 3, para un mecanismo asíncrono como ABT, el paso de mediación solo es necesario en una ronda de negociación cuando un agente recibe un mensaje de negociación. Para el agente i, comenzando inicialmente con (wl = 1, (0 ≤ l < m); pi = i, (0 ≤ i < n)) y Fi contiene a todos los agentes que comparten las restricciones con el agente i, su estrategia ABT impulsada por BDI se describe de la siguiente manera. Paso 1 - Percepción: Actualizar Pi al recibir los mensajes de información de los vecinos (en Fi). Actualizar Ci para que sea la lista de 526 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) restricciones que solo consisten en agentes en Fi que tienen igual o mayor prioridad que este agente. Paso 2 - Creencia: La función de creencia GB (Pi, Ci) devolverá un valor bi ∈ {0, 1, 2}, decidido de la siguiente manera: • bi = 0 cuando el agente i puede encontrar una opción óptima, es decir, si (Si(vi) = 0 o vi está en la lista de valores malos) y (∃a ∈ Di)(Si(a) = 0) y a no está en una lista de valores de dominio llamada lista de valores malos. Inicialmente esta lista está vacía y se limpiará cuando un vecino de mayor prioridad cambie su asignación de variable. • bi = 1 cuando no puede encontrar una opción óptima, es decir, si (∀a ∈ Di)(Si(a) = 0) o a está en la lista de valores malos. • bi = 2 cuando su asignación de variable actual es una opción óptima, es decir, si Si(vi) = 0 y vi no está en la lista de valores malos. Paso 3 - Deseo: La función de deseo GD (bi) devolverá un conjunto de deseos denotado por DS, decidido de la siguiente manera: • Si bi = 0, entonces DS = {a | (a = vi), (Si(a) = 0) y a no está en la lista de valores malos}. • Si bi = 1, entonces DS = ∅, el agente también encuentra al agente k que se determina por {k | pk = min(pj) con el agente j ∈ Fi y pk > pi }. • Si bi = 2, entonces DS = ∅. Paso 4 - Intención: La función de intención GI (DS) devolverá una intención, decidida de la siguiente manera: • Si DS = ∅, entonces selecciona un valor arbitrario (digamos, vi) de DS como la intención. • Si DS = ∅, entonces asigna nil como la intención (para denotar su falta). Paso 5 - Ejecución: • Si el agente i tiene un valor de dominio como su intención, el agente actualizará su asignación de variable con este valor. • Si bi = 1, el agente i enviará un mensaje de negociación al agente k, luego eliminará a k de Fi y comenzará su siguiente ronda de negociación. El mensaje de negociación contendrá la lista de asignaciones variables de aquellos agentes en su lista de vecinos Fi que tienen una prioridad más alta que el agente i en la imagen actual Pi. Mediación: Cuando el agente i recibe un mensaje de negociación, se llevan a cabo varios subpasos, de la siguiente manera: • Si la lista de agentes asociados con el mensaje de negociación contiene agentes que no están en Fi, se agregarán estos agentes a Fi y se les solicitará a estos agentes que se agreguen a sí mismos a sus listas de vecinos. La solicitud se considera como un tipo de mensaje de negociación. • El agente i primero verificará si el agente remitente está actualizado con su valor actual vi. El agente añadirá vi a su lista de valores malos si es así, o de lo contrario enviará su valor actual al agente remitente. Siguiendo este paso, el agente i procede a la siguiente ronda de negociación. 4.2 Búsqueda de Compromiso Débil Asincrónica La Figura 3 presenta el modelo de negociación BDI que incorpora la estrategia de Compromiso Débil Asincrónico (AWC). El modelo es similar al de incorporar la estrategia ABT (ver Figura 2). Esto no es sorprendente; AWC y ABT se encuentran estratégicamente similares, diferenciándose solo en los detalles de algunos pasos de razonamiento. El punto distintivo de AWC es que cuando el agente no puede encontrar una asignación de variable adecuada, cambiará su prioridad a la más alta entre sus miembros del grupo ({i} ∪ Fi). Para el agente i, comenzando inicialmente con (wl = 1, (0 ≤ l < m); pi = i, (0 ≤ i < n)) y Fi contiene a todos los agentes que comparten las restricciones con el agente i, su estrategia AWC impulsada por BDI se describe de la siguiente manera. Paso 1 - Percepción: Este paso es idéntico al paso de Percepción de ABT. Paso 2 - Creencia: La función de creencia GB (Pi, Ci) devolverá un valor bi ∈ {0, 1, 2}, decidido de la siguiente manera: Creencia de Percepción Deseo Intención Mediación Ejecución P B D I Información Mensaje Información Mensaje Negociación Mensaje Negociación Mensaje Negociación Mensaje Figura 3: Protocolo BDI con estrategia de CompromisoDébil Asincrónico • bi = 0 cuando el agente puede encontrar una opción óptima, es decir, si (Si(vi) = 0 o la asignación xi = vi y las asignaciones de variables actuales de los vecinos en Fi que tienen mayor prioridad forman un nogood [4]) almacenado en una lista llamada lista de nogoods y ∃a ∈ Di, Si(a) = 0 (inicialmente la lista está vacía). • bi = 1 cuando el agente no puede encontrar ninguna opción óptima, es decir, si ∀a ∈ Di, Si(a) = 0. • bi = 2 cuando la asignación actual es una opción óptima, es decir, si Si(vi) = 0 y el estado actual no es un nogood en la lista de nogoods. Paso 3 - Deseo: La función de deseo GD (bi) devolverá un conjunto de deseos DS, decidido de la siguiente manera: • Si bi = 0, entonces DS = {a | (a = vi), (Si(a) = 0) y se minimiza el número de violaciones de restricciones con agentes de menor prioridad }. • Si bi = 1, entonces DS = {a | a ∈ Di y se minimiza el número de violaciones de todas las restricciones relevantes }. • Si bi = 2, entonces DS = ∅. Si bi = 1, el agente i encontrará una lista Ki de vecinos de mayor prioridad, definida por Ki = {k | agente k ∈ Fi y pk > pi}. Paso 4 - Intención: Este paso es similar al paso de Intención de ABT. Sin embargo, para esta estrategia, el mensaje de negociación contendrá las asignaciones de variables (de la imagen actual Pi) para todos los agentes en Ki. Esta lista de tareas se considera como un nogood. Si el mismo mensaje de negociación hubiera sido enviado anteriormente, el agente i no tendría ninguna intención. De lo contrario, el agente enviará el mensaje y guardará el nogood en la lista de nogoods. Paso 5 - Ejecución: • Si el agente i tiene un valor de dominio como su intención, el agente actualizará su asignación de variable con este valor. • Si bi = 1, enviará el mensaje de negociación a sus vecinos en Ki, y establecerá pi = max{pj} + 1, con el agente j ∈ Fi. Mediación: Este paso es idéntico al paso de Mediación de ABT, excepto que el agente i ahora agregará el nogood contenido en el mensaje de negociación recibido a su propia lista de nogoods. 4.3 Distribución de la Ruptura. La Figura 4 presenta el modelo de negociación BDI que incorpora la estrategia de Distribución de la Ruptura (DBO). Esencialmente, mediante esta estrategia sincrónica, cada agente buscará de forma iterativa mejorar reduciendo el peso total de las restricciones violadas. La iteración continuará hasta que ningún agente pueda mejorar más, momento en el que si algunas restricciones siguen siendo violadas, los pesos de The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 527 Percepción Creencia Deseo Intención Mediación Ejecución P B D I I A Información Mensaje Información Mensaje Negociación Mensaje Negociación Figura 4: Protocolo BDI con estrategia de Distribución de Rompimiento, estas restricciones se incrementarán en 1 para ayudar a salir de un mínimo local. Para el agente i, comenzando inicialmente con (wl = 1, (0 ≤ l < m), pi = i, (0 ≤ i < n)) y Fi contiene todos los agentes que comparten las restricciones con el agente i, su estrategia de DBO impulsada por BDI se describe de la siguiente manera. Paso 1 - Percepción: Actualizar Pi al recibir los mensajes de información de los vecinos (en Fi). Actualizar Ci para que sea la lista de sus restricciones relevantes. Paso 2 - Creencia: La función de creencia GB (Pi, Ci) devolverá un valor bi ∈ {0, 1, 2}, decidido de la siguiente manera: • bi = 0 cuando el agente i puede encontrar una opción para reducir el número de violaciones de las restricciones en Ci, es decir, si ∃a ∈ Di, Si(a) < Si(vi). • bi = 1 cuando no puede encontrar ninguna opción para mejorar la situación, es decir, si ∀a ∈ Di, a = vi, Si(a) ≥ Si(vi). • bi = 2 cuando su asignación actual es una opción óptima, es decir, si Si(vi) = 0. Paso 3 - Deseo: La función de deseo GD (bi) devolverá un conjunto de deseos DS, decidido de la siguiente manera: • Si bi = 0, entonces DS = {a | a = vi, Si(a) < Si(vi) y (Si(vi)−Si(a)) está maximizado }. (max{(Si(vi)−Si(a))} será referenciado como hmax i en pasos posteriores, y define la reducción máxima en violaciones de restricciones). • De lo contrario, DS = ∅. Paso 4 - Intención: La función de intención GI (DS) devolverá una intención, decidida de la siguiente manera: • Si DS = ∅, entonces selecciona un valor arbitrario (digamos, vi) de DS como la intención. • Si DS = ∅, entonces asigna nil como la intención. A continuación, el agente i enviará su intención a todos sus vecinos. A cambio, recibirá las intenciones de estos agentes antes de proceder al paso de Mediación. Mediación: El agente i recibe todas las intenciones de sus vecinos. Si encuentra que la intención recibida de un agente vecino j está asociada con hmax j > hmax i, el agente cancelará automáticamente su intención actual. Paso 5 - Ejecución: • Si el agente i no canceló su intención, actualizará su asignación de variables con el valor previsto. Si todas las intenciones recibidas y la propia son intenciones nulas, el agente aumentará el peso de cada restricción actualmente violada en 1.5. La Figura 5 presenta el modelo de negociación BDI que incorpora la estrategia de Consejo Mutuo No Solicitado (UMA). A diferencia de cuando se utilizan las estrategias de la sección anterior, un agente DCSP que utiliza UMA no solo enviará un mensaje de negociación al concluir su paso de Intención, sino también al concluir su paso de Deseo. El mensaje de negociación que envía para concluir el paso del Deseo constituye un consejo no solicitado para todos sus vecinos. A su vez, el agente esperará recibir consejos no solicitados de todos sus vecinos antes de proceder a determinar su intención. Para el agente i, comenzando inicialmente con (wl = 1, (0 ≤ l < m), pi = i, (0 ≤ i < n)) y Fi contiene todos los agentes que comparten las restricciones con el agente i, su estrategia UMA impulsada por BDI se describe de la siguiente manera. Paso 1 - Percepción: Actualizar Pi al recibir los mensajes de información de los vecinos (en Fi). Actualizar Ci para que sea la lista de restricciones relevantes para el agente i. Paso 2 - Creencia: La función de creencia GB (Pi, Ci) devolverá un valor bi ∈ {0, 1, 2}, decidido de la siguiente manera: • bi = 0 cuando el agente i puede encontrar una opción para reducir el número de violaciones de las restricciones en Ci, es decir, si ∃a ∈ Di, Si(a) < Si(vi) y la asignación xi = a y las asignaciones variables actuales de sus vecinos no forman un estado local almacenado en una lista llamada lista de estados malos (inicialmente esta lista está vacía). • bi = 1 cuando no puede encontrar un valor a tal que a ∈ Di, Si(a) < Si(vi), y la asignación xi = a y las asignaciones variables actuales de sus vecinos no forman un estado local almacenado en la lista de estados malos. • bi = 2 cuando su asignación actual es una opción óptima, es decir, si Si(vi) = 0. Paso 3 - Deseo: La función de deseo GD (bi) devolverá un conjunto de deseos DS, decidido de la siguiente manera: • Si bi = 0, entonces DS = {a | a = vi, Si(a) < Si(vi) y (Si(vi) − Si(a)) está maximizado } y la asignación xi = a y las asignaciones variables actuales del agente vecino no forman un estado en la lista de estados malos. En este caso, DS se llama un conjunto de deseos voluntarios. max{(Si(vi)−Si(a))} será referido como hmax i en los pasos siguientes, y define 528 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) la reducción máxima en violaciones de restricciones. También se le conoce como una mejora). • Si bi = 1, entonces DS = {a | a = vi, Si(a) se minimiza} y la asignación xi = a y las asignaciones de variables actuales del agente vecino no forman un estado en la lista de estados malos. En este caso, DS se llama un conjunto de deseos reacios • Si bi = 2, entonces DS = ∅. Si bi = 0, el agente i enviará un mensaje de negociación que contiene hmax i a todos sus vecinos. Este mensaje se llama un consejo voluntario. Si bi = 1, el agente i enviará un mensaje de negociación llamado "consejo de cambio" a los vecinos en Fi que comparten las restricciones violadas con el agente i. El agente i recibe consejos de todos sus vecinos y los almacena en una lista llamada A, antes de pasar al siguiente paso. Paso 4 - Intención: La función de intención GI (DS, A) devolverá una intención, decidida de la siguiente manera: • Si hay un consejo voluntario de un agente j que está asociado con hmax j > hmax i , asignar nil como la intención. • Si DS = ∅, DS es un conjunto de deseos voluntarios y hmax i es la mayor mejora entre aquellos asociados con los consejos voluntarios recibidos, seleccionar un valor arbitrario (digamos, vi) de DS como la intención. Esta intención se llama intención voluntaria. • Si DS = ∅, DS es un conjunto de deseos reacios y el agente i recibe algunos consejos de cambio, selecciona un valor arbitrario (digamos, vi) de DS como la intención. Esta intención se llama intención renuente. • Si DS = ∅, entonces asigna nil como la intención. A continuación, si la mejora hmax i es la mayor mejora y es igual a algunas mejoras asociadas con los consejos voluntarios recibidos, el agente i enviará su intención calculada a todos sus vecinos. Si el agente i tiene una intención reacia, también la enviará a todos sus vecinos. En ambos casos, el agente i adjuntará el número de consejos de cambio recibidos en la ronda de negociación actual con su intención. A cambio, el agente i recibirá las intenciones de sus vecinos antes de proceder al paso de Mediación. Mediación: Si el agente i no envía su intención antes de este paso, es decir, si el agente tiene una intención nula o una intención voluntaria con la mayor mejora, procederá al siguiente paso. De lo contrario, el agente i seleccionará la mejor intención entre todas las intenciones recibidas, incluyendo la suya propia (si la hubiera). Los criterios para seleccionar la mejor intención se enumeran y se aplican en orden descendente de importancia de la siguiente manera: • Se prefiere una intención voluntaria sobre una intención reacia. • Se selecciona una intención voluntaria (si la hay) con la mayor mejora. • Si no hay una intención voluntaria, se selecciona la intención reacia con el menor número de violaciones de restricciones. • Se selecciona la intención de un agente que ha recibido un mayor número de consejos de cambio en la ronda de negociación actual. • Se selecciona la intención de un agente con la prioridad más alta. Si la intención seleccionada no es la intención del agente, cancelará su intención. Paso 5 - Ejecución: Si el agente i no cancela su intención, actualizará su asignación de variables con el valor previsto. Condición de Terminación: Dado que cada agente no tiene información completa sobre el estado global, es posible que no sepa cuándo ha alcanzado una solución, es decir, cuando todos los agentes se encuentran en un estado global estable. Por lo tanto, se necesita un observador que se encargue de hacer un seguimiento de los mensajes de negociación comunicados en el entorno. Después de un cierto período de tiempo en el que no hay más comunicación de mensajes (lo cual sucede cuando todos los agentes ya no tienen intención de actualizar sus asignaciones de variables), el observador informará a los agentes en el entorno que se ha encontrado una solución. 1 2 3 4 5 6 7 8 9 10 Figura 6: Problema de ejemplo 5.1 Un ejemplo Para ilustrar cómo funciona UMA, consideremos un problema de grafo de 2 colores [6] como se muestra en la Figura 6. En este ejemplo, cada agente tiene una variable de color que representa un nodo. Hay 10 variables de color que comparten el mismo dominio {Negro, Blanco}. El siguiente registro detalla el resultado de cada paso en cada ronda de negociación ejecutada. Ronda 1: Paso 1 - Percepción: Cada agente obtiene las asignaciones de color actuales de los nodos (agentes) adyacentes a él, es decir, sus vecinos. Paso 2 - Creencia: Los agentes que han tenido mejoras positivas son el agente 1 (este agente cree que debería cambiar su color a Blanco), el agente 2 (este cree que debería cambiar su color a Blanco), el agente 7 (este agente cree que debería cambiar su color a Negro) y el agente 10 (este agente cree que debería cambiar su valor a Negro). En esta ronda de negociación, las mejoras logradas por estos agentes son 1. Los agentes que no tienen ninguna mejora son los agentes 4, 5 y 8. Los agentes 3, 6 y 9 no necesitan cambiar ya que todas sus restricciones relevantes están satisfechas. Paso 3 - Deseo: Los agentes 1, 2, 7 y 10 tienen el deseo voluntario (color blanco para los agentes 1, 2 y color negro para los agentes 7, 10). Estos agentes enviarán los consejos voluntarios a todos sus vecinos. Mientras tanto, los agentes 4, 5 y 8 tienen los deseos reacios (color blanco para el agente 4 y color negro para los agentes 5, 8). El agente 4 enviará un aviso de cambio al agente 2, ya que el agente 2 está compartiendo la restricción violada con él. De manera similar, los agentes 5 y 8 enviarán avisos de cambio a los agentes 7 y 10 respectivamente. Los agentes 3, 6 y 9 no tienen ningún deseo de actualizar sus asignaciones de color. Paso 4 - Intención: Los agentes 2, 7 y 10 reciben los consejos de cambio de los agentes 4, 5 y 8, respectivamente. Ellos forman sus intenciones voluntarias. Los agentes 4, 5 y 8 reciben los consejos voluntarios de los agentes 2, 7 y 10, por lo tanto, no tendrán ninguna intención. Los agentes 3, 6 y 9 no tienen ninguna intención. A continuación, la intención de los agentes será enviada a todos sus vecinos. Mediación: El agente 1 encuentra que la intención del agente 2 es mejor que su propia intención. Esto se debe a que, aunque ambos agentes tienen intenciones voluntarias de mejora de 1, el agente 2 ha recibido un consejo de cambio del agente 4 mientras que el agente 1 no ha recibido ninguno. Por lo tanto, el agente 1 cancela su intención. El agente 2 mantendrá su intención. Los agentes 7 y 10 mantienen sus intenciones ya que ninguno de sus vecinos tiene una intención. El resto de los agentes no hacen nada en este paso ya que no tienen ninguna intención. Paso 5 - Ejecución: El Agente 2 cambia su color a Blanco. Los agentes 7 y 10 cambian sus colores a negro. El nuevo estado después de la ronda 1 se muestra en la Figura 7. Ronda 2: Paso 1 - Percepción: Los agentes obtienen las asignaciones de color actuales de sus vecinos. Paso 2 - Creencia: El agente 3 es el único agente que tiene una mejora positiva, que es 1. Cree que debería cambiar su The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 529 1 2 3 4 5 6 7 8 9 10 Figura 7: El gráfico después de la ronda 1 se colorea de negro. El agente 2 no tiene ninguna mejora positiva. El resto de los agentes no necesitan realizar ningún cambio, ya que todas sus restricciones relevantes están satisfechas. No tendrán deseo, y por lo tanto no tendrán intención. Paso 3 - Deseo: El agente 3 desea cambiar su color a negro voluntariamente, por lo tanto, envía un consejo voluntario a su vecino, es decir, el agente 2. El Agente 2 no tiene ningún valor para su conjunto de deseos reacios, ya que la única opción, el color negro, llevará al Agente 2 y a sus vecinos al estado anterior, que se sabe que es un estado malo. Dado que el agente 2 está compartiendo la violación de la restricción con el agente 3, envía un aviso de cambio al agente 3. Paso 4 - Intención: El agente 3 tendrá una intención voluntaria mientras que el agente 2 no tendrá ninguna intención al recibir el consejo voluntario del agente 3. Mediación: El Agente 3 mantendrá su intención como su único vecino, ya que el Agente 2 no tiene ninguna intención. Paso 5 - Ejecución: El Agente 3 cambia su color a Negro. El nuevo estado después de la ronda 2 se muestra en la Figura 8. Ronda 3: En esta ronda, cada agente descubre que no tiene deseo y, por lo tanto, no tiene intención de revisar su asignación de variables. A continuación, sin más negociaciones en el entorno de comunicación de mensajes, el observador informará a todos los agentes que se ha encontrado una solución. Para facilitar comparaciones creíbles con estrategias existentes, medimos el tiempo de ejecución en términos de ciclos computacionales según se define en [4], y construimos un simulador que pudiera reproducir los resultados publicados para ABT y AWC. La definición de un ciclo computacional es la siguiente. • En un ciclo, cada agente recibe todos los mensajes entrantes, realiza cálculos locales y envía una respuesta. • Un mensaje enviado en el tiempo t será recibido en el tiempo t + 1. El retraso de red se desprecia. • Cada agente tiene su propio reloj. El valor inicial del reloj es 0. Los agentes adjuntan su valor de reloj como una marca de tiempo en el mensaje saliente y utilizan la marca de tiempo en el mensaje entrante para actualizar su propio valor de reloj. Se consideraron cuatro problemas de referencia [6], a saber, n-reinas y coloreado de nodos para grafos dispersos, densos y críticos. Para cada problema, se generó un número finito de casos de prueba para varios tamaños de problema n. El tiempo de ejecución máximo se estableció en 0 200 400 600 800 1000 10 50 100 Número de reinas Ciclos Retroceso Asincrónico Compromiso Débil Asincrónico Consejo Mutuo No Solicitado Figura 9: Relación entre el tiempo de ejecución y el tamaño del problema 10000 ciclos para la coloración de nodos para grafos críticos y 1000 ciclos para otros problemas. El programa simulador fue terminado después de este período y se consideró que el algoritmo fallaba un caso de prueba si no encontraba una solución para entonces. En tal caso, el tiempo de ejecución de la prueba se contó como 1000 ciclos. 5.2.1 Evaluación con el problema de las n-reinas El problema de las n-reinas es un problema tradicional de satisfacción de restricciones. Se generaron 10 casos de prueba para cada tamaño de problema n ∈ {10, 50 y 100}. La Figura 9 muestra el tiempo de ejecución para diferentes tamaños de problema cuando se ejecutaron ABT, AWC y UMA. 5.2.2 Evaluación con el problema de coloreo de grafos El problema de coloreo de grafos puede ser caracterizado por tres parámetros: (i) el número de colores k, el número de nodos/agentes n y el número de enlaces m. Basado en la proporción m/n, el problema puede ser clasificado en tres tipos [3]: (i) disperso (con m/n = 2), (ii) crítico (con m/n = 2.7 o 4.7) y (iii) denso (con m/n = (n − 1)/4). Para este problema, no incluimos ABT en nuestros resultados empíricos ya que se encontró que su tasa de falla era muy alta. Se esperaba este bajo rendimiento de ABT ya que el problema de colorear grafos es más difícil que el problema de las n-reinas, en el cual ABT ya no tuvo un buen desempeño (ver Figura 9). Los tipos de problemas de coloración dispersa y densa son relativamente fáciles, mientras que el tipo crítico es difícil de resolver. En los experimentos, fijamos k = 3. Se crearon 10 casos de prueba utilizando el método descrito en [13] para cada valor de n ∈ {60, 90, 120}, para cada tipo de problema. Los resultados de la simulación para cada tipo de problema se muestran en las Figuras 10 - 12. 0 40 80 120 160 200 60 90 120 150 Número de Nodos Ciclos Asincrónico Compromiso Débil No Solicitado Consejo Mutuo Figura 10: Comparación entre AWC y UMA (coloración de gráficos dispersos) 5.3 Discusión 5.3.1 Comparación con ABT y AWC 530 The Sixth Intl. La Figura 10 muestra que el rendimiento promedio de UMA es ligeramente mejor que AWC para el problema disperso. UMA supera a AWC en la resolución del problema crítico, como se muestra en la Figura 11. Se observó que la última estrategia falló en algunos casos de prueba. Sin embargo, como se observa en la Figura 12, ambas estrategias son muy eficientes al resolver el problema denso, con AWC mostrando un rendimiento ligeramente mejor. El rendimiento de UMA, en el peor caso (complejidad temporal), es similar al de todas las estrategias evaluadas. El peor caso ocurre cuando se alcanzan todos los posibles estados globales de la búsqueda. Dado que solo unos pocos agentes tienen el derecho de cambiar sus asignaciones variables en una ronda de negociación, se reduce el número de ciclos computacionales redundantes y mensajes de información. Como observamos en el retroceso en ABT y AWC, la diferencia en el orden de los mensajes entrantes puede resultar en un número diferente de ciclos computacionales a ser ejecutados por los agentes. 5.3.2 Comparación con DBO El rendimiento computacional de UMA es posiblemente mejor que el de DBO por las siguientes razones: • UMA puede garantizar que habrá una reasignación de variables después de cada ronda de negociación, mientras que DBO no puede. • UMA introduce una ronda de comunicación adicional (la de enviar un mensaje y esperar una respuesta) en comparación con DBO, lo cual ocurre debido a la necesidad de comunicar consejos no solicitados. Aunque esto aumenta el costo de comunicación por ronda de negociación, observamos en nuestras simulaciones que el costo de comunicación total incurrido por UMA es menor debido al número significativamente menor de rondas de negociación. • Utilizando UMA, en el peor de los casos, un agente solo realizará 2 o 3 viajes de comunicación por ronda de negociación, tras lo cual el agente o su vecino realizarán una actualización de asignación variable. Usando DBO, este número de viajes de ida y vuelta es incierto ya que cada agente podría tener que aumentar los pesos de las restricciones violadas hasta que un agente tenga una mejora positiva; esto podría resultar en un bucle infinito [3]. 6. CONCLUSIÓN Aplicando la negociación automatizada al DCSP, este documento ha propuesto un protocolo que prescribe el razonamiento genérico de un agente de DCSP en una arquitectura BDI. Nuestro trabajo muestra que varios algoritmos DCSP conocidos, a saber, ABT, AWC y DBO, pueden ser descritos como mecanismos que comparten el mismo protocolo propuesto, y solo difieren en las estrategias empleadas para los pasos de razonamiento por ronda de negociación según lo dicta el protocolo. Importante, esto significa que podría proporcionar un marco unificado para DCSP que no solo brinde una visión más clara desde la perspectiva de un agente BDI de los enfoques existentes de DCSP, sino que también abra oportunidades para mejorar o desarrollar nuevas estrategias. Hacia el último, hemos propuesto y formulado una nueva estrategia: la estrategia UMA. Los resultados empíricos y nuestra discusión sugieren que UMA es superior a ABT, AWC y DBO en algunos aspectos específicos. Se observó en nuestras simulaciones que UMA posee la propiedad de completitud. El trabajo futuro intentará establecer formalmente esta propiedad, así como formalizar otros algoritmos DSCP existentes como mecanismos de negociación BDI, incluido el reciente esfuerzo que emplea un mediador de grupo [5]. La idea de que los agentes de DCSP utilicen diferentes estrategias en el mismo entorno también será investigada. 7. REFERENCIAS [1] P. J. Modi, H. Jung, M. Tambe, W.-M. Shen y S. Kulkarni, Asignación dinámica de recursos distribuidos: un enfoque de satisfacción de restricciones distribuidas, en Notas de Conferencia en Ciencias de la Computación, 2001, p. 264. [2] H. Schlenker y U. Geske, Simulación de grandes redes ferroviarias utilizando satisfacción de restricciones distribuidas, en 2da Conferencia Internacional IEEE sobre Informática Industrial (INDIN-04), 2004, pp. 441-446. [3] M. Yokoo, Satisfacción de restricciones distribuidas: Fundamentos de cooperación en sistemas multiagente. Springer Verlag, 2000, Serie Springer sobre Tecnología de Agentes. [4] M. Yokoo, E. H. Durfee, T. Ishida y K. Kuwabara, El problema de satisfacción de restricciones distribuidas: formalización y algoritmos, IEEE Transactions on Knowledge and Data Engineering, vol. 10, no. 5, pp. 673-685, septiembre/octubre 1998. [5] R. Mailler y V. Lesser, Uso de mediación cooperativa para resolver problemas de satisfacción de restricciones distribuidas, en Actas de la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS-04), 2004, pp. 446-453. [6] E. Tsang, Fundamentos de la Satisfacción de Restricciones. Academic Press, 1993. [7] R. Mailler, R. Vincent, V. Lesser, T. Middlekoop, y J. Shen, Negociación cooperativa en tiempo real suave para la asignación de recursos distribuidos, Simposio de otoño de AAAI sobre Métodos de Negociación para Sistemas Cooperativos Autónomos, noviembre de 2001. [8] M. Yokoo, K. Suzuki, y K. Hirayama, Satisfacción de restricciones distribuidas seguras: alcanzando acuerdos sin revelar información privada, Inteligencia Artificial, vol. 161, no. 1-2, pp. 229-246, 2005. [9] J. S. Rosenschein y G. Zlotkin, Reglas de Encuentro. El MIT Press, 1994. [10] M. Yokoo y K. Hirayama, Algoritmo de satisfacción de restricciones distribuidas para problemas locales complejos, en Actas de la Tercera Conferencia Internacional sobre Sistemas Multiagente (ICMAS-98), 1998, pp. 372-379. [11] M. E. Bratman, Intenciones, Planes y Razón Práctica. Harvard University Press, Cambridge, M.A, 1987. [12] G. Weiss, Ed., Sistema Multiagente: Un Enfoque Moderno para la Inteligencia Artificial Distribuida. The MIT Press, Londres, Reino Unido, 1999. [13] S. Minton, M. D. Johnson, A. B. Philips y P. Laird, Minimización de conflictos: Un método heurístico de reparación para problemas de satisfacción de restricciones y programación, Inteligencia Artificial, vol. e58, núm. 1-3, pp. 161-205, 1992. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 531