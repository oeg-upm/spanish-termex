Nuggets interesantes y su impacto en las preguntas de definición respondiendo a Kian-Wei Kor Departamento de Informática Escuela de Computación Universidad Nacional de Singapur dkor@comp.nus.edu.sg Tat-Seng Chua Departamento de Ciencias de la Computación Escuela de Computación de la Universidad Nacional de Singapur@comp.nus.edu.sg Enfoques actuales abstractos para identificar oraciones de definición en el contexto de la respuesta de preguntas implica principalmente el uso de patrones lingüísticos o sintácticos para identificar pepitas informativas. Esto es insuficiente ya que no abordan el factor de novedad que también debe poseer una pepita definitiva. Este documento propone abordar la deficiencia construyendo un modelo de interés humano a partir del conocimiento externo. Se espera que dicho modelo permita el cálculo del interés humano en la oración con respecto al tema. Comparamos y contrastamos nuestro modelo con los modelos de respuesta de preguntas de definición actuales para mostrar que la interesante juega un factor importante en la respuesta de las preguntas de definición. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: modelos de recuperación;H.1.2 [Sistemas de usuario/máquina]: Algoritmos de términos generales de factores humanos, factores humanos, experimentación 1. Pregunta de definición Respuesta de respuesta Definición La respuesta se introdujo por primera vez en la Conferencia de recuperación de texto de la Conferencia de respuesta de respuesta a la tarea principal en 2003. Las preguntas de definición, también llamadas otras preguntas en los últimos años, se definen de la siguiente manera. Dada un tema de pregunta X, la tarea de un sistema de control de calidad definitivo es similar a responder la pregunta ¿Qué es X?¿O quién es x?. El sistema de control de calidad definitivo es buscar a través de un corpus de noticias y devolver un conjunto de respuestas que mejor describe el tema de la pregunta. Cada respuesta debe ser una pepita única específica del tema que constituya una faceta en la definición del tema de la pregunta.1.1 Los dos aspectos de las pepitas de tema oficialmente, las pepitas de respuesta específicas del tema o simplemente las pepitas de temas se describen como pepitas informativas. Cada pepita informativa es un fragmento de oración que describe alguna información objetiva sobre el tema. Dependiendo del tipo de tema y el dominio, esto puede incluir propiedades del tema, relaciones que el tema tiene con una entidad estrechamente relacionada o eventos que le sucedieron al tema. Desde la observación de la respuesta establecida para la respuesta de preguntas de definición de TREC 2003 a 2005, parece que un número significativo de temas que las pepitas no pueden describirse simplemente como pepitas informativas. Más bien, estas pepitas de tema tienen una calidad de trivia asociada con ellas. Por lo general, estos están fuera de la información ordinaria sobre un tema que puede despertar un interés de los lectores humanos. Por esta razón, decidimos definir las pepitas de respuesta que pueden evocar el interés humano como pepitas interesantes. En esencia, las pepitas interesantes responden las preguntas por las que es X famosa?, ¿Qué define x?¿O de qué se trata extraordinariamente x?. Ahora tenemos dos perspectivas muy diferentes sobre lo que constituye una respuesta a las preguntas de definición. Una respuesta puede ser una información objetiva importante sobre el tema o algún aspecto novedoso e interesante sobre el tema. Esta dualidad de informatividad e interesante se puede observar claramente en las cinco pepitas de respuesta vitales para un tema TREC 2005 de George Foreman. Ciertas respuestas nuggets son más informativas, mientras que otras pepitas son más interesantes en la naturaleza. Nuggets informativos: se graduó de Job Corps.- Se convirtió en el campeón mundial más antiguo en la historia del boxeo. Nuggets interesantes: ha prestado su nombre a la línea de productos de preparación de alimentos.- Waveed American Flag después de ganar el campeonato de las Olímpicas de 1968.- Regresó al boxeo después de 10 años. Como boxeador profesional de peso pesado profesional afroamericano, un lector humano promedio encontraría que las últimas tres pepitas sobre George Foreman son interesantes porque los boxeadores generalmente no prestan sus nombres a los productos de preparación de alimentos, ni los boxeadores se retiran durante 10 años antes de regresar al ring y convertirse en el ring y convertirse enEl campeón de boxeo más antiguo del mundo. El onda de Foreman de la bandera estadounidense en los Juegos Olímpicos es interesante porque la acción inocente hizo que algunos afroamericanos acusaran a Foreman de ser un tío Tom. Como se ve aquí, Nuggets interesantes tiene un factor sorpresa o una calidad única que los hace interesantes para los lectores humanos.1.2 Identificación de pepitas interesantes Dado que la descripción oficial original de las definiciones comprende identificar pepitas informativas, la mayoría de las investigaciones se han centrado completamente en identificar pepitas informativas. En este artículo, nos centramos en explorar las propiedades de las pepitas interesantes y desarrollar formas de identificar pepitas tan interesantes. Un sistema de respuesta de preguntas de definición de modelo de interés humano se desarrolla con énfasis en la identificación de pepitas interesantes para evaluar el impacto de las pepitas interesantes en el desempeño de un sistema de respuesta de preguntas de definición. Además, experimentamos con la combinación del modelo de interés humano con un sistema de respuesta de preguntas de definición basado en patrones léxicos para capturar pepitas informativas e interesantes.2. Trabajo relacionado Actualmente hay dos métodos generales para la respuesta de preguntas de definición. El método más común utiliza un enfoque léxico basado en el patrón fue propuesto por primera vez por Blair-Goldensohn et al.[1] y Xu et al.[14]. Ambos grupos usaron predominantemente patrones como cópulas y apositivos, así como patrones lexicosintácticos diseñados manualmente para identificar oraciones que contienen pepitas informativas. Por ejemplo, Xu et al.Usó 40 patrones estructurados definidos manualmente en su sistema de respuesta de preguntas de definición de 2003. Desde entonces, en un intento por capturar una clase más amplia de pepitas informativas, se han creado muchos sistemas de complejidad creciente. Un sistema reciente de Harabagiu et al.[6] creó un sistema de respuesta de preguntas de definición que combina el uso de 150 patrones positivos y negativos definidos manualmente, las relaciones con la entidad nombradas y plantillas de extracción de información especialmente elaboradas para 33 dominios objetivo. Aquí, una plantilla de músicos puede contener patrones léxicos que identifican información como el estilo musical de los músicos, las canciones cantadas por el músico y la banda, si los hay, a los que pertenece el músico. Como se puede imaginar, este es un enfoque intensivo de conocimiento que requiere un lingüista experto para definir manualmente todos los patrones léxicos o sintácticos posibles necesarios para identificar tipos específicos de información. Este proceso requiere mucho trabajo manual, experiencia y no es escalable. Esto conduce al desarrollo del enfoque de patrón suave de Cui et al.[4, 11]. En lugar de codificar patrones manualmente, las respuestas a las evaluaciones de respuesta de preguntas de definición anteriores se convirtieron en patrones genéricos y se capacita un modelo probabilístico para identificar tales patrones en las oraciones. Dada una posible oración de respuesta, el modelo probabilístico genera una probabilidad que indica cómo es probable que la oración coincida con uno o más patrones que el modelo ha visto en el entrenamiento. Se ha demostrado que dicho enfoque de patrones léxicososintácticos es experto en identificar pepitas informativas objetivas, como una fecha de nacimiento de personas, o el nombre de un CEO de compañías. Sin embargo, estos patrones son aplicables a nivel mundial a todos los temas o a un conjunto específico de entidades como músicos u organizaciones. Esto contrasta directamente con las pepitas interesantes que son altamente específicas para los temas individuales y no para un conjunto de entidades. Por ejemplo, las pepitas interesantes para George Foreman son específicas, solo George Foreman y ningún otro boxeador o ser humano. La especificidad del tema o la relevancia del tema es, por lo tanto, un criterio importante que ayuda a identificar pepitas interesantes. Esto lleva a la exploración del segundo enfoque basado en la relevancia que se ha utilizado en la respuesta de las preguntas de definición. Predominantemente, este enfoque se ha utilizado como un método de respaldo para identificar oraciones de definición cuando el método primario de los patrones léxicosisintácticos no pudo encontrar un número suficiente de pepitas informativas [1]. Un enfoque similar también se ha utilizado como sistema de referencia para TREC 2003 [14]. Más recientemente, Chen et al.[3] Adaptó un modelo de lenguaje bi-gram o bi-término para la respuesta de preguntas de definición. En general, el enfoque basado en la relevancia requiere un corpus de definición que contenga documentos altamente relevantes para el tema. El sistema de referencia en TREC 2003 simplemente usa las palabras temáticas como su corpus de definición. Blair-Goldensohn et al.[1] utiliza un alumno de máquina para incluir en las oraciones de Corpus definitonales que probablemente sean definitionales. Chen et al.[3] Recoja fragmentos de Google para construir su corpus de definición. Desde el corpus de definición, se crea un vector centralide de definición o se selecciona un conjunto de palabras centroides. Este vector centralide o un conjunto de palabras centroides se considera muy indicativas del tema. Luego, los sistemas pueden usar este centroide para identificar respuestas de definición utilizando una variedad de métricas de distancia para comparar con las oraciones encontradas en el conjunto de documentos recuperados para el tema. Blairgoldensohn et al.[1] utiliza similitud de coseno para clasificar las oraciones por centralidad. Chen et al.[3] construye un modelo de lenguaje BigRam utilizando los 350 términos de fragmentos de Google que se producen más frecuentes, descritos en su documento como un centroide ordenado, para estimar la probabilidad de que una oración sea similar al centroide ordenado. Como se describe aquí, el enfoque basado en la relevancia es altamente específico para los temas individuales debido a su dependencia de un corpus de definición específico del tema. Sin embargo, si las oraciones individuales se ven como un documento, los enfoques basados en relevancia utilizan esencialmente las palabras centroides específicas del tema recopiladas como una forma de recuperación de documentos con expansión de consulta automatizada para identificar oraciones muy relevantes. Por lo tanto, dichos métodos identifican oraciones relevantes y no oraciones que contienen pepitas de definición. Sin embargo, el sistema de referencia TREC 2003 [14] superó a todos menos a otro sistema. El modelo de lenguaje bi-término [3] puede informar resultados que son altamente competitivos para los resultados de vanguardia utilizando este enfoque basado en la recuperación. En TREC 2006, una suma ponderada simple de todos los términos modelo con términos ponderados utilizando únicamente los fragmentos de Google superaron a todos los demás sistemas por un margen significativo [7]. Creemos que las pepitas interesantes a menudo vienen en forma de trivia, hechos novedosos o raros sobre el tema que tienden a coincidir fuertemente con la mención directa de las palabras clave del tema. Esto puede explicar por qué el método basado en la relevancia puede funcionar de manera competitiva en la respuesta de las preguntas de definición. Sin embargo, simplemente comparar con un solo vector centralide o un conjunto de palabras centroide puede haber enfatizado sobre relevancia del tema y solo ha identificado pepitas de definición interesantes de manera indirecta. Aún así, los métodos de recuperación basados en relevancia se pueden utilizar como punto de partida para identificar pepitas interesantes. Describiremos cómo ampliamos tales métodos para identificar pepitas interesantes en la siguiente sección.3. Modelo de interés humano Obtener un sistema informático para identificar oraciones que un lector humano encontraría interesante es una tarea difícil. Sin embargo, hay muchos documentos en la red mundial que contienen resúmenes concisos y escritos humanos sobre cualquier tema. Además, estos documentos se escriben explícitamente para los seres humanos y contendrán información sobre el tema que la mayoría de los lectores humanos estarían interesados. Suponiendo que podemos identificar dichos documentos relevantes en la web, podemos aprovecharlos para ayudar a identificar respuestas definitionales a dichos temas. Podemos suponer que la mayoría de las oraciones encontradas dentro de estos documentos web contendrán facetas interesantes sobre el tema en cuestión. Esto simplifica enormemente el problema al de encontrar dentro de las oraciones del corpus de Aquaint similares a las que se encuentran en los documentos web. Este enfoque se ha utilizado con éxito en varios sistemas de respuesta a preguntas de Factoid y Listas [11] y creemos que el uso de dicho enfoque para la respuesta de definición u otra respuesta está justificada. La identificación de pepitas interesantes requiere calcular maquinaria para comprender el conocimiento mundial y la visión humana. Esta sigue siendo una tarea muy desafiante y el uso de documentos escritos humanos simplifica drásticamente la complejidad de la tarea. En este documento, informamos sobre dicho enfoque experimentando con un algoritmo de comparación de términos ponderado de distancia de edición de edición de nivel simple simple. Utilizamos el algoritmo de distancia de edición para obtener la similitud de un par de oraciones, con una oración proveniente de recursos web y la otra oración seleccionada del Corpus de Aquaint. A través de una serie de experimentos, mostraremos que incluso un enfoque tan simple puede ser muy efectivo en la respuesta de preguntas definitivas.3.1 Recursos web Existe en los artículos de Internet sobre cualquier tema que un humano pueda pensar. Además, muchos de estos artículos están ubicados en varios sitios web prominentes, lo que los convierte en una fuente de conocimiento mundial fácilmente accesible. Para nuestro trabajo para identificar pepitas interesantes, nos centramos en encontrar artículos cortos de uno o dos páginas en Internet que sean muy relevantes para nuestro tema deseado. Dichos artículos son útiles ya que contienen información concisa sobre el tema. Más importante aún, los artículos están escritos por humanos, para lectores humanos y, por lo tanto, contienen el conocimiento crítico del mundo humano que actualmente no puede capturar un sistema informático. Aprovechamos este conocimiento mundial mediante la recopilación de artículos para cada tema de los siguientes recursos externos para construir nuestro corpus de interés para cada tema. Wikipedia es una enciclopedia de contenido libre basada en la web escrita en colaboración por voluntarios. Este recurso ha sido utilizado por muchos sistemas de respuesta de preguntas como fuente de conocimiento sobre cada tema. Utilizamos una instantánea de Wikipedia tomada en marzo de 2006 e incluimos el artículo más relevante en el Corpus de Interest. Newslibrary es un archivo de búsqueda de artículos de noticias de más de 100 agencias de periódicos diferentes. Para cada tema, descargamos los 50 artículos más relevantes e incluimos el título y el primer párrafo de cada artículo en el Corpus de Interest. Los fragmentos de Google se recuperan emitiendo el tema como una consulta para el motor de búsqueda de Google. De los resultados de la búsqueda, extrajimos los 100 fragmentos principales. Si bien los fragmentos de Google no son artículos, encontramos que proporcionan una amplia cobertura de información autorativa sobre la mayoría de los temas. Debido a su cobertura integral de una amplia variedad de temas, los recursos anteriores forman la mayor parte de nuestro corpus de interés. También extraemos documentos de otros recursos. Sin embargo, como estos recursos son de naturaleza más específica, no siempre obtenemos ningún documento relevante. Estos recursos se enumeran a continuación. Biography.com es el sitio web del canal de cable de la televisión de biografía. El sitio web de los canales contiene biografías de búsqueda en más de 25,000 personas notables. Si el tema es una persona y podemos encontrar una biografía relevante en la persona, la incluimos en nuestro corpus de interés. Bartleby.com contiene una copia de búsqueda de varios recursos, incluida la Enciclopedia de Columbia, el World Factbook y varios diccionarios ingleses.S9.com es un diccionario de biografía sobre más de 33,000 personas notables. Al igual que Biography.com, incluimos la biografía más relevante que podemos encontrar en el Corpus de Interest. Definiciones de Google Google Search Engine ofrece una característica llamada definiciones que proporciona la definición para una consulta, si tiene una. Utilizamos esta función y extraemos cualquier definición que el motor de búsqueda de Google haya encontrado para cada tema en el Corpus de Interest. Figura 1: Arquitectura del modelo de interés humano. Wordnet Wordnet es un conocido léxico semántico electrónico para el idioma inglés. Además de agrupar palabras en inglés en conjuntos de sinónimos llamados sinsets, también proporciona una definición breve sobre el significado de las palabras que se encuentran en cada synset. Agregamos esta definición corta, si hay una, en nuestro Corpus de Interest. Tenemos dos usos principales para este Corpus de interés específico del tema, como fuente de oraciones que contienen pepitas interesantes y como modelo de lenguaje unigram de términos temáticos, I. 3.2 Múltiples centroides interesantes que hemos visto que las pepitas interesantes son altamente específicas para un tema. Enfoques basados en relevancia, como el modelo de lenguaje BigRam utilizado por Chen et al.[3] se centran en identificar oraciones altamente relevantes y recoger las pepitas de respuesta de definición como una consecuencia indirecta. Creemos que el uso de una sola colección de palabras centroides ha enfatizado la relevancia del tema y elegir en su lugar usar múltiples centroides. Dado que las oraciones en el corpus de interés de los artículos que recopilamos de Internet probablemente contengan pepitas que son de interés para los lectores humanos, esencialmente podemos usar cada oración como pseudocentroides. Cada oración en el Corpus de intereses esencialmente plantea un aspecto diferente del tema para su consideración como una oración de interés para los lectores humanos. Al realizar una comparación de oraciones por pares entre oraciones en el corpus de intereses y oraciones candidatas recuperadas del Corpus Aquaint, aumentamos el número de comparaciones de oraciones de O (N) a O (NM). Aquí, n es el número de oraciones candidatas potenciales y M es el número de oraciones en el corpus de intereses. A cambio, obtenemos una lista diversa de respuestas clasificadas que son individualmente similares a varias oraciones que se encuentran en el Corpus de interés de los temas. Una respuesta solo puede estar altamente clasificada si es muy similar a una oración en el Corpus de Interest, y también es muy relevante para el tema.3.3 Implementación La Figura 1 muestra la arquitectura del sistema para el sistema de control de calidad de definición basado en el interés humano propuesto. El módulo de recuperación de Aquaint que se muestra en la Figura 1 reutiliza un módulo de recuperación de documentos de un sistema actual de contestadores de preguntas de factoid y de preguntas que hemos implementado. Dado un conjunto de palabras que describen el tema, el módulo de recuperación de Aquaint consulta la expansión de la expansión de Google y busca un índice de documentos de Aquaint para recuperar los 800 documentos más relevantes para su consideración. El módulo de recuperación web, por otro lado, busca los recursos en línea descritos en la Sección 3.1 para obtener documentos interesantes para completar el Corpus de Interés. El Him Ranker, o el Módulo de clasificación del Modelo de Interés Humano, es la implementación de lo que se describe en este documento. El módulo primero construye el modelo de idioma unigram, I, a partir de los documentos web recopilados. Este modelo de idioma se utilizará para soportar la importancia de los términos dentro de las oraciones. A continuación, se utiliza un frase de oraciones para segmentar los 800 documentos recuperados en oraciones individuales. Cada una de estas oraciones puede ser una frase de respuesta potencial que se clasificará independientemente por la interesante. Clasificamos oraciones por interesante utilizando oraciones tanto del Corpus de Interés de documentos externos como del modelo de idioma unigram que creamos anteriormente que usamos para peso. Una oración candidata en nuestros 800 documentos de Aquaint relevantes principales se considera interesante si es muy similar en contenido a una oración que se encuentra en nuestra colección de documentos web externos. Para lograr esto, realizamos una comparación de similitud por pares entre una oración candidata y oraciones en nuestros documentos externos utilizando un algoritmo de distancia de edición a plazo ponderado. Los pesos de término se utilizan para ajustar la importancia relativa de cada término único que se encuentra en el corpus de intereses. Cuando ambas oraciones comparten el mismo término, el puntaje de similitud se incrementa en las dos veces el peso de los términos y cada término diferente disminuye el puntaje de similitud por el peso de términos diferentes. Elegimos el puntaje de similitud más alto para una sentencia candidata como el puntaje del modelo de interés humano para la sentencia candidata. De esta manera, cada oración candidata está clasificada por la interesante. Finalmente, para obtener el conjunto de respuestas, seleccionamos las 12 oraciones más altas y no redundantes como respuestas de definición para el tema.4. Experimentos iniciales El sistema basado en intereses humanos descrito en la sección anterior está diseñado para identificar solo pepitas interesantes y no pepitas informativas. Por lo tanto, se puede describir como un sistema discapacitado que solo trata con la mitad del problema en la respuesta de las preguntas de definición. Esto se hace para explorar cómo la interesante juega un factor en las respuestas de definición. Para comparar y contrastar las diferencias entre las pepitas informativas e interesantes, también implementamos el modelo BigRam de patrón suave propuesto por Cui et al.[4, 11]. Para garantizar resultados comparables, ambos sistemas reciben datos de entrada idénticos. Dado que ambos sistemas requieren el uso de recursos externos, ambos se proporcionan los mismos artículos web recuperados por nuestro módulo de recuperación web. Ambos sistemas también clasifican el mismo conjunto de oraciones candidatas en forma de 800 documentos más relevantes que recuperan nuestro módulo de recuperación de Aquaint. Para los experimentos, utilizamos el conjunto de preguntas TREC 2004 para ajustar los parámetros del sistema y utilizar los conjuntos de preguntas TREC 2005 para probar los dos sistemas. Ambos sistemas se evalúan los resultados utilizando la metodología de puntuación estándar para las definiciones TREC. TREC proporciona una lista de pepitas vitales y bien para cada tema de la pregunta. Cada pregunta se califica en Nugget RetRink (NR) y Nugget Precision (NP) y se calcula una sola puntuación final utilizando la medición F (ver ecuación 1) con β = 3 para enfatizar el retiro de Nugget. Aquí, NR es el número de pepitas vitales devueltas divididas por el número total de pepitas vitales, mientras que NP se calcula utilizando una función de longitud de carácter mínima permitida definida en [12]. La evaluación se realiza automáticamente usando Pourpre V1.0C [10]. Fscore = β2 ∗ np ∗ nr (β2 + 1) np + nr (1) sistema f3-score mejor sistema trec 2005 0.2480 patrón suave (sp) 0.2872 modelo de interés humano (HIT) 0.3031 Tabla 1: rendimiento en trec 2005 set de preguntasFigura 2: rendimiento por tipos de entidades.4.1 Informatividad versus Interesante Nuestro primer experimento compara el rendimiento de identificar únicamente pepitas interesantes contra la identificación únicamente de pepitas informativas. Comparamos los resultados alcanzados por el modelo de interés humano que solo identifican pepitas interesantes con los resultados del modelo de búsqueda de patrones sintácticos, así como el resultado del sistema de definición de mayor rendimiento en TREC 2005 [13]. La Tabla 1 muestra la puntuación F3 de los tres sistemas para el conjunto de preguntas TREC 2005. El modelo de interés humano supera claramente tanto el patrón suave como el mejor sistema TREC 2005 con una puntuación F3 de 0.303. El resultado también es comparable con el resultado de una carrera manual humana, que alcanzó una puntuación F3 de 0.299 en el mismo conjunto de preguntas [9]. Este resultado es una confirmación de que las nuggets interesantes juegan un papel importante en la captura de respuestas de definición, y puede ser más vital que usar información para encontrar patrones léxicos. Para obtener una mejor perspectiva de qué tan bien se desempeña el modelo de interés humano para diferentes tipos de temas, dividimos manualmente los temas de TREC 2005 en cuatro amplias categorías de persona, organización, cosa y evento como se enumeran en la Tabla 3. Estas categorías se ajustan a la división general de TREC de temas de preguntas en 4 tipos principales de entidad [13]. El rendimiento del modelo de interés humano y el modelo BigRam de patrón suave para cada tipo de entidad se pueden ver en la Figura 2. Ambos sistemas exhiben un comportamiento consistente en todos los tipos de entidades, con el mejor rendimiento proveniente de temas de persona y organización y el peor rendimiento de las cosas y los temas de eventos. Esto se puede atribuir principalmente a nuestra selección de recursos basados en la web para el corpus de definición utilizado por ambos sistemas. En general, es más difícil localizar un solo artículo web que describa un evento o un objeto general. Sin embargo, dado el mismo conjunto de información basada en la web, el modelo de interés humano supera constantemente el modelo de patrón suave para los cuatro tipos de entidades. Esto sugiere que el modelo de interés humano está mejor capaz de aprovechar la información que se encuentra en los recursos web para identificar respuestas de definición.5. Refinamientos alentados por los resultados experimentales iniciales, exploramos dos optimización adicional del algoritmo básico.5.1 Ponderación de términos interesantes La palabra trivia se refiere a las cositas de información sin importancia o poco común. Como hemos señalado, las pepitas interesantes a menudo tienen una cualidad trivial que les hace interesarse a los seres humanos. A partir de esta descripción de pepitas y trivia interesantes, planteamos la hipótesis de que es probable que ocurran nuggets interesantes raramente en un corporaciones de texto. Existe la posibilidad de que algunos términos de baja frecuencia realmente sean importantes para identificar pepitas interesantes. Un modelo de lenguaje unigram estándar no capturaría estos términos de baja frecuencia como términos importantes. Para explorar esta posibilidad, experimentamos con tres esquemas de ponderación a término que pueden proporcionar más peso a ciertos términos de baja frecuencia. Los esquemas de ponderación que consideramos incluyen TFIDF de uso común, así como la divergencia teórica de Kullback-Leiber de la información y la divergencia de Jensen-Shannon [8]. TFIDF, o frecuencia de término × frecuencia de documento inverso, es un esquema de ponderación de recuperación de información estándar que equilibra la importancia de un término en un documento y en un corpus. Para nuestros experimentos, calculamos el peso de cada término como tf × log (n nt), donde tf es el término frecuencia, nt es el número de oraciones en el corpus de intereses que tiene el término y n es el número total de oraciones en elCorpus de interés. La divergencia de Kullback-Leibbler (Ecuación 2) también se llama divergencia de KL o entropía relativa, puede considerarse que mide la diferencia entre dos distribuciones de probabilidad. Aquí, tratamos al Corpus de Aquaint como un modelo de idioma unigram de inglés general [15], A, y el Corpus de Interest como un modelo de idioma unigram que consiste en términos específicos del tema y términos generales de inglés, I. Es probable que las palabras en inglés en general tengan distribuciones similares tanto en los modelos de idiomas I como en A. Por lo tanto, el uso de la divergencia de KL como un esquema de ponderación de término hará que se dan pesos fuertes a términos específicos de temas porque su distribución en el corpus de interés ocurren significativamente más a menudo o menos a menudo que en el inglés general. De esta manera, los términos centroides de alta frecuencia, así como los términos raros pero específicos del tema de baja frecuencia, se identifican y se ponderan altamente usando divergencia KL. Dkl (i a) = t i (t) log i (t) a (t) (2) Debido a la distribución de términos de la ley de potencia en el lenguaje natural, solo hay un pequeño número de términos muy frecuentes y una gran cantidad de rarosTérminos tanto en I como en A. Si bien los términos comunes en inglés consisten en palabras de parada, los términos comunes en el corpus específico del tema, I, consisten en palabras de parada y palabras temáticas relevantes. Estas palabras específicas del tema de alta frecuencia ocurren con mucha más frecuencia en I que en A. Como resultado, encontramos que la divergencia de KL tiene un sesgo hacia términos de tema altamente frecuentes, ya que estamos midiendo la disimilitud directa contra un modelo de inglés general donde tales términos temáticos son muy raros. Por esta razón, exploramos otra medida de divergencia como un posible esquema de ponderación de término. La divergencia o la divergencia de Jensen-Shannon se extiende sobre la divergencia de KL como se ve en la ecuación 3. Al igual que con la divergencia KL, también usamos la divergencia JS para medir la diferencia entre nuestros dos modelos de lenguaje, I y A. DJS (i a) = 1 2 ¢ dkl i i+a 2 ¡+dkl a i+a 2 ¡£ (3 (3) Figura 3: rendimiento por varios esquemas de ponderación a término en el modelo de interés humano. Sin embargo, la divergencia JS tiene propiedades adicionales1 de ser simétricos y no negativos como se ve en la ecuación 4. La propiedad simétrica ofrece una medida más equilibrada de disimilitud y evita el sesgo que tiene la divergencia KL. DJS (i a) = djs (a i) = 0 i = a> 0 i <> a (4) Realizamos otro experimento, sustituyendo el esquema de ponderación del modelo de lenguaje unigram que utilizamos en los experimentos iniciales con los esquemas de ponderación de tres términos descritos anteriormente. Como referencia de límite inferior, incluimos un esquema de ponderación de término que consiste en una constante 1 para todos los términos. La Figura 3 muestra el resultado de aplicar los cinco esquemas de ponderación a término en el modelo de interés humano. TFIDF realizó lo peor como habíamos anticipado. La razón es que la mayoría de los términos solo aparecen una vez dentro de cada oración, lo que resulta en una frecuencia de término de 1 para la mayoría de los términos. Esto hace que el componente de las FDI sea el factor principal en la puntuación de las oraciones. A medida que calculamos la frecuencia de documentos inversos para los términos en el Corpus de Intereses recopilado de los recursos web, las FDI sean de gran peso, términos de temas de alta frecuencia y términos relevantes. Esto da como resultado que TFIDF favorezca todos los términos de baja frecuencia en términos de alta frecuencia en el corpus de intereses. A pesar de esto, el esquema de ponderación TFIDF solo obtuvo un ligero 0.0085 inferior a nuestra referencia límite inferior de pesos constantes. Vemos esto como una indicación positiva de que los términos de baja frecuencia pueden ser útiles para encontrar pepitas interesantes. Tanto la divergencia KL como JS funcionó marginalmente mejor que el esquema probabilístico del modelo de lenguaje uniforme que utilizamos en nuestros experimentos iniciales. A partir de la inspección de la lista de términos ponderados, observamos que, si bien los términos relevantes de baja frecuencia aumentaron en resistencia, los términos relevantes de alta frecuencia aún dominan la parte superior de la lista de términos ponderado. Solo un puñado de términos de baja frecuencia se ponderaron tan fuertemente como las palabras clave de los temas y combinadas con su baja frecuencia, pueden haber limitado el impacto de la re-peso de dichos términos. Sin embargo, creemos que a pesar de esto, la divergencia de Jensen-Shannon proporciona un aumento pequeño pero medible en el rendimiento de nuestro modelo de interés humano.1 JS Divergencia también tiene la propiedad de estar limitado, lo que permite que los resultados se traten como una probabilidad si es necesario. Sin embargo, la propiedad limitada no se requiere aquí, ya que solo estamos tratando la divergencia calculada por la divergencia JS como Pesos de término 5.2 Selección de recursos web en uno de nuestros experimentos iniciales, observamos que la calidad de los recursos web incluidos en el Corpus de Interest puede tener unImpacto directo en los resultados que obtenemos. Queríamos determinar qué impacto tiene la elección de los recursos web en el desempeño de nuestro modelo de interés humano. Por esta razón, dividimos nuestra colección de recursos web en cuatro grupos principales enumerados aquí: N - Noticias: Título y primer párrafo de los 50 artículos más relevantes que se encuentran en el biblioteca de periódicos. W - Wikipedia: texto del artículo más relevante que se encuentra en Wikipedia. S - Fragmentos: fragmentos extraídos de los 100 enlaces más relevantes después de consultar Google. M - Fuentes misceláneas: combinación de contenido (cuando esté disponible) de fuentes secundarias, incluidas Biography.com, S9.com, Bartleby.com, definiciones de Google y definiciones de WordNet. Realizamos una gama de carreras en el conjunto de preguntas TREC 2005 utilizando todas las combinaciones posibles de los cuatro grupos de recursos web anteriores para identificar la mejor combinación posible. Todas las ejecuciones se realizaron en el modelo de interés humano utilizando la divergencia JS como esquema de ponderación del término. Las ejecuciones se clasificaron en puntaje F3 descendente y las 3 mejores ejecuciones de mejor rendimiento para cada clase de entidad se enumeran en la Tabla 2 junto con las puntuaciones F3 informadas anteriores de la Figura 2 como referencia de línea de base. Se puede observar una tendencia consistente para cada clase de entidad. Para los temas de persona y eventos, los artículos de biblioteca de periódicos son la principal fuente de pepitas interesantes con fragmentos de Google y artículos diversos que ofrecen evidencia de apoyo adicional. Esto parece intuitivo para los eventos, ya que los periódicos se centran predominantemente en informar eventos de ruptura y, por lo tanto, son excelentes fuentes de pepitas interesantes. Esperábamos que Wikipedia en lugar de los artículos de noticias fuera una mejor fuente de datos interesantes sobre las personas y nos sorprendió descubrir que los artículos de noticias superaron a Wikipedia. Creemos que la razón es porque las personas seleccionadas como temas hasta ahora han sido celebridades o figuras públicas bien conocidas. Es probable que los lectores humanos estén interesados en los eventos de noticias que destinen a estas personalidades. Por el contrario de los temas de organización y cosas, la mejor fuente de pepitas interesantes proviene del artículo más relevante de Wikipedias sobre el tema con Google Fnippets nuevamente proporcionando información adicional para las organizaciones. Con un Oracle que puede clasificar temas por clase de entidad con una precisión del 100% y mediante el uso de los mejores recursos web para cada clase de entidad como se muestra en la Tabla 2, podemos lograr una puntuación F3 de 0.3158.6. Uniendo la información con interesante, hasta ahora hemos estado comparando el modelo de interés humano con el modelo de patrón suave para comprender las diferencias entre pepitas interesantes e informativas. Sin embargo, desde la perspectiva de un lector humano, las pepitas informativas e interesantes son útiles y definitivamente. Las pepitas informativas presentan una descripción general del tema, mientras que las pepitas interesantes brindan a los lectores una profundidad y una visión adicionales al proporcionar aspectos novedosos y únicos sobre el tema. Creemos que un buen sistema de respuesta de preguntas de definición debería proporcionar al lector una mezcla combinada de ambos tipos de pepitas como un conjunto de respuestas de definición. Rango de persona orgía de la organización del evento Base Base Unigram Scheming, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S W+S W+S W+S W+S W+S W+S W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Tabla 2: Top 3 ejecutas usando diferentes recursos web para cada clase de entidad ahora ahora We Now We Wetener dos expertos muy diferentes para identificar definiciones. El modelo BigRam de patrón suave propuesto por Cui et al.es un experto en la identificación de pepitas informativas. El modelo de interés humano que hemos descrito en este documento, por otro lado, es un experto en encontrar pepitas interesantes. Inicialmente esperábamos unificar los dos sistemas de respuesta de preguntas de definición separadas aplicando un método de aprendizaje de conjunto [5], como votar o aumentar para lograr una buena mezcla de pepitas informativas e interesantes en nuestro conjunto de respuestas. Sin embargo, ninguno de los métodos de aprendizaje de conjunto que intentamos podría superar a nuestro modelo de interés humano. La razón es que ambos sistemas están recogiendo oraciones muy diferentes como respuestas de definición. En esencia, nuestros dos expertos están en desacuerdo sobre qué oraciones son definitivas. En las 10 oraciones principales de ambos sistemas, solo el 4.4% de estas oraciones aparecieron en ambos conjuntos de respuestas. Las respuestas restantes fueron completamente diferentes. Incluso cuando examinamos las 500 oraciones principales generadas por ambos sistemas, la tasa de acuerdo seguía siendo un 5.3%extremadamente baja. Sin embargo, a pesar de la baja tasa de acuerdo entre ambos sistemas, cada sistema individual aún puede alcanzar una puntuación F3 relativamente alta. Existe la clara posibilidad de que cada sistema pueda seleccionar diferentes oraciones con diferentes estructuras sintácticas, pero en realidad tiene el mismo contenido semántico o similar. Esto podría dar como resultado que ambos sistemas tengan las mismas pepitas marcadas como correctas a pesar de que las oraciones de respuesta de origen son estructuralmente diferentes. Desafortunadamente, no podemos verificar esto automáticamente, ya que el software de evaluación que estamos utilizando no informa correctamente las pepitas de respuesta identificadas correctamente. Para verificar si ambos sistemas seleccionan las mismas pepitas de respuesta, seleccionamos aleatoriamente un subconjunto de 10 temas del conjunto de preguntas TREC 2005 e identificamos manualmente las pepitas de respuesta correctas (según lo definido por los accesorios TREC) de ambos sistemas. Cuando comparamos las pepitas de respuesta encontradas por ambos sistemas para este subconjunto de temas, encontramos que la tasa de acuerdo de pepita entre ambos sistemas era del 16,6%. Si bien la tasa de acuerdo de Nugget es más alta que la tasa de acuerdo de oración, ambos sistemas generalmente siguen recogiendo diferentes pepitas de respuesta. Vemos esto como una indicación adicional de que las definiciones están formadas por una mezcla de pepitas informativas e interesantes. También es una indicación de que, en general, las pepitas interesantes e informativas son bastante diferentes en la naturaleza. Por lo tanto, existen razones racionales y motivación práctica para unificar respuestas de los enfoques basados en patrones y basados en el corpus. Sin embargo, las diferencias entre los dos sistemas también causan problemas cuando intentamos combinar ambos conjuntos de respuestas. Actualmente, el mejor enfoque que encontramos para combinar ambos conjuntos de respuestas es fusionar y volver a clasificar ambos conjuntos de respuestas con acuerdos de aumento. Primero normalizamos las 1,000 oraciones clasificadas principales de cada sistema, para obtener el puntaje del modelo de interés humano normalizado, él (s) y el puntaje del modelo BigRam de patrón suave normalizado, SP (s), para cada oración única, s.Para cada oración, los dos puntajes separados para luego se unifican en una sola puntuación utilizando la Ecuación 5. Cuando solo un sistema cree que la oración es definitiva, simplemente conservamos que los sistemas normalizaron la puntuación como puntaje unificado. Cuando ambos sistemas están de acuerdo en que la oración es definitiva, el puntaje de oraciones se ve impulsado por el grado de acuerdo entre ambos sistemas. Puntaje (s) = max (shim, ssp) 1-min (shim, ssp) (5) Para mantener un conjunto diverso de respuestas, así como para garantizar que las oraciones similares no tengan una clasificación similar, volvemos a clasificar aún másNuestra lista combinada de respuestas utilizando relevancia marginal máxima o MMR [2]. Usando el enfoque descrito aquí, logramos una puntuación F3 de 0.3081. Esta puntuación es equivalente a la puntuación inicial del modelo de interés humano de 0.3031, pero no supera al modelo de modelo de interés humano optimizado.7. Conclusión Este documento ha presentado una perspectiva novedosa para responder preguntas de definición a través de la identificación de pepitas interesantes. Las pepitas interesantes son información poco común sobre el tema que puede evocar una curiosidad de los lectores humanos. La noción de un lector humano promedio es una consideración importante en nuestro enfoque. Esto es muy diferente del enfoque de patrón léxico-sintáctico donde el contexto de un lector humano ni siquiera se considera al encontrar respuestas para la respuesta de las preguntas de definición. Usando esta perspectiva, hemos demostrado que utilizando una combinación de un corpus externo cuidadosamente seleccionado, que coincide con múltiples centroides y teniendo en cuenta términos raros pero altamente específicos de temas, podemos construir un módulo de respuesta de pregunta definitivo que se centre más en identificar pepitas que sonde interés para los seres humanos. Los resultados experimentales han demostrado que este enfoque puede superar significativamente a los sistemas de contestadores de preguntas definitionales de última generación. Además, demostramos que se requieren al menos dos tipos diferentes de respuestas que se requieren nuggets para formar un conjunto más exhaustivo de respuestas de definición. Lo que parece ser un buen conjunto de respuestas de definición es una información general que proporciona una visión general informativa rápida mezclada con algunos aspectos novedosos o interesantes sobre el tema. Por lo tanto, creemos que un buen sistema de respuesta de preguntas de definición necesitaría recoger tipos de pepitas informativos e interesantes para proporcionar una cobertura de definición completa sobre todos los aspectos importantes del tema. Si bien hemos intentado construir dicho sistema combinando nuestro modelo de interés humano propuesto con el modelo BigRam de patrón blando de Cui et al., Las diferencias inherentes entre ambos tipos de pepitas aparentemente causadas por las bajas tasas de acuerdo entre ambos modelos han hecho que esto sea difíciltarea. De hecho, esto es natural ya que los dos modelos han sido diseñados para identificar dos tipos muy diferentes de respuestas de definición utilizando tipos de características muy diferentes. Como resultado, actualmente solo podemos lograr un sistema híbrido que tenga el mismo nivel de rendimiento que nuestro modelo de interés humano propuesto. Abordamos el problema de la respuesta de las preguntas definitivas desde una perspectiva novedosa, con la noción de que el factor de interés juega un papel en la identificación de respuestas definitivas. Aunque los métodos que utilizamos son simples, se ha demostrado que experimentalmente son efectivos. Nuestro enfoque también puede proporcionar una idea de algunas anomalías en los ensayos de respuesta de preguntas de definición pasadas. Por ejemplo, el sistema de definición superior en la reciente evaluación de TREC 2006 pudo superar significativamente a todos los demás sistemas utilizando probabilidades unigram relativamente simples extraídas de los fragmentos de Google. Sospechamos que el principal contribuyente a la organización de temas de tipo de entidad de rendimiento de sistemas de la Universidad DePauw, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefónica de España, Lions Club International, Amway, McDonalds Corporation, Harley-Davidson, EE. UU. Academia Naval, OPEP, OTAN, Oficina Internacional de la Unión Postal Universal (UPU), Organización de la Conferencia Islámica (OIC), PBGC Persona Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il Thing F16, Bollywood, Viagra, Howdy Dooddod Show, Museum, Museo, Meteoritas, Virginia Wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, Kudzu, Medalla de Honor de EE. UU., Tsunami, Genoma, Acuerdo de Alimentos para el Propiedad, Shiite, Kinmen Island Event Submarine Kursk Fregads, Miss Universo 2000Coronada, Port Arthur Massacre, Francia gana la Copa Mundial en fútbol, clips de avión cables de cable en el complejo italiano, tiroteo en la escuela Kip Kinkel, accidente del vuelo 990 de Egyptairio, Preakness 1998, primer debate presidencial de 2000 Bush-Gore, 1998 acusación y juicio de Susan McDougal, Regreso de Hong Kong a la soberanía china, 1998 Juegos Olímpicos de Nagano, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens Eruption, 1998 Baseball World Series, Hindenburg Desastre, Huracán Mitch Tabla 3: Temas de TREC 2005 agrupados porEl tipo de entidad es el algoritmo de PageRank Googles, que consideró principalmente el número de vínculos, tiene un efecto indirecto de clasificar los documentos web por grado de interés humano. En nuestro trabajo futuro, buscamos mejorar aún más el sistema combinado incorporando más evidencia en apoyo de las respuestas de definición correctas o para filtrar las respuestas obviamente incorrectas.8. Referencias [1] S. Blair-Goldensohn, K. R. McKeown y A. H. Schlaikjer. Un enfoque híbrido para las preguntas de definición de la pista de control de calidad. En TREC 03: Actas de la 12ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell y J. Goldstein. El uso de MMR, Reranking basado en la diversidad para reordenar documentos y producir resúmenes. En investigación y desarrollo en recuperación de información, páginas 335-336, 1998. [3] Y. Chen, M. Zhou y S. Wang. Respuestas para volver a ser un control de definición utilizando el modelado de idiomas. En Actas de la 21a Conferencia Internacional sobre Lingüística Computacional y la 44ª Reunión Anual de la Asociación de Lingüística Computacional, páginas 1081-1088, Sydney, Australia, julio de 2006. Asociación de Lingüística Computacional.[4] H. Cui, M.-Y. Kan y T.-S.Chua. Modelos genéricos de patrones suaves para la respuesta de preguntas de definición. En Sigir 05: Actas de la 28ª Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 384-391, Nueva York, NY, EE. UU., 2005. ACM Press.[5] T. G. Dietterich. Métodos de conjunto en el aprendizaje automático. Notas de conferencia en informática, 1857: 1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl y P. Wang. Empleando dos sistemas de respuesta de preguntas en TREC 2005. En TREC 05: Actas de la 14ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible y B. Webber. Experimentos en la Universidad de Edimburgo para la pista de control de calidad TREC 2006. En Trec 06 Notebook: Actas de la 14ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2006. Instituto Nacional de Normas y Tecnología.[8] J. Lin. Medidas de divergencia basadas en la entropía de Shannon. IEEE Transactions on Information Theory, 37 (1): 145 - 151, enero de 1991. [9] J. Lin, E. Abels, D. Demner -Fushman, D. W. Oard, P. Wu e Y. Wu. Una colección de pistas en Maryland: Hard, Enterprise, QA y Genomics, ¡Oh, Dios mío! En TREC 05: Actas de la 14a Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. [10] J. Lin y D. Demner-Fushman. Evaluar automáticamente las respuestas a las preguntas de definición. En Actas de la Conferencia de Tecnología del Lenguaje Humano y la Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, páginas 931-938, Vancouver, Columbia Británica, Canadá, octubre de 2005. Asociación de Lingüística Computacional.[11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S.Chua y M.-Y. Kan. Uso del análisis de relaciones sintácticas y semánticas en la respuesta de cuestión. En TREC 05: Actas de la 14a Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees. Descripción general de la pista de respuesta a la pregunta TREC 2003. En la Conferencia de Recuperación de Textos 2003, Gaithersburg, Maryland, 2003. Instituto Nacional de Normas y Tecnología.[13] E. M. Voorhees. Descripción general de la pista de respuesta a la pregunta TREC 2005. En TREC 05: Actas de la 14ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. Instituto Nacional de Normas y Tecnología.[14] J. Xu, A. Licuanan y R. Weischedel. TREC 2003 QA en BBN: Respondiendo preguntas de definición. En TREC 03: Actas de la 12ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2003. [15] D. Zhang y W. S. Lee. Un enfoque de modelado de idiomas para la respuesta de las preguntas del pasaje. En TREC 03: Actas de la 12ª Conferencia de recuperación de texto, Gaithersburg, Maryland, 2003.