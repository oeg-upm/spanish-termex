Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro de Recuperación de Información Inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu RESUMEN Los métodos de bajo costo para adquirir juicios de relevancia pueden ser una bendición para los investigadores que necesitan evaluar nuevas tareas o temas de recuperación pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se puedan confiar al ser reutilizados para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que las evaluaciones sean reutilizables: la confianza en una evaluación de nuevos sistemas puede ser evaluada con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren esfuerzo adicional del evaluador. Usar este método garantiza prácticamente la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y Descriptores de Asignaturas: H.3 Almacenamiento y Recuperación de Información; H.3.4 Sistemas y Software: Evaluación del Rendimiento Términos Generales: Experimentación, Medición, Confiabilidad 1. INTRODUCCIÓN Imagina a un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha construido un sistema para realizar la tarea y quiere evaluarlo. Dado que la tarea es nueva, es poco probable que existan juicios de relevancia vigentes. Ella no tiene el tiempo ni los recursos para juzgar cada documento, ni siquiera cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tenga un grado razonable de confianza en sus conclusiones. Pero ¿qué sucede cuando ella desarrolla un nuevo sistema y necesita evaluarlo? ¿Otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación en recuperación de información, pero es solo un problema parcialmente resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento; simplemente hay demasiados. La solución utilizada por NIST en TREC (Text REtrieval Conference) es el método de agrupación [19, 20]: todos los sistemas competidores contribuyen con N documentos a un grupo, y cada documento en ese grupo es evaluado. Este método crea grandes conjuntos de juicios que son reutilizables para entrenar o evaluar nuevos sistemas que no contribuyeron al conjunto [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación proporciona miles de juicios de relevancia, pero requiere muchas horas de tiempo de anotadores (remunerados). Como resultado, ha habido una serie de artículos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al. [11], Zobel [21], Sanderson y Joho [17], Carterette et al. [8], y Aslam et al. [4], entre otros. Como veremos, los juicios que estos métodos producen pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. ¿Volviendo a nuestra investigadora hipotética, puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia al evaluar sistemas no vistos. Si bien podemos decir que tuvo razón el 75% del tiempo, o que tuvo una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas es probable que estén equivocados ni cuán seguros deberíamos estar en uno en particular. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es qué tan precisamente podemos evaluar los nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no haya recuperado ninguno de los documentos evaluados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones, y, más importante aún, si podemos confiar en nuestras estimaciones de confianza. Aunque la confianza no sea alta, siempre y cuando podamos confiar en ello, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, por pequeño que sea, se vuelve reutilizable en cierta medida. Pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir las evaluaciones de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponible para los investigadores crecería de forma exponencial con el tiempo. EVALUACIÓN ROBUSTA Anteriormente dimos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y tenemos, por ejemplo, un 75% de confianza en que el sistema A es mejor que el sistema B, nos gustaría que no haya más del 25% de posibilidad de que nuestra evaluación de la calidad relativa de los sistemas cambie a medida que continuamos evaluando documentos. Nuestra evaluación debe ser robusta ante juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea menor que cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Otras tareas de evaluación podrían ser definidas; estimar la magnitud de la diferencia o los valores de las medidas mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, consideramos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer acerca de una estimación de probabilidad es qué significa. ¿Qué significa tener un 75% de confianza de que el sistema A es mejor que el sistema B? Como se describe arriba, queremos que signifique que si seguimos evaluando documentos, solo habrá un 25% de probabilidad de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos no evaluados, específicamente que cada documento no evaluado tenía la misma probabilidad de ser relevante o no relevante. Esta suposición casi con seguridad no es realista en la mayoría de las aplicaciones de IR. Resulta que es esta suposición la que determina si se pueden confiar en las estimaciones de confianza. Antes de profundizar en esto, definimos formalmente la confianza. 2.1 Estimación de la confianza La precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar documentos relevantes de manera alta (precisión) como su capacidad para recuperar documentos relevantes (recuperación). Normalmente se escribe como la precisión media en los rangos de documentos relevantes: AP = 1 |R| i∈R prec@r(i) donde R es el conjunto de documentos relevantes y r(i) es el rango del documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos están ordenados por rango, podemos expresar la precisión como prec@i = 1/i i j=1 Xj. La precisión promedio se convierte entonces en la ecuación cuadrática AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj donde aij = 1/ max{r(i), r(j)}. Usar aij en lugar de 1/i nos permite numerar los documentos de forma arbitraria. Para ver por qué esto es cierto, considera un ejemplo simple: una lista de 3 documentos con los documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 porque xA = 0, xB = 1, xC = 1. Aunque el orden B, A, C es diferente de la etiqueta A, B, C, no afecta al cálculo. Ahora podemos ver que la precisión promedio en sí misma es una variable aleatoria con una distribución sobre todas las posibles asignaciones de relevancia a todos los documentos. Esta variable aleatoria tiene una esperanza, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento i sea relevante: pi = p(Xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos las valoraciones de relevancia, pero creemos que pA = 0.4, pB = 0.8, pC = 0.7. Podemos entonces calcular, por ejemplo. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. 

La traducción al español sería: P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la esperanza y la varianza: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP converge asintóticamente a una distribución normal con la esperanza y la varianza definidas anteriormente. Para nuestra tarea de evaluación comparativa, estamos interesados en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 − AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos están ordenados arbitrariamente: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij donde bij está definido de manera análoga a aij para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP sea menor que cero. Dado que los temas son independientes, podemos extender fácilmente esto para referirnos a la precisión promedio (MAP). MAP también se distribuye normalmente; su esperanza y varianza son: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 La confianza puede entonces estimarse calculando la esperanza y varianza y utilizando la función de densidad normal para encontrar P(ΔMAP < 0). 2.2 Confianza y Robustez Habiendo definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección ante juicios faltantes. 1 Estas son en realidad aproximaciones a la verdadera esperanza y varianza, pero el error es despreciable O(n2−n ). Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de m juicios de relevancia xm = {x1, x2, ..., xm} (usando x minúscula en lugar de X mayúscula para distinguir entre documentos juzgados y no juzgados); estos son los juicios contra los cuales calculamos la confianza. Sea Zα el subconjunto de pares en Z para los cuales predecimos que ΔMAP = −1 con confianza α dadas las evaluaciones xm. Para que las estimaciones de confianza sean precisas, necesitamos que al menos α · |Zα| de estos pares realmente tengan ΔMAP = −1 después de haber evaluado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza; nuestra evaluación será robusta ante juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Las suposiciones en las que se basan son las probabilidades de relevancia pi. Necesitamos que esto sea realista. Sostenemos que la mejor distribución posible de relevancia p(Xi) es aquella que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones infundadas. Esto se conoce como el principio de máxima entropía [13]. La entropía de una variable aleatoria X con distribución p(X) se define como H(p) = − i p(X = i) log p(X = i). Esto ha encontrado una amplia variedad de usos en la informática y la recuperación de información. La distribución de entropía máxima es aquella que maximiza H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para la relevancia al estimar la confianza. Teorema 1. Si p(Xn |I, xm ) = argmaxpH(p), las estimaciones de confianza serán precisas, donde xm es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos cuya relevancia deseamos estimar, e I es alguna información sobre los documentos (no especificada hasta ahora). Por el momento prescindimos de la prueba, pero es bastante simple. Esto indica que cuanto mejores sean las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una colección de pruebas reutilizable se convierte entonces en la tarea de estimar la relevancia de los documentos no evaluados. El teorema y su demostración no dicen absolutamente nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida en la que estamos interesados. Esto significa que los mismos cálculos de probabilidad pueden informarnos sobre la precisión promedio, así como sobre la precisión, la recuperación, el bpref, etc. Además, podríamos asumir que la relevancia de los documentos i y j es independiente y lograr el mismo resultado, lo cual afirmamos como un corolario: Corolario 1. Si p(Xi|I, xm) = argmaxpH(p), las estimaciones de confianza serán precisas. La tarea, por lo tanto, consiste en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acerquemos a la distribución de entropía máxima relevante, más nos acercaremos a la robustez. 3. PREDICIENDO LA RELEVANCIA En nuestra declaración del Teorema 1, dejamos sin especificar la naturaleza de la información I. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes; básicamente cualquier cosa que pueda ser modelada puede ser utilizada como información para predecir la relevancia. Una fuente natural de información son los propios sistemas de recuperación: cómo clasificaron los documentos evaluados, con qué frecuencia no lograron clasificar documentos relevantes, cómo se desempeñan en diferentes temas, y así sucesivamente. Si tratamos cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en uno de agregación de opiniones de expertos. Esto es similar al problema de metabúsqueda o fusión de datos en el que la tarea consiste en tomar k sistemas de entrada y fusionarlos en un solo ranking. Aslam et al. [3] identificaron previamente una conexión entre la evaluación y la metabúsqueda. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos insertar en la Ecuación 1; los algoritmos de metabusqueda no tienen tal requisito. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y somos capaces de reestimar la relevancia dada cada nuevo juicio. A la luz de lo anterior, introducimos un modelo probabilístico para la combinación de expertos. 3.1 Un Modelo para la Agregación de Opiniones de Expertos Supongamos que cada experto j proporciona una probabilidad de relevancia qij = pj(Xi = 1). La información sobre la relevancia del documento i será entonces el conjunto de k opiniones de expertos I = qi = (qi1, qi2, · · · , qik). La distribución de probabilidad que deseamos encontrar es aquella que maximiza la entropía de pi = p(Xi = 1|qi). Resulta que encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la verosimilitud [5]. Blower [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) donde λ1, · · · , λk son los parámetros de regresión. Incluimos una distribución beta a priori para p(λj) con parámetros α, β. Esto se puede ver como un tipo de suavizado para tener en cuenta el hecho de que los datos de entrenamiento están altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Un modelo de la misma forma fue demostrado por Clemen & Winkler como el mejor para agregar probabilidades de expertos [10]. Un enfoque similar motivado por la entropía máxima se ha utilizado para la agregación de expertos [15]. Aslam & Montague [1] utilizaron un modelo similar para la metabúsqueda, pero asumieron independencia entre los expertos. ¿De dónde vienen los qij? Utilizar puntuaciones sin procesar y no calibradas como predictores no funcionará porque las distribuciones de puntuaciones varían demasiado entre los temas. Un clasificador de modelado de lenguaje, por ejemplo, suele otorgar una puntuación mucho más alta al documento recuperado en la parte superior para una consulta corta que al documento recuperado en la parte superior para una consulta larga. Podríamos entrenar un modelo predictivo separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no suficientes para entrenar un modelo con confianza. Además, parece razonable asumir que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas no vistos. En cambio, calibraremos las puntuaciones de cada experto de forma individual para que las puntuaciones puedan ser comparadas tanto dentro de un tema como entre temas. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas). 3.2 Calibración de Expertos Cada experto nos proporciona una puntuación y un rango para cada documento. Necesitamos convertir estos en probabilidades. Un método como el utilizado por Manmatha et al. [14] podría ser utilizado para convertir puntuaciones en probabilidades de relevancia. El método de preferencia por pares de Carterette & Petkova [9] también podría ser utilizado, interpretando la clasificación de un documento sobre otro como una expresión de preferencia. Que q∗ ij sea la probabilidad autoinformada por el experto j de que el documento i sea relevante. De manera intuitiva, parece claro que q∗ ij debería disminuir con el rango, y debería ser cero si el documento i no está clasificado (el experto no consideró que fuera relevante). El modelo de preferencia por pares puede manejar fácilmente estos dos requisitos, por lo que lo utilizaremos. Que θrj (i) sea el coeficiente de relevancia del documento en la posición rj(i). Queremos encontrar los θs que maximizan la función de verosimilitud: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) Incluimos nuevamente una distribución beta previa en p(θrj(i)) con parámetros |Rt| + 1 y |Nt| + 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos parámetros previos garantiza que las probabilidades resultantes se concentren alrededor de la proporción de documentos relevantes que se han descubierto para el tema t. Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen más documentos relevantes. Después de encontrar el Θ que maximiza la verosimilitud, tenemos q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no clasificado sea relevante es 0. Dado que q∗ ij se basa en la posición en la que se recupera un documento en lugar de en la identidad del documento en sí, las probabilidades son idénticas de un experto a otro, por ejemplo, si el experto E colocó el documento A en la posición 1, y el experto D colocó el documento B en la posición 1, tendremos que q∗ AE = q∗ BD. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior proporciona probabilidades independientes del tema para cada documento. Pero supongamos que un experto que informa una probabilidad del 90% solo tiene razón el 50% del tiempo. Su opinión debería ser descartada basándose en su desempeño observado. Específicamente, queremos aprender una función de calibración qij = Cj(q∗ ij) que garantice que las probabilidades predichas estén ajustadas a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este momento. El método de calibración SVM de Platts [16] ajusta una función sigmoide entre q∗ ij y las evaluaciones de relevancia para obtener qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Dado que q∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se introducen en el modelo (2) para encontrar las probabilidades de los documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1 y E2 han clasificado los documentos A, B, C para el tema T1 y los documentos D, E, F para el tema T2. El primer paso es obtener q∗ ij. A continuación se realiza la calibración al rendimiento real para encontrar qij. Finalmente obtenemos pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren en los datos que toman como entrada y en lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. rangos → probabilidades (por sistema por tema). Esto nos da q∗ ij, la probabilidad autoinformada por el experto j de la relevancia del documento i. Esto es no supervisado; no requiere datos etiquetados (aunque si los tenemos, los usamos para establecer parámetros previos). 2. probabilidades → probabilidades calibradas (por sistema). Esto nos da qij = Cj (q∗ ij), la probabilidad calibrada por el experto j de la relevancia del documento i. Esto es semisupervisado; tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos. 3. probabilidades calibradas → probabilidades de documentos. Esto nos da pi = p(Xi = 1|qi), la probabilidad de relevancia del documento i dada las probabilidades de experto calibradas qij. Esto es supervisado; aprendemos coeficientes de un conjunto de documentos juzgados y los utilizamos para estimar la relevancia de los documentos no juzgados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Por lo tanto, se puede implementar en un lenguaje de programación estadística como R en unas pocas líneas de código. El uso de priors beta (conjugados) garantiza que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se entrena y aplica lo suficientemente rápido como para ser utilizado en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTOS Se están considerando tres hipótesis. El primero, y más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas lo suficientemente robustas como para ser reutilizables; es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al conjunto. Las otras dos hipótesis se relacionan con la mejora que observamos al utilizar estimaciones de relevancia mejores que las que utilizamos en nuestro trabajo anterior [8]. Estos son (a) que se necesitan menos temas de relevancia, número de ejecuciones, número de documentos evaluados y número de documentos relevantes para cada uno de nuestros conjuntos de datos y (b) la precisión de las predicciones es mayor que si simplemente asumiéramos que pi = 0.5 para todos los documentos no evaluados. 4.1 Datos Obtuvimos ejecuciones completas de búsqueda ad-hoc presentadas en las TRECs 3 a 8. Cada ejecución clasifica como máximo 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista Web de TREC 13, la pista Robust2 de TREC 14 y la pista Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Reservamos el conjunto TREC 4 (ad-hoc 95) para entrenamiento, los TRECs 3 y 5-8 (ad-hoc 94 y 96-99) para pruebas principales, y los conjuntos restantes para pruebas adicionales. Utilizamos los archivos qrels recopilados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos las listas clasificadas en 100 documentos. No hay razón por la que no pudiéramos profundizar más, pero el cálculo de la varianza es O(n3) y, por lo tanto, muy consumidor de tiempo. Debido a la naturaleza del rango recíproco de AP, no perdemos mucha información al truncar en el rango 100. 4.2 Algoritmos Compararemos tres algoritmos para adquirir juicios de relevancia. La línea base es una variación del agrupamiento de TREC que llamaremos agrupamiento incremental (IP). Este algoritmo toma un número k como entrada y presenta los primeros k documentos en orden de rango (sin tener en cuenta el tema) para ser evaluados. No estima la relevancia de los documentos no evaluados; simplemente asume que cualquier documento no evaluado no es relevante. El segundo algoritmo es el presentado en Carterette et al. [8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que sean para determinar si existe una diferencia en la precisión media promedio. Para este enfoque pi = 0.5 para todos los i; no hay estimación de probabilidades. Llamaremos a esto MTC por colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de las probabilidades de relevancia. Llamaremos a esto RTC por colección de pruebas robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos pi para todos los documentos no evaluados i utilizando el modelo de agregación de expertos de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben ser configurados. Entrenamos utilizando el conjunto ad-hoc 95. Limitamos 2 robustos aquí significa recuperación robusta; esto es diferente de nuestro objetivo de evaluación robusta. Algoritmo 1 (MTC) Dadas dos listas clasificadas y un nivel de confianza α, predecir el signo de ΔMAP. 1: pi ← 0.5 para todos los documentos i 2: mientras P(ΔMAP < 0) < α hacer 3: calcular el peso wi para todos los documentos i no clasificados (ver Carterette et al. [8] para más detalles) 4: j ← argmaxiwi 5: xj ← 1 si el documento j es relevante, 0 en caso contrario 6: pj ← xj 7: fin mientras la búsqueda de priors uniformes con una varianza relativamente alta. Para la agregación de expertos, los parámetros previos son α = β = 1. 4.3 Diseño Experimental Primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios y evaluar un nuevo conjunto de sistemas. Para cada prueba experimental: 1. Selecciona un subconjunto aleatorio de k ejecuciones. De esos k, elige un c inicial < k para evaluar. 3. Ejecutar RTC al 95% de confianza en el c inicial de aproximadamente 4. Utilizando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todos los k experimentos. Calcular EMAP para todas las ejecuciones k, y P(ΔMAP < 0) para todos los pares de ejecuciones. Hacemos lo mismo para MTC, pero omitimos el paso 4. Ten en cuenta que después de evaluar los primeros c sistemas, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos c = 2: construiremos un conjunto de juicios a partir de evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que realizamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis. 4.4 Evaluación Experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que realiza es al menos su confianza estimada. Una forma de evaluar la robustez es agrupar pares por su confianza, luego calcular la precisión sobre todos los pares en cada grupo. Nos gustaría que la precisión no fuera menor que la puntuación de confianza más baja en el contenedor, pero preferiblemente más alta. Dado que las estadísticas resumen son útiles, ideamos la siguiente métrica. Supongamos que somos una casa de apuestas tomando apuestas sobre si ΔMAP < 0. Utilizamos RTC o MTC para establecer las probabilidades O = P (ΔMAP <0) 1−P (ΔMAP <0) . Supongamos que un apostador apuesta $1 a que ΔMAP ≥ 0. Si resulta que ΔMAP < 0, ganamos el dólar. De lo contrario, pagamos O. Si nuestras estimaciones de confianza son perfectamente precisas, empatamos. Si la confianza es mayor que la precisión, perdemos dinero; ganamos si la precisión es mayor que la confianza. Contra intuitivamente, el resultado más deseable es empatar: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o evaluado más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de realizar juicios de relevancia adicionales, por lo que consideraremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación par a par i es: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 si ΔMAP < 0 y 0 en caso contrario, y Pi = P(ΔMAP < 0). La estadística resumen es W, la media de Wi. Ten en cuenta que a medida que Pi aumenta, perdemos más por estar equivocados. Esto es como debería ser: la penalización debería ser grande por fallar en las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin límite a medida que las predicciones se acercan a la certeza, limitamos −Wi a 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar el 95% de confianza en el primer par de sistemas. La mediana es más interesante que la media: la mayoría de los pares requieren unos cientos de juicios, pero algunos pares requieren varios miles. La distribución es, por lo tanto, altamente sesgada y la media se ve fuertemente afectada por esos valores atípicos. Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de k sistemas por un pequeño conjunto de juicios y la clasificación real utilizando el conjunto completo de juicios. El tau de Kendall, una estadística no paramétrica basada en intercambios de pares entre dos listas, es una evaluación estándar para este tipo de estudio. Va desde −1 (perfectamente anticorrelacionado) hasta 1 (clasificaciones idénticas), con 0 significando que la mitad de los pares están intercambiados. Como mencionamos en la introducción, sin embargo, una medida de precisión como la correlación de rangos no es una buena evaluación de la reutilización. Lo incluimos por completitud. 4.4.1 Prueba de hipótesis Realizar múltiples pruebas permite el uso de pruebas de hipótesis estadísticas para comparar algoritmos. El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas. Como mencionamos anteriormente, estamos más interesados en la mediana del número de juicios que en la media. Una prueba para la diferencia en la mediana es la prueba de rango con signo de Wilcoxon. También podemos usar una prueba t pareada para probar una diferencia en la media. Para la correlación de rangos, podemos usar una prueba t pareada para probar si hay una diferencia en τ. 5. RESULTADOS Y ANÁLISIS La comparación entre MTC y RTC se muestra en la Tabla 2. Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos. No podemos reutilizar las evaluaciones de relevancia con mucha confianza. Pero con RTC, los resultados son muy robustos. Existe una ligera disminución en la precisión cuando la confianza supera el 0.95; sin embargo, las predicciones de confianza son confiables. La media de Wi muestra que RTC está mucho más cerca de 0 que MTC. La distribución de las puntuaciones de confianza muestra que al menos el 80% de confianza se logra más del 35% del tiempo, lo que indica que ninguno de los algoritmos está siendo demasiado conservador en sus estimaciones de confianza. Las estimaciones de confianza son bastante bajas en general; esto se debe a que hemos construido una colección de pruebas a partir de solo dos sistemas iniciales. Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza al generalizar a nuevos sistemas. Los resultados más detallados de ambos algoritmos se muestran en la Figura 2. La línea sólida es el resultado ideal que daría W = 0. RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcance aproximadamente 0.97. Después de eso, hay una ligera disminución en la precisión que discutimos a continuación. Ten en cuenta que tanto la confianza MTC RTC % en la precisión del intervalo % en la precisión del intervalo 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 mediana juzgada 251 235 media τ 0.393 0.555 Tabla 2: Confianza en que P(ΔMAP < 0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC. Cada contenedor contiene más de 1,000 pruebas de los conjuntos ad hoc 3, 5-8. RTC es mucho más robusto que MTC. W está definido en la Sección 4.4; cuanto más cerca de 0, mejor. La mediana juzgada es el número de juicios necesarios para alcanzar un 95% de confianza en los dos primeros sistemas. La media τ es la correlación de rango promedio para los 10 sistemas. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 precisión confianza punto de equilibrio RTC MTC Figura 2: Confianza vs. precisión de MTC y RTC. La línea sólida es el resultado perfecto que daría W = 0; el rendimiento debería estar en o por encima de esta línea. Cada punto representa al menos 500 comparaciones en pares. Los algoritmos están claramente por encima de la línea hasta aproximadamente una confianza de 0.7. Esto se debe a que el rendimiento base en estos conjuntos de datos es alto; es bastante fácil lograr un 75% de precisión haciendo muy poco trabajo [7]. Número de juicios: La mediana del número de juicios requeridos por MTC para alcanzar un 95% de confianza en los dos primeros sistemas es de 251, un promedio de 5 por tema. La mediana requerida por RTC es 235, aproximadamente 4.7 por tema. Aunque los números son cercanos, la mediana de RTCs es significativamente más baja según una prueba de Wilcoxon pareada (p < 0.0001). Para comparación, una piscina de profundidad 100 resultaría en un mínimo de 5,000 juicios para cada par. La diferencia en las medias es mucho mayor. MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema. (Recordemos que las medias están fuertemente sesgadas por algunas parejas que requieren miles de juicios). Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). El diez por ciento de los conjuntos resultaron en 100 o menos juicios (menos de dos por tema). El rendimiento en estos es muy alto: W = 0.41 y un 99.7% de precisión cuando la confianza es de al menos 0.9. Esto demuestra que incluso las colecciones pequeñas pueden ser reutilizables. Para el 50% de los conjuntos con más de 235 juicios, la precisión es del 93% cuando la confianza es de al menos 0.9. Correlación de rangos: MTC y RTC clasifican los 10 sistemas por EMAP (Eq. (1)) calculado utilizando sus respectivas estimaciones de probabilidad. La correlación de rango τ promedio entre el MAP verdadero y el EMAP es de 0.393 para MTC y 0.555 para RTC. Esta diferencia es significativa mediante una prueba t de muestras pareadas (p < 0.0001). Ten en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia. Es más importante que estimemos correctamente la confianza en cada comparación de pares. Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por MAP utilizando solo esos juicios (todos los documentos no juzgados se consideraron no relevantes). Calculamos la correlación τ con la clasificación real. La correlación media τ es de 0.398, lo cual no es significativamente diferente de MTC, pero es significativamente menor que RTC. El uso de estimaciones uniformes de probabilidad es indistinguible de la línea base, mientras que la estimación de relevancia mediante la agregación de expertos mejora el rendimiento en gran medida: casi un 40% más que tanto MTC como IP. Sobreajuste: Es posible sobreajustar: si provienen demasiados juicios de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco fiables. Vimos esto en la Tabla 2 y la Figura 2, donde RTC muestra una disminución en la precisión cuando la confianza está alrededor del 97%. De hecho, el número de juicios realizados antes de una predicción incorrecta es más de un 50% mayor que el número realizado antes de una predicción correcta. El sobreajuste es difícil de cuantificar exactamente, porque hacer más juicios de relevancia no siempre lo causa: a niveles de confianza más altos, se hacen más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confianzas. Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión; la diferencia parece estar cuando un sistema tiene muchos más juicios que el otro. Comparaciones por pares: Nuestras comparaciones por pares se dividen en uno de tres grupos: 1. las dos ejecuciones originales de las que se obtienen los juicios de relevancia; 2. una de las ejecuciones originales vs. una de las nuevas ejecuciones; 3. dos nuevas ejecuciones. La Tabla 3 muestra los resultados de confianza frente a precisión para cada uno de estos tres grupos. Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales. Esto se debe muy probablemente a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP. Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos. Peor caso: El caso intuitivamente más probable de producir un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común. Si queremos que los juicios sean reutilizables, deberíamos ser capaces de generalizar incluso a ejecuciones que son muy diferentes de las utilizadas para adquirir los juicios de relevancia. Una medida simple de similitud de dos ejecuciones es el porcentaje promedio de documentos que recuperaron en común para cada tema [2]. Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud. Los resultados se muestran en la confianza de precisión de dos ejecuciones originales, una ejecución original, ninguna ejecución original. 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza vs. precisión de RTC al comparar las dos ejecuciones originales, una ejecución original y una nueva ejecución, y dos nuevas ejecuciones. RTC es robusto en los tres casos. precisión cuando la confianza es similar 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza vs. precisión de RTC cuando un par de sistemas recuperó el 0-30% de documentos en común (dividido en 0%-10%, 10%-20% y 20%-30%). RTC es robusto en los tres casos. Tabla 4. El rendimiento es de hecho muy robusto incluso cuando la similitud es baja. Cuando las dos corridas comparten muy pocos documentos en común, W es en realidad positivo. Tanto MTC como IP tuvieron un rendimiento bastante deficiente en estos casos. Cuando la similitud estaba entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC. Por conjunto de datos: Todos los resultados anteriores han sido solo en las colecciones ad-hoc. Realizamos los mismos experimentos en nuestros conjuntos de datos adicionales y desglosamos los resultados por conjunto de datos para ver cómo varía el rendimiento. Los resultados en la Tabla 5 muestran todo acerca de cada conjunto, incluyendo la precisión agrupada, W, la media de τ, y el número medio de juicios para alcanzar el 95% de confianza en los dos primeros sistemas. Los resultados son altamente consistentes de una colección a otra, lo que sugiere que nuestro método no está sobreajustado a ningún conjunto de datos en particular. CONCLUSIONES Y TRABAJO FUTURO En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una colección de pruebas y presentado un modelo que es capaz de lograr la reutilización con conjuntos muy pequeños de juicios de relevancia. La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos preciso que RTC, que puede eliminar el sesgo para proporcionar una evaluación sólida. Las estimaciones de confianza de RTC, además de ser precisas, proporcionan una guía para obtener juicios adicionales: enfóquese en juzgar documentos de las comparaciones de menor confianza. A largo plazo, observamos pequeños conjuntos de juicios de relevancia, precisión, confianza ad-hoc 94, ad-hoc 96, ad-hoc 97, ad-hoc 98, ad-hoc 99, web 04, robust 05, terabyte 05, 0.5 − 0.6 64.1%, 61.8%, 62.2%, 62.0%, 59.4%, 64.3%, 61.5%, 61.6%, 0.6 − 0.7 76.1%, 77.8%, 74.5%, 78.2%, 74.3%, 78.1%, 75.9%, 75.9%, 0.7 − 0.8 75.2%, 78.9%, 77.6%, 80.0%, 78.6%, 82.6%, 77.5%, 80.4%, 0.8 − 0.9 83.2%, 85.5%, 84.6%, 84.9%, 86.8%, 84.5%, 86.7%, 87.7%, 0.9 − 0.95 93.0%, 93.6%, 92.8%, 93.7%, 92.6%, 94.2%, 93.9%, 94.2%, 0.95 − 0.99 93.1%, 94.3%, 93.1%, 93.7%, 92.8%, 95.0%, 93.9%, 91.6%, 1.0 99.2%, 96.8%, 98.7%, 99.5%, 99.6%, 100%, 99.2%, 98.3%, W -0.34, -0.34, -0.48, -0.35, -0.44, -0.07, -0.41, -0.67, mediana juzgada 235, 276, 243, 213, 179, 448, 310, 320, media τ 0.538, 0.573, 0.556, 0.579, 0.532, 0.596, 0.565, 0.574. Tabla 5: Precisión, W, media τ y mediana del número de juicios para los 8 conjuntos de pruebas. Los resultados son altamente consistentes en todos los conjuntos de datos. A medida que pasa el tiempo, el número de juicios crece hasta que hay un 100% de confianza en cada evaluación, y hay una colección completa de pruebas para la tarea. Vemos un uso adicional para este método en escenarios como la recuperación web en los que el corpus cambia con frecuencia. Podría aplicarse a la evaluación en una colección de pruebas dinámica según lo definido por Soboroff [18]. El modelo que presentamos en la Sección 3 no es en absoluto la única posibilidad para crear una colección de pruebas robusta. Un modelo de agregación de expertos más simple podría funcionar igual de bien o mejor (aunque todos nuestros esfuerzos por simplificar fallaron). Además de la agregación de expertos, podríamos estimar probabilidades al observar similitudes entre documentos. Esta es un área obvia para futuras exploraciones. Además, será valioso investigar el problema del sobreajuste: las circunstancias en las que ocurre y qué se puede hacer para prevenirlo. Mientras tanto, limitar las estimaciones de confianza al 95% es un truco que resuelve el problema. Tenemos muchos más resultados experimentales que desafortunadamente no tuvimos espacio para incluir, pero que refuerzan la idea de que RTC es altamente robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación de sistemas de forma pareja. Agradecimientos Este trabajo fue apoyado en parte por el Centro de Recuperación de Información Inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material es responsabilidad del autor y no necesariamente refleja la del patrocinador. REFERENCIAS [1] J. Aslam y M. Montague. Modelos para Metabúsqueda. En Actas de SIGIR, páginas 275-285, 2001. [2] J. Aslam y R. Savell. Sobre la efectividad de evaluar sistemas de recuperación en ausencia de juicios de relevancia. En Actas de SIGIR, páginas 361-362, 2003. [3] J. A. Aslam, V. Pavlu y R. Savell. Un modelo unificado para metabúsqueda, agrupación y evaluación de sistemas. En Actas de CIKM, páginas 484-491, 2003. [4] J. A. Aslam, V. Pavlu y E. Yilmaz. Un método estadístico para la evaluación de sistemas utilizando juicios incompletos. En Actas de SIGIR, páginas 541-548, 2006. [5] A. L. Berger, S. D. Pietra y V. J. D. Pietra. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22(1):39-71, 1996. [6] D. J. Blower. Una derivación sencilla de la regresión logística desde la perspectiva bayesiana y de máxima entropía. En Actas del 23º Taller Internacional sobre Inferencia Bayesiana y Métodos de Entropía Máxima en Ciencia e Ingeniería, páginas 30-43, 2004. [7] B. Carterette y J. Allan. Metodología de investigación en estudios del esfuerzo del evaluador para la evaluación de recuperación. En Actas de RIAO, 2007. [8] B. Carterette, J. Allan y R. K. Sitaraman. Colecciones de prueba mínimas para evaluación de recuperación. En Actas de SIGIR, páginas 268-275, 2006. [9] B. Carterette y D. I. Petkova. Aprendiendo un ranking a partir de preferencias por pares. En Actas de SIGIR, 2006. [10] R. T. Clemen y R. L. Winkler. Unanimidad y compromiso entre pronosticadores de probabilidad. Ciencia de la Gestión, 36(7):767-779, julio de 1990. [11] G. V. Cormack, C. R. Palmer y C. L. Clarke. Construcción eficiente de grandes colecciones de pruebas. En Actas de SIGIR, páginas 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern y D. B. Rubin. Análisis de datos bayesianos. Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.
Chapman & Hall/CRC, 2004. [13] E. T. Jaynes. Teoría de la probabilidad: La lógica de la ciencia. Cambridge University Press, 2003. [14] R. Manmatha y H. Sever. Un enfoque formal para la normalización de puntuaciones en metabúsqueda. En Actas de HLT, páginas 88-93, 2002. [15] I. J. Myung, S. Ramamoorti y J. Andrew D. Baily. Agregación de predicciones de expertos mediante entropía máxima. Ciencia de la Gestión, 42(10):1420-1436, octubre de 1996. [16] J. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparación con métodos de verosimilitud regularizados. páginas 61-74, 2000. [17] M. Sanderson y H. Joho. Formando colecciones de pruebas sin agrupamiento de sistemas. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, páginas 33-40, 2004. [18] I. Soboroff. Colecciones de pruebas dinámicas: midiendo la efectividad de la búsqueda en la web en vivo. En Actas de SIGIR, páginas 276-283, 2006. [19] K. Sparck Jones y C. J. van Rijsbergen. Colecciones de pruebas de recuperación de información. Revista de Documentación, 32(1):59-75, 1976. [20] E. M. Voorhees y D. K. Harman, editores. TREC: Experimento y Evaluación en Recuperación de Información. MIT Press, 2005. [21] J. Zobel. 

MIT Press, 2005. [21] J. Zobel. ¿Qué tan confiables son los resultados de los experimentos de recuperación de información a gran escala? En Actas de SIGIR, páginas 307-314, 1998.