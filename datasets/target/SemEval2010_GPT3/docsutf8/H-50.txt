Un enfoque de clasificación para la agregación de rangos en la recuperación de información. Mohamed Farah Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia farah@lamsade.dauphine.fr Daniel Vanderpooten Lamsade, Universidad Paris Dauphine Place du Mal de Lattre de Tassigny 75775 París Cedex 16, Francia vdp@lamsade.dauphine.fr RESUMEN La investigación en Recuperación de Información suele mostrar una mejora en el rendimiento cuando se combinan muchas fuentes de evidencia para producir una clasificación de documentos (por ejemplo, textos, imágenes, sonidos, etc.). En este artículo, nos enfocamos en el problema de agregación de rangos, también llamado problema de fusión de datos, donde los rankings de documentos, buscados en la misma colección y proporcionados por múltiples métodos, se combinan para producir un nuevo ranking. En este contexto, proponemos un método de agregación de rangos dentro de un marco de múltiples criterios utilizando mecanismos de agregación basados en reglas de decisión que identifican razones positivas y negativas para juzgar si un documento debería obtener un rango mejor que otro. Mostramos que el método propuesto se desempeña bien con las características distintivas de la Recuperación de Información. Se informan los resultados experimentales que muestran que el método sugerido tiene un mejor rendimiento que los operadores conocidos CombSUM y CombMNZ. Categorías y Descriptores de Asignaturas: H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información - Modelos de recuperación. Términos generales: Algoritmos, Medición, Experimentación, Rendimiento, Teoría. 1. INTRODUCCIÓN Una amplia gama de enfoques actuales de Recuperación de Información (IR) se basan en diversos modelos de búsqueda (Booleano, Espacio Vectorial, Probabilístico, de Lenguaje, etc. [2]) con el fin de recuperar documentos relevantes en respuesta a una solicitud del usuario. Las listas de resultados producidas por estos enfoques dependen de la definición exacta del concepto de relevancia. Los enfoques de agregación de rangos, también llamados enfoques de fusión de datos, consisten en combinar estas listas de resultados para producir un nuevo y, con suerte, mejor ranking. Tales enfoques dan lugar a motores de búsqueda en la web en el contexto de Internet. Consideramos, en lo siguiente, casos donde solo se disponen de rangos y no se proporciona otra información adicional como las puntuaciones de relevancia. Esto corresponde de hecho a la realidad, donde solo se dispone de información ordinal. La fusión de datos también es relevante en otros contextos, como cuando el usuario escribe varias consultas de su necesidad de información (por ejemplo, una consulta booleana y una consulta en lenguaje natural) [4], o cuando hay disponibles muchos documentos sustitutos [16]. Varios estudios argumentaron que la agregación de rangos tiene el potencial de combinar de manera efectiva todas las diversas fuentes de evidencia consideradas en varios métodos de entrada. Por ejemplo, experimentos realizados en [16], [30], [4] y [19] mostraron que los documentos que aparecen en las listas de la mayoría de los métodos de entrada tienen más probabilidades de ser relevantes. Además, Lee [19] y Vogt y Cottrell [31] encontraron que varios enfoques de recuperación a menudo devuelven documentos irrelevantes muy diferentes, pero muchos de los mismos documentos relevantes. Bartell et al. [3] también encontraron que los métodos de agregación de rangos mejoran el rendimiento con respecto a los métodos de entrada, incluso cuando algunos de ellos tienen un rendimiento individual débil. Estos métodos también tienden a suavizar los sesgos de los métodos de entrada según Montague y Aslam [22]. La fusión de datos ha demostrado recientemente mejorar el rendimiento tanto en las tareas de recuperación ad-hoc como en la categorización dentro de la pista genómica TREC en 2005 [1]. El problema de la agregación de rangos se abordó en varios campos, como i) en la teoría de la elección social que estudia algoritmos de votación que especifican ganadores de elecciones o ganadores de competiciones en torneos [29], ii) en estadística al estudiar la correlación entre clasificaciones, iii) en bases de datos distribuidas cuando los resultados de diferentes bases de datos deben combinarse [12], y iv) en filtrado colaborativo [23]. La mayoría de los métodos actuales de agregación de rangos consideran cada ranking de entrada como una permutación sobre el mismo conjunto de elementos. También dan una interpretación rígida al ranking exacto de los elementos. Ambas suposiciones no son válidas en el contexto de IR, como se demostrará en las siguientes secciones. El resto del documento está organizado de la siguiente manera. Primero revisamos los métodos actuales de agregación de rangos en la Sección 2. Luego detallamos las especificidades del problema de fusión de datos en el contexto de la IR (Sección 3). En la Sección 4, presentamos un nuevo método de agregación que se ha demostrado que se ajusta mejor al contexto de IR. Los resultados experimentales se presentan en la Sección 5 y las conclusiones se proporcionan en una sección final. 2. TRABAJO RELACIONADO Como señaló Riker [25], podemos distinguir dos familias de métodos de agregación de rangos: métodos posicionales que asignan puntuaciones a los elementos a clasificar según los rangos que reciben y métodos mayoritarios que se basan en comparaciones de pares de elementos a clasificar. Estos dos grupos de métodos tienen sus raíces en las obras pioneras de Borda [5] y Condorcet [7], respectivamente, en la literatura de elección social. 2.1 Preliminares Primero introducimos algunas notaciones básicas para presentar los métodos de agregación de rangos de manera uniforme. Sea D = {d1, d2, . . . , dnd} un conjunto de nd documentos. Una lista o un ranking j es un orden definido en Dj ⊆ D (j = 1, . . . , n). Por lo tanto, di j di significa que di está clasificado mejor que di en j. Cuando Dj = D, se dice que j es una lista completa. De lo contrario, es una lista parcial. Si di pertenece a Dj, rj i denota la clasificación o posición de di en j. Suponemos que la mejor respuesta (documento) se asigna a la posición 1 y la peor se asigna a la posición |Dj|. Sea D el conjunto de todas las permutaciones en D o todos los subconjuntos de D. Un perfil es una n-tupla de clasificaciones PR = (1, 2, ..., n). Restringir PR a los rankings que contienen el documento di define PRi. También llamamos al número de clasificaciones que contienen el documento di los aciertos de rango de di [19]. El problema de agregación de rangos o fusión de datos consiste en encontrar una función de clasificación o mecanismo Ψ (también llamado función de bienestar social en la terminología de la teoría de la elección social) definido por: Ψ: n D → D PR = (1, 2, . . . , n) → σ = Ψ(PR) donde σ se llama un ranking de consenso. 2.2 Métodos posicionales 2.2.1 Recuento de Borda Este método [5] asigna primero una puntuación n j=1 rj i a cada documento di. Los documentos se clasifican luego por orden creciente de esta puntuación, rompiendo los empates, si los hubiera, de forma arbitraria. 2.2.2 Métodos de Combinación Lineal Esta familia de métodos básicamente combina las puntuaciones de los documentos. Cuando se utilizan para el problema de agregación de rangos, se asume que los rangos son puntajes o desempeños que se combinan utilizando operadores de agregación como la suma ponderada o alguna variación de la misma [3, 31, 17, 28]. Por ejemplo, Callan et al. [6] utilizaron el modelo de redes de inferencia [30] para combinar clasificaciones. Fox y Shaw propusieron varias estrategias de combinación que son CombSUM, CombMIN, CombMAX, CombANZ y CombMNZ. Los tres primeros operadores corresponden a los operadores de suma, mínimo y máximo, respectivamente. CombANZ y CombMNZ respectivamente dividen y multiplican la puntuación de CombSUM por los hits de rango. Se muestra en [19] que los operadores CombSUM y CombMNZ tienen un mejor rendimiento que los demás. Los motores de búsqueda de metadatos como SavvySearch y MetaCrawler utilizan la estrategia CombSUM para fusionar clasificaciones. 2.2.3 Agregación óptima de Footrule En este método, una clasificación de consenso minimiza la distancia de Footrule de Spearman de las clasificaciones de entrada [21]. Formalmente, dadas dos listas completas j y j, esta distancia está dada por F(j, j) = Σd i=1 |rj i − rj i|. Se extiende a varias listas de la siguiente manera. Dado un perfil PR y un ranking de consenso σ, la distancia de Spearman footrule de σ a PR está dada por F(σ, PR) = Σ j=1 n F(σ, j). Cook y Kress propusieron un método similar que consiste en optimizar la distancia D( j, j ) = 1 2 nd i,i =1 |rj i,i − rj i,i |, donde rj i,i = rj i −rj i . Esta formulación tiene la ventaja de que considera la intensidad de las preferencias. Métodos Probabilísticos Este tipo de métodos asumen que el rendimiento de los métodos de entrada en una serie de consultas de entrenamiento es indicativo de su rendimiento futuro. Durante el proceso de entrenamiento, se calculan las probabilidades de relevancia. Para consultas posteriores, los documentos se clasifican según estas probabilidades. Por ejemplo, en [20], cada ranking de entrada j se divide en varios segmentos, y se calcula la probabilidad condicional de relevancia (R) de cada documento di dependiendo del segmento k en el que se encuentre, es decir, prob(R|di, k, j). Para consultas posteriores, la puntuación de cada documento di se da por n j=1 prob(R|di,k, j ) k. Le Calve y Savoy sugieren utilizar un enfoque de regresión logística para combinar puntajes. Se necesita datos de entrenamiento para inferir los parámetros del modelo. 2.3 Métodos Mayoritarios 2.3.1 Procedimiento de Condorcet La regla original de Condorcet [7] especifica que un ganador de la elección es cualquier elemento que vence o empata con cada otro elemento en un concurso de a pares. Formalmente, sea C(diσdi ) = { j∈ PR : di j di } la coalición de clasificaciones que son concordantes con el establecimiento de diσdi, es decir, con la proposición de que di debería ser clasificado mejor que di en la clasificación final σ. di vence o empata con di si y solo si |C(diσdi )| ≥ |C(di σdi)|. La aplicación repetitiva del algoritmo de Condorcet puede producir una clasificación de elementos de forma natural: selecciona al ganador de Condorcet, elimínalo de las listas y repite los dos pasos anteriores hasta que no haya más documentos por clasificar. Dado que no siempre hay ganadores de Condorcet, se han desarrollado variaciones del procedimiento de Condorcet dentro de la teoría de ayuda a la decisión de múltiples criterios, con métodos como ELECTRE [26]. 2.3.2 Agregación Óptima de Kemeny Como en la sección 2.2.3, una clasificación de consenso minimiza una distancia geométrica de las clasificaciones de entrada, donde se utiliza la distancia de Kendall tau en lugar de la distancia de regla de pie de Spearman. Formalmente, dadas dos listas completas j y j , la distancia de Kendall tau se define como K( j, j ) = |{(di, di ) : i < i , rj i < rj i , rj i > rj i }|, es decir, el número de desacuerdos en pares entre las dos listas. Es fácil demostrar que la clasificación de consenso corresponde a la mediana geométrica de las clasificaciones de entrada y que el problema de agregación óptima de Kemeny corresponde al problema del conjunto mínimo de aristas de retroalimentación. Métodos de cadena de Markov (MCs) han sido utilizados por Dwork et al. [11] como un método natural para obtener una clasificación de consenso donde los estados corresponden a los documentos a ser clasificados y las probabilidades de transición varían dependiendo de la interpretación del evento de transición. En la misma referencia, los autores propusieron cuatro MC específicos y las pruebas experimentales habían demostrado que el siguiente MC es el que mejor rendimiento tiene (ver también [24]): • MC4: pasar del estado actual di al siguiente estado di eligiendo primero un documento di de manera uniforme de D. Si para la mayoría de las clasificaciones tenemos rj i ≤ rj i , entonces pasar a di, de lo contrario, quedarse en di. La clasificación de consenso corresponde a la distribución estacionaria de MC4.3. 3.1 Limitada importancia de las clasificaciones Las posiciones exactas de los documentos en una clasificación de entrada tienen una importancia limitada y no deben ser sobredimensionadas. Por ejemplo, al tener tres documentos relevantes en las tres primeras posiciones, cualquier perturbación de estos tres elementos tendrá el mismo valor. De hecho, en el contexto de IR, el orden completo proporcionado por un método de entrada puede ocultar empates. En este caso, llamamos a tales clasificaciones semiórdenes. Esto fue descrito en [13] como el problema de la agregación con empates. Por lo tanto, es importante construir la clasificación de consenso basada en información sólida: los documentos con posiciones cercanas en j tienen más probabilidades de tener intereses o relevancia similares. Por lo tanto, una ligera perturbación en la clasificación inicial no tiene sentido. • Suponiendo que el documento di está mejor clasificado que el documento di en una clasificación j, di es más probable que sea definitivamente más relevante que di en j cuando el número de posiciones intermedias entre di y di aumenta. 3.2 Listas Parciales En aplicaciones del mundo real, como los motores de búsqueda, las clasificaciones proporcionadas por los métodos de entrada suelen ser listas parciales. Esto fue descrito en [14] como el problema de tener que fusionar los mejores k resultados de varias listas de entrada. Por ejemplo, en los experimentos realizados por Dwork et al. [11], los autores encontraron que entre los 100 mejores documentos de 7 motores de búsqueda de entrada, el 67% de los documentos estaban presentes en solo un motor de búsqueda, mientras que menos de dos documentos estaban presentes en todos los motores de búsqueda. La agregación de rangos de listas parciales plantea cuatro dificultades principales que exponemos a continuación, proponiendo para cada una de ellas varias suposiciones de trabajo: 1. Las listas parciales pueden tener diversas longitudes, lo cual puede favorecer a las listas largas. Por lo tanto, consideramos las siguientes dos hipótesis de trabajo: H1 k: Solo consideramos los k mejores documentos de cada clasificación de entrada. Hola a todos: Consideramos todos los documentos de cada clasificación de entrada. 2. Dado que hay diferentes documentos en las clasificaciones de entrada, debemos decidir qué documentos deben mantenerse en la clasificación de consenso. Por lo tanto, se consideran dos hipótesis de trabajo: H2 k: Solo consideramos documentos que estén presentes en al menos k clasificaciones de entrada (k > 1). Hola a todos: Consideramos todos los documentos que están clasificados en al menos una clasificación de entrada. De ahora en adelante, llamaremos documentos que se mantendrán en la clasificación de consenso, documentos candidatos, y documentos que serán excluidos de la clasificación de consenso, documentos excluidos. También llamamos a un documento candidato que falta en uno o más rankings, un documento faltante. 3. Algunos documentos candidatos faltan en algunas clasificaciones de entrada. Las razones principales por las que falta un documento son que no fue indexado o que fue indexado pero considerado irrelevante; generalmente esta información no está disponible. Consideramos las siguientes dos hipótesis de trabajo: H3 sí: Cada documento faltante en cada j se le asigna una posición. H3 no: No se hace ninguna suposición, es decir, cada documento faltante se considera ni mejor ni peor que cualquier otro documento. 4. Cuando se cumple la suposición H2 k, cada clasificación de entrada puede contener documentos que no serán considerados en la clasificación de consenso. En cuanto a las posiciones de los documentos candidatos, podemos considerar las siguientes hipótesis de trabajo: H4 init: Las posiciones iniciales de los documentos candidatos se mantienen en cada clasificación de entrada. H4 nuevo: Los documentos candidatos reciben nuevas posiciones en cada clasificación de entrada, después de descartar los excluidos. En el contexto de la recuperación de información, los métodos de agregación de rangos necesitan decidir de manera más o menos explícita qué supuestos retener con respecto a las dificultades mencionadas anteriormente. 4. Enfoque de clasificación para la agregación de rangos 4.1 Presentación Los métodos posicionales consideran implícitamente que las posiciones de los documentos en las clasificaciones de entrada son puntajes, otorgando así un significado cardinal a una información ordinal. Esto constituye una suposición fuerte que es cuestionable, especialmente cuando las clasificaciones de entrada tienen longitudes diferentes. Además, para los métodos posicionales, las suposiciones H3 y H4, que suelen ser arbitrarias, tienen un fuerte impacto en los resultados. Por ejemplo, consideremos un ranking de entrada de 500 documentos de entre 1000 documentos candidatos. Ya sea que asignemos a cada uno de los documentos faltantes la posición 1, 501, 750 o 1000 -correspondiente a variaciones de H3 sí- dará lugar a resultados muy contrastantes, especialmente en lo que respecta a la parte superior de la clasificación de consenso. Los métodos mayoritarios no sufren de las desventajas mencionadas anteriormente de los métodos posicionales, ya que construyen clasificaciones de consenso explotando solo la información ordinal contenida en las clasificaciones de entrada. Sin embargo, ellos suponen que tales clasificaciones son órdenes completos, ignorando que pueden ocultar empates. Por lo tanto, los métodos mayoritarios basan las clasificaciones de consenso en información discriminante ilusoria en lugar de información menos discriminante pero más robusta. Tratando de superar los límites de los métodos actuales de agregación de rangos, descubrimos que los enfoques de superación, que inicialmente se utilizaron para problemas de agregación de múltiples criterios [26], también pueden ser utilizados con el propósito de agregación de rangos, donde cada clasificación desempeña el papel de un criterio. Por lo tanto, para decidir si un documento di debería ser clasificado mejor que di en la clasificación de consenso σ, se deben cumplir las dos siguientes condiciones: • una condición de concordancia que garantiza que la mayoría de las clasificaciones de entrada sean concordantes con di en σ (principio de mayoría). • una condición de discordancia que garantiza que ninguna de las clasificaciones de entrada discordantes refute fuertemente a di en σ (principio de respeto a las minorías). Formalmente, la coalición de concordancia con diσdi es Csp (diσdi) = { j∈ PR : rj i ≤ rj i − sp}, donde sp es un umbral de preferencia que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una situación de indiferencia y una de preferencia entre documentos. La coalición de discordancia con diσdi es Dsv (diσdi) = {j ∈ PR: rj i ≥ rj i + sv}, donde sv es un umbral de veto que representa la variación de las posiciones de los documentos, ya sea de forma absoluta o relativa a la longitud de la clasificación, que establece los límites entre una oposición débil y fuerte a diσdi. Dependiendo de la definición exacta de las coaliciones de concordancia y discordancia precedentes que conducen a la definición de algunas reglas de decisión, se pueden definir varias relaciones de prelación. Pueden ser más o menos exigentes dependiendo de i) los valores de los umbrales sp y sv, ii) la importancia o tamaño mínimo cmin requerido para la coalición de concordancia, y iii) la importancia o tamaño máximo dmax de la coalición de discordancia. Una relación de superación genérica puede definirse de la siguiente manera: diS(sp,sv,cmin,dmax)di ⇔ |Csp (diσdi )| ≥ cmin Y |Dsv (diσdi )| ≤ dmax Esta expresión define una familia de relaciones de superación anidadas ya que S(sp,sv,cmin,dmax) ⊆ S(sp,sv,cmin,dmax) cuando cmin ≥ cmin y/o dmax ≤ dmax y/o sp ≥ sp y/o sv ≤ sv. Esta expresión también generaliza la regla de la mayoría que corresponde a la relación particular S(0,∞, n 2 ,n). También satisface propiedades importantes de los métodos de agregación de rangos, llamadas neutralidad, optimalidad de Pareto, propiedad de Condorcet y propiedad de Condorcet extendida, en la literatura de elección social [29]. Las relaciones de jerarquización no son necesariamente transitivas y no necesariamente corresponden a clasificaciones, ya que pueden existir ciclos dirigidos. Por lo tanto, necesitamos procedimientos específicos para obtener un ranking de consenso. Proponemos el siguiente procedimiento que encuentra sus raíces en [27]. Consiste en dividir el conjunto de documentos en r clases clasificadas. Cada clase Ch contiene documentos con la misma relevancia y resultados de la aplicación de todas las relaciones (si es posible) al conjunto de documentos restantes después de que se calculen las clases anteriores. Los documentos dentro de la misma clase de equivalencia se clasifican de forma arbitraria. Formalmente, sea • R el conjunto de documentos candidatos para una consulta, • S1 , S2 , . . . una familia de relaciones de superación anidadas, • Fk(di, E) = |{di ∈ E : di Sk di }| sea el número de documentos en E(E ⊆ R) que podrían considerarse peores que di según la relación Sk , • fk(di, E) = |{di ∈ E : di Sk di}| sea el número de documentos en E que podrían considerarse mejores que di según Sk , • sk(di, E) = Fk(di, E) − fk(di, E) sea la calificación de di en E según Sk. Cada clase Ch resulta de un proceso de destilación. Corresponde al último destilado de una serie de conjuntos E0 ⊇ E1 ⊇ . . . donde E0 = R \ (C1 ∪ . . . ∪ Ch−1) y Ek es un subconjunto reducido de Ek−1 resultante de la aplicación del siguiente procedimiento: 1. calcular para cada di ∈ Ek−1 su calificación según Sk, es decir, sk(di, Ek−1), 2. definir smax = maxdi∈Ek−1 {sk(di, Ek−1)}, luego 3. Ek = {di ∈ Ek−1 : sk(di, Ek−1) = smax} Cuando se utiliza una relación de clasificación, el proceso de destilación se detiene después de la primera aplicación del procedimiento anterior, es decir, Ch corresponde al destilado E1. Cuando se utilizan diferentes relaciones de clasificación, el proceso de destilación se detiene cuando se han utilizado todas las relaciones de clasificación predefinidas o cuando |Ek| = 1. 4.2 Ejemplo ilustrativo Esta sección ilustra los conceptos y procedimientos de la sección 4.1. Consideremos un conjunto de documentos candidatos R = {d1, d2, d3, d4, d5}. La siguiente tabla proporciona un perfil PR de diferentes clasificaciones de los documentos de R: PR = (1, 2, 3, 4). Tabla 1: Clasificación de documentos rj i 1 2 3 4 d1 1 3 1 5 d2 2 1 3 3 d3 3 2 2 1 d4 4 4 5 2 d5 5 5 4 4 Supongamos que los umbrales de preferencia y veto están establecidos en los valores 1 y 4 respectivamente, y que los umbrales de concordancia y discordancia están establecidos en los valores 2 y 1 respectivamente. Las siguientes tablas muestran las matrices de concordancia, discordancia y de clasificación por orden de preferencia. Cada entrada csp (di, di) (dsv (di, di)) en la matriz de concordancia (discordancia) da el número de clasificaciones que son concordantes (discordantes) con diσdi, es decir, csp (di, di) = |Csp (diσdi)| y dsv (di, di) = |Dsv (diσdi)|. Tabla 2: Cálculo de la relación de superación d1 d2 d3 d4 d5 d1 - 2 2 3 3 d2 2 - 2 3 4 d3 2 2 - 4 4 d4 1 1 0 - 3 d5 1 0 0 1 Matriz de Concordancia d1 d2 d3 d4 d5 d1 - 0 1 0 0 d2 0 - 0 0 0 d3 0 0 - 0 0 d4 1 0 0 - 0 d5 1 1 0 0 Matriz de Discordancia d1 d2 d3 d4 d5 d1 - 1 1 1 1 d2 1 - 1 1 1 d3 1 1 - 1 1 d4 0 0 0 - 1 d5 0 0 0 0 Matriz de Superación (S1) Por ejemplo, la coalición de concordancia para la afirmación d1σd4 es C1(d1σd4) = { 1, 2, 3} y la coalición de discordancia para la misma afirmación es D4(d1σd4) = ∅. Por lo tanto, c1(d1, d4) = 3, d4(d1, d4) = 0 y d1S1 d4 se cumple. Observa que Fk(di, R) (fk(di, R)) se obtiene sumando los valores de la fila (columna) i-ésima de la matriz de clasificación. La clasificación de consenso se obtiene de la siguiente manera: para obtener la primera clase C1, calculamos las calificaciones de todos los documentos de E0 = R con respecto a S1. Son respectivamente 2, 2, 2, -2 y -4. Por lo tanto, smax es igual a 2 y C1 = E1 = {d1, d2, d3}. Observe que, si hubiéramos utilizado una segunda relación de clasificación S2(⊇ S1), estos tres documentos podrían haber sido posiblemente discriminados. En esta etapa, eliminamos los documentos de C1 de la matriz de clasificación y calculamos la siguiente clase C2: calculamos las nuevas calificaciones de los documentos de E0 = R \ C1 = {d4, d5}. Son respectivamente 1 y -1. Entonces C3 = E1 = {d4}. El último documento d5 es el único documento de la última clase C3. Por lo tanto, la clasificación de consenso es {d1, d2, d3} → {d4} → {d5}. 5. EXPERIMENTOS Y RESULTADOS 5.1 Configuración de la Prueba Para facilitar la investigación empírica de la metodología propuesta, desarrollamos un motor de búsqueda prototipo que implementa una versión de nuestro enfoque de clasificación para la agregación de rangos. En este artículo, aplicamos nuestro enfoque a la tarea de Destilación de Temas (TD) de la pista web TREC-2004 [10]. En esta tarea, hay 75 temas donde solo se proporciona una breve descripción de cada uno. Para cada consulta, conservamos las clasificaciones de las 10 mejores ejecuciones de la tarea TD proporcionadas por los equipos participantes en TREC-2004. Las actuaciones de estas carreras se informan en la tabla 3. Tabla 3: Rendimientos de las 10 mejores ejecuciones de la tarea TD de TREC-2004. ID de ejecución MAP P@10 S@1 S@5 S@10 uogWebCAU150 17.9% 24.9% 50.7% 77.3% 89.3% MSRAmixed1 17.8% 25.1% 38.7% 72.0% 88.0% MSRC04C12 16.5% 23.1% 38.7% 74.7% 80.0% humW04rdpl 16.3% 23.1% 37.3% 78.7% 90.7% THUIRmix042 14.7% 20.5% 21.3% 58.7% 74.7% UAmsT04MWScb 14.6% 20.9% 36.0% 66.7% 76.0% ICT04CIIS1AT 14.1% 20.8% 33.3% 64.0% 78.7% SJTUINCMIX5 12.9% 18.9% 29.3% 57.3% 72.0% MU04web1 11.5% 19.9% 33.3% 64.0% 76.0% MeijiHILw3 11.5% 15.3% 30.7% 54.7% 64.0% Promedio 14.7% 21.2% 34.9% 66.8% 78.94% Para cada consulta, cada ejecución proporciona un ranking de aproximadamente 1000 documentos. El número de documentos recuperados por todas estas ejecuciones varía de 543 a 5769. Su número promedio (mediana) es 3340 (3386). Vale la pena señalar que encontramos distribuciones similares de los documentos entre las clasificaciones como en [11]. Para la evaluación, utilizamos la herramienta estándar trec eval que es utilizada por la comunidad TREC para calcular las medidas estándar de efectividad del sistema que son la Precisión Promedio Media (MAP) y el Éxito@n (S@n) para n=1, 5 y 10. Nuestro enfoque de efectividad se compara con algunos resultados oficiales de alto rendimiento de TREC-2004, así como con algunos algoritmos estándar de agregación de rangos. En los experimentos, las pruebas de significancia se basan principalmente en la estadística t de Student, la cual se calcula en función de los valores de MAP de las ejecuciones comparadas. En las tablas de la siguiente sección, las diferencias estadísticamente significativas se marcan con un asterisco. Los valores entre corchetes de la primera columna de cada tabla indican el valor del parámetro de la ejecución correspondiente. 5.2 Resultados Realizamos varias series de ejecuciones para i) estudiar las variaciones de rendimiento del enfoque de clasificación cuando se ajustan los parámetros y suposiciones de trabajo, ii) comparar el rendimiento del enfoque de clasificación con estrategias estándar de agregación de rangos, y iii) verificar si la agregación de rangos funciona mejor que las mejores clasificaciones de entrada. Configuramos nuestro módulo de ejecución básico con los siguientes parámetros. Consideramos que cada clasificación de entrada es un orden completo (sp = 0) y que una clasificación de entrada refuta fuertemente a diσdi cuando la diferencia de posiciones de ambos documentos es lo suficientemente grande (sv = 75%). Los umbrales de preferencia y veto se calculan de forma proporcional al número de documentos retenidos en cada clasificación de entrada. Por lo tanto, pueden variar de un ranking a otro. Además, para aceptar la afirmación diσdi, supusimos que la mayoría de las clasificaciones deben ser concordantes (cmin = 50%) y que cada clasificación de entrada puede imponer su veto (dmax = 0). Los umbrales de concordancia y discordancia se calculan para cada tupla (di, di) como el porcentaje de las clasificaciones de entrada de PRi ∩ PRi. Por lo tanto, nuestra elección de parámetros conduce a la definición de la relación de superación S(0,75%,50%,0). Para probar el mcm de ejecución, habíamos elegido las siguientes suposiciones. Retuvimos los 100 mejores documentos de cada clasificación de entrada (H1 100), solo consideramos documentos que estén presentes en al menos la mitad de las clasificaciones de entrada (H2 5) y asumimos H3 no y H4 nuevo. En estas condiciones, el número de documentos exitosos fue de aproximadamente 100 en promedio, y el tiempo de cálculo por consulta fue inferior a un segundo. Obviamente, modificar las suposiciones de trabajo debería tener un impacto más profundo en el rendimiento que ajustar los parámetros de nuestro modelo. Esto fue validado por experimentos preliminares. Por lo tanto, a partir de ahora comenzamos estudiando la variación del rendimiento cuando se consideran diferentes conjuntos de suposiciones. Después, estudiamos el impacto de ajustar los parámetros. Finalmente, comparamos el rendimiento de nuestro modelo con respecto a las clasificaciones de entrada, así como con algunos algoritmos estándar de fusión de datos. La tabla 4 resume la variación del rendimiento del enfoque de clasificación por jerarquías bajo diferentes hipótesis de trabajo. En la Tabla 4: Impacto de las suposiciones de trabajo, se muestra que en la ejecución mcm22, en la que los documentos faltantes se colocan todos en la misma última posición de cada clasificación de entrada, se produce una disminución en el rendimiento con respecto a la ejecución mcm. Además, S@1 pasa de 41.33% a 34.67% (-16.11%). Esto muestra que varios documentos relevantes que inicialmente se ubicaron en la primera posición del ranking de consenso en mcm, pierden esta primera posición pero siguen clasificados en los 5 primeros documentos, ya que S@5 no cambió. También concluimos que los documentos que tienen posiciones bastante buenas en algunas clasificaciones de entrada son más propensos a ser relevantes, aunque falten en otras clasificaciones. Por consiguiente, cuando faltan en ciertas clasificaciones, asignarles rangos más bajos a estos documentos es perjudicial para el rendimiento. Además, a partir de la Tabla 4, encontramos que las actuaciones de las ejecuciones mcm y mcm23 son similares. Por lo tanto, el enfoque de clasificación no está sujeto a mantener las posiciones iniciales de los documentos candidatos o a recalcularlas descartando los excluidos. De la misma Tabla 4, el rendimiento del enfoque de clasificación aumenta significativamente para las ejecuciones mcm24 y mcm25. Por lo tanto, ya sea que consideremos todos los documentos presentes en la mitad de las clasificaciones (mcm24) o consideremos todos los documentos clasificados en las primeras 100 posiciones en una o más clasificaciones (mcm25), se incrementan los rendimientos. Este resultado era predecible ya que en ambos casos tenemos información más detallada sobre la importancia relativa de los documentos. Las tablas 5 y 6 confirman esta evidencia. La Tabla 5, donde los valores entre corchetes de la primera columna indican el número de documentos que se retienen de cada clasificación de entrada, muestra que seleccionar más documentos de cada clasificación de entrada conduce a un aumento en el rendimiento. Vale la pena mencionar que seleccionar más de 600 documentos de cada clasificación de entrada no mejora el rendimiento. Tabla 5: Impacto del número de documentos retenidos. Identificador de ejecución MAP S@1 S@5 S@10 mcm (100) 18.47% 41.33% 81.33% 86.67% mcm24-1 (200) 19.32% (+4.60%) 42.67% 78.67% 88.00% mcm24-2 (400) 19.88% (+7.63%*) 37.33% 80.00% 88.00% mcm24-3 (600) 20.80% (+12.62%*) 40.00% 80.00% 88.00% mcm24-4 (800) 20.66% (+11.86%*) 40.00% 78.67% 86.67% mcm24 (1000) 20.67% (+11.91%*) 38.66% 80.00% 86.66% La Tabla 6 informa ejecuciones correspondientes a variaciones de H2 k. Los valores entre corchetes son éxitos de rango. Por ejemplo, en la ejecución mcm32, solo se consideraron exitosos los documentos que estaban presentes en 3 o más clasificaciones de entrada. Esta tabla muestra que el rendimiento es significativamente mejor cuando se consideran documentos raros, mientras que disminuye significativamente cuando estos documentos son descartados. Por lo tanto, concluimos que muchos de los documentos relevantes son recuperados por un conjunto bastante pequeño de modelos de RI. Tabla 6: Rendimiento considerando diferentes éxitos de rango. Identificación de ejecución MAP S@1 S@5 S@10 mcm25 (1) 21.68% (+17.38%*) 40.00% 78.67% 89.33% mcm32 (3) 18.98% (+2.76%) 38.67% 80.00% 85.33% mcm (5) 18.47% 41.33% 81.33% 86.67% mcm33 (7) 15.83% (-14.29%*) 37.33% 78.67% 85.33% mcm34 (9) 10.96% (-40.66%*) 36.11% 66.67% 70.83% mcm35 (10) 7.42% (-59.83%*) 39.22% 62.75% 64.70% Para las ejecuciones mcm24 y mcm25, el número de documentos exitosos fue de aproximadamente 1000 y, por lo tanto, el tiempo de cálculo por consulta aumentó y se situó en alrededor de 5 segundos. 5.2.2 Impacto de la Variación de los Parámetros. La Tabla 7 muestra la variación de rendimiento del enfoque de clasificación por preferencias cuando se consideran diferentes umbrales de preferencia. Encontramos una mejora en el rendimiento hasta valores de umbral de aproximadamente el 5%, luego hay una disminución en el rendimiento que se vuelve significativa para valores de umbral superiores al 10%. Además, S@1 mejora del 41.33% al 46.67% cuando el umbral de preferencia cambia de 0 a 5%. Por lo tanto, podemos concluir que las clasificaciones de entrada son semiordeles en lugar de órdenes completos. La Tabla 8 muestra la evolución de las medidas de rendimiento con respecto al umbral de concordancia. Podemos concluir que para colocar el documento di antes de di en la clasificación de consenso, en la Tabla 7: Impacto de la variación del umbral de preferencia del 0 al 12.5%. Ejecutar Id MAP S@1 S@5 S@10 mcm (0%) 18.47% 41.33% 81.33% 86.67% mcm1 (1%) 18.57% (+0.54%) 41.33% 81.33% 86.67% mcm2 (2.5%) 18.63% (+0.87%) 42.67% 78.67% 86.67% mcm3 (5%) 18.69% (+1.19%) 46.67% 81.33% 86.67% mcm4 (7.5%) 18.24% (-1.25%) 46.67% 81.33% 86.67% mcm5 (10%) 17.93% (-2.92%) 40.00% 82.67% 86.67% mcm5b (12.5%) 17.51% (-5.20%*) 41.33% 80.00% 86.67% al menos la mitad de las clasificaciones de entrada de PRi ∩ PRi deben ser concordantes. El rendimiento disminuye significativamente para valores muy bajos y muy altos del umbral de concordancia. De hecho, para tales valores, la condición de concordancia se cumple o bien siempre por demasiados pares de documentos o no se cumple en absoluto, respectivamente. Por lo tanto, la relación de clasificación se vuelve demasiado débil o demasiado fuerte respectivamente. Tabla 8: Impacto de la variación de cmin Run Id MAP S@1 S@5 S@10 mcm11 (20%) 17.63% (-4.55%*) 41.33% 76.00% 85.33% mcm12 (40%) 18.37% (-0.54%) 42.67% 76.00% 86.67% mcm (50%) 18.47% 41.33% 81.33% 86.67% mcm13 (60%) 18.42% (-0.27%) 40.00% 78.67% 86.67% mcm14 (80%) 17.43% (-5.63%*) 40.00% 78.67% 86.67% mcm15 (100%) 16.12% (-12.72%*) 41.33% 70.67% 85.33% En los experimentos, variar el umbral de veto, así como el umbral de discordancia dentro de intervalos razonables, no tiene un impacto significativo en las medidas de rendimiento. De hecho, las ejecuciones con diferentes umbrales de veto (sv ∈ [50%; 100%]) tuvieron un rendimiento similar, aunque hay una ligera ventaja para las ejecuciones con valores de umbral altos, lo que significa que es mejor no permitir que las clasificaciones de entrada veten fácilmente. Además, el ajuste del umbral de discordancia se realizó para valores del 50% y 75% del umbral de veto. Para estas ejecuciones no observamos ninguna variación de rendimiento notable, aunque para umbrales de discordancia bajos (dmax < 20%), el rendimiento disminuyó ligeramente. 5.2.3 Impacto de la Variación del Número de Clasificaciones de Entrada Para estudiar la evolución del rendimiento cuando se consideran diferentes conjuntos de clasificaciones de entrada, realizamos tres ejecuciones adicionales donde se consideran 2, 4 y 6 de los conjuntos de clasificaciones de entrada con mejor rendimiento. Los resultados reportados en la Tabla 9 parecen ser contraintuitivos y tampoco respaldan hallazgos previos en la investigación sobre la agregación de rangos [3]. Sin embargo, este resultado muestra que las clasificaciones de bajo rendimiento aportan más ruido que información para establecer la clasificación de consenso. Por lo tanto, cuando se consideran, el rendimiento disminuye. Tabla 9: Rendimiento considerando diferentes conjuntos de clasificaciones de entrada con mejor rendimiento. Identificador de ejecución MAP S@1 S@5 S@10 mcm (10) 18.47% 41.33% 81.33% 86.67% mcm27 (6) 18.60% (+0.70%) 41.33% 80.00% 85.33% mcm28 (4) 19.02% (+2.98%) 40.00% 86.67% 88.00% mcm29 (2) 18.33% (-0.76%) 44.00% 76.00% 88.00% 5.2.4 Comparación del rendimiento de diferentes métodos de agregación de clasificaciones. En este conjunto de ejecuciones, comparamos el enfoque de clasificación con algunos métodos de agregación de clasificaciones estándar que han demostrado tener un rendimiento aceptable en estudios anteriores: consideramos dos métodos posicionales que son las estrategias CombSUM y CombMNZ. También examinamos el rendimiento de un método mayoritario que es el método de la cadena de Markov (MC4). Para las comparaciones, consideramos una relación de superación específica S∗ = S(5%,50%,50%,30%) que resulta en buenos rendimientos generales al ajustar todos los parámetros. La primera fila de la Tabla 10 muestra el rendimiento de los métodos de agregación de rangos con respecto a un conjunto de suposiciones básicas A1 = (H1 100, H2 5, H4 nuevo): solo consideramos los 100 primeros documentos de cada clasificación, luego retenemos los documentos presentes en 5 o más clasificaciones y actualizamos los rangos de los documentos exitosos. Para los métodos posicionales, colocamos los documentos faltantes en la cola de la clasificación (H3 sí), mientras que para nuestro método, al igual que para MC4, conservamos la hipótesis H3 no. Las tres filas siguientes de la Tabla 10 informan sobre los rendimientos al cambiar un elemento del conjunto de suposiciones básicas: la segunda fila corresponde al conjunto de suposiciones A2 = (H1 1000, H2 5, H4 nuevo), es decir, cambiar el número de documentos retenidos de 100 a 1000. La tercera fila corresponde al conjunto de supuestos A3 = (H1 100, H2 todos, H4 nuevos), es decir, considerando los documentos presentes en al menos un ranking. La cuarta fila corresponde al conjunto de supuestos A4 = (H1 100, H2 5, H4 init), es decir, manteniendo los rangos originales de los documentos exitosos. La quinta fila de la Tabla 10, etiquetada como A5, muestra el rendimiento cuando se consideran todas las 225 consultas de la pista web de TREC-2004. Obviamente, el nivel de rendimiento no se puede comparar con las líneas anteriores ya que las consultas adicionales son diferentes de las consultas TD y corresponden a otras tareas (tareas de Página de Inicio y Página Nombrada [10]) de la pista web TREC-2004. Este conjunto de pruebas tiene como objetivo demostrar si el rendimiento relativo de los diferentes métodos depende de la tarea. La última fila de la Tabla 10, etiquetada como A6, informa el rendimiento de los diversos métodos considerando la tarea TD de TREC2002 en lugar de TREC-2004: fusionamos los resultados de las clasificaciones de entrada de las 10 mejores ejecuciones oficiales para cada una de las 50 consultas TD [9] considerando el conjunto de suposiciones A1 de la primera fila. Esto tiene como objetivo mostrar si el rendimiento relativo de los diferentes métodos cambia de un año a otro. Los valores entre corchetes de la Tabla 10 son variaciones del rendimiento de cada método de agregación de rangos con respecto al rendimiento del enfoque de superación. Tabla 10: Rendimiento (MAP) de diferentes métodos de agregación de rangos bajo 3 colecciones de pruebas diferentes mcm combSUM combMNZ markov A1 18.79% 17.54% (-6.65%*) 17.08% (-9.10%*) 18.63% (-0.85%) A2 21.36% 19.18% (-10.21%*) 18.61% (-12.87%*) 21.33% (-0.14%) A3 21.92% 21.38% (-2.46%) 20.88% (-4.74%) 19.35% (-11.72%*) A4 18.64% 17.58% (-5.69%*) 17.18% (-7.83%*) 18.63% (-0.05%) A5 55.39% 52.16% (-5.83%*) 49.70% (-10.27%*) 53.30% (-3.77%) A6 16.95% 15.65% (-7.67%*) 14.57% (-14.04%*) 16.39% (-3.30%) Del análisis de la tabla 10 se puede establecer lo siguiente: • para todas las ejecuciones, considerar todos los documentos en cada clasificación de entrada (A2) mejora significativamente el rendimiento (MAP aumenta en promedio un 11.62%). Esto es predecible ya que algunos documentos relevantes inicialmente no reportados recibirían mejores posiciones en la clasificación de consenso. • para todas las ejecuciones, considerar documentos incluso aquellos presentes en solo una clasificación de entrada (A3) mejora significativamente el rendimiento. Para mcm, combSUM y combMNZ, la mejora del rendimiento es más importante (el MAP aumenta en promedio un 20.27%) que para la ejecución de Markov (el MAP aumenta un 3.86%). • preservar las posiciones iniciales de los documentos (A4) o volver a calcularlas (A1) no tiene una influencia notable en el rendimiento para ambos métodos posicional y mayoritario. • considerar todas las consultas de la pista web de TREC2004 (A5) así como las consultas TD de la pista web de TREC-2002 (A6) no altera el rendimiento relativo de los diferentes métodos de fusión de datos. • considerando las consultas TD de la pista web de TREC2002, los rendimientos de todos los métodos de fusión de datos son más bajos que el del mejor ranking de entrada que tiene un valor de MAP de 18.58%. Esto se debe a que la mayoría de las clasificaciones de entrada fusionadas tienen un rendimiento muy bajo en comparación con la mejor, lo que añade más ruido a la clasificación de consenso. • los rendimientos de los métodos de fusión de datos mcm y markov son significativamente mejores que el de la mejor clasificación de entrada uogWebCAU150. Esto sigue siendo cierto solo para las ejecuciones combSUM y combMNZ bajo las suposiciones H1 todas o H2 todas. Esto demuestra que los métodos mayoritarios son menos sensibles a suposiciones que los métodos posicionales. El enfoque de superación siempre tiene un rendimiento significativamente mejor que los métodos posicionales combSUM y combMNZ. También tiene un mejor rendimiento que el método de la cadena de Markov, especialmente bajo la suposición H2 donde la diferencia de rendimientos se vuelve significativa. 6. CONCLUSIONES En este artículo, abordamos el problema de agregación de rangos donde se deben fusionar listas de documentos diferentes, pero no disjuntas. Notamos que las clasificaciones de entrada pueden ocultar empates, por lo que no deben considerarse como órdenes completos. Solo se debe utilizar información sólida de cada clasificación de entrada. Los métodos actuales de agregación de rangos, y especialmente los métodos posicionales (por ejemplo, combSUM [15]), no fueron diseñados inicialmente para trabajar con tales clasificaciones. Deben adaptarse teniendo en cuenta supuestos de trabajo específicos. Proponemos un nuevo método de clasificación para la agregación de rangos que está bien adaptado al contexto de la RI. De hecho, clasifica dos documentos con respecto a la intensidad de la diferencia de sus posiciones en cada clasificación de entrada y también considera el número de clasificaciones de entrada que son concordantes y discordantes a favor de un documento específico. Tampoco es necesario hacer suposiciones específicas sobre las posiciones de los documentos faltantes. Esta es una característica importante, ya que la ausencia de un documento en un ranking no necesariamente debe interpretarse de forma negativa. Los resultados experimentales muestran que el método de clasificación supera significativamente a los populares métodos clásicos de fusión de datos posicionales como las estrategias combSUM y combMNZ. También supera en rendimiento a los métodos mayoritarios de buen rendimiento, como el método de la cadena de Markov. Estos resultados se prueban con diferentes colecciones de pruebas y consultas. De los experimentos, también podemos concluir que para mejorar el rendimiento, deberíamos fusionar listas de resultados de modelos de IR con buen desempeño, y que los métodos de fusión de datos mayoritarios funcionan mejor que los métodos posicionales. El método propuesto puede tener un impacto real en el rendimiento de la metabúsqueda web, ya que la mayoría de los motores de búsqueda primarios solo proporcionan clasificaciones, mientras que la mayoría de los enfoques actuales necesitan puntuaciones para fusionar las listas de resultados en una sola lista. El trabajo adicional implica investigar si el enfoque de clasificación por jerarquía funciona bien en varios otros contextos, por ejemplo, utilizando las puntuaciones de los documentos o alguna combinación de los rangos y puntuaciones de los documentos. Agradecimientos Los autores desean agradecer a Jacques Savoy por sus valiosos comentarios sobre una versión preliminar de este artículo. 7. REFERENCIAS [1] A. Aronson, D. Demner-Fushman, S. Humphrey, J. Lin, H. Liu, P. Ruch, M. Ruiz, L. Smith, L. Tanabe y W. Wilbur. Fusión de enfoques intensivos en conocimiento y estadísticos para recuperar y anotar documentos genómicos textuales. En Actas de TREC2005. Publicación del NIST, 2005. [2] R. A. Baeza-Yates y B. A. Ribeiro-Neto. Recuperación de información moderna. ACM Press, 1999. [3] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de múltiples sistemas de recuperación clasificados. En Actas ACM-SIGIR94, páginas 173-181. Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. 

Springer-Verlag, 1994. [4] N. J. Belkin, P. Kantor, E. A. Fox, y J. A. Shaw. Combinando evidencia de múltiples representaciones de consulta para la recuperación de información. IPM, 31(3):431-448, 1995. [5] J. Borda. 

IPM, 31(3):431-448, 1995. [5] J. Borda. Memoria sobre las elecciones por voto secreto. Historia de la Academia de Ciencias, 1781. [6] J. P. Callan, Z. Lu y W. B. Croft. Buscando colecciones distribuidas con redes de inferencia. En Actas ACM-SIGIR95, páginas 21-28, 1995. [7] M. Condorcet. Ensayo sobre la aplicación del análisis de probabilidad a las decisiones tomadas por mayoría de votos. Imprimerie Royale, París, 1785. [8] W. D. Cook y M. Kress. Clasificación ordinal con intensidad de preferencia. Ciencia de la Gestión, 31(1):26-32, 1985. [9] N. Craswell y D. Hawking. Resumen de la pista web TREC-2002. En Actas de TREC2002. Publicación del NIST, 2002. [10] N. Craswell y D. Hawking. Resumen de la TREC-2004 Web Track. En Actas de TREC2004. Publicación del NIST, 2004. [11] C. Dwork, S. R. Kumar, M. Naor y D. Sivakumar. Métodos de agregación de clasificaciones para la Web. En Actas WWW2001, páginas 613-622, 2001. [12] R. Fagin. Combinando información difusa de múltiples sistemas. JCSS, 58(1):83-99, 1999. [13] R. Fagin, R. Kumar, M. Mahdian, D. Sivakumar y E. Vee. Comparando y agregando clasificaciones con empates. En PODS, páginas 47-58, 2004. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas de los k mejores. SIAM J. en Matemáticas Discretas, 17(1):134-160, 2003. [15] E. A. Zorro y J. A. Shaw. Combinación de múltiples búsquedas. En Actas de TREC3. Publicación del NIST, 1994. [16] J. Katzer, M. McGill, J. Tessier, W. Frakes y P. DasGupta. Un estudio de la superposición entre representaciones de documentos. Tecnología de la Información: Investigación y Desarrollo, 1(4):261-274, 1982. [17] L. S. Larkey, M. E. Connell y J. Callan. Selección de colecciones y fusión de resultados con patentes de EE. UU. organizadas por tema y datos de TREC. En las Actas ACM-CIKM2000, páginas 282-289. ACM Press, 2000. [18] A.
ACM Press, 2000. [18] A. Le Calv´e y J. Savoy. Estrategia de fusión de bases de datos basada en regresión logística. IPM, 36(3):341-359, 2000. [19] J. H. Lee. 

IPM, 36(3):341-359, 2000. [19] J. H. Lee. Análisis de la combinación de múltiples evidencias. En Actas ACM-SIGIR97, páginas 267-276, 1997. [20] D. Lillis, F. Toolan, R. Collier y J. Dunnion. Probfuse: un enfoque probabilístico para la fusión de datos. En las Actas ACM-SIGIR2006, páginas 139-146. ACM Press, 2006. [21] J. I. Marden. 

ACM Press, 2006. [21] J. I. Marden. Analizando y modelando datos de rango. Número 64 en Monografías sobre Estadística y Probabilidad Aplicada. Chapman & Hall, 1995. [22] M. Montague y J. A. Aslam. Consistencia de la metabúsqueda. En las Actas ACM-SIGIR2001, páginas 386-387. ACM Press, 2001. [23] D. M. Pennock y E. Horvitz. Análisis de los fundamentos axiomáticos del filtrado colaborativo. En el Taller sobre Inteligencia Artificial para el Comercio Electrónico en la 16ª Conferencia Nacional de Inteligencia Artificial, 1999. [24] M. E. Renda y U. Straccia. Búsqueda web metasearch: métodos de agregación de rango basados en rango vs. puntuación. En las Actas de ACM-SAC2003, páginas 841-846. ACM Press, 2003. [25] W. H. Riker. 

ACM Press, 2003. [25] W. H. Riker. Liberalismo contra populismo. Waveland Press, 1982. [26] B. Roy. 

Waveland Press, 1982. [26] B. Roy. El enfoque de jerarquización y los fundamentos de los métodos ELECTRE. Teoría y decisión, 31:49-73, 1991. [27] B. Roy y J. Hugonnard. Clasificación de proyectos de extensión de líneas suburbanas en el sistema de metro de París mediante un método multicriterio. Investigación en Transporte, 16A(4):301-312, 1982. [28] L. Si y J. Callan. Utilizando datos muestreados y regresión para fusionar resultados de motores de búsqueda. En las Actas ACM-SIGIR2002, páginas 19-26. ACM Press, 2002. [29] M. Truchon. 

ACM Press, 2002. [29] M. Truchon. Una extensión del criterio de Condorcet y órdenes de Kemeny. Cuaderno 9813, Centro de Investigación en Economía y Finanzas Aplicadas, octubre de 1998. [30] H. Turtle y W. B. Croft. Redes de inferencia para la recuperación de documentos. En Actas de ACM-SIGIR90, páginas 1-24. ACM Press, 1990. [31] C. C. Vogt y G. W. Cottrell. Fusión a través de una combinación lineal de puntuaciones. Recuperación de información, 1(3):151-173, 1999.