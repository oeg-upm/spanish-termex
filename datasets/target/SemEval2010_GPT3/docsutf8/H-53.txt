Stemming sensible al contexto para la búsqueda web Fuchun Peng Nawaaz Ahmed Xin Li Yumao Lu Yahoo! Tradicionalmente, el truncamiento se ha aplicado a tareas de Recuperación de Información transformando las palabras en documentos a su forma raíz antes de indexarlas, y aplicando una transformación similar a los términos de consulta. Aunque aumenta la recuperación, esta estrategia ingenua no funciona bien para la Búsqueda en la Web, ya que disminuye la precisión y requiere una cantidad significativa de cálculos adicionales. En este artículo, proponemos un método de derivación sensible al contexto que aborda estos dos problemas. Dos propiedades únicas hacen que nuestro enfoque sea factible para la Búsqueda en la Web. Primero, basados en modelado estadístico del lenguaje, realizamos un análisis sensible al contexto en el lado de la consulta. Predecimos con precisión cuál de sus variantes morfológicas es útil para expandir un término de consulta antes de enviar la consulta al motor de búsqueda. Esto reduce drásticamente la cantidad de expansiones incorrectas, lo que a su vez disminuye el costo de la computación adicional y mejora la precisión al mismo tiempo. Segundo, nuestro enfoque realiza una coincidencia de documentos sensible al contexto para esas variantes ampliadas. Esta estrategia conservadora sirve como salvaguarda contra el truncamiento espurio, y resulta ser muy importante para mejorar la precisión. Usando el manejo de la pluralización de palabras como un ejemplo de nuestro enfoque de derivación, nuestros experimentos en un importante motor de búsqueda web muestran que al derivar solo el 29% del tráfico de consultas, podemos mejorar la relevancia medida por la Ganancia Acumulativa Descontada promedio (DCG5) en un 6.1% en estas consultas y en un 1.8% sobre todo el tráfico de consultas. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Almacenamiento y Recuperación de Información-Formulación de Consultas Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN La búsqueda en la web se ha convertido en una herramienta importante en nuestra vida diaria para buscar información. Uno de los problemas importantes en la búsqueda web es que las consultas de los usuarios a menudo no están formuladas de la mejor manera para obtener resultados óptimos. Por ejemplo, "zapatilla para correr" es una consulta que ocurre con frecuencia en los registros de consulta. Sin embargo, es mucho más probable que la búsqueda "zapatillas para correr" proporcione mejores resultados de búsqueda que la consulta original, ya que los documentos que coinciden con la intención de esta consulta suelen contener las palabras "zapatillas para correr". Formular correctamente una consulta requiere que el usuario prediga con precisión qué forma de palabra se utiliza en los documentos que mejor satisfacen sus necesidades de información. Esto es difícil incluso para usuarios experimentados, y especialmente difícil para hablantes no nativos. Una solución tradicional es utilizar el stemming [16, 18], el proceso de transformar palabras flexionadas o derivadas a su forma raíz para que un término de búsqueda coincida y recupere documentos que contengan todas las formas del término. Por lo tanto, la palabra "run" coincidirá con "running", "ran", "runs", y "shoe" coincidirá con "shoes" y "shoeing". El truncamiento se puede realizar tanto en los términos de un documento durante la indexación (y aplicando la misma transformación a los términos de la consulta durante el procesamiento de la consulta) o expandiendo la consulta con las variantes durante el procesamiento de la consulta. El truncamiento durante la indexación permite muy poca flexibilidad durante el procesamiento de consultas, mientras que el truncamiento mediante la expansión de consultas permite manejar cada consulta de manera diferente, y por lo tanto es preferible. Aunque el truncamiento tradicional aumenta la recuperación al emparejar variantes de palabras [13], puede reducir la precisión al recuperar demasiados documentos que han sido emparejados incorrectamente. Al examinar los resultados de aplicar el truncamiento a un gran número de consultas, generalmente se encuentra que casi igual cantidad de consultas se benefician y se ven perjudicadas por la técnica [6]. Además, reduce el rendimiento del sistema porque el motor de búsqueda tiene que hacer coincidir todas las variantes de palabras. Como mostraremos en los experimentos, esto es cierto incluso si simplificamos el proceso de derivación a la manipulación de pluralización, que es el proceso de convertir una palabra de su forma plural a singular, o viceversa. Por lo tanto, es necesario ser muy cauteloso al utilizar el stemming en los motores de búsqueda web. Un problema del truncamiento tradicional es su transformación ciega de todos los términos de consulta, es decir, siempre realiza la misma transformación para la misma palabra de consulta sin considerar el contexto de la palabra. Por ejemplo, la palabra "book" tiene cuatro formas: book, books, booking, booked, y la palabra "store" tiene cuatro formas: store, stores, storing, stored. Para la consulta de la librería, expandir ambas palabras a todas sus variantes aumenta significativamente el costo de computación y perjudica la precisión, ya que no todas las variantes son útiles para esta consulta. Transformar librería para que coincida con librerías está bien, pero hacer coincidir almacenamiento de libros o tienda de reservas no lo está. Un método de ponderación que otorga pesos más pequeños a las palabras variantes alivia los problemas hasta cierto punto si los pesos reflejan con precisión la importancia de la variante en esta consulta particular. Sin embargo, el peso uniforme no va a funcionar y un peso dependiente de la consulta sigue siendo un problema desafiante sin resolver [20]. Un segundo problema del truncamiento tradicional es su emparejamiento ciego de todas las ocurrencias en los documentos. Para la consulta de la librería, una transformación que permita que las tiendas variantes se emparejen hará que cada aparición de tiendas en el documento se trate de manera equivalente al término de consulta tienda. Por lo tanto, se emparejará un documento que contenga el fragmento de leer un libro en las cafeterías, lo que provocará que se seleccionen muchos documentos incorrectos. Aunque esperamos que la función de clasificación pueda manejar correctamente estos, con muchos más candidatos para clasificar, el riesgo de cometer errores aumenta. Para aliviar estos dos problemas, proponemos un enfoque de reducción de palabras raíz sensible al contexto para la búsqueda en la web. Nuestra solución consiste en dos análisis contextuales sensibles, uno en el lado de la consulta y otro en el lado del documento. En el lado de la consulta, proponemos un enfoque basado en modelado del lenguaje estadístico para predecir qué variantes de palabras son mejores formas que la palabra original con fines de búsqueda y expandir la consulta solo con esas formas. En el lado del documento, proponemos un emparejamiento conservador sensible al contexto para las variantes de palabras transformadas, solo emparejando las ocurrencias en el documento en el contexto de otros términos en la consulta. Nuestro modelo es simple pero efectivo y eficiente, lo que lo hace factible de ser utilizado en motores de búsqueda web comerciales reales. Utilizamos el manejo de pluralización como un ejemplo continuo para nuestro enfoque de derivación. La motivación para usar el manejo de pluralización como ejemplo es mostrar que incluso un truncamiento tan simple, si se maneja correctamente, puede brindar beneficios significativos a la relevancia de la búsqueda. Hasta donde sabemos, ninguna investigación previa ha investigado sistemáticamente el uso de la pluralización en la búsqueda en la web. Como debemos señalar, el método que proponemos no se limita al manejo de la pluralización, es una técnica de derivación general y también se puede aplicar a la expansión de consultas en general. Los experimentos sobre el truncamiento general producen mejoras significativas adicionales sobre el manejo de la pluralización para consultas largas, aunque los detalles no se informarán en este artículo. En el resto del documento, primero presentamos el trabajo relacionado y distinguimos nuestro método de trabajos anteriores en la Sección 2. Describimos los detalles del enfoque de derivación sensible al contexto en la Sección 3. Luego realizamos experimentos extensos en un importante motor de búsqueda web para respaldar nuestras afirmaciones en la Sección 4, seguidos de discusiones en la Sección 5. Finalmente, concluimos el artículo en la Sección 6.2. TRABAJO RELACIONADO El stemming es una tecnología estudiada desde hace mucho tiempo. Se han desarrollado muchos stemmers, como el stemmer Lovins [16] y el stemmer Porter [18]. El stemmer de Porter es ampliamente utilizado debido a su simplicidad y efectividad en muchas aplicaciones. Sin embargo, el algoritmo de Porter comete muchos errores porque sus reglas simples no pueden describir completamente la morfología del inglés. El análisis de corpus se utiliza para mejorar el stemmer de Porter [26] mediante la creación de clases de equivalencia para palabras que son morfológicamente similares y que ocurren en un contexto similar, medido por la información mutua esperada [23]. Utilizamos un enfoque de corpus similar para el stemming al calcular la similitud entre dos palabras basada en sus características de contexto distribucional, que pueden ser más que solo palabras adyacentes [15], y luego solo mantenemos las palabras morfológicamente similares como candidatas. El uso de la derivación en la recuperación de información también es una técnica bien conocida [8, 10]. Sin embargo, se informó previamente que la efectividad del truncamiento para los sistemas de consulta en inglés era bastante limitada. Lennon et al. [17] compararon los algoritmos de Lovins y Porter y encontraron poco mejoramiento en el rendimiento de recuperación. Más tarde, Harman [9] compara tres técnicas generales de derivación en experimentos de recuperación de texto, incluido el manejo de pluralización (llamado S stemmer en el artículo). También propusieron el truncamiento selectivo basado en la longitud de la consulta y la importancia del término, pero no se informaron resultados positivos. Por otro lado, Krovetz [14] realizó comparaciones sobre un pequeño número de documentos (de 400 a 12k) y mostró una mejora dramática en la precisión (de hasta un 45%). Sin embargo, debido al número limitado de consultas probadas (menos de 100) y al tamaño reducido de la colección, los resultados son difíciles de generalizar para la búsqueda en la web. Estos resultados mixtos, en su mayoría fracasos, llevaron a los primeros investigadores de IR a considerar que el truncamiento era irrelevante en general para el inglés [4], aunque investigaciones recientes han demostrado que el truncamiento tiene mayores beneficios para la recuperación en otros idiomas [2]. Sospechamos que los fracasos anteriores se debieron principalmente a los dos problemas que mencionamos en la introducción. El stemming ciego, o un stemming selectivo basado en la longitud de la consulta como se utiliza en [9], no es suficiente. El truncamiento debe decidirse caso por caso, no solo a nivel de consulta sino también a nivel de documento. Como demostraremos, si se maneja correctamente, se puede lograr una mejora significativa. Un problema más general relacionado con el stemming es la reformulación de consultas [3, 12] y la expansión de consultas que amplía las palabras no solo con variantes de palabras [7, 22, 24, 25]. Para decidir qué palabras expandidas usar, las personas a menudo utilizan técnicas de retroalimentación de seudorelevancia que envían la consulta original a un motor de búsqueda y recuperan los documentos principales, extraen palabras relevantes de estos documentos principales como palabras de consulta adicionales y vuelven a enviar la consulta expandida nuevamente [21]. Esto normalmente requiere enviar una consulta varias veces al motor de búsqueda y no es rentable para procesar la gran cantidad de consultas involucradas en la búsqueda web. Además, la expansión de consultas, incluida la reformulación de consultas [3, 12], tiene un alto riesgo de cambiar la intención del usuario (llamado desviación de consulta). Dado que las palabras expandidas pueden tener diferentes significados, agregarlas a la consulta podría potencialmente cambiar la intención de la consulta original. Por lo tanto, la expansión de consultas basada en pseudorelevancia y la reformulación de consultas pueden proporcionar sugerencias a los usuarios para su refinamiento interactivo, pero difícilmente pueden ser utilizadas directamente para la búsqueda en la web. Por otro lado, el truncamiento es mucho más conservador ya que la mayoría de las veces, conserva la intención de búsqueda original. Si bien la mayoría de los trabajos sobre la expansión de consultas se centran en mejorar la recuperación, nuestro trabajo se enfoca en aumentar tanto la recuperación como la precisión. El aumento en el recuerdo es evidente. Con el stemming de calidad, los buenos documentos que no fueron seleccionados antes del stemming serán promovidos y aquellos documentos de baja calidad serán degradados. En la expansión selectiva de consultas, Cronen-Townsend et al. [6] propusieron un método para la expansión selectiva de consultas basado en comparar la divergencia de Kullback-Leibler de los resultados de la consulta no expandida y los resultados de la consulta expandida. Esto es similar a la retroalimentación de relevancia en el sentido de que requiere múltiples pasadas de recuperación. Si una palabra puede expandirse en varias palabras, se requiere ejecutar este proceso varias veces para decidir cuál palabra expandida es útil. Es costoso implementar esto en motores de búsqueda web en producción. Nuestro método predice la calidad de la expansión basándose en información sin conexión sin enviar la consulta a un motor de búsqueda. En resumen, proponemos un enfoque novedoso para abordar un problema antiguo, pero aún importante y desafiante para la búsqueda en la web: el stemming. Nuestro enfoque es único en el sentido de que realiza el truncamiento predictivo de forma individualizada para cada consulta sin retroalimentación de relevancia de la Web, utilizando el contexto de las variantes en los documentos para preservar la precisión. Es simple, pero muy eficiente y efectivo, lo que hace factible el truncamiento en tiempo real para la búsqueda en la web. Nuestros resultados confirmarán a los investigadores que el truncamiento es realmente muy importante para la recuperación de información a gran escala. STEMMING CONTEXTUAL SENSIBLE 3.1 Resumen Nuestro sistema tiene cuatro componentes como se ilustra en la Figura 1: generación de candidatos, segmentación de consultas y detección de palabras clave, derivación de consultas sensible al contexto y coincidencia de documentos sensible al contexto. La generación de candidatos (componente 1) se realiza fuera de línea y los candidatos generados se almacenan en un diccionario. Para una consulta de entrada, primero segmentamos la consulta en conceptos y detectamos la palabra principal de cada concepto (componente 2). Luego utilizamos modelado de lenguaje estadístico para decidir si una variante particular es útil (componente 3), y finalmente, para las variantes expandidas, realizamos un emparejamiento de documentos sensible al contexto (componente 4). A continuación discutimos cada uno de los componentes con más detalle. Componente 4: coincidencia de documentos sensibles al contexto Consulta de entrada: y detección de palabras clave Componente 2: segmento Componente 1: generación de candidatos comparaciones −> comparación Componente 3: decisión de expansión selectiva de palabras: comparaciones −> comparación ejemplo: comparaciones de precios de hoteles salida: comparaciones de hoteles hotel −> hoteles Figura 1: Componentes del sistema 3.2 Generación de candidatos de expansión Una de las formas de generar candidatos es utilizando el stemmer de Porter [18]. El stemmer de Porter simplemente utiliza reglas morfológicas para convertir una palabra a su forma base. No tiene conocimiento del significado semántico de las palabras y a veces comete errores graves, como cambiar "executive" por "execution", "news" por "new" y "paste" por "past". Una forma más conservadora se basa en utilizar análisis de corpus para mejorar los resultados del stemmer de Porter [26]. El análisis de corpus que realizamos se basa en la similitud distribucional de palabras [15]. La razón de utilizar la similitud distribucional de palabras es que las variantes verdaderas tienden a ser utilizadas en contextos similares. En el cálculo de similitud de palabras distribucionales, cada palabra se representa con un vector de características derivadas del contexto de la palabra. Utilizamos los bigramas a la izquierda y a la derecha de la palabra como sus características de contexto, mediante la extracción de un gran corpus web. La similitud entre dos palabras es la similitud coseno entre los dos vectores de características correspondientes. Las 20 palabras similares a "desarrollar" se muestran en la siguiente tabla. rango candidato puntaje rango candidato puntaje 0 desarrollar 1 10 berts 0.119 1 desarrollando 0.339 11 wads 0.116 2 desarrollado 0.176 12 desarrollador 0.107 3 incubadora 0.160 13 promoviendo 0.100 4 desarrolla 0.150 14 desarrollo 0.091 5 desarrollo 0.148 15 reingeniería 0.090 6 tutoría 0.138 16 construir 0.083 7 analizando 0.128 17 construir 0.081 8 desarrollo 0.128 18 educativo 0.081 9 automatización 0.126 19 instituto 0.077 Tabla 1: Los 20 candidatos más similares a la palabra "desarrollar". La puntuación de la columna es la puntuación de similitud. Para determinar los candidatos de derivación, aplicamos algunas reglas morfológicas del stemmer de Porter [18] a la lista de similitud. Después de aplicar estas reglas, para la palabra desarrollar, los candidatos de derivación son desarrollando, desarrollado, desarrolla, desarrollo, desarrollo, desarrollador, y desarrollo. Para el propósito de manejo de pluralización, solo se conserva el candidato desarrolla. Una cosa que observamos al observar las palabras distribucionalmente similares es que están estrechamente relacionadas semánticamente. Estas palabras podrían servir como candidatas para la expansión general de consultas, un tema que investigaremos en el futuro. 3.3 Segmentación e identificación de palabras clave Para consultas largas, es bastante importante detectar los conceptos en la consulta y las palabras más importantes para esos conceptos. Primero dividimos una consulta en segmentos, cada segmento representando un concepto que normalmente es una frase nominal. Para cada una de las frases nominales, luego detectamos la palabra más importante a la que llamamos la palabra principal. La segmentación también se utiliza en la coincidencia sensible a documentos (sección 3.5) para hacer cumplir la proximidad. Para dividir una consulta en segmentos, tenemos que definir un criterio para medir la fuerza de la relación entre las palabras. Un método efectivo es utilizar la información mutua como indicador para decidir si dividir o no dos palabras [19]. Utilizamos un registro de 25 millones de consultas y recopilamos las frecuencias de bigramas y unigramas a partir de él. Para cada consulta entrante, calculamos la información mutua de dos palabras adyacentes; si supera un umbral predefinido, no dividimos la consulta entre esas dos palabras y pasamos a la siguiente palabra. Continuamos este proceso hasta que la información mutua entre dos palabras esté por debajo del umbral, luego creamos un límite conceptual aquí. La Tabla 2 muestra algunos ejemplos de segmentación de consultas. La forma ideal de encontrar la palabra principal de un concepto es realizar un análisis sintáctico para determinar la estructura de dependencia de la consulta. El análisis de consultas es más difícil que el análisis de oraciones, ya que muchas consultas no son gramaticales y son muy cortas. Aplicar un analizador entrenado en oraciones de documentos a consultas tendrá un rendimiento deficiente. En nuestra solución, solo utilizamos reglas heurísticas simples, y funciona muy bien en la práctica para el inglés. Para una frase nominal en inglés, la palabra principal suele ser la última palabra no detenida, a menos que la frase siga un patrón particular, como XYZ de/en/a/desde UVW. En tales casos, la palabra principal suele ser la última palabra no detenida de XYZ. 3.4 Expansión de palabras sensibles al contexto Después de detectar cuáles son las palabras más importantes para expandir, debemos decidir si las expansiones serán útiles. Nuestras estadísticas muestran que aproximadamente la mitad de las consultas pueden ser transformadas mediante la pluralización a través de un truncamiento ingenuo. Entre esta mitad, aproximadamente el 25% de las consultas mejoran la relevancia cuando se transforman, la mayoría (alrededor del 50%) no cambian sus 5 resultados principales, y el 25% restante tiene un rendimiento peor. Por lo tanto, es extremadamente importante identificar qué consultas no deben ser truncadas con el fin de maximizar la mejora de relevancia y minimizar el costo de truncamiento. Además, para una consulta con múltiples palabras que pueden ser transformadas, o una palabra con múltiples variantes, no todas las expansiones son útiles. Tomando la comparación de precios de hoteles como ejemplo, decidimos que hotel y comparación de precios son dos conceptos. Las palabras clave hotel y comparación se pueden expandir a hoteles y comparaciones. ¿Son útiles ambas transformaciones? Para probar si una expansión es útil, tenemos que saber si es probable que la consulta expandida obtenga más documentos relevantes de la web, lo cual puede cuantificarse mediante la probabilidad de que la consulta ocurra como una cadena en la web. Cuanto más probable sea que ocurra una consulta en la web, más documentos relevantes puede devolver esta consulta. Ahora todo el problema se convierte en cómo calcular la probabilidad de que ocurra la consulta en la Web. Calcular la probabilidad de que una cadena ocurra en un corpus es un problema bien conocido de modelado del lenguaje. El objetivo del modelado del lenguaje es predecir la probabilidad de secuencias de palabras que ocurren naturalmente, s = w1w2...wN; o más simplemente, asignar una alta probabilidad a las secuencias de palabras que realmente ocurren (y una baja probabilidad a las secuencias de palabras que nunca ocurren). El enfoque más simple y exitoso para el modelado del lenguaje sigue estando basado en el modelo n-gramo. Por la regla de la cadena de probabilidad, se puede escribir la probabilidad de cualquier secuencia de palabras como Pr(w1w2...wN) = ΠY i=1 Pr(wi|w1...wi−1) (1). Un modelo n-grama aproxima esta probabilidad asumiendo que las únicas palabras relevantes para predecir Pr(wi|w1...wi−1) son las n − 1 palabras anteriores; es decir, La probabilidad de una palabra wi dado w1...wi−1 es igual a la probabilidad de wi dado wi−n+1...wi−1. Una estimación directa de máxima verosimilitud de las probabilidades de n-gramas a partir de un corpus se obtiene a partir de la frecuencia observada de cada uno de los patrones Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) donde #(.) denota el número de ocurrencias de un n-grama específico en el corpus de entrenamiento. Aunque se podría intentar usar modelos de n-gramos simples para capturar dependencias a largo plazo en el lenguaje, intentar hacerlo directamente crea inmediatamente problemas de datos dispersos: Utilizar n-gramos de longitud hasta n implica estimar la probabilidad de eventos de longitud Wn, donde W es el tamaño del vocabulario de palabras. Esto rápidamente abruma los recursos computacionales y de datos modernos incluso para opciones modestas de n (más allá de 3 a 6). Además, debido a la naturaleza de cola pesada del lenguaje (es decir, La ley de Zipf) es probable que uno se encuentre con n-gramas novedosos que nunca fueron observados durante el entrenamiento en ningún corpus de prueba, por lo tanto, algún mecanismo para asignar una probabilidad distinta de cero a los n-gramas novedosos es un problema central e inevitable en la modelización estadística del lenguaje. Un enfoque estándar para suavizar las estimaciones de probabilidad para hacer frente a problemas de datos escasos (y para hacer frente a posibles n-gramas faltantes) es utilizar algún tipo de estimador de retroceso. Pr(wi|wi−n+1...wi−1) = 8 >>< >>: ˆPr(wi|wi−n+1...wi−1), si #(wi−n+1...wi) > 0 β(wi−n+1...wi−1) × Pr(wi|wi−n+2...wi−1), de lo contrario (3) donde ˆPr(wi|wi−n+1...wi−1) = descuento #(wi−n+1...wi) #(wi−n+1...wi−1) (4) es la probabilidad descontada y β(wi−n+1...wi−1) es una constante de normalización β(wi−n+1...wi−1) = 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+1...wi−1) 1 − X x∈(wi−n+1...wi−1x) ˆPr(x|wi−n+2...wi−1) (5) La probabilidad descontada (4) se puede calcular con diferentes técnicas de suavizado, incluido el suavizado absoluto, el suavizado de Good-Turing, el suavizado lineal y el suavizado de Witten-Bell [5]. Utilizamos suavizado absoluto en nuestros experimentos. Dado que la probabilidad de una cadena, Pr(w1w2...wN), es un número muy pequeño y difícil de interpretar, utilizamos la entropía, tal como se define a continuación, para puntuar la cadena. Entropía = − 1 N log2 Pr(w1w2...wN ) (6) Ahora volviendo al ejemplo de la comparación de precios de hoteles, hay cuatro variantes de esta consulta, y la entropía de estos cuatro candidatos se muestra en la Tabla 3. Podemos ver que todas las alternativas son menos probables que la consulta de entrada. Por lo tanto, no es útil hacer una expansión para esta consulta. Por otro lado, si la consulta de entrada son comparaciones de precios de hoteles, que es la segunda alternativa en la tabla, entonces hay una mejor alternativa que la consulta de entrada, y por lo tanto debería ser ampliada. Para tolerar las variaciones en la estimación de probabilidad, relajamos el criterio de selección para esas alternativas de consulta si sus puntuaciones están dentro de una cierta distancia (10% en nuestros experimentos) de la mejor puntuación. Variaciones de la consulta Comparación de precios de hoteles 6.177 comparaciones de precios de hoteles 6.597 comparación de precios de hoteles 6.937 comparaciones de precios de hoteles 7.360 Tabla 3: Variaciones de la consulta comparación de precios de hoteles clasificadas por puntaje de entropía, con la consulta original en negrita. 3.5 Coincidencia de documentos sensibles al contexto Aun después de saber qué variantes de palabras son probablemente útiles, debemos ser conservadores en la coincidencia de documentos para las variantes ampliadas. Para la consulta de comparaciones de precios de hoteles, decidimos que la palabra "comparaciones" se amplía para incluir "comparación". Sin embargo, no todos los casos de comparación en el documento son de interés. Una página que trata sobre comparar el servicio al cliente puede contener todas las palabras comparaciones de precios de hoteles comparación. Esta página no es una buena página para la consulta. Si aceptamos coincidencias de cada ocurrencia de comparación, afectará la precisión de recuperación y esta es una de las principales razones por las que la mayoría de los enfoques de derivación no funcionan bien para la recuperación de información. Para abordar este problema, tenemos una restricción de proximidad que considera el contexto alrededor de la variante expandida en el documento. Una coincidencia de variante se considera válida solo si la variante ocurre en el mismo contexto que la palabra original lo hace. El contexto son los segmentos continuos de la izquierda o de la derecha de la palabra original. Tomando la misma consulta como ejemplo, el contexto de las comparaciones es el precio. La comparación de palabras ampliada solo es válida si está en el mismo contexto de comparaciones, que es después de la palabra precio. Por lo tanto, solo debemos emparejar aquellas ocurrencias de comparación en el documento si ocurren después de la palabra precio. Considerando el hecho de que las consultas y los documentos pueden no representar la intención de la misma manera exacta, relajamos esta restricción de proximidad para permitir ocurrencias variantes dentro de una ventana de un tamaño fijo. Si la comparación de palabras expandidas ocurre dentro del contexto de precio dentro de una ventana, se considera válida. Cuanto más pequeño sea el tamaño de la ventana, más restrictiva será la coincidencia. Utilizamos un tamaño de ventana de 4, que normalmente captura contextos que incluyen las frases nominales que contienen y las adyacentes. 4. EVALUACIÓN EXPERIMENTAL 4.1 Métricas de evaluación Mediremos tanto la mejora de relevancia como el costo de derivación necesario para lograr la relevancia. 1 un segmento de contexto no puede ser una sola palabra de parada. 4.1.1 Medición de relevancia Utilizamos una variante de la Ganancia Acumulada Descontada Promedio (DCG), un esquema recientemente popularizado para medir la relevancia de los motores de búsqueda [1, 11]. Dada una consulta y una lista clasificada de K documentos (K se establece en 5 en nuestros experimentos), el puntaje DCG(K) para esta consulta se calcula de la siguiente manera: DCG(K) = Σ k=1 a K gk log2(1 + k) . (7) donde gk es el peso para el documento en la posición k. Un mayor grado de relevancia corresponde a un peso mayor. Una página se califica en una de las cinco escalas: Perfecto, Excelente, Bueno, Regular, Malo, con pesos correspondientes. Utilizamos DCG para representar el DCG promedio(5) sobre un conjunto de consultas de prueba. 4.1.2 Costo de derivación Otro métrica es medir el costo adicional incurrido por la derivación. Dado el mismo nivel de mejora en la relevancia, preferimos un método de truncamiento que tenga un menor costo adicional. Medimos esto por el porcentaje de consultas que realmente se reducen, sobre todas las consultas que podrían ser reducidas. 4.2 Preparación de datos Muestreamos aleatoriamente 870 consultas de un registro de consultas de tres meses, con 290 de cada mes. Entre todas estas 870 consultas, eliminamos todas las consultas con errores ortográficos ya que las consultas con errores ortográficos no son de interés para el stemming. También eliminamos todas las consultas de una sola palabra, ya que reducir las consultas de una sola palabra sin contexto tiene un alto riesgo de cambiar la intención de la consulta, especialmente para palabras cortas. Al final, tenemos 529 consultas correctamente escritas con al menos 2 palabras. 4.3 Stemming ingenuo para la búsqueda en la Web Antes de explicar en detalle los experimentos y resultados, nos gustaría describir la forma tradicional de usar el stemming para la búsqueda en la Web, conocida como el modelo ingenuo. Esto es para tratar cada variante de palabra equivalente para todas las posibles palabras en la consulta. La consulta de la librería se transformará en (libro O libros)(tienda O tiendas) al limitar el truncamiento al manejo de pluralización solamente, donde O es un operador que denota la equivalencia de los argumentos izquierdo y derecho. 4.4 Configuración experimental El modelo base es el modelo sin truncamiento. Primero ejecutamos el modelo ingenuo para ver qué tan bien se desempeña en comparación con el punto de referencia. Luego mejoramos el modelo de derivación ingenua mediante la coincidencia sensible al documento, denominado modelo de coincidencia sensible al documento. Este modelo realiza el mismo stemming que el modelo ingenuo en el lado de la consulta, pero realiza un emparejamiento conservador en el lado del documento utilizando la estrategia descrita en la sección 3.5. El modelo ingenuo y el modelo de coincidencia sensible al documento son los que mejor responden a la mayoría de las consultas. De las 529 consultas, hay 408 consultas que se originan, lo que corresponde al 46.7% del tráfico de consultas (de un total de 870). Luego mejoramos aún más el modelo de coincidencia sensible de documentos desde el lado de la consulta con un proceso de derivación de palabras selectivo basado en modelado estadístico del lenguaje (sección 3.4), denominado modelo de derivación selectiva. Basado en la predicción del modelado del lenguaje, este modelo deriva solo un subconjunto de las 408 consultas derivadas por el modelo de coincidencia sensible al documento. Experimentamos con un modelo de lenguaje unigrama y un modelo de lenguaje bigrama. Dado que solo nos importa cuánto podemos mejorar el modelo ingenuo, solo utilizaremos estas 408 consultas (todas las consultas que se ven afectadas por el modelo de derivación ingenuo) en los experimentos. Para tener una idea de cómo se desempeñan estos modelos, también tenemos un modelo oráculo que proporciona el rendimiento máximo que un stemmer puede lograr en estos datos. El modelo del oráculo solo expande una palabra si el truncamiento dará mejores resultados. Para analizar la influencia del manejo de la pluralización en diferentes categorías de consultas, dividimos las consultas en consultas cortas y consultas largas. Entre las 408 consultas generadas por el modelo ingenuo, hay 272 consultas cortas con 2 o 3 palabras, y 136 consultas largas con al menos 4 palabras. Resumimos los resultados generales en la Tabla 4, y presentamos los resultados de las consultas cortas y largas por separado en la Tabla 5. Cada fila en la Tabla 4 es una estrategia de derivación descrita en la sección 4.4. La primera columna es el nombre de la estrategia. La segunda columna es el número de consultas afectadas por esta estrategia; esta columna mide el costo de derivación, y los números deberían ser bajos para el mismo nivel de dcg. La tercera columna es la puntuación DCG promedio sobre todas las consultas probadas en esta categoría (incluyendo aquellas que no fueron truncadas por la estrategia). La cuarta columna es la mejora relativa sobre la línea base, y la última columna es el valor p de la prueba de significancia de Wilcoxon. Hay varias observaciones sobre los resultados. Podemos ver que el truncado ingenuo solo logra una mejora estadísticamente insignificante del 1.5%. Mirando la Tabla 5, se observa una mejora del 2.7% en las consultas cortas. Sin embargo, también perjudica las consultas largas en un -2.4%. En general, la mejora se cancela. La razón por la que mejora las consultas cortas es que la mayoría de las consultas cortas solo tienen una palabra que se puede reducir a su raíz. Por lo tanto, pluralizar ciegamente consultas cortas es relativamente seguro. Sin embargo, para consultas largas, la mayoría de las consultas pueden tener varias palabras que pueden ser pluralizadas. Expandir todos ellos sin selección perjudicará significativamente la precisión. La aplicación de un stemming sensible al contexto del documento proporciona un aumento significativo en el rendimiento, desde un 2.7% hasta un 4.2% para consultas cortas y desde un -2.4% hasta un -1.6% para consultas largas, con un aumento general del 1.5% al 2.8%. La mejora proviene de la coincidencia de documentos sensible al contexto conservador. Una palabra expandida es válida solo si ocurre dentro del contexto de la consulta original en el documento. Esto reduce muchas coincidencias falsas. Sin embargo, aún observamos que para consultas largas, el truncamiento sensible al contexto no logra mejorar el rendimiento porque sigue seleccionando demasiados documentos y le presenta a la función de clasificación un problema difícil. Si bien el tamaño de ventana elegido de 4 es el que mejor funciona entre todas las opciones, aún permite coincidencias espurias. Es posible que el tamaño de la ventana deba elegirse según la consulta para garantizar restricciones de proximidad más estrictas para diferentes tipos de frases nominales. La pluralización selectiva de palabras ayuda aún más a resolver el problema enfrentado por el truncamiento sensible al contexto del documento. No se aplica el truncamiento a cada palabra que coloca toda la carga en el algoritmo de clasificación, sino que intenta eliminar el truncamiento innecesario en primer lugar. Al predecir qué variantes de palabras van a ser útiles, podemos reducir drásticamente el número de palabras derivadas, mejorando así tanto la recuperación como la precisión. Con el modelo de lenguaje unigrama, podemos reducir el costo de derivación en un 26.7% (de 408/408 a 300/408) y aumentar la mejora general de DCG del 2.8% al 3.4%. En particular, proporciona mejoras significativas en consultas largas. La ganancia de DCG se cambia de negativa a positiva, de -1.6% a 1.1%. Esto confirma nuestra hipótesis de que reducir la expansión innecesaria de palabras conduce a una mejora en la precisión. Para consultas cortas también observamos tanto la mejora de dcg como la reducción del costo de derivación con el modelo de lenguaje unigrama. Las ventajas de la expansión predictiva de palabras con un modelo de lenguaje se potencian aún más con un mejor modelo de lenguaje de bigrama. La ganancia total de DCG aumenta de 3.4% a 3.9%, y el costo de derivación se reduce drásticamente de 408/408 a 250/408, correspondiente a solo el 29% del tráfico de consultas (250 de 870) y una mejora total de DCG del 1.8% en todo el tráfico de consultas. Para consultas cortas, el modelo de lenguaje de bigrama mejora la ganancia de DCG del 4.4% al 4.7%, y reduce el costo de derivación de 272/272 a 150/272. Para consultas largas, el modelo de lenguaje de bigrama mejora la ganancia de DCG del 1.1% al 2.5%, y reduce el costo de derivación de 136/136 a 100/136. Observamos que el modelo de lenguaje de bigrama proporciona un mayor aumento para consultas largas. Esto se debe a que la incertidumbre en las consultas largas es mayor y se necesita un modelo de lenguaje más potente. Hacemos la hipótesis de que un modelo de lenguaje de trigramas proporcionaría un impulso adicional para consultas largas y dejamos esto para investigaciones futuras. Considerando el límite superior estricto de 2 en la mejora que se puede obtener del manejo de la pluralización (a través del modelo oráculo), el rendimiento actual en consultas cortas es muy satisfactorio. Para consultas breves, la ganancia máxima de DCG es del 6.3% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 4.7% con un modelo de lenguaje de bigrama. Para consultas largas, la ganancia máxima de DCG es del 4.6% para el manejo perfecto de la pluralización, nuestra ganancia actual es del 2.5% con un modelo de lenguaje de bigrama. Podríamos obtener beneficios adicionales con un modelo de lenguaje más potente para consultas largas. Sin embargo, las dificultades de las consultas largas provienen de muchos otros aspectos, incluyendo la proximidad y el problema de la segmentación. Estos problemas deben ser abordados por separado. Al observar el límite superior de reducción de gastos generales para el truncamiento de Oracle, el 75% (308/408) de los truncamientos ingenuos son inútiles. Actualmente capturamos alrededor de la mitad de ellos. La reducción adicional de los gastos generales requiere sacrificar la ganancia de la DCG. Ahora podemos comparar las estrategias de derivación desde un aspecto diferente. En lugar de analizar la influencia sobre todas las consultas como describimos anteriormente, la Tabla 6 resume las mejoras de DCG solo sobre las consultas afectadas. Podemos ver que el número de consultas afectadas disminuye a medida que la estrategia de derivación se vuelve más precisa (mejora en el dcg). Para el modelo de lenguaje de bigrama, sobre las 250/408 consultas truncadas, la mejora en DCG es del 6.1%. Una observación interesante es que el dcg promedio disminuye con un modelo mejor, lo que indica que una estrategia de derivación mejorada deriva consultas más difíciles (consultas con bajo dcg). 5. Como mencionamos en la Sección 1, estamos tratando de predecir la probabilidad de que una cadena ocurra en la Web. El modelo de lenguaje debe describir la ocurrencia de la cadena en la web. Sin embargo, el registro de consultas también es un buen recurso. Tenga en cuenta que este límite superior es solo para el manejo de pluralización, no para el truncamiento general. El stemming general proporciona un límite superior del 8%, lo cual es bastante sustancial en términos de nuestras métricas. Las consultas afectadas dcg dcg Mejora p-valor línea de base 0/408 7.102 N/A N/A modelo ingenuo 408/408 7.206 1.5% 0.22 modelo sensible al contexto del documento 408/408 7.302 2.8% 0.014 modelo selectivo: unigram LM 300/408 7.321 3.4% 0.001 modelo selectivo: bigram LM 250/408 7.381 3.9% 0.001 modelo oráculo 100/408 7.519 5.9% 0.001 Tabla 4: Comparación de resultados de diferentes estrategias de derivación en todas las consultas afectadas por la derivación ingenua Resultados de Consultas Cortas Consultas Afectadas dcg Mejora p-valor línea de base 0/272 N/A N/A modelo ingenuo 272/272 2.7% 0.48 modelo sensible al contexto del documento 272/272 4.2% 0.002 modelo selectivo: unigram LM 185/272 4.4% 0.001 modelo selectivo: bigram LM 150/272 4.7% 0.001 modelo oráculo 71/272 6.3% 0.001 Resultados de Consultas Largas Consultas Afectadas dcg Mejora p-valor línea de base 0/136 N/A N/A modelo ingenuo 136/136 -2.4% 0.25 modelo sensible al contexto del documento 136/136 -1.6% 0.27 modelo selectivo: unigram LM 115/136 1.1% 0.001 modelo selectivo: bigram LM 100/136 2.5% 0.001 modelo oráculo 29/136 4.6% 0.001 Tabla 5: Comparación de resultados de diferentes estrategias de derivación en consultas cortas y largas en general Los usuarios reformulan una consulta utilizando muchas variantes diferentes para obtener buenos resultados. Para probar la hipótesis de que podemos aprender probabilidades de transformación confiables a partir del registro de consultas, entrenamos un modelo de lenguaje con las mismas 25 millones de consultas principales utilizadas para aprender la segmentación, y lo utilizamos para la predicción. Observamos una ligera disminución en el rendimiento en comparación con el modelo entrenado en frecuencias web. En particular, el rendimiento para el modelo de lenguaje unigrama no se vio afectado, pero la ganancia de dcg para el modelo de lenguaje bigrama cambió de 4.7% a 4.5% para consultas cortas. Por lo tanto, el registro de consultas puede servir como una buena aproximación de las frecuencias web. 5.2 Cómo la lingüística ayuda. Algunos conocimientos lingüísticos son útiles en el stemming. Para el manejo de la pluralización, la pluralización y la despluralización no son simétricas. Una palabra en plural utilizada en una consulta indica una intención especial. Por ejemplo, la consulta hoteles en Nueva York está buscando una lista de hoteles en Nueva York, no el hotel específico de Nueva York que podría estar ubicado en California. Una simple equivalencia de hotel a hoteles podría impulsar una página en particular sobre hoteles en Nueva York al primer puesto. Para capturar esta intención, tenemos que asegurarnos de que el documento sea una página general sobre hoteles en Nueva York. Esto lo hacemos exigiendo que la palabra en plural hoteles aparezca en el documento. Por otro lado, convertir una palabra singular a plural es más seguro ya que una página de propósito general normalmente contiene información específica. Observamos una ligera disminución general en el dcg, aunque no estadísticamente significativa, para el stemming sensible al contexto del documento si no consideramos esta propiedad asimétrica. Análisis de errores. Un tipo de errores que notamos, aunque raros pero que afectan seriamente la relevancia, es el cambio de intención de búsqueda después del stemming. En general, la pluralización o despluralización mantiene la intención original. Sin embargo, la intención podría cambiar en algunos casos. Por ejemplo, para una consulta de este tipo, trabajo en Apple, pluralizamos trabajo a trabajos. Este truncamiento hace que la consulta original sea ambigua. El trabajo de consulta O trabajos en Apple tiene dos intenciones. Una de las oportunidades laborales en Apple, y otra es una persona que trabaja en Apple, Steve Jobs, quien es el CEO y cofundador de la empresa. Por lo tanto, los resultados después de la reducción de consultas devuelven a Steve Jobs como uno de los resultados en el top 5. Una solución es realizar un análisis basado en el conjunto de resultados para verificar si la intención ha cambiado. Esto es similar a la retroalimentación de relevancia y requiere una clasificación en la segunda fase. Un segundo tipo de errores es el problema de reconocimiento de entidades/conceptos. Estos incluyen dos tipos. Una de ellas es que la variante de la palabra derivada ahora coincide con parte de una entidad o concepto. Por ejemplo, la consulta "cookies en San Francisco" se pluraliza como "cookies" O "cookie en San Francisco". Los resultados coincidirán con el tarro de galletas en San Francisco. Aunque "cookie" todavía significa lo mismo que "galletas", el concepto de "cookie jar" es diferente. Otro tipo es la palabra no derivada que coincide con una entidad o concepto debido al truncamiento de las otras palabras. Por ejemplo, la cita ICE se pluraliza como cita OR citas ICE. La intención original de esta consulta es buscar la cotización de acciones para el símbolo ICE. Sin embargo, notamos que entre los resultados principales, uno de los resultados es Citas sobre comida: Helado. Esto se empareja debido a las consultas afectadas dcg antiguas nuevas dcg dcg Mejora del modelo ingenuo 408/408 7.102 7.206 1.5% modelo sensible al contexto del documento 408/408 7.102 7.302 2.8% modelo selectivo: unigram LM 300/408 5.904 6.187 4.8% modelo selectivo: bigram LM 250/408 5.551 5.891 6.1% Tabla 6: Comparación de resultados solo sobre las consultas derivadas: la columna dcg antigua/nueva es la puntuación dcg sobre las consultas afectadas antes/después de aplicar la pluralización de la palabra entre comillas. La palabra inalterada ICE coincide con parte de la frase nominal helado aquí. Para resolver este tipo de problema, tenemos que analizar los documentos y reconocer "cookie jar" y "ice cream" como conceptos en lugar de dos palabras independientes. Un tercer tipo de errores ocurre en consultas largas. Para la consulta de software lector de códigos de barras, se pluralizan dos palabras. Códigos y lector. De hecho, el lector de códigos de barras en la consulta original es un concepto sólido y las palabras internas no deben ser cambiadas. Este es el problema de segmentación y detección de entidades y frases nominales en consultas, el cual estamos abordando activamente. Para consultas largas, debemos identificar correctamente los conceptos en la consulta y aumentar la proximidad de las palabras dentro de un concepto. 6. CONCLUSIONES Y TRABAJOS FUTUROS Hemos presentado una forma simple pero elegante de derivar palabras para la búsqueda en la web. Mejora el stemming ingenuo en dos aspectos: la expansión selectiva de palabras en el lado de la consulta y la coincidencia conservadora de ocurrencias de palabras en el lado del documento. Usando el manejo de pluralización como ejemplo, experimentos con datos de un motor de búsqueda web importante muestran que mejora significativamente la relevancia web y reduce el costo de derivación. También mejora significativamente la tasa de clics en la web (detalles no reportados en el artículo). Para el trabajo futuro, estamos investigando los problemas que identificamos en la sección de análisis de errores. Estos incluyen: errores de concordancia entre entidades y frases nominales, y una segmentación mejorada. 7. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, 2006. [2] E. Airio. Normalización de palabras y descomposición en IR monolingüe y bilingüe. Recuperación de información, 9:249-271, 2006. [3] P. Anick. Utilizando retroalimentación terminológica para el refinamiento de la búsqueda web: un estudio basado en registros. En SIGIR, 2003. [4] R. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. ACM Press/Addison Wesley, 1999. [5] S. Chen y J. Goodman. Un estudio empírico de técnicas de suavizado para modelado del lenguaje. Informe técnico TR-10-98, Universidad de Harvard, 1998. [6] S. Cronen-Townsend, Y. Zhou y B. Croft. Un marco para la expansión selectiva de consultas. En CIKM, 2004. [7] H. Fang y C. Zhai. Coincidencia de términos semánticos en enfoques axiomáticos para la recuperación de información. En SIGIR, 2006. [8] W. B. Frakes. Confluencia de términos para la recuperación de información. En C. J. Rijsbergen, editor, Investigación y Desarrollo en Recuperación de Información, páginas 383-389. Cambridge University Press, 1984. [9] D. Harman.
Cambridge University Press, 1984. [9] D. Harman. ¿Qué tan efectivo es el sufijado? JASIS, 42(1):7-15, 1991. [10] D. Hull.
JASIS, 42(1):7-15, 1991. [10] D. Hull. Algoritmos de derivación: un estudio de caso para una evaluación detallada. JASIS, 47(1):70-84, 1996. [11] K. Jarvelin y J. Kekalainen. Evaluación basada en la ganancia acumulada de técnicas de RI. ACM TOIS, 20:422-446, 2002. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando Sustituciones de Consultas. En WWW, 2006. [13] W. Kraaij y R. Pohlmann. Viendo el Stemming como Mejora de la Recuperación. En SIGIR, 1996. [14] R. Krovetz. Viendo la morfología como un proceso de inferencia. En SIGIR, 1993. [15] D. Lin. Recuperación automática y agrupamiento de palabras similares. En COLING-ACL, 1998. [16] J. B. Lovins. Desarrollo de un algoritmo de derivación. Traducción Mecánica y Lingüística Computacional, II:22-31, 1968. [17] M. Lennon, D. Peirce, B. Tarry y P. Willett. Una evaluación de algunos algoritmos de confluencia para la recuperación de información. Revista de Ciencia de la Información, 3:177-188, 1981. [18] M. Porter. Un algoritmo para el recorte de sufijos. Programa, 14(3):130-137, 1980. [19] K. M. Risvik, T. Mikolajewski y P. Boros. Segmentación de consultas para búsqueda web. En WWW, 2003. [20] S. E. Robertson. Selección de términos para la expansión de consultas. Revista de Documentación, 46(4):359-364, 1990. [21] G. Salton y C. Buckley. Mejorando el rendimiento de recuperación mediante retroalimentación de relevancia. JASIS, 41(4):288 - 297, 1999. [22] R. Sun, C.-H. Ong, y T.-S. Chua. Extracción de relaciones de dependencia para la expansión de consultas en la recuperación de pasajes. En SIGIR, 2006. [23] C. Van Rijsbergen. Recuperación de información. Butterworths, segunda versión, 1979. [24] B. Vélez, R. Weiss, M. A. Sheldon y D. K. Gifford. Refinamiento de consultas rápido y efectivo. En SIGIR, 1997. [25] J. Xu y B. Croft. Expansión de consultas utilizando análisis local y global de documentos. En SIGIR, 1996. [26] J. Xu y B. Croft. Extracción de raíces basada en corpus utilizando la coocurrencia de variantes de palabras. ACM TOIS, 16 (1):61-81, 1998.