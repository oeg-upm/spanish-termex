Extracción automática de títulos de documentos generales utilizando aprendizaje automático. En este documento, proponemos un enfoque de aprendizaje automático para la extracción de títulos de documentos generales. Por documentos generales nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos, incluyendo presentaciones, capítulos de libros, artículos técnicos, folletos, informes y cartas. Anteriormente, se han propuesto métodos principalmente para la extracción de títulos de artículos de investigación. No ha quedado claro si sería posible realizar la extracción automática de títulos de documentos generales. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. En nuestro enfoque, anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento, entrenamos modelos de aprendizaje automático y realizamos la extracción de títulos utilizando los modelos entrenados. Nuestro método es único en que principalmente utilizamos información de formato como el tamaño de fuente como características en los modelos. Resulta que el uso de información de formato puede llevar a una extracción bastante precisa de documentos generales. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente en un experimento con datos de intranet. Otros hallazgos importantes en este trabajo incluyen que podemos entrenar modelos en un dominio y aplicarlos a otro dominio, y más sorprendentemente, incluso podemos entrenar modelos en un idioma y aplicarlos a otro idioma. Además, podemos mejorar significativamente los resultados de clasificación de búsqueda en la recuperación de documentos utilizando los títulos extraídos. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de Búsqueda; H.4.1 [Aplicaciones de Sistemas de Información]: Automatización de Oficinas - Procesamiento de Texto; D.2.8 [Ingeniería de Software]: Métricas - medidas de complejidad, medidas de rendimiento Términos Generales Algoritmos, Experimentación, Rendimiento. 1. METADATA La información de metadatos de los documentos es útil para muchos tipos de procesamiento de documentos, como la búsqueda, la navegación y el filtrado. Idealmente, los metadatos son definidos por los autores de los documentos y luego son utilizados por varios sistemas. Sin embargo, las personas rara vez definen los metadatos de los documentos por sí mismas, incluso cuando tienen herramientas convenientes para la definición de metadatos [26]. Por lo tanto, la extracción automática de metadatos de los cuerpos de los documentos resulta ser un tema de investigación importante. Se han propuesto métodos para realizar la tarea. Sin embargo, el enfoque estaba principalmente en la extracción de los documentos de investigación. Por ejemplo, Han et al. [10] propusieron un método basado en aprendizaje automático para realizar extracciones de artículos de investigación. Formalizaron el problema como el de clasificación y emplearon Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron características lingüísticas en el modelo. En este artículo, consideramos la extracción de metadatos de documentos generales. Por documentos generales, nos referimos a documentos que pueden pertenecer a cualquiera de varios géneros específicos. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigación sobre la extracción de información de ellos. Los artículos de investigación suelen tener estilos bien formados y características notables. Por el contrario, los estilos de los documentos generales pueden variar considerablemente. No se ha aclarado si un enfoque basado en aprendizaje automático puede funcionar bien para esta tarea. Hay muchos tipos de metadatos: título, autor, fecha de creación, etc. Como estudio de caso, consideramos la extracción de títulos en este artículo. Los documentos generales pueden estar en muchos formatos de archivo diferentes: Microsoft Office, PDF (PS), etc. Como estudio de caso, consideramos la extracción de Office, incluyendo Word y PowerPoint. Tomamos un enfoque de aprendizaje automático. Anotamos títulos en documentos de muestra (para Word y PowerPoint respectivamente) y los tomamos como datos de entrenamiento para entrenar varios tipos de modelos, y realizamos la extracción de títulos utilizando uno de los tipos de modelos entrenados. En los modelos, principalmente utilizamos información de formato como el tamaño de fuente como características. Empleamos los siguientes modelos: Modelo de Entropía Máxima, Perceptrón con Márgenes Desiguales, Modelo de Entropía Máxima de Markov y Perceptrón Votado. En este documento, también investigamos los siguientes tres problemas, que no parecían haber sido examinados previamente. (1) Comparación entre modelos: entre los modelos anteriores, ¿cuál modelo funciona mejor para la extracción de títulos?; (2) Generalidad del modelo: si es posible entrenar un modelo en un dominio y aplicarlo a otro dominio, y si es posible entrenar un modelo en un idioma y aplicarlo a otro idioma; (3) Utilidad de los títulos extraídos: si los títulos extraídos pueden mejorar el procesamiento de documentos como la búsqueda. Los resultados experimentales indican que nuestro enfoque funciona bien para la extracción de títulos de documentos generales. Nuestro método puede superar significativamente a los baselines: uno que siempre utiliza las primeras líneas como títulos y el otro que siempre utiliza las líneas en los tamaños de fuente más grandes como títulos. La precisión y la exhaustividad para la extracción de títulos de Word son 0.810 y 0.837 respectivamente, y la precisión y la exhaustividad para la extracción de títulos de PowerPoint son 0.875 y 0.895 respectivamente. Resulta que el uso de características de formato es la clave para la exitosa extracción de títulos. (1) Hemos observado que los modelos basados en Perceptrón tienen un mejor rendimiento en términos de precisión de extracción. (2) Hemos verificado empíricamente que los modelos entrenados con nuestro enfoque son genéricos en el sentido de que pueden ser entrenados en un dominio y aplicados en otro, y pueden ser entrenados en un idioma y aplicados en otro. (3) Hemos descubierto que al utilizar los títulos extraídos podemos mejorar significativamente la precisión de la recuperación de documentos (en un 10%). Concluimos que podemos realizar de manera fiable la extracción de títulos de documentos generales y utilizar los resultados extraídos para mejorar aplicaciones reales. El resto del documento está organizado de la siguiente manera. En la sección 2, presentamos el trabajo relacionado, y en la sección 3, explicamos la motivación y el contexto del problema de nuestro trabajo. En la sección 4, describimos nuestro método de extracción de títulos, y en la sección 5, describimos nuestro método de recuperación de documentos utilizando los títulos extraídos. La sección 6 presenta nuestros resultados experimentales. Hacemos observaciones finales en la sección 7.2. TRABAJO RELACIONADO 2.1 Se han propuesto métodos de extracción de metadatos de documentos para realizar extracción automática de metadatos de documentos; sin embargo, el enfoque principal ha sido la extracción de artículos de investigación. Los métodos propuestos se dividen en dos categorías: el enfoque basado en reglas y el enfoque basado en aprendizaje automático. Giuffrida et al. [9], por ejemplo, desarrollaron un sistema basado en reglas para extraer automáticamente metadatos de artículos de investigación en Postscript. Utilizaron reglas como que los títulos suelen ubicarse en las partes superiores de las primeras páginas y suelen estar en los tamaños de fuente más grandes. Liddy et al. [14] y Yilmazel et al. [23] realizaron extracción de metadatos de materiales educativos utilizando tecnologías de procesamiento del lenguaje natural basadas en reglas. Mao et al. [16] también llevaron a cabo la extracción automática de metadatos de artículos de investigación utilizando reglas sobre la información de formato. El enfoque basado en reglas puede lograr un alto rendimiento. Sin embargo, también tiene desventajas. Es menos adaptable y robusto en comparación con el enfoque de aprendizaje automático. Han et al. [10], por ejemplo, realizaron extracción de metadatos con enfoque de aprendizaje automático. Consideraron el problema como el de clasificar las líneas en un documento en las categorías de metadatos y propusieron utilizar Máquinas de Vectores de Soporte como clasificador. Principalmente utilizaron información lingüística como características. Informaron de una alta precisión de extracción de información de artículos de investigación en términos de precisión y recuperación. 2.2 Extracción de Información La extracción de metadatos puede ser vista como una aplicación de extracción de información, en la cual, dada una secuencia de instancias, identificamos una subsecuencia que representa la información en la que estamos interesados. Los Modelos Ocultos de Markov [6], el Modelo de Entropía Máxima [1, 4], el Modelo de Entropía Máxima de Markov [17], las Máquinas de Vectores de Soporte [3], el Campo Aleatorio Condicional [12] y el Perceptrón Votado [2] son modelos ampliamente utilizados para la extracción de información. La extracción de información se ha aplicado, por ejemplo, al etiquetado de partes del discurso [20], al reconocimiento de entidades nombradas [25] y a la extracción de tablas [19]. 2.3 Búsqueda utilizando la información del título La información del título es útil para la recuperación de documentos. En el sistema Citeseer, por ejemplo, Giles et al. lograron extraer títulos de artículos de investigación y utilizar los títulos extraídos en la búsqueda de metadatos de los artículos [8]. En la búsqueda web, los campos de título (es decir, propiedades de archivo) y los textos de anclaje de las páginas web (documentos HTML) se pueden ver como títulos de las páginas [5]. Muchos motores de búsqueda parecen utilizarlos para la recuperación de páginas web [7, 11, 18, 22]. Zhang et al. encontraron que las páginas web con metadatos bien definidos son más fáciles de recuperar que aquellas sin metadatos bien definidos [24]. Hasta donde sabemos, no se ha realizado ninguna investigación sobre el uso de títulos extraídos de documentos generales (por ejemplo, documentos de Office) para la búsqueda de los documentos. 146 3. MOTIVACIÓN Y DEFINICIÓN DEL PROBLEMA Consideramos el problema de extraer automáticamente títulos de documentos generales. Por documentos generales, nos referimos a documentos que pertenecen a uno de varios géneros específicos. Los documentos pueden ser presentaciones, libros, capítulos de libros, artículos técnicos, folletos, informes, memorandos, especificaciones, cartas, anuncios o currículums. Los documentos generales están más ampliamente disponibles en bibliotecas digitales, intranets e internet, por lo que se necesita urgentemente investigar la extracción de títulos de los mismos. La Figura 1 muestra una estimación de las distribuciones de formatos de archivo en intranet e internet [15]. Office y PDF son los principales formatos de archivo en la intranet. Incluso en internet, los documentos en los formatos siguen siendo significativos, dada su tamaño extremadamente grande. En este documento, sin pérdida de generalidad, tomamos como ejemplo los documentos de Office. Figura 1. Distribuciones de formatos de archivo en internet e intranet. Para los documentos de Office, los usuarios pueden definir títulos como propiedades de archivo utilizando una función proporcionada por Office. Encontramos en un experimento, sin embargo, que los usuarios rara vez utilizan la función y, por lo tanto, los títulos en las propiedades de los archivos suelen ser muy inexactos. Es decir, los títulos en las propiedades del archivo suelen ser inconsistentes con los verdaderos títulos en los cuerpos de los archivos que son creados por los autores y son visibles para los lectores. Recopilamos 6,000 documentos de Word y 6,000 documentos de PowerPoint de una intranet y de internet, y examinamos cuántos títulos en las propiedades de los archivos son correctos. Descubrimos que sorprendentemente la precisión fue solo de 0.265 (cf., Sección 6.3 para más detalles). Se pueden considerar varias razones. Por ejemplo, si se crea un archivo nuevo copiando un archivo antiguo, entonces la propiedad de archivo del nuevo archivo también se copiará del archivo antiguo. En otro experimento, descubrimos que Google utiliza los títulos en las propiedades de archivo de documentos de Office en la búsqueda y navegación, pero los títulos no son muy precisos. Creamos 50 consultas para buscar documentos de Word y PowerPoint y examinamos los 15 resultados principales de cada consulta devueltos por Google. Descubrimos que casi todos los títulos presentados en los resultados de búsqueda eran de las propiedades de archivo de los documentos. Sin embargo, solo 0.272 de ellos fueron correctos. De hecho, los títulos verdaderos suelen encontrarse al principio de los cuerpos de los documentos. Si podemos extraer con precisión los títulos de los cuerpos de los documentos, entonces podemos aprovechar la información de título confiable en el procesamiento de documentos. Este es exactamente el problema que abordamos en este artículo. Más específicamente, dado un documento de Word, debemos extraer el título de la región superior de la primera página. Dado un documento de PowerPoint, debemos extraer el título de la primera diapositiva. Un título a veces consiste en un título principal y uno o dos subtítulos. Solo consideramos la extracción del título principal. Como líneas base para la extracción de títulos, utilizamos la de siempre usar las primeras líneas como títulos y la de siempre usar las líneas con tamaños de fuente más grandes como títulos. Figura 2. Extracción de título de documento de Word. Figura 3. Extracción de títulos de un documento de PowerPoint. A continuación, definimos una especificación para los juicios humanos en la anotación de datos de títulos. Los datos anotados se utilizarán en el entrenamiento y prueba de los métodos de extracción de títulos. Resumen de la especificación: El título de un documento debe ser identificado en base al sentido común, si no hay dificultad en la identificación. Sin embargo, hay muchos casos en los que la identificación no es fácil. Hay algunas reglas definidas en la especificación que guían la identificación para tales casos. Las reglas incluyen que un título suele estar en líneas consecutivas en el mismo formato, un documento puede no tener título, los títulos en imágenes no se consideran, un título no debe contener palabras como borrador, 147 whitepaper, etc., si es difícil determinar cuál es el título, seleccionar el que tenga el tamaño de fuente más grande y, si aún es difícil determinar cuál es el título, seleccionar el primer candidato. (La especificación cubre todos los casos que hemos encontrado en la anotación de datos). Las figuras 2 y 3 muestran ejemplos de documentos de Office de los cuales realizamos la extracción de títulos. En la Figura 2, Diferencias en las Implementaciones de la API de Win32 entre los Sistemas Operativos de Windows es el título del documento de Word. En la parte superior de esta página hay una imagen de Microsoft Windows que por lo tanto es ignorada. En la Figura 3, "Construyendo Ventajas Competitivas a través de una Infraestructura Ágil" es el título del documento de PowerPoint. Hemos desarrollado una herramienta para la anotación de títulos por anotadores humanos. La Figura 4 muestra una captura de pantalla de la herramienta. Figura 4. Herramienta de anotación de títulos. 4. MÉTODO DE EXTRACCIÓN DE TÍTULOS 4.1 El método de extracción de títulos basado en aprendizaje automático consta de entrenamiento y extracción. El mismo paso de preprocesamiento ocurre antes del entrenamiento y la extracción. Durante el preprocesamiento, se extraen una serie de unidades para el procesamiento de la región superior de la primera página de un documento de Word o de la primera diapositiva de un documento de PowerPoint. Si una línea (las líneas están separadas por símbolos de retorno) solo tiene un único formato, entonces la línea se convertirá en una unidad. Si una línea tiene varias partes y cada una de ellas tiene su propio formato, entonces cada parte se convertirá en una unidad. Cada unidad será tratada como una instancia en el aprendizaje. Una unidad contiene no solo información de contenido (información lingüística) sino también información de formato. La entrada al preprocesamiento es un documento y la salida del preprocesamiento es una secuencia de unidades (instancias). La Figura 5 muestra las unidades obtenidas del documento en la Figura 2. Figura 5. Ejemplo de unidades. En el aprendizaje, la entrada son secuencias de unidades donde cada secuencia corresponde a un documento. Tomamos unidades etiquetadas (etiquetadas como title_begin, title_end u otras) en las secuencias como datos de entrenamiento y construimos modelos para identificar si una unidad es title_begin, title_end u otra. Empleamos cuatro tipos de modelos: Perceptrón, Entropía Máxima (ME), Modelo de Perceptrón de Markov (PMM) y Modelo de Entropía Máxima de Markov (MEMM). En la extracción, la entrada es una secuencia de unidades de un documento. Empleamos un tipo de modelo para identificar si una unidad es título_inicio, título_fin u otro. Luego extraemos las unidades desde la unidad etiquetada con title_begin hasta la unidad etiquetada con title_end. El resultado es el título extraído del documento. La característica única de nuestro enfoque es que principalmente utilizamos información de formato para la extracción de títulos. Nuestra suposición es que aunque los documentos generales varían en estilos, sus formatos tienen ciertos patrones y podemos aprender y utilizar esos patrones para la extracción de títulos. Esto contrasta con el trabajo de Han et al., en el cual solo se utilizan características lingüísticas para la extracción de artículos de investigación. 4.2 Modelos De hecho, los cuatro modelos pueden considerarse dentro del mismo marco de extracción de metadatos. Por eso los aplicamos juntos a nuestro problema actual. Cada entrada es una secuencia de instancias kxxx L21 junto con una secuencia de etiquetas kyyy L21. ix e iy representan una instancia y su etiqueta, respectivamente (ki ,,2,1 L=). Recuerda que una instancia aquí representa una unidad. Una etiqueta representa title_begin, title_end, u otro. Aquí, k es el número de unidades en un documento. En el aprendizaje, entrenamos un modelo que puede ser generalmente denotado como una distribución de probabilidad condicional )|( 11 kk XXYYP LL donde iX e iY denotan variables aleatorias que toman instancias ix e etiquetas iy como valores, respectivamente ( ki ,,2,1 L= ). Herramienta de extracción de herramientas de aprendizaje 21121 2222122221 1121111211 nknnknn kk kk yyyxxx yyyxxx yyyxxx LL LL LL LL → → → )|(maxarg 11 mkmmkm xxyyP LL )|( 11 kk XXYYP LL Distribución condicional mkmm xxx L21 Figura 6. Modelo de extracción de metadatos. Podemos hacer suposiciones sobre el modelo general para simplificarlo lo suficiente para el entrenamiento. Por ejemplo, podemos asumir que kYY ,,1 L son independientes entre sí dado kXX ,,1 L. De esta manera, descomponemos el modelo en varios clasificadores. Entrenamos los clasificadores localmente utilizando los datos etiquetados. Como clasificador, empleamos el modelo Perceptrón o de Máxima Entropía. También podemos asumir que la propiedad de Markov de primer orden se cumple para kYY ,,1 L dado kXX ,,1 L. Por lo tanto, obtenemos una serie de clasificadores. Sin embargo, los clasificadores están condicionados por la etiqueta previa. Cuando empleamos el modelo Perceptrón o de Entropía Máxima como clasificador, los modelos se convierten en un Modelo Markov Perceptrón o un Modelo Markov de Entropía Máxima, respectivamente. Es decir, los dos modelos son más precisos. En la extracción, dada una nueva secuencia de instancias, recurrimos a uno de los modelos construidos para asignar una secuencia de etiquetas a la secuencia de instancias, es decir, realizar la extracción. Para Perceptrón y ME, asignamos etiquetas localmente y luego combinamos los resultados globalmente utilizando heurísticas. Específicamente, primero identificamos el título más probable. Entonces encontramos el título más probable dentro de tres unidades después del título inicial. Finalmente, extraemos como título las unidades entre title_begin y title_end. Para PMM y MEMM, empleamos el algoritmo de Viterbi para encontrar la secuencia de etiquetas globalmente óptima. En este artículo, para el Perceptrón, en realidad empleamos una variante mejorada de él, llamada Perceptrón con Margen Desigual [13]. Esta versión de Perceptrón puede funcionar bien especialmente cuando el número de instancias positivas y el número de instancias negativas difieren considerablemente, que es exactamente el caso en nuestro problema. También empleamos una versión mejorada del Modelo Perceptrón Markov en la cual el modelo Perceptrón es el llamado Perceptrón Votado [2]. Además, en el entrenamiento, los parámetros del modelo se actualizan de forma global en lugar de localmente. 4.3 Características Hay dos tipos de características: características de formato y características lingüísticas. Principalmente usamos el primero. Las características se utilizan tanto para los clasificadores de inicio de título como para los de fin de título. 4.3.1 Características de formato Tamaño de fuente: Hay cuatro características binarias que representan el tamaño de fuente normalizado de la unidad (recordemos que una unidad tiene solo un tipo de fuente). Si el tamaño de fuente de la unidad es el más grande en el documento, entonces la primera característica será 1, de lo contrario, 0. Si el tamaño de fuente es el más pequeño en el documento, entonces la cuarta característica será 1, de lo contrario, 0. Si el tamaño de la fuente es mayor que el tamaño de fuente promedio y no es el más grande en el documento, entonces la segunda característica será 1, de lo contrario, 0. Si el tamaño de la fuente es inferior al tamaño promedio de la fuente y no el más pequeño, la tercera característica será 1, de lo contrario, 0. Es necesario realizar la normalización de los tamaños de fuente. Por ejemplo, en un documento el tamaño de fuente más grande podría ser de 12pt, mientras que en otro el más pequeño podría ser de 18pt. Negrita: Esta característica binaria representa si la unidad actual está en negrita o no. Alineación: Hay cuatro características binarias que representan respectivamente la ubicación de la unidad actual: izquierda, centro, derecha y alineación desconocida. El siguiente formato de características con respecto al contexto juega un papel importante en la extracción de títulos. Unidad vecina vacía: Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad actual son líneas en blanco. Cambio de tamaño de fuente: Hay dos características binarias que representan, respectivamente, si el tamaño de fuente de la unidad anterior y el tamaño de fuente de la siguiente unidad difieren del de la unidad actual. Cambio de alineación: Hay dos características binarias que representan, respectivamente, si la alineación de la unidad anterior y la alineación de la siguiente difieren de la de la actual. Hay dos características binarias que representan, respectivamente, si la unidad anterior y la unidad siguiente están en el mismo párrafo que la unidad actual. 4.3.2 Características lingüísticas Las características lingüísticas se basan en palabras clave. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras positivas. Las palabras positivas incluyen título:, asunto:, línea de asunto: Por ejemplo, en algunos documentos las líneas de títulos y autores tienen los mismos formatos. Sin embargo, si las líneas comienzan con una de las palabras positivas, es probable que sean líneas de título. Esta característica binaria representa si la unidad actual comienza o no con una de las palabras negativas. Las palabras negativas incluyen To, By, creado por, actualizado por, etc. Hay más palabras negativas que positivas. Las características lingüísticas mencionadas anteriormente dependen del idioma. Recuento de palabras: Un título no debe ser demasiado largo. Creamos heurísticamente cuatro intervalos: [1, 2], [3, 6], [7, 9] y [9, ∞) y definimos una característica para cada intervalo. Si el número de palabras en un título cae en un intervalo, entonces la característica correspondiente será 1; de lo contrario, 0. Carácter de finalización: Esta característica representa si la unidad termina con :, -, u otros caracteres especiales. Un título generalmente no termina con un carácter como ese. 5. MÉTODO DE RECUPERACIÓN DE DOCUMENTOS Describimos nuestro método de recuperación de documentos utilizando títulos extraídos. Normalmente, en la recuperación de información, un documento se divide en varios campos que incluyen cuerpo, título y texto de anclaje. Una función de clasificación en la búsqueda puede utilizar diferentes pesos para diferentes campos del documento. Además, los títulos suelen recibir un peso alto, lo que indica que son importantes para la recuperación de documentos. Como se explicó anteriormente, nuestro experimento ha demostrado que un número significativo de documentos en realidad tienen títulos incorrectos en las propiedades del archivo, por lo tanto, además de usarlos, utilizamos los títulos extraídos como un campo más del documento. Al hacer esto, intentamos mejorar la precisión general. En este artículo, empleamos una modificación de BM25 que permite la ponderación de campos [21]. Como campos, hacemos uso del cuerpo, título, título extraído y ancla. Primero, para cada término en la consulta contamos la frecuencia del término en cada campo del documento; luego, cada frecuencia de campo se pondera según el parámetro de peso correspondiente: ∑= f tfft tfwwtf De manera similar, calculamos la longitud del documento como una suma ponderada de las longitudes de cada campo. La longitud promedio del documento en el corpus se convierte en el promedio de todas las longitudes de documentos ponderadas. ∑= f ff dlwwdl En nuestros experimentos usamos 75.0,8.11 == bk. El peso para el contenido fue de 1.0, para el título fue de 10.0, para el enlace fue de 10.0 y para el título extraído fue de 5.0. RESULTADOS EXPERIMENTALES 6.1 Conjuntos de datos y medidas de evaluación. Utilizamos dos conjuntos de datos en nuestros experimentos. Primero, descargamos y seleccionamos aleatoriamente 5,000 documentos de Word y 5,000 documentos de PowerPoint de una intranet de Microsoft. Lo llamamos MS de ahora en adelante. Segundo, descargamos y seleccionamos aleatoriamente 500 documentos de Word y 500 documentos de PowerPoint de los dominios DotGov y DotCom en internet, respectivamente. La Figura 7 muestra las distribuciones de los géneros de los documentos. Vemos que los documentos son, de hecho, documentos generales tal como los definimos. Figura 7. Distribuciones de géneros de documentos. Tercero, también se descargó un conjunto de datos en chino de internet. Incluye 500 documentos de Word y 500 documentos de PowerPoint en chino. Etiquetamos manualmente los títulos de todos los documentos, basándonos en nuestra especificación. No todos los documentos en los dos conjuntos de datos tienen títulos. La Tabla 1 muestra los porcentajes de los documentos que tienen títulos. Vemos que DotCom y DotGov tienen más documentos de PowerPoint con títulos que MS. Esto podría deberse a que los documentos de PowerPoint publicados en internet son más formales que los de la intranet. Tabla 1. En nuestros experimentos, realizamos evaluaciones sobre la extracción de títulos en términos de precisión, recuperación y medida F. Las medidas de evaluación se definen de la siguiente manera: Precisión: P = A / (A + B) Recall: R = A / (A + C) F-measure: F1 = 2PR / (P + R) Aquí, A, B, C y D son números de documentos como los definidos en la Tabla 2. Tabla 2. Tabla de contingencia con respecto a la extracción de títulos. ¿Es título? ¿No es título? Extraído A B No extraído C D 6.2 Baselines Probamos las precisiones de los dos baselines descritos en la sección 4.2. Se denominan como el tamaño de fuente más grande y la primera línea respectivamente. 6.3 Precisión de los títulos en las propiedades del archivo Investigamos cuántos títulos en las propiedades del archivo de los documentos son confiables. Consideramos los títulos anotados por humanos como títulos verdaderos y probamos cuántos títulos en las propiedades del archivo pueden coincidir aproximadamente con los títulos verdaderos. Utilizamos la Distancia de Edición para realizar la coincidencia aproximada. (La coincidencia aproximada solo se utiliza en esta evaluación). Esto se debe a que a veces los títulos anotados por humanos pueden ser ligeramente diferentes de los títulos en las propiedades del archivo en la superficie, por ejemplo, contener espacios adicionales. Dadas las cadenas A y B: si ((D == 0) o (D / (La + Lb) < θ)) entonces la cadena A = cadena B D: Distancia de edición entre la cadena A y la cadena B La: longitud de la cadena A Lb: longitud de la cadena B θ: 0.1 ∑ × ++− + = t t n N wtf avwdl wdl bbk kwtf FBM )log( ))1(( )1( 25 1 1 150 Tabla 3. Precisión de los títulos en las propiedades del archivo Tipo de archivo Dominio Precisión Recall F1 MS 0.299 0.311 0.305 DotCom 0.210 0.214 0.212 Word DotGov 0.182 0.177 0.180 MS 0.229 0.245 0.237 DotCom 0.185 0.186 0.186 PowerPoint DotGov 0.180 0.182 0.181 6.4 Comparación con Baselines Realizamos la extracción de títulos del primer conjunto de datos (Word y PowerPoint en MS). Como modelo, utilizamos el Perceptrón. Realizamos validación cruzada de 4 pliegues. Por lo tanto, todos los resultados reportados aquí son aquellos promediados en 4 ensayos. Las tablas 4 y 5 muestran los resultados. Vemos que el Perceptrón supera significativamente a los baselines. En la evaluación, utilizamos coincidencia exacta entre los títulos reales anotados por humanos y los títulos extraídos. Tabla 4. Precisión de la extracción de títulos con el Modelo Perceptrón de Palabra Precisión Recall F1 0.810 0.837 0.823 Tamaño de fuente más grande 0.700 0.758 0.727 Líneas base Primera línea 0.707 0.767 0.736 Tabla 5. Vemos que el enfoque de aprendizaje automático puede lograr un buen rendimiento en la extracción de títulos. Para los documentos de Word, tanto la precisión como la exhaustividad del enfoque son un 8 por ciento más altas que las de las líneas de base. Para PowerPoint, tanto la precisión como la exhaustividad del enfoque son un 2 por ciento más altas que las de las líneas de base. Realizamos pruebas de significancia. Los resultados se muestran en la Tabla 6. Aquí, "Largest" denota la línea base de usar el tamaño de fuente más grande, "First" denota la línea base de usar la primera línea. Los resultados indican que las mejoras del aprendizaje automático sobre las líneas de base son estadísticamente significativas (en el sentido de que el valor p < 0.05) Tabla 6. Vemos, a partir de los resultados, que los dos baselines pueden funcionar bien para la extracción de títulos, lo que sugiere que el tamaño de fuente y la información de posición son las características más útiles para la extracción de títulos. Sin embargo, también es evidente que utilizar solo estas dos características no es suficiente. Hay casos en los que todas las líneas tienen el mismo tamaño de fuente (es decir, el tamaño de fuente más grande), o casos en los que las líneas con el tamaño de fuente más grande solo contienen descripciones generales como Confidencial, Documento blanco, etc. Para esos casos, el método del tamaño de fuente más grande no puede funcionar bien. Por razones similares, el método de la primera línea por sí solo tampoco puede funcionar bien. Con la combinación de diferentes características (evidencia en el juicio del título), el Perceptrón puede superar a Largest y First. Investigamos el rendimiento de utilizar únicamente características lingüísticas. Descubrimos que no funciona bien. Parece que las características del formato desempeñan roles importantes y las características lingüísticas son complementos. Figura 8. Un documento de Word de ejemplo. Figura 9. Un documento de PowerPoint de ejemplo. Realizamos un análisis de errores en los resultados del Perceptrón. Encontramos que los errores caían en tres categorías. (1) Aproximadamente un tercio de los errores estaban relacionados con casos difíciles. En estos documentos, los diseños de las primeras páginas eran difíciles de entender, incluso para los humanos. Las figuras 8 y 9 muestran ejemplos. (2) Casi una cuarta parte de los errores provienen de los documentos que no tienen títulos verdaderos, sino que solo contienen viñetas. Dado que realizamos la extracción desde las regiones superiores, es difícil deshacernos de estos errores con el enfoque actual. Las confusiones entre los títulos principales y los subtítulos fueron otro tipo de error. Dado que solo etiquetamos los títulos principales como títulos, las extracciones de ambos títulos se consideraron incorrectas. Este tipo de error no afecta mucho al procesamiento de documentos como la búsqueda. 6.5 Comparación entre Modelos Para comparar el rendimiento de diferentes modelos de aprendizaje automático, realizamos otro experimento. Nuevamente, realizamos una validación cruzada de 4 pliegues en el primer conjunto de datos (MS). La tabla 7 y 8 muestran los resultados de los cuatro modelos. Resulta que Perceptrón y PMM tienen el mejor rendimiento, seguidos por MEMM, y ME tiene el peor rendimiento. En general, los modelos markovianos tienen un mejor rendimiento que sus contrapartes clasificadoras o igual de bueno. Esto parece ser porque los modelos markovianos se entrenan de forma global, mientras que los clasificadores se entrenan de forma local. Los modelos basados en el Perceptrón tienen un mejor rendimiento que sus contrapartes basadas en el ME. Esto parece ser porque los modelos basados en Perceptrón están creados para realizar mejores clasificaciones, mientras que los modelos de ME se construyen para una mejor predicción. Tabla 7. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con Modelo de Palabra Precisión Recall F1 Perceptrón 0.810 0.837 0.823 MEMM 0.797 0.824 0.810 PMM 0.827 0.823 0.825 ME 0.801 0.621 0.699 Tabla 8. Comparación entre diferentes modelos de aprendizaje para la extracción de títulos con el Modelo de PowerPoint Precisión Recall F1 Perceptrón 0.875 0.895 0.885 MEMM 0.841 0.861 0.851 PMM 0.873 0.896 0.885 ME 0.753 0.766 0.759 Adaptación de dominio Aplicamos el modelo entrenado con el primer conjunto de datos (MS) al segundo conjunto de datos (DotCom y DotGov). Las tablas 9-12 muestran los resultados. Tabla 9. Precisión de la extracción de títulos con Word en DotGov Precisión Recall F1 Modelo Perceptrón 0.716 0.759 0.737 Tamaño de fuente más grande 0.549 0.619 0.582 Baselines Primera línea 0.462 0.521 0.490 Tabla 10. Precisión de la extracción de títulos con PowerPoint en DotGov Precision Recall F1 Modelo Perceptrón 0.900 0.906 0.903 Tamaño de fuente más grande 0.871 0.888 0.879 Baselines Primera línea 0.554 0.564 0.559 Tabla 11. Precisión de la extracción de títulos con Word en DotCom Precisión Recall F1 Modelo Perceptrón 0.832 0.880 0.855 Tamaño de fuente más grande 0.676 0.753 0.712 Baselines Primera línea 0.577 0.643 0.608 Tabla 12. Rendimiento de la extracción de títulos de documentos de PowerPoint en el modelo de Precisión, Recall y F1 de Perceptrón DotCom 0.910 0.903 0.907 Tamaño de fuente más grande 0.864 0.886 0.875 Baselines Primera línea 0.570 0.585 0.577 A partir de los resultados, vemos que los modelos pueden adaptarse bien a diferentes dominios. Casi no hay disminución en la precisión. Los resultados indican que los patrones de formatos de títulos existen en diferentes dominios, y es posible construir un modelo independiente del dominio utilizando principalmente información de formato. 6.7 Adaptación de idioma Aplicamos el modelo entrenado con los datos en inglés (MS) al conjunto de datos en chino. Las tablas 13-14 muestran los resultados. Tabla 13. Precisión de la extracción de títulos con Word en chino: Precisión Recall F1 Modelo Perceptrón 0.817 0.805 0.811 Tamaño de fuente más grande 0.722 0.755 0.738 Baselines Primera línea 0.743 0.777 0.760 Tabla 14. Vemos que los modelos se pueden adaptar a un idioma diferente. Solo hay pequeñas caídas en la precisión. Obviamente, las características lingüísticas no funcionan para el chino, pero el efecto de no usarlas es insignificante. Los resultados indican que los patrones de formatos de títulos existen en diferentes idiomas. A partir de los resultados de adaptación de dominio y adaptación de lenguaje, concluimos que el uso de información de formato es clave para una extracción exitosa de documentos generales. 6.8 Búsqueda con Títulos Extraídos Realizamos experimentos utilizando la extracción de títulos para la recuperación de documentos. Como línea base, empleamos BM25 sin utilizar títulos extraídos. El mecanismo de clasificación fue como se describe en la Sección 5. Los pesos fueron establecidos heurísticamente. No realizamos la optimización de los pesos. La evaluación se llevó a cabo en un corpus de 1.3 millones de documentos extraídos de la intranet de Microsoft utilizando 100 consultas de evaluación obtenidas de los registros de consultas del motor de búsqueda de esta intranet. 50 consultas fueron del conjunto más popular, mientras que otras 50 consultas fueron elegidas al azar. A los usuarios se les pidió que proporcionaran juicios sobre el grado de relevancia del documento en una escala del 1 al 5 (1 significando perjudicial, 2 - malo, 3 - regular, 4 - bueno y 5 - excelente). La figura 10 muestra los resultados. En el gráfico se obtuvieron dos conjuntos de resultados de precisión al considerar documentos buenos o excelentes como relevantes (tres barras izquierdas con umbral de relevancia 0.5), o al considerar solo documentos excelentes como relevantes (tres barras derechas con umbral de relevancia 1.0). Resultados de clasificación de búsqueda. La Figura 10 muestra diferentes resultados de recuperación de documentos con diferentes funciones de clasificación en términos de precisión @10, precisión @5 y rango recíproco: • Barra azul - BM25 incluyendo los campos cuerpo, título (propiedad de archivo) y texto de anclaje. • Barra morada - BM25 incluyendo los campos cuerpo, título (propiedad de archivo), texto de anclaje y título extraído. Con el campo adicional de título extraído incluido en BM25, la precisión @10 aumentó de 0.132 a 0.145, o aproximadamente un 10%. Por lo tanto, se puede afirmar que el uso del título extraído puede mejorar efectivamente la precisión de la recuperación de documentos. 7. CONCLUSIÓN En este documento, hemos investigado el problema de extraer automáticamente títulos de documentos generales. Hemos intentado utilizar un enfoque de aprendizaje automático para abordar el problema. Trabajos anteriores mostraron que el enfoque de aprendizaje automático puede funcionar bien para la extracción de metadatos de artículos de investigación. En este artículo, demostramos que el enfoque también puede funcionar para la extracción de documentos generales. Nuestros resultados experimentales indicaron que el enfoque de aprendizaje automático puede funcionar significativamente mejor que las líneas base en la extracción de títulos de documentos de Office. Trabajos anteriores sobre extracción de metadatos principalmente utilizaron características lingüísticas en los documentos, mientras que nosotros principalmente utilizamos información de formato. Parecía que el uso de información de formato es clave para llevar a cabo con éxito la extracción de títulos de documentos generales. Probamos diferentes modelos de aprendizaje automático incluyendo Perceptrón, Entropía Máxima, Modelo de Markov de Entropía Máxima y Perceptrón Votado. Descubrimos que el rendimiento de los modelos Perceptrón fue el mejor. Aplicamos modelos construidos en un dominio a otro dominio y aplicamos modelos entrenados en un idioma a otro idioma. Encontramos que las precisiones no disminuyeron sustancialmente en diferentes dominios y en diferentes idiomas, lo que indica que los modelos eran genéricos. También intentamos utilizar los títulos extraídos en la recuperación de documentos. Observamos una mejora significativa en el rendimiento de clasificación de documentos para la búsqueda al utilizar la información del título extraída. Todas las investigaciones anteriores no se llevaron a cabo, y a través de nuestras investigaciones verificamos la generalidad y la importancia del enfoque de extracción de títulos. 8. AGRADECIMIENTOS Agradecemos a Chunyu Wei y Bojuan Zhao por su trabajo en la anotación de datos. Reconocemos a Jinzhu Li por su ayuda en la realización de los experimentos. Agradecemos a Ming Zhou, John Chen, Jun Xu y a los revisores anónimos de JCDL05 por sus valiosos comentarios sobre este artículo. 9. REFERENCIAS [1] Berger, A. L., Della Pietra, S. A., y Della Pietra, V. J. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Lingüística Computacional, 22:39-71, 1996. [2] Collins, M. Métodos de entrenamiento discriminativo para modelos ocultos de Markov: teoría y experimentos con algoritmos de perceptrón. En Actas de la Conferencia sobre Métodos Empíricos en Procesamiento de Lenguaje Natural, 1-8, 2002. [3] Cortes, C. y Vapnik, V. Redes de vectores de soporte. Aprendizaje automático, 20:273-297, 1995. [4] Chieu, H. L. y Ng, H. T. Un enfoque de entropía máxima para la extracción de información de texto semiestructurado y libre. En Actas de la Decimoctava Conferencia Nacional sobre Inteligencia Artificial, 768-791, 2002. [5] Evans, D. K., Klavans, J. L., y McKeown, K. R. Columbia newsblaster: resumen multilingüe de noticias en la web. En Actas de la conferencia de Tecnología del Lenguaje Humano / capítulo norteamericano de la reunión anual de la Asociación de Lingüística Computacional, 1-4, 2004. [6] Ghahramani, Z. y Jordan, M. I. Modelos ocultos de Markov factorial. Aprendizaje automático, 29:245-273, 1997. [7] Gheel, J. y Anderson, T. Datos y metadatos para encontrar y recordar, En Actas de la Conferencia Internacional de Visualización de Información de 1999, 446-451, 1999. [8] Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., Lawrence, S., Rangaswamy, A., y Pal, N. eBizSearch: un motor de búsqueda de nicho para e-Business. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 413-414, 2003. [9] Giuffrida, G., Shek, E. C., y Yang, J. Extracción de metadatos basada en conocimiento de archivos PostScript. En Actas de la Quinta Conferencia de Bibliotecas Digitales de la ACM, 77-84, 2000. [10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., y Fox, E. A. Extracción automática de metadatos de documentos utilizando máquinas de vectores de soporte. En Actas de la Tercera Conferencia Conjunta ACM/IEEE-CS sobre Bibliotecas Digitales, 37-48, 2003. [11] Kobayashi, M., y Takeda, K. Recuperación de información en la Web. ACM Computing Surveys, 32:144-173, 2000. [12] Lafferty, J., McCallum, A., y Pereira, F. Campos aleatorios condicionales: modelos probabilísticos para segmentar y etiquetar datos de secuencias. En Actas de la Decimoctava Conferencia Internacional sobre Aprendizaje Automático, 282-289, 2001. [13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., y Kandola, J. S. El algoritmo del perceptrón con márgenes desiguales. En Actas de la Decimonovena Conferencia Internacional sobre Aprendizaje Automático, 379-386, 2002. [14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., y Silverstein, J. Generación y evaluación automática de metadatos. En Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 401-402, 2002. [15] Littlefield, A. Recuperación efectiva de información empresarial en nuevos formatos de contenido. En Actas de la Séptima Conferencia de Motores de Búsqueda, http://www.infonortics.com/searchengines/sh02/02prog.html, 2002. [16] Mao, S., Kim, J. W., y Thoma, G. R. Un sistema de generación de características dinámicas para la extracción automatizada de metadatos en la preservación de materiales digitales. En Actas del Primer Taller Internacional sobre Análisis de Imágenes de Documentos para Bibliotecas, 225-232, 2004. [17] McCallum, A., Freitag, D., y Pereira, F. Modelos de máxima entropía de Markov para extracción de información y segmentación. En Actas de la Decimoséptima Conferencia Internacional sobre Aprendizaje Automático, 591-598, 2000. [18] Murphy, L. D. Metadatos de documentos digitales en organizaciones: roles, enfoques analíticos y futuras direcciones de investigación. En Actas de la Trigésimo-Primera Conferencia Internacional Anual de Ciencias de Sistemas de Hawái, 267-276, 1998. [19] Pinto, D., McCallum, A., Wei, X., y Croft, W. B. Extracción de tablas utilizando campos aleatorios condicionales. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 235242, 2003. [20] Ratnaparkhi, A. Modelos estadísticos no supervisados para la unión de frases preposicionales. En Actas de la Decimoséptima Conferencia Internacional de Lingüística Computacional. 1079-1085, 1998. [21] Robertson, S., Zaragoza, H., y Taylor, M. Extensión simple de BM25 a múltiples campos ponderados, En Actas de la Decimotercera Conferencia de ACM sobre Información y Gestión del Conocimiento, 42-49, 2004. [22] Yi, J. y Sundaresan, N. Minería web basada en metadatos para relevancia, En Actas del Simposio Internacional de Ingeniería y Aplicaciones de Bases de Datos 2000, 113-121, 2000. [23] Yilmazel, O., Finneran, C. M., y Liddy, E. D. MetaExtract: Un sistema de procesamiento de lenguaje natural para asignar automáticamente metadatos. En Actas de la Conferencia Conjunta ACM/IEEE sobre Bibliotecas Digitales de 2004, 241-242, 2004. [24] Zhang, J. y Dimitroff, A. Respuesta de los motores de búsqueda de Internet a la implementación de metadatos Dublin Core. Revista de Ciencia de la Información, 30:310-320, 2004. [25] Zhang, L., Pan, Y., y Zhang, T. Reconocimiento y uso de entidades nombradas: reconocimiento de entidades nombradas enfocado utilizando aprendizaje automático. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 281-288, 2004. [26] http://dublincore.org/groups/corporate/Seattle/ 154