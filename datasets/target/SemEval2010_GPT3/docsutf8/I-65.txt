Modelos gráficos para soluciones en línea a POMDP interactivos de Prashant Doshi Dept. Departamento de Ciencias de la Computación Universidad de Georgia Athens, GA 30602, EE. UU. pdoshi@cs.uga.edu Yifeng Zeng Dept. de Ciencias de la Computación Universidad de Aalborg DK-9220 Aalborg, Dinamarca yfzeng@cs.aau.edu Qiongyu Chen Dept. de Ciencias de la Computación Universidad Nacional de Singapur 117543, Singapur chenqy@comp.nus.edu.sg RESUMEN Desarrollamos una nueva representación gráfica para procesos de decisión de Markov parcialmente observables interactivos (I-POMDPs) que es significativamente más transparente y semánticamente clara que la representación anterior. Estos modelos gráficos llamados diagramas de influencia dinámica interactiva (I-DIDs) buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de probabilidad y decisión, y las dependencias entre las variables. Los I-DIDs generalizan los DIDs, los cuales pueden ser vistos como representaciones gráficas de POMDPs, a entornos multiagentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes que interactúan entre sí. Usando varios ejemplos, mostramos cómo se pueden aplicar los I-DIDs y demostramos su utilidad. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial Distribuida]: Sistemas Multiagentes Términos Generales Teoría 1. Las Procesos de Decisión de Markov Interactivos Parcialmente Observables (IPOMDPs) proporcionan un marco para la toma de decisiones secuenciales en entornos multiagentes parcialmente observables. Generalizan los POMDPs [13] a entornos multiagentes al incluir los modelos computables de los otros agentes en el espacio de estados junto con los estados del entorno físico. Los modelos abarcan toda la información que influye en los comportamientos de los agentes, incluyendo sus preferencias, capacidades y creencias, y por lo tanto son análogos a los tipos en los juegos bayesianos [11]. I-POMDPs adoptan un enfoque subjetivo para comprender el comportamiento estratégico, arraigado en un marco de teoría de decisiones que toma la perspectiva de los tomadores de decisiones en la interacción. En [15], Polich y Gmytrasiewicz introdujeron diagramas de influencia dinámica interactivos (I-DIDs) como las representaciones computacionales de I-POMDPs. Los I-DIDs generalizan los DIDs [12], los cuales pueden ser vistos como contrapartes computacionales de POMDPs, a entornos de múltiples agentes de la misma manera que los I-POMDPs generalizan los POMDPs. Los I-DIDs contribuyen a una creciente línea de trabajo [19] que incluye diagramas de influencia multiagente (MAIDs) [14], y más recientemente, redes de diagramas de influencia (NIDs) [8]. Estos formalismos buscan modelar explícitamente la estructura que a menudo está presente en problemas del mundo real al descomponer la situación en variables de azar y decisiones, y las dependencias entre las variables. Los MAIDs proporcionan una alternativa a las formas normales y extensivas de juego utilizando un formalismo gráfico para representar juegos de información imperfecta con un nodo de decisión para las acciones de cada agente y nodos de azar que capturan la información privada de los agentes. Los MAIDs analizan objetivamente el juego, calculando eficientemente el perfil de equilibrio de Nash al explotar la estructura de independencia. NIDs extienden los MAIDs para incluir la incertidumbre de los agentes sobre el juego que se está jugando y sobre los modelos de los otros agentes. Cada modelo es un MAID y la red de MAIDs se colapsa, de abajo hacia arriba, en un solo MAID para calcular el equilibrio del juego teniendo en cuenta los diferentes modelos de cada agente. Los formalismos gráficos como MAIDs y NIDs abren una área de investigación prometedora que tiene como objetivo representar las interacciones multiagente de manera más transparente. Sin embargo, los MAIDs proporcionan un análisis del juego desde un punto de vista externo y la aplicabilidad de ambos está limitada a juegos estáticos de un solo jugador. Los asuntos son más complejos cuando consideramos interacciones que se extienden en el tiempo, donde las predicciones sobre las acciones futuras de otros deben hacerse utilizando modelos que cambian a medida que los agentes actúan y observan. Los I-DIDs abordan esta brecha al permitir la representación de modelos de otros agentes como los valores de un nodo de modelo especial. Tanto los modelos de otros agentes como las creencias originales de los agentes sobre estos modelos se actualizan con el tiempo utilizando implementaciones especializadas. En este documento, mejoramos la representación preliminar previa del I-DID mostrada en [15] utilizando la idea de que el I-ID estático es un tipo de NID. Por lo tanto, podemos utilizar construcciones de lenguaje específicas de NID, como multiplexores, para representar de manera más transparente el nodo del modelo y, posteriormente, el I-ID. Además, aclaramos la semántica del enlace de política de propósito especial introducido en la representación de I-DID por [15], y mostramos que podría ser reemplazado por enlaces de dependencia tradicionales. En la representación previa del I-DID, la actualización de la creencia de los agentes sobre los modelos de los demás a medida que los agentes actúan y reciben observaciones se denotaba utilizando un enlace especial llamado el enlace de actualización del modelo que conectaba los nodos del modelo a lo largo del tiempo. Explicamos la semántica de este enlace mostrando cómo puede ser implementado utilizando los enlaces de dependencia tradicionales entre los nodos de probabilidad que constituyen los nodos del modelo. El resultado final es una representación de I-DID que es significativamente más transparente, semánticamente clara y capaz de ser implementada utilizando los algoritmos estándar para resolver DIDs. Mostramos cómo los IDIDs pueden ser utilizados para modelar la incertidumbre de un agente sobre los modelos de otros, que a su vez pueden ser I-DIDs. La solución al I-DID es una política que prescribe lo que el agente debe hacer con el tiempo, dadas sus creencias sobre el estado físico y otros modelos. Análogos a los DIDs, los I-DIDs pueden ser utilizados para calcular la política de un agente en línea mientras el agente actúa y observa en un entorno poblado por otros agentes interactuantes. 2. ANTECEDENTES: Los IPOMDPS ANIDADOS FINITAMENTE generalizan los POMDPs a entornos multiagentes al incluir los modelos de otros agentes como parte del espacio de estados [9]. Dado que otros agentes también pueden razonar sobre otros, el espacio de estado interactivo está estratégicamente anidado; contiene creencias sobre los modelos de otros agentes y sus creencias sobre los demás. Para simplificar la presentación, consideramos un agente, i, que está interactuando con otro agente, j. Un I-POMDP finitamente anidado del agente i con un nivel de estrategia l se define como la tupla: I-POMDPi,l = ISi,l, A, Ti, Ωi, Oi, Ri donde: • ISi,l denota un conjunto de estados interactivos definido como, ISi,l = S × Mj,l−1, donde Mj,l−1 = {Θj,l−1 ∪ SMj}, para l ≥ 1, e ISi,0 = S, donde S es el conjunto de estados del entorno físico. Θj,l−1 es el conjunto de modelos intencionales computables del agente j: θj,l−1 = bj,l−1, ˆθj donde el marco, ˆθj = A, Ωj, Tj, Oj, Rj, OCj. Aquí, j es Bayesiano racional y OCj es el criterio de optimalidad de j. SMj es el conjunto de modelos subintencionales de j. Ejemplos simples de modelos subintencionales incluyen un modelo sin información [10] y un modelo de juego ficticio [6], ambos de los cuales son independientes de la historia. Damos una construcción recursiva de abajo hacia arriba del espacio de estados interactivo a continuación. Sí,0 = S, Θj,0 = { bj,0, ˆθj | bj,0 ∈ Δ(ISj,0)} Sí,1 = S × {Θj,0 ∪ SMj}, Θj,1 = { bj,1, ˆθj | bj,1 ∈ Δ(ISj,1)} . . . . . . Sí, l = S × {Θj,l−1 ∪ SMj}, Θj,l = { bj,l, ˆθj | bj,l ∈ Δ(ISj,l)} Formulaciones similares de espacios anidados han aparecido en [1, 3]. • A = Ai × Aj es el conjunto de acciones conjuntas de todos los agentes en el entorno; • Ti : S ×A×S → [0, 1], describe el efecto de las acciones conjuntas en los estados físicos del entorno; • Ωi es el conjunto de observaciones del agente i; • Oi : S × A × Ωi → [0, 1] da la probabilidad de las observaciones dadas el estado físico y la acción conjunta; • Ri : ISi × A → R describe las preferencias del agente sobre sus estados interactivos. Normalmente solo importarán los estados físicos. La política del agente es el mapeo, Ω∗ i → Δ(Ai), donde Ω∗ i es el conjunto de todos los historiales de observación del agente i. Dado que la creencia sobre los estados interactivos forma una estadística suficiente [9], la política también puede ser representada como un mapeo del conjunto de todas las creencias del agente i a una distribución sobre sus acciones, Δ(ISi) → Δ(Ai). 2.1 Actualización de Creencias Análogo a los POMDPs, un agente dentro del marco I-POMDP actualiza su creencia a medida que actúa y observa. Sin embargo, hay dos diferencias que complican la actualización de creencias en entornos multiagentes en comparación con los entornos de un solo agente. Primero, dado que el estado del entorno físico depende de las acciones de ambos agentes, la predicción de cómo cambia el estado físico debe hacerse basándose en la predicción de sus acciones. Segundo, los cambios en los modelos de js deben ser incluidos en la actualización de creencias. Específicamente, si j es intencional, entonces se debe incluir una actualización de las creencias de j debido a su acción y observación. En otras palabras, i tiene que actualizar su creencia basándose en su predicción de lo que j observaría y cómo j actualizaría su creencia. Si el modelo de js es subintencional, entonces las observaciones probables de js se añaden al historial de observaciones contenido en el modelo. Formalmente, tenemos: Pr(ist |at−1 i , bt−1 i,l ) = β ISt−1:mt−1 j =θt j bt−1 i,l (ist−1 ) × at−1 j Pr(at−1 j |θt−1 j,l−1)Oi(st , at−1 i , at−1 j , ot i) ×Ti(st−1 , at−1 i , at−1 j , st ) ot j Oj(st , at−1 i , at−1 j , ot j) ×τ(SEθt j (bt−1 j,l−1, at−1 j , ot j) − bt j,l−1) (1) donde β es la constante de normalización, τ es 1 si su argumento es 0, de lo contrario es 0, Pr(at−1 j |θt−1 j,l−1) es la probabilidad de que at−1 j sea Bayes racional para el agente descrito por el modelo θt−1 j,l−1, y SE(·) es una abreviatura para la actualización de creencias. Para una versión de la actualización de creencias cuando el modelo js es subintencional, consulte [9]. Si el agente j también se modela como un I-POMDP, entonces la actualización de creencias invoca la actualización de creencias de j (a través del término SEθt j ( bt−1 j,l−1 , at−1 j , ot j)), lo que a su vez podría invocar la actualización de creencias de is y así sucesivamente. Esta recursión en la anidación de creencias llega al nivel 0. En este nivel, la actualización de creencias del agente se reduce a una actualización de creencias de un POMDP. Para ilustraciones de la actualización de creencias, detalles adicionales sobre I-POMDPs y cómo se comparan con otros marcos multiagentes, consulte [9]. Iteración de Valor Cada estado de creencia en un I-POMDP finitamente anidado tiene un valor asociado que refleja la máxima ganancia que el agente puede esperar en este estado de creencia: Un ( bi,l, θi ) = max ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is)+ γ oi∈Ωi Pr(oi|ai, bi,l)Un−1 ( SEθi (bi,l, ai, oi), θi ) (2) donde, ERi(is, ai) = aj Ri(is, ai, aj)Pr(aj|mj,l−1) (ya que is = (s, mj,l−1)). La ecuación 2 es una base para la iteración de valor en I-POMDPs. La acción óptima del agente, a∗ i, para el caso de horizonte finito con descuento, es un elemento del conjunto de acciones óptimas para el estado de creencia, OPT(θi), definido como: OPT( bi,l, θi ) = argmax ai∈Ai is∈ISi,l ERi(is, ai)bi,l(is) +γ oi∈Ωi Pr(oi|ai, bi,l)Un ( SEθi (bi,l, ai, oi), θi ) (3) 3. Diagramas de influencia interactivos. Una extensión ingenua de los diagramas de influencia (IDs) a entornos poblados por múltiples agentes es posible tratando a los otros agentes como autómatas, representados mediante nodos de probabilidad. Sin embargo, este enfoque asume que las acciones de los agentes están controladas utilizando una distribución de probabilidad que no cambia con el tiempo. Los diagramas de influencia interactivos (I-IDs) adoptan un enfoque más sofisticado al generalizar los IDs para hacerlos aplicables a entornos compartidos con otros agentes que pueden actuar, observar y actualizar sus creencias. 3.1 Sintaxis Además de los nodos habituales de probabilidad, decisión y utilidad, los IIDs incluyen un nuevo tipo de nodo llamado nodo de modelo. Mostramos un nivel general l I-ID en la Fig. 1(a), donde el nodo del modelo (Mj,l−1) está representado con un hexágono. Observamos que la distribución de probabilidad sobre el nodo de probabilidad, S, y el nodo del modelo juntos representan la creencia del agente sobre sus estados interactivos. Además del modelo 1, el modelo de nivel 0 es un POMDP: las acciones de otros agentes se tratan como eventos exógenos y se incorporan en las funciones T, O y R. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: (a) Un nivel genérico l I-ID para el agente i situado con otro agente j. El hexágono es el nodo modelo (Mj,l−1) cuya estructura mostramos en (b). Los miembros del nodo modelo son ellos mismos I-ID (m1 j,l−1, m2 j,l−1; los diagramas no se muestran aquí por simplicidad) cuyos nodos de decisión se asignan a los nodos de probabilidad correspondientes (A1 j , A2 j). Dependiendo del valor del nodo, Mod[Mj], la distribución de cada uno de los nodos de probabilidad se asigna al nodo Aj. (c) El I-ID transformado con el nodo del modelo reemplazado por los nodos de probabilidad y las relaciones entre ellos. Los I-IDs difieren de los IDs al tener un enlace discontinuo (llamado enlace de política en [15]) entre el nodo del modelo y un nodo de probabilidad, Aj, que representa la distribución sobre las acciones de los otros agentes dada su modelo. En ausencia de otros agentes, el nodo del modelo y el nodo de probabilidad, Aj, desaparecen y los I-ID se convierten en ID tradicionales. El nodo del modelo contiene los modelos computacionales alternativos atribuidos por i al otro agente del conjunto, Θj,l−1 ∪ SMj, donde Θj,l−1 y SMj fueron definidos previamente en la Sección 2. Por lo tanto, un modelo en el nodo del modelo puede ser en sí mismo un I-ID o ID, y la recursión termina cuando un modelo es un ID o subintencional. Dado que el nodo del modelo contiene los modelos alternativos del otro agente como sus valores, su representación no es trivial. En particular, algunos de los modelos dentro del nodo son I-IDs que, al resolverse, generan la política óptima de los agentes en sus nodos de decisión. Cada nodo de decisión se asigna al nodo de probabilidad correspondiente, digamos A1 j, de la siguiente manera: si OPT es el conjunto de acciones óptimas obtenidas al resolver el I-ID (o ID), entonces Pr(aj ∈ A1 j) = 1 |OPT| si aj ∈ OPT, 0 en caso contrario. Tomando prestados los conocimientos de trabajos anteriores [8], observamos que el nodo del modelo y el enlace de política discontinua que lo conecta con el nodo de probabilidad, Aj, podrían representarse como se muestra en la Fig. 1(b). El nodo de decisión de cada nivel l − 1 I-ID se transforma en un nodo de probabilidad, como mencionamos anteriormente, de modo que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que al resto se les asigna probabilidad cero. Los diferentes nodos de probabilidad (A1 j , A2 j ), uno para cada modelo, y adicionalmente, el nodo de probabilidad etiquetado Mod[Mj] forman los padres del nodo de probabilidad, Aj. Por lo tanto, hay tantos nodos de acción (A1 j , A2 j ) en Mj,l−1 como el número de modelos en el soporte de las creencias del agente. La tabla de probabilidad condicional del nodo de probabilidad, Aj, es un multiplexor que asume la distribución de cada uno de los nodos de acción (A1 j , A2 j ) dependiendo del valor de Mod[Mj]. Los valores de Mod[Mj] denotan los diferentes modelos de j. En otras palabras, cuando Mod[Mj] tiene el valor m1 j,l−1, el nodo de probabilidad Aj asume la distribución del nodo A1 j, y Aj asume la distribución de A2 j cuando Mod[Mj] tiene el valor m2 j,l−1. La distribución sobre el nodo, Mod[Mj], es la creencia del agente sobre los modelos de j dado un estado físico. Para más agentes, tendremos tantos nodos de modelo como agentes haya. Observa que la Fig. 1(b) aclara la semántica del enlace de política y muestra cómo puede ser representado utilizando los enlaces de dependencia tradicionales. En la Fig. 1(c), mostramos el I-ID transformado cuando el nodo del modelo es reemplazado por los nodos de probabilidad y las relaciones entre ellos. A diferencia de la representación en [15], no hay enlaces de políticas especiales, sino que el I-ID está compuesto únicamente por aquellos tipos de nodos que se encuentran en IDs tradicionales y relaciones de dependencia entre los nodos. Esto permite que los I-IDs sean representados e implementados utilizando herramientas de aplicación convencionales que apuntan a los IDs. Ten en cuenta que podemos ver el nivel l I-ID como un NID. Específicamente, cada uno de los modelos del nivel l − 1 dentro del nodo del modelo son bloques en el NID (ver Fig. 2). Si el nivel l = 1, cada bloque es un ID tradicional, de lo contrario, si l > 1, cada bloque dentro del NID puede ser un NID en sí mismo. Ten en cuenta que dentro de los I-IDs (o IDs) en cada nivel, solo hay un nodo de decisión único. Por lo tanto, nuestro NID no contiene ningún MAID. Figura 2: Un nivel l I-ID representado como un NID. Las probabilidades asignadas a los bloques del NID son creencias sobre modelos js condicionados a un estado físico. 3.2 Solución La solución de un I-ID procede de manera ascendente y se implementa de forma recursiva. Comenzamos resolviendo los modelos de nivel 0, que, si son intencionales, son identificadores tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones de los otros agentes, las cuales se ingresan en los nodos de probabilidad correspondientes encontrados en el nodo del modelo del nivel 1 I-ID. El mapeo de los nodos de decisión de los modelos de nivel 0 a los nodos de probabilidad se realiza de manera que las acciones con el mayor valor en el nodo de decisión se les asignan probabilidades uniformes en el nodo de probabilidad, mientras que el resto se les asigna probabilidad cero. Dadas las distribuciones sobre las acciones dentro de los diferentes nodos de probabilidad (uno para cada modelo del otro agente), el I-ID de nivel 1 se transforma como se muestra en la Fig. 1(c). Durante la transformación, la tabla de probabilidad condicional (CPT) del nodo, Aj, se llena de tal manera que el nodo asume la distribución de cada uno de los nodos de probabilidad dependiendo del valor del nodo, Mod[Mj]. Como mencionamos anteriormente, los valores del nodo Mod[Mj] denotan los diferentes modelos del otro agente, y su distribución es la creencia del agente sobre los modelos de j condicionados al estado físico. El nivel 1 I-ID transformado es un ID tradicional que puede ser resuelto como us816 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 3: (a) Un I-DID genérico de dos niveles de tiempo para el agente i en un entorno con otro agente j. Observa el enlace de actualización del modelo punteado que denota la actualización de los modelos de j y la distribución de los modelos a lo largo del tiempo. (b) La semántica del enlace de actualización del modelo utilizando el método estándar de maximización de utilidad esperada [18]. Este procedimiento se lleva a cabo hasta el nivel l I-ID cuya solución proporciona el conjunto no vacío de acciones óptimas que el agente debe realizar dada su creencia. Ten en cuenta que, al igual que los IDs, los I-IDs son adecuados para la toma de decisiones en línea cuando se conoce la creencia actual de los agentes. 4. Diagramas de influencia dinámica interactivos (I-DIDs) Los diagramas de influencia dinámica interactivos (I-DIDs) extienden los I-IDs (y NIDs) para permitir la toma de decisiones secuencial a lo largo de varios pasos de tiempo. Así como los DIDs son representaciones gráficas estructuradas de POMDPs, los I-DIDs son los análogos gráficos en línea para I-POMDPs finitamente anidados. Los I-DIDs pueden ser utilizados para optimizar sobre una mirada anticipada finita dadas las creencias iniciales mientras se interactúa con otros agentes, posiblemente similares. 4.1 Sintaxis Representamos un I-DID de dos cortes temporales en la Fig. 3(a). Además de los nodos del modelo y el enlace de política discontinuo, lo que diferencia a un I-DID de un DID es el enlace de actualización del modelo mostrado como una flecha punteada en la Fig. 3(a). Explicamos la semántica del nodo del modelo y el enlace de la política en la sección anterior; a continuación, describimos las actualizaciones del modelo. La actualización del nodo del modelo con el tiempo implica dos pasos: primero, dados los modelos en el tiempo t, identificamos el conjunto actualizado de modelos que residen en el nodo del modelo en el tiempo t + 1. Recuerda de la Sección 2 que el modelo intencional de un agente incluye sus creencias. Debido a que los agentes actúan y reciben observaciones, sus modelos se actualizan para reflejar sus creencias cambiadas. Dado que el conjunto de acciones óptimas para un modelo podría incluir todas las acciones, y el agente podría recibir cualquiera de las |Ωj| posibles observaciones, el conjunto actualizado en el paso de tiempo t + 1 tendrá como máximo |Mt j,l−1||Aj||Ωj| modelos. Aquí, |Mt j,l−1| es el número de modelos en el paso de tiempo t, |Aj| y |Ωj| son los espacios más grandes de acciones y observaciones respectivamente, entre todos los modelos. Segundo, calculamos la nueva distribución sobre los modelos actualizados dados la distribución original y la probabilidad de que el agente realice la acción y reciba la observación que llevó al modelo actualizado. Estos pasos son parte de la actualización de creencias del agente formalizada utilizando la Ecuación 1. En la Fig. 3(b), mostramos cómo se implementa el enlace de actualización del modelo punteado en el I-DID. Si cada uno de los dos modelos de nivel l − 1 atribuidos a j en el paso de tiempo t resulta en una acción, y j podría hacer una de dos posibles observaciones, entonces el nodo del modelo en el paso de tiempo t + 1 contiene cuatro modelos actualizados (mt+1,1 j,l−1 ,mt+1,2 j,l−1 , mt+1,3 j,l−1 , y mt+1,4 j,l−1 ). Estos modelos difieren en sus creencias iniciales, cada una de las cuales es el resultado de j actualizar sus creencias debido a su acción y una posible observación. Los nodos de decisión en cada uno de los I-DIDs o DIDs que representan los modelos de nivel inferior se asignan a la Figura 4 correspondiente: I-DID transformado con los nodos del modelo y el enlace de actualización del modelo reemplazados por los nodos de probabilidad y las relaciones (en negrita) de los nodos de probabilidad, como se mencionó anteriormente. A continuación, describimos cómo se calcula la distribución sobre el conjunto actualizado de modelos (la distribución sobre el nodo de probabilidad Mod[Mt+1 j ] en Mt+1 j,l−1). La probabilidad de que el modelo actualizado js sea, digamos mt+1,1 j,l−1, depende de la probabilidad de que j realice la acción y reciba la observación que condujo a este modelo, y de la distribución previa sobre los modelos en el paso de tiempo t. Dado que el nodo de probabilidad At j asume la distribución de cada uno de los nodos de acción basados en el valor de Mod[Mt j], la probabilidad de la acción está dada por este nodo de probabilidad. Para obtener la probabilidad de una observación posible js, introducimos el nodo de probabilidad Oj, que dependiendo del valor de Mod[Mt j], asume la distribución del nodo de observación en el modelo de nivel inferior denominado Mod[Mt j]. Dado que la probabilidad de las observaciones js depende del estado físico y de las acciones conjuntas de ambos agentes, el nodo Oj está vinculado con St+1, At j y At i. De manera análoga a At j, la tabla de probabilidad condicional de Oj también es un multiplexor modulado por Mod[Mt j]. Finalmente, la distribución de los modelos previos en el tiempo t se obtiene a partir del nodo de probabilidad, Mod[Mt j ] en Mt j,l−1. Por consiguiente, los nodos de probabilidad, Mod[Mt j], At j y Oj, forman los padres de Mod[Mt+1 j] en Mt+1 j,l−1. Ten en cuenta que el enlace de actualización del modelo puede ser reemplazado por los enlaces de dependencia entre los nodos de probabilidad que constituyen los nodos del modelo en las dos etapas temporales. En la Fig. 4 mostramos los dos cortes temporales I-DID con los nodos del modelo reemplazados por los nodos de probabilidad y las relaciones entre ellos. Los nodos de probabilidad y los enlaces de dependencia que no están en negrita son estándar, generalmente encontrados en DIDs. La expansión del I-DID a lo largo de más pasos de tiempo requiere la repetición de los dos pasos de actualizar el conjunto de modelos que forman el 2. Nota que Oj representa la observación j en el tiempo t + 1. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 817 valores del nodo del modelo y añadiendo las relaciones entre los nodos de probabilidad, tantas veces como enlaces de actualización del modelo haya. Observamos que el conjunto posible de modelos del otro agente j crece exponencialmente con el número de pasos de tiempo. Por ejemplo, después de T pasos, puede haber como máximo |Mt=1 j,l−1|(|Aj||Ωj|)T −1 modelos candidatos residiendo en el nodo del modelo. 4.2 Solución Análoga a los I-ID, la solución a un I-DID de nivel l para el agente i expandido a lo largo de T pasos de tiempo puede llevarse a cabo de forma recursiva. Con el propósito de ilustración, dejemos l=1 y T=2. El método de solución utiliza la técnica estándar de anticipación, proyectando las secuencias de acciones y observaciones de los agentes hacia adelante desde el estado de creencia actual [17], y encontrando las posibles creencias que podría tener en el siguiente paso temporal. Dado que el agente i también tiene una creencia sobre los modelos de j, la búsqueda anticipada incluye descubrir los posibles modelos que j podría tener en el futuro. Por consiguiente, cada uno de los modelos subintencionales o de nivel 0 de js (representados utilizando un DID estándar) en el primer paso de tiempo debe resolverse para obtener su conjunto óptimo de acciones. Estas acciones se combinan con el conjunto de observaciones posibles que j podría hacer en ese modelo, lo que resulta en un conjunto actualizado de modelos candidatos (que incluyen las creencias actualizadas) que podrían describir el comportamiento de j. Las creencias sobre este conjunto actualizado de modelos candidatos se calculan utilizando los métodos estándar de inferencia que utilizan las relaciones de dependencia entre los nodos del modelo, como se muestra en la Figura 3(b). Observamos la naturaleza recursiva de esta solución: al resolver el agente de nivel 1 I-DID, los DIDs de nivel 0 deben ser resueltos. Si el anidamiento de los modelos es más profundo, todos los modelos en todos los niveles, comenzando desde el nivel 0, se resuelven de manera ascendente. Breve descripción del algoritmo recursivo para resolver el agente es el Algoritmo para resolver I-DID. Entrada: nivel l ≥ 1 I-ID o nivel 0 ID, T Fase de Expansión 1. Para t desde 1 hasta T − 1, hacer 2. Si l ≥ 1, entonces llenar Mt+1 j,l−1 3. Para cada mt j en el rango (Mt j,l−1) hacer 4. Llame recursivamente al algoritmo con el l - 1 I-ID (o ID) que representa mt j y el horizonte, T - t + 1 5. Mapea el nodo de decisión del I-ID (o ID) resuelto, OPT(mt j), a un nodo de probabilidad Aj 6. Para cada aj en OPT(mt j) hacer 7. Para cada oj en Oj (parte de mt j) hacer 8. Actualizar la creencia js, bt+1 j ← SE(bt j, aj, oj) 9. mt+1 j ← Nuevo I-ID (o ID) con bt+1 j como la creencia inicial 10. Rango(Mt+1 j,l−1) ∪ ← {mt+1 j } 11. Añadir el nodo del modelo, Mt+1 j,l−1, y los enlaces de dependencia entre Mt j,l−1 y Mt+1 j,l−1 (mostrados en la Fig. 3(b)) 12. Agregue los nodos de probabilidad, decisión y utilidad para la franja de tiempo t + 1 y los enlaces de dependencia entre ellos 13. Establezca las CPTs para cada nodo de probabilidad y nodo de utilidad de la Fase de Mirada Adelante 14. Aplica el método estándar de anticipación y respaldo para resolver la Figura 5 expandida de I-DID: Algoritmo para resolver un nivel l ≥ 0 I-DID. Nivel l I-DID expandido durante T pasos de tiempo con otro agente j en la Figura 5. Adoptamos un enfoque de dos fases: Dado un I-ID de nivel l (descrito previamente en la Sección 3) con todos los modelos de niveles inferiores representados también como I-IDs o IDs (si es nivel 0), el primer paso es expandir el I-ID de nivel l a lo largo de T pasos de tiempo añadiendo los enlaces de dependencia y las tablas de probabilidad condicional para cada nodo. Nos enfocamos especialmente en establecer y poblar los nodos del modelo (líneas 3-11). Ten en cuenta que Range(·) devuelve los valores (modelos de nivel inferior) de la variable aleatoria dada como entrada (nodo del modelo). En la segunda fase, utilizamos una técnica estándar de anticipación proyectando las secuencias de acción y observación durante T pasos de tiempo en el futuro, y respaldando los valores de utilidad de las creencias alcanzables. Similar a los I-IDs, los I-DIDs se reducen a DIDs en ausencia de otros agentes. Como mencionamos anteriormente, los modelos de nivel 0 son los DIDs tradicionales. Sus soluciones proporcionan distribuciones de probabilidad sobre las acciones del agente modelado en ese nivel a los I-DIDs en el nivel 1. Dadas las distribuciones de probabilidad sobre las acciones de otros agentes, los IDIDs de nivel 1 pueden resolverse como DIDs y proporcionar distribuciones de probabilidad a modelos de niveles superiores. Suponga que el número de modelos considerados en cada nivel está limitado por un número, M. Resolver un I-DID de nivel l es equivalente a resolver O(Ml) DIDs. Para ilustrar la utilidad de los I-DIDs, los aplicamos a tres dominios de problemas. Describimos, en particular, la formulación del I-DID y las prescripciones óptimas obtenidas al resolverlo. 5.1 Seguimiento-Liderazgo en el Problema del Tigre Multiagente Comenzamos nuestras ilustraciones del uso de I-IDs e I-DIDs con una versión ligeramente modificada del problema del tigre multiagente discutido en [9]. El problema tiene dos agentes, cada uno de los cuales puede abrir la puerta derecha (OR), la puerta izquierda (OL) o escuchar (L). Además de escuchar gruñidos (desde la izquierda (GL) o desde la derecha (GR)) cuando escuchan, los agentes también escuchan chirridos (desde la izquierda (CL), desde la derecha (CR) o sin chirridos (S)), que indican ruidosamente que los otros agentes están abriendo una de las puertas. Cuando se abre cualquier puerta, el tigre persiste en su ubicación original con una probabilidad del 95%. El agente i escucha gruñidos con una fiabilidad del 65% y crujidos con una fiabilidad del 95%. El agente J, por otro lado, escucha gruñidos con una fiabilidad del 95%. Por lo tanto, la configuración es tal que el agente i escucha la apertura de puertas del agente j de manera más confiable que los rugidos de los tigres. Esto sugiere que podría usar acciones de JavaScript como indicación de la ubicación del tigre, como discutimos a continuación. Las preferencias de cada agente son como en el juego de un solo agente discutido en [13]. Las funciones de transición, observación y recompensa se muestran en [16]. Un buen indicador de la utilidad de los métodos normativos para la toma de decisiones como los I-DIDs es la aparición de comportamientos sociales realistas en sus prescripciones. En entornos del persistente problema del tigre multiagente que reflejan situaciones del mundo real, demostramos la seguidismo entre los agentes y, como se muestra en [15], el engaño entre agentes que creen que están en una relación de tipo seguidor-líder. En particular, analizamos las condiciones situacionales y epistemológicas suficientes para su surgimiento. El comportamiento de seguidores, por ejemplo, resulta del agente conociendo sus propias debilidades, evaluando las fortalezas, preferencias y posibles comportamientos del otro, y dándose cuenta de que es mejor para él seguir las acciones de los demás para maximizar sus ganancias. Consideremos una configuración particular del problema del tigre en la que el agente i cree que las preferencias de j están alineadas con las suyas: ambos solo quieren obtener el oro, y la audición de j es más confiable en comparación consigo mismo. Como ejemplo, supongamos que j, al escuchar, puede discernir la ubicación del tigre el 95% de las veces en comparación con su precisión del 65%. Además, el agente i no tiene ninguna información inicial sobre la ubicación de los tigres. En otras palabras, la creencia anidada de un solo nivel, bi,1, asigna 0.5 a cada una de las dos ubicaciones del tigre. Además, considera dos modelos de j, que difieren en los niveles iniciales de creencias planas de j. Esto se representa en el nivel 1 I-ID mostrado en la Fig. 6(a). Según un modelo, j asigna una probabilidad de 0.9 a que el tigre esté detrás de la puerta izquierda, mientras que el otro 818 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 6: (a) Nivel 1 I-ID del agente i, (b) dos IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). El modelo asigna 0.1 a esa ubicación (ver Fig. 6(b)). El agente i está indeciso sobre estos dos modelos de j. Si variamos la capacidad auditiva, y resolvemos el nivel 1 correspondiente I-ID expandido en tres pasos de tiempo, obtenemos las políticas de comportamiento normativas mostradas en la Figura 7 que exhiben comportamiento de seguidores. Si la probabilidad de escuchar correctamente los gruñidos es de 0.65, entonces, como se muestra en la política en la Fig. 7(a), i comienza a seguir condicionalmente las acciones de j: i abre la misma puerta que j abrió anteriormente si su evaluación propia de la ubicación de los tigres confirma la elección de j. Si i pierde la capacidad de interpretar correctamente los gruñidos por completo, sigue ciegamente a j y abre la misma puerta que j abrió anteriormente (Fig. 7(b)). Figura 7: Emergencia de (a) seguidores condicionales, y (b) seguidores ciegos en el problema del tigre. Los comportamientos de interés están en negrita. * es un comodín y representa cualquiera de las observaciones. Observamos que un solo nivel de anidación de creencias, creencias sobre los modelos de los demás, fue suficiente para que surgiera el seguidismo en el problema del tigre. Sin embargo, los requisitos epistemológicos para el surgimiento del liderazgo son más complejos. Para que un agente, digamos j, surja como líder, primero debe surgir el seguidismo en el otro agente i. Como mencionamos anteriormente, si i está seguro de que sus preferencias son idénticas a las de j, y cree que j tiene un mejor sentido del oído, i seguirá las acciones de j con el tiempo. El agente j emerge como líder si cree que lo seguiré, lo que implica que la creencia de j debe estar anidada dos niveles de profundidad para permitirle reconocer su papel de liderazgo. Darse cuenta de que lo seguiré presenta a J una oportunidad para influir en sus acciones en beneficio del bien colectivo o de su interés propio solamente. Por ejemplo, en el problema del tigre, consideremos una situación en la que si tanto i como j abren la puerta correcta, entonces cada uno recibe un pago de 20 que es el doble del original. Si solo j selecciona la puerta correcta, obtiene la recompensa de 10. Por otro lado, si ambos agentes eligen la puerta incorrecta, sus penalizaciones se reducen a la mitad. En este entorno, es tanto en el mejor interés de j como en el beneficio colectivo que utilice su experiencia para seleccionar la puerta correcta, y así ser un buen líder. Sin embargo, considera un problema ligeramente diferente en el que j se beneficia de la pérdida de i y es penalizado si i se beneficia. Específicamente, dejemos que la ganancia se reste de js, indicando que j es antagonista hacia i: si j elige la puerta correcta y i la incorrecta, entonces la pérdida de 100 se convierte en la ganancia de js. El agente J cree que yo incorrectamente pienso que las preferencias de J son aquellas que promueven el bien colectivo y que comienza creyendo con un 99% de confianza dónde está el tigre. Dado que creo que mis preferencias son similares a las de j, y que j comienza creyendo casi con certeza que una de las dos ubicaciones es la correcta (dos modelos de nivel 0 de j), comenzaré siguiendo las acciones de j. Mostramos la política normativa para resolver su I-DID anidado individualmente durante tres pasos de tiempo en la Fig. 8(a). La política demuestra que seguiré ciegamente las acciones de js. Dado que el tigre persiste en su ubicación original con una probabilidad del 0.95, seleccionaré nuevamente la misma puerta. Si j comienza el juego con una probabilidad del 99% de que el tigre esté a la derecha, resolver su I-DID anidado dos niveles profundamente, resulta en la política mostrada en la Fig. 8(b). Aunque j está casi seguro de que OL es la acción correcta, comenzará seleccionando OR, seguido de OL. La intención del agente js es engañar a i, a quien cree que seguirá las acciones de js, para así obtener $110 en el segundo paso de tiempo, lo cual es más de lo que j ganaría si fuera honesto. Figura 8: Emergencia del engaño entre agentes en el problema del tigre. Los comportamientos de interés están en negrita. * denota como antes. (a) El agente es una política que demuestra que seguirá ciegamente las acciones de js. (b) Aunque j está casi seguro de que el tigre está a la derecha, comenzará seleccionando OR, seguido de OL, para engañar a i. 5.2 Altruismo y reciprocidad en el problema del bien público El problema del bien público (PG) [7], consiste en un grupo de M agentes, cada uno de los cuales debe contribuir con algún recurso a una olla pública o guardarlo para sí mismos. Dado que los recursos aportados al fondo público se comparten entre todos los agentes, son menos valiosos para el agente cuando están en el fondo público. Sin embargo, si todos los agentes eligen contribuir con sus recursos, entonces la recompensa para cada agente es mayor que si nadie contribuye. Dado que un agente recibe su parte del bote público independientemente de si ha contribuido o no, la acción dominante es que cada agente no contribuya y en su lugar se aproveche de las contribuciones de los demás. Sin embargo, los comportamientos de los jugadores humanos en simulaciones empíricas del problema de PG difieren de las predicciones normativas. Los experimentos revelan que muchos jugadores inicialmente contribuyen con una gran cantidad al bote público, y continúan contribuyendo cuando se juega el problema del PG repetidamente, aunque en cantidades decrecientes [4]. Muchos de estos experimentos [5] informan que un pequeño grupo central de jugadores contribuye persistentemente al bote público incluso cuando todos los demás están desertando. Estos experimentos también revelan que los jugadores que contribuyen persistentemente tienen preferencias altruistas o recíprocas que coinciden con la cooperación esperada de los demás. Para simplificar, asumimos que el juego se juega entre M = 2 agentes, i y j. Que cada agente esté inicialmente dotado con una cantidad de recursos XT. Mientras que la formulación clásica del juego PG permite que cada agente contribuya con cualquier cantidad de recursos (≤ XT) al bote público, simplificamos el espacio de acciones al permitir dos acciones posibles. Cada agente puede elegir contribuir (C) una cantidad fija de recursos, o no contribuir. La última acción es deThe Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 819 fue señalada como defectuosa (D). Suponemos que las acciones no son observables por otros. El valor de los recursos en la bolsa pública se descuenta por ci para cada agente i, donde ci es el retorno privado marginal. Suponemos que ci < 1 para que el agente no se beneficie lo suficiente como para contribuir al bote público en beneficio propio. Simultáneamente, ciM > 1, lo que hace que la contribución colectiva sea óptima de Pareto. i/j C D C 2ciXT , 2cjXT ciXT − cp, XT + cjXT − P D XT + ciXT − P, cjXT − cp XT , XT Tabla 1: El juego PG de un solo disparo con castigo. Para fomentar las contribuciones, los agentes contribuyentes castigan a los que no colaboran pero incurren en un pequeño costo por administrar el castigo. Sea P la sanción impuesta al agente que se desvía y cp el costo no nulo de castigar al agente contribuyente. Para simplificar, asumimos que el costo de castigar es el mismo para ambos agentes. El juego PG de un solo disparo con castigo se muestra en la Tabla 1. Si ci = cj, cp > 0, y si P > XT − ciXT, entonces la deserción ya no es una acción dominante. Si P < XT − ciXT, entonces la deserción es la acción dominante para ambos. Si P = XT − ciXT, entonces el juego no es resoluble por dominancia. Figura 9: (a) Nivel 1 I-ID del agente i, (b) IDs de nivel 0 del agente j con nodos de decisión mapeados a los nodos de probabilidad, A1 j y A2 j, en (a). Formulamos una versión secuencial del problema PG con castigo desde la perspectiva del agente i. Aunque en el juego de PG repetido, la cantidad en la olla pública se revela a todos los agentes después de cada ronda de acciones, asumimos en nuestra formulación que está oculta para los agentes. Cada agente puede contribuir con una cantidad fija, xc, o desertar. Un agente al realizar una acción recibe una observación de abundante (PY) o escasa (MR) simbolizando el estado de la olla pública. Ten en cuenta que las observaciones también son indirectamente indicativas de las acciones del agente js porque el estado del bote público es influenciado por ellas. La cantidad de recursos en el agente es una olla privada, es perfectamente observable para i. Los pagos son análogos a la Tabla 1. Tomando prestado de las investigaciones empíricas del problema PG [5], construimos IDs de nivel 0 para j que modelan tipos altruistas y no altruistas (Fig. 9(b)). Específicamente, nuestro agente altruista tiene un alto retorno privado marginal (cj está cerca de 1) y no castiga a los demás que incumplen. Que xc = 1 y el agente de nivel 0 sea castigado la mitad de las veces que se desvía. Con una acción restante, ambos tipos de agentes eligen contribuir para evitar ser castigados. Con dos acciones por delante, el tipo altruista elige contribuir, mientras que el otro defecta. Esto se debe a que cj para el tipo altruista es cercano a 1, por lo tanto, el castigo esperado, 0.5P > (1 − cj), que el tipo altruista evita. Debido a que cj para el tipo no altruista es menor, prefiere no contribuir. Con tres pasos por delante, el agente altruista contribuye para evitar el castigo (0.5P > 2(1 − cj)), mientras que el tipo no altruista se desentiende. Para más de tres pasos, mientras el agente altruista continúa contribuyendo al bote público dependiendo de qué tan cercano esté su retorno privado marginal a 1, el tipo no altruista prescribe la deserción. Analizamos las decisiones de un agente altruista i modelado usando un nivel 1 I-DID expandido en 3 pasos de tiempo. i atribuye los dos modelos de nivel 0, mencionados anteriormente, a j (ver Fig. 9). Si creo con una probabilidad de 1 que j es altruista, elijo contribuir en cada uno de los tres pasos. Este comportamiento persiste cuando i no está al tanto de si j es altruista (Fig. 10(a)), y cuando i asigna una alta probabilidad a que j sea del tipo no altruista. Sin embargo, cuando i cree con una probabilidad de 1 que j no es altruista y por lo tanto seguramente va a traicionar, i elige traicionar para evitar ser castigado y porque su beneficio privado marginal es menor que 1. Estos resultados demuestran que el comportamiento de nuestro tipo altruista se asemeja al encontrado experimentalmente. El agente de nivel 1 no altruista elige desertar independientemente de lo altruista que crea que sea el otro agente. Analizamos el comportamiento de un tipo de agente recíproco que se ajusta a la cooperación o defección esperada. El retorno privado marginal del tipo recíproco es similar al del tipo no altruista, sin embargo, obtiene una mayor recompensa cuando su acción es similar a la del otro. Consideramos el caso en el que el agente recíproco i no está seguro de si j es altruista y cree que es probable que el bote público esté medio lleno. Para esta creencia previa, yo elijo defectar. Al recibir una observación de abundancia, decide contribuir, mientras que una observación de escasez lo hace defectar (Fig. 10(b)). Esto se debe a que una observación de abundancia indica que es probable que la olla esté más llena que a la mitad, lo cual resulta de la acción de js para contribuir. Por lo tanto, entre los dos modelos atribuidos a j, es probable que su tipo sea altruista, lo que hace probable que j contribuya nuevamente en el próximo paso de tiempo. El agente i, por lo tanto, elige contribuir para corresponder la acción de js. Un razonamiento análogo lleva a uno a defectar cuando observa una olla escasa. Con una acción por hacer, creo que si j contribuye, también elegiré contribuir para evitar castigo independientemente de sus observaciones. Figura 10: (a) Un agente altruista de nivel 1 siempre contribuye. (b) Un agente recíproco i comienza defraudando y luego elige contribuir o defraudar basándose en su observación de abundancia (indicando que j es probablemente altruista) o escasez (j no es altruista). 5.3 Estrategias en el póker de dos jugadores El póker es un popular juego de cartas de suma cero que ha recibido mucha atención en la comunidad de investigación de IA como un banco de pruebas [2]. El póker se juega entre M ≥ 2 jugadores, en el que cada jugador recibe una mano de cartas de una baraja. Si bien existen varias variantes de póker con diferentes niveles de complejidad, consideramos una versión simple en la que cada jugador tiene tres turnos durante los cuales el jugador puede intercambiar una carta (E), mantener la mano existente (K), retirarse (F) y abandonar el juego, o apostar (C), lo que requiere que todos los jugadores muestren sus manos. Para mantener las cosas simples, dejemos que M = 2, y que cada jugador reciba una mano compuesta por una sola carta sacada del mismo palo. Por lo tanto, durante un enfrentamiento, el jugador que tenga la carta numéricamente más alta (el 2 es el más bajo, el as es el más alto) gana el bote. Durante un intercambio de cartas, la carta descartada se coloca ya sea en la pila L, indicando al otro agente que era una carta de número bajo menor que 8, o en la 820 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) H, indicando que la carta tenía un rango mayor o igual a 8. Ten en cuenta que, por ejemplo, si se descarta una carta de número bajo, la probabilidad de recibir una carta baja a cambio se reduce. Mostramos el nivel 1 I-ID para el Poker simplificado de dos jugadores en la Fig. 11. Consideramos dos modelos (tipos de personalidad) del agente j. El tipo conservador cree que es probable que su oponente tenga una carta de alto número en su mano. Por otro lado, el agente agresivo j cree con alta probabilidad que su oponente tiene una carta de número más bajo. Por lo tanto, los dos tipos difieren en sus creencias sobre la mano de sus oponentes. En ambos estos modelos de nivel 0, se asume que el oponente realiza sus acciones siguiendo una distribución fija y uniforme. Con tres acciones restantes, independientemente de su mano (a menos que sea un as), el agente agresivo elige intercambiar su carta, con la intención de mejorar su mano actual. Esto se debe a que cree que el otro tiene una carta baja, lo que mejora sus posibilidades de obtener una carta alta durante el intercambio. El agente conservador elige quedarse con su carta, sin importar su mano, porque considera que sus posibilidades de obtener una carta alta son escasas, ya que cree que su oponente tiene una. Figura 11: (a) Nivel 1 I-ID del agente i. La observación revela información sobre la mano de js del paso de tiempo anterior, (b) IDs de nivel 0 del agente j cuyos nodos de decisión están mapeados a los nodos de probabilidad, A1 j , A2 j , en (a). La política de un agente de nivel 1 i que cree que cada carta, excepto la suya, tiene la misma probabilidad de estar en su mano (tipo de personalidad neutral) y j podría ser de tipo agresivo o conservador, se muestra en la Figura 12. Su mano contiene la carta numerada 8. El agente comienza por mantener su carta. Al ver que j no intercambió una carta (N), i cree con probabilidad 1 que j es conservador y, por lo tanto, mantendrá sus cartas. i responde manteniendo su carta o intercambiándola porque j tiene igual probabilidad de tener una carta más baja o más alta. Si observo que j descartó su carta en la pila L o H, creo que j es agresivo. Al observar a L, i se da cuenta de que j tenía una carta baja, y es probable que tenga una carta alta después de su intercambio. Dado que la probabilidad de recibir una carta baja es alta en este momento, i decide quedarse con su carta. Al observar la carta H, creyendo que la probabilidad de recibir una carta de número alto es alta, i decide intercambiar su carta. En el paso final, i decide llamar independientemente de su historial de observaciones porque su creencia de que j tiene una carta más alta no es lo suficientemente alta como para concluir que es mejor retirarse y renunciar al pago. Esto se debe en parte al hecho de que una observación de, digamos, L, restablece las creencias del agente en el paso de tiempo anterior sobre las cartas de números bajos solamente. 6. DISCUSIÓN Mostramos cómo los DIDs pueden ser extendidos a los I-DIDs que permiten la toma de decisiones secuenciales en línea en entornos multiagentes inciertos. Nuestra representación gráfica de I-DIDs mejora la Figura 12 anterior: Un agente de nivel 1 es una política de tres pasos en el problema de Poker. i comienza creyendo que j tiene la misma probabilidad de ser agresivo o conservador y podría tener cualquier carta en su mano con igual probabilidad. Trabaja significativamente al ser más transparente, semánticamente claro y capaz de ser resuelto utilizando algoritmos estándar que apuntan a los DIDs. Los I-DIDs extienden los NIDs para permitir la toma de decisiones secuenciales a lo largo de múltiples pasos de tiempo en presencia de otros agentes que interactúan. Los I-DIDs pueden ser vistos como representaciones gráficas concisas para IPOMDPs que proporcionan una forma de explotar la estructura del problema y llevar a cabo la toma de decisiones en línea a medida que el agente actúa y observa dadas sus creencias previas. Actualmente estamos investigando formas de resolver I-DIDs aproximadamente con límites demostrables en la calidad de la solución. Agradecimiento: Agradecemos a Piotr Gmytrasiewicz por algunas discusiones útiles relacionadas con este trabajo. El primer autor desea agradecer el apoyo de una beca UGARF. 7. REFERENCIAS [1] R. J. Aumann. Epistemología interactiva i: Conocimiento. Revista Internacional de Teoría de Juegos, 28:263-300, 1999. [2] D. Billings, A. Davidson, J. Schaeffer y D. Szafron. El desafío del póker. AIJ, 2001. [3] A. Brandenburger y E. Dekel. Jerarquías de creencias y conocimiento común. Revista de Teoría Económica, 59:189-198, 1993. [4] C. Camerer. Teoría de juegos conductual: Experimentos en interacción estratégica. Princeton University Press, 2003. [5] E. Fehr y S. Gachter. Cooperación y castigo en experimentos de bienes públicos. Revista Económica Americana, 90(4):980-994, 2000. [6] D. Fudenberg y D. K. Levine. La Teoría del Aprendizaje en los Juegos. MIT Press, 1998. [7] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [8] Y. Gal y A. Pfeffer. Un lenguaje para modelar los procesos de toma de decisiones de agentes en juegos. En AAMAS, 2003. [9] P. Gmytrasiewicz y P. Doshi. Un marco para la planificación secuencial en entornos multiagentes. JAIR, 24:49-79, 2005. [10] P. Gmytrasiewicz y E. Durfee. Coordinación racional en entornos multiagente. JAAMAS, 3(4):319-350, 2000. [11] J. C. Harsanyi. Juegos con información incompleta jugados por jugadores bayesianos. Ciencia de la Gestión, 14(3):159-182, 1967. [12] R. A. Howard y J. E. Matheson. Diagramas de influencia. En R. A. Howard y J. E. Matheson, editores, Los Principios y Aplicaciones del Análisis de Decisiones. Grupo de Decisiones Estratégicas, Menlo Park, CA 94025, 1984. [13] L. Kaelbling, M. Littman y A. Cassandra. Planificación y actuación en dominios estocásticos parcialmente observables. Revista de Inteligencia Artificial, 2, 1998. [14] D. Koller y B. Milch. Diagramas de influencia multiagente para representar y resolver juegos. En IJCAI, páginas 1027-1034, 2001. [15] K. Polich y P. Gmytrasiewicz. Diagramas de influencia interactivos y dinámicos. En el taller GTDT, AAMAS, 2006. [16] B. Rathnas, P. Doshi y P. J. Gmytrasiewicz. Soluciones exactas para POMDP interactivos utilizando equivalencia conductual. En la Conferencia de Agentes Autónomos y Sistemas Multiagente (AAMAS), 2006. [17] S. Russell y P. Norvig. Inteligencia Artificial: Un Enfoque Moderno (Segunda Edición). Prentice Hall, 2003. [18] R. D. Shachter. 

Prentice Hall, 2003. [18] R. D. Shachter. Evaluando diagramas de influencia. Investigación de Operaciones, 34(6):871-882, 1986. [19] D. Suryadi y P. Gmytrasiewicz. Aprendiendo modelos de otros agentes utilizando diagramas de influencia. En UM, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 821