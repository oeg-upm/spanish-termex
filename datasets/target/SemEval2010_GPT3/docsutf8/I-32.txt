Un modelo de entorno adversarial para agentes racionales limitados en interacciones de suma cero Inon Zuckerman1, Sarit Kraus1, Jeffrey S. Rosenschein2, Gal Kaminka1 1 Departamento de Ciencias de la Computación 2 Escuela de Ingeniería Universidad Bar-Ilan y Ciencias de la Computación Ramat-Gan, Israel Universidad Hebrea, Jerusalén, Israel {zukermi, sarit, galk}@cs.biu.ac.il jeff@cs.huji.ac.il RESUMEN Los entornos multiagentes a menudo no son cooperativos ni colaborativos; en muchos casos, los agentes tienen intereses conflictivos, lo que lleva a interacciones adversariales. Este documento presenta un modelo formal de Entorno Adversarial para agentes racionales acotados que operan en un entorno de suma cero. En tales entornos, los intentos de utilizar métodos de búsqueda basados en utilidad clásica pueden plantear una variedad de dificultades (por ejemplo, modelar implícitamente al oponente como un maximizador de utilidad omnisciente, en lugar de aprovechar un modelo de oponente más matizado y explícito). Definimos un Entorno Adversario al describir los estados mentales de un agente en dicho entorno. Luego presentamos axiomas de comportamiento que están destinados a servir como principios de diseño para construir agentes adversarios. Exploramos la aplicación de nuestro enfoque mediante el análisis de archivos de registro de juegos de Conecta Cuatro completados, y presentamos un análisis empírico de la adecuación de los axiomas. Categorías y Descriptores de Asignaturas I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Agentes Inteligentes, Sistemas Multiagente; I.2.4 [Inteligencia Artificial]: Formalismos y Métodos de Representación del Conocimiento - Lógica Modal Términos Generales Diseño, Teoría 1. Introducción: En las primeras investigaciones en sistemas multiagente (SMA) se consideraban grupos cooperativos de agentes; debido a que los agentes individuales tenían recursos limitados o acceso limitado a la información (por ejemplo, potencia de procesamiento limitada, cobertura sensorial limitada), trabajaban juntos por diseño para resolver problemas que individualmente no podían resolver, o al menos no podían resolver de manera eficiente. Sin embargo, la investigación de MAS pronto comenzó a considerar agentes interactivos con intereses individualizados, como representantes de diferentes humanos u organizaciones con intereses no idénticos. Cuando las interacciones son guiadas por intereses diversos, los participantes pueden tener que superar desacuerdos, interacciones no cooperativas e incluso intentos intencionales de dañarse mutuamente. Cuando ocurren este tipo de interacciones, los entornos requieren un comportamiento apropiado por parte de los agentes situados en ellos. Llamamos a estos entornos Entornos Adversarios, y llamamos a los agentes en conflicto Adversarios. Los modelos de cooperación y trabajo en equipo han sido ampliamente explorados en MAS a través de la axiomatización de estados mentales (por ejemplo, [8, 4, 5]). Sin embargo, ninguna de estas investigaciones abordó los dominios adversarios y sus implicaciones para el comportamiento de los agentes. Nuestro artículo aborda este problema al proporcionar un modelo formal y axiomatizado de estados mentales para un subconjunto de dominios adversariales, específicamente entornos adversariales simples de suma cero. Los encuentros de suma cero simples existen, por supuesto, en varios juegos de dos jugadores (por ejemplo, Ajedrez, Damas), pero también existen en juegos de n jugadores (por ejemplo, Risk, Diplomacia), subastas de un solo bien y en otros lugares. En estos entornos recientes en particular, el uso de una búsqueda adversarial basada en utilidad (como el algoritmo Min-Max) no siempre proporciona una solución adecuada; la función de recompensa podría ser bastante compleja o difícil de cuantificar, y existen limitaciones computacionales naturales en agentes racionales acotados. Además, los métodos de búsqueda tradicionales (como Min-Max) no hacen uso de un modelo del oponente, lo cual ha demostrado ser una adición valiosa a la planificación adversarial [9, 3, 11]. En este artículo, desarrollamos un modelo formal y axiomatizado para agentes racionales limitados que se encuentran en un entorno adversarial de suma cero. El modelo utiliza diferentes operadores de modalidad, y sus principales fundamentos son el modelo SharedPlans [4] para el comportamiento colaborativo. Exploramos las propiedades del entorno y los estados mentales de los agentes para derivar axiomas de comportamiento; estos axiomas de comportamiento constituyen un modelo formal que sirve como especificación y guía de diseño para el diseño de agentes en tales entornos. Luego investigamos el comportamiento de nuestro modelo empíricamente utilizando el juego de mesa Conecta Cuatro. Mostramos que este juego se ajusta a nuestra definición de entorno, y analizamos el comportamiento de los jugadores utilizando un gran conjunto de registros de partidas completadas 550 978-81-904262-7-5 (RPS) c 2007 archivos de IFAAMAS. Además, utilizamos los resultados presentados en [9] para discutir la importancia del modelado del oponente en nuestro dominio adversarial de Conecta Cuatro. El artículo continúa de la siguiente manera. La sección 2 presenta la formalización de los modelos. La sección 3 presenta el análisis empírico y sus resultados. Discutimos el trabajo relacionado en la Sección 4, y concluimos y presentamos las direcciones futuras en la Sección 5. ENTORNOS ADVERSARIOS El modelo de entorno adversario (denominado como AE) está destinado a guiar el diseño de agentes al proporcionar una especificación de las capacidades y actitudes mentales de un agente en un entorno adversario. Nos enfocamos aquí en tipos específicos de entornos adversarios, especificados de la siguiente manera: 1. Interacciones de suma cero: las utilidades positivas y negativas de todos los agentes suman cero; 2. Agentes adversarios simples: todos los agentes en el entorno son agentes adversarios; 3. AEs bilaterales: AEs con exactamente dos agentes; 4. AEs multilaterales: AEs de tres o más agentes. Trabajaremos en instanciaciones tanto bilaterales como multilaterales de entornos de suma cero y simples. En particular, nuestro modelo de entorno adversarial tratará con interacciones que consisten en N agentes (N ≥ 2), donde todos los agentes son adversarios y solo un agente puede tener éxito. Ejemplos de tales entornos van desde juegos de mesa (por ejemplo, Ajedrez, Conecta Cuatro y Diplomacia) hasta ciertos entornos económicos (por ejemplo, subastas de N postores sobre un único bien). 2.1 Resumen del Modelo Nuestro enfoque es formalizar las actitudes mentales y comportamientos de un único agente adversario; consideramos cómo un único agente percibe el AE. La siguiente lista especifica las condiciones y estados mentales de un agente en un AE simple de suma cero: 1. El agente tiene la intención individual de que su propio objetivo se complete; 2. El agente tiene la creencia individual de que él y sus adversarios persiguen objetivos completamente opuestos (definidos a continuación) y que solo puede haber un ganador; 3. El agente tiene la creencia individual de que cada adversario tiene la intención de completar su propio objetivo conflictivo completo; 4. El agente tiene una creencia individual en el perfil (parcial) de sus adversarios. El ítem 3 es necesario, ya que podría ser el caso de que algún agente tenga un objetivo completamente conflictivo y esté considerando adoptar la intención de completarlo, pero aún no está comprometido a lograrlo. Esto podría ocurrir porque el agente aún no ha reflexionado sobre los efectos que adoptar esa intención podría tener en las otras intenciones que actualmente mantiene. En tales casos, es posible que ni siquiera se considere en un entorno adversarial. El ítem 4 establece que el agente debe tener alguna creencia sobre los perfiles de sus adversarios. El perfil representa todo el conocimiento que el agente tiene sobre su adversario: sus debilidades, capacidades estratégicas, objetivos, intenciones, confiabilidad y más. Puede ser dado explícitamente o puede ser aprendido a partir de observaciones de encuentros pasados. 2.2 Definiciones de modelos para estados mentales Utilizamos las definiciones de los operadores modales, predicados y metapredicados de Grosz y Krauss, tal como se definen en su formalización de SharedPlan [4]. Recordamos aquí algunos de los predicados y operadores que se utilizan en esa formalización: Int.To(Ai, α, Tn, Tα, C) representa las intenciones de Ai en el tiempo Tn de realizar una acción α en el tiempo Tα en el contexto de C. Int.Th(Ai, prop, Tn, Tprop, C) representa las intenciones de Ai en el tiempo Tn de que una cierta proposición prop se cumpla en el tiempo Tprop en el contexto de C. Los operadores de intención potencial, Pot.Int.To(...) y Pot.Int.Th(...), se utilizan para representar el estado mental cuando un agente considera adoptar una intención, pero no ha deliberado sobre la interacción de las otras intenciones que tiene. El operador Bel(Ai, f, Tf) representa al agente Ai creyendo en la afirmación expresada en la fórmula f, en el tiempo Tf. MB(A, f, Tf) representa la creencia mutua para un grupo de agentes A. Una instantánea del sistema encuentra que nuestro entorno se encuentra en algún estado e ∈ E de estados variables ambientales, y cada adversario en cualquier LAi ∈ L de posibles estados locales. En cualquier paso de tiempo dado, el sistema estará en algún mundo w del conjunto de todos los mundos posibles w ∈ W, donde w = E×LA1 ×LA2 × ...LAn, y n es el número de adversarios. Por ejemplo, en un juego de póker Texas Holdem, el estado local de un agente podría ser su propio conjunto de cartas (que es desconocido para su adversario) mientras que el entorno consistirá en el bote de apuestas y las cartas comunitarias (que son visibles para ambos jugadores). Una función de utilidad bajo esta formalización se define como un mapeo de un mundo posible w ∈ W a un elemento en , que expresa la deseabilidad del mundo, desde la perspectiva de un único agente. Normalmente normalizamos el rango a [0,1], donde 0 representa el mundo menos deseable posible, y 1 es el mundo más deseable. La implementación de la función de utilidad depende del dominio en cuestión. La siguiente lista especifica nuevos predicados, funciones, variables y constantes utilizadas en conjunto con las definiciones originales para la formalización del entorno adversarial: 1. φ es una acción nula (el agente no hace nada). 2. GAi es el conjunto de objetivos del agente Ai. Cada objetivo es un conjunto de predicados cuya satisfacción hace que el objetivo esté completo (usamos G∗ Ai ∈ GAi para representar un objetivo arbitrario del agente Ai). 3. gAi es el conjunto de subobjetivos del agente Ai. Los subobjetivos son predicados cuya satisfacción representa un hito importante hacia el logro del objetivo completo. gG∗ Ai ⊆ gAi es el conjunto de subobjetivos que son importantes para la finalización del objetivo G∗ Ai (usaremos g∗ G∗ Ai ∈ gG∗ Ai para representar un subobjetivo arbitrario). 4. P Aj Ai es el objeto de perfil que el agente Ai tiene sobre el agente Aj. 5. CA es un conjunto general de acciones para todos los agentes en A que se derivan de las restricciones del entorno. CAi ⊆ CA es el conjunto de acciones posibles del agente Ai. La expresión Do(Ai, α, Tα, w) se cumple cuando Ai realiza la acción α durante el intervalo de tiempo Tα en el mundo w. 7. La afirmación Achieve(G∗ Ai , α, w) es verdadera cuando se logra el objetivo G∗ Ai siguiendo la finalización de la acción α en el mundo w ∈ W, donde α ∈ CAi. El perfil (Ai, PAi Ai) es verdadero cuando el agente Ai tiene un perfil de objeto para el agente Aj. Definición 1. El conflicto total (FulConf) describe una interacción de suma cero donde solo se puede completar un solo objetivo de los objetivos en conflicto. FulConf(G∗ Ai , G∗ Aj ) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj ) (Achieve(G∗ Ai , α, w) ⇒ ¬Achieve(G∗ Aj , β, w)) ∨ (∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗ Aj , β, w) ⇒ ¬Achieve(G∗ Ai , α, w)) Definición 2. El Conocimiento Adversarial (AdvKnow) es una función que devuelve un valor que representa la cantidad de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 551 agente de conocimiento Ai tiene sobre el perfil del agente Aj, en el tiempo Tn. Cuanto mayor sea el valor, más conocimiento tiene el agente Ai. AdvKnow: P Aj Ai × Tn → Definición 3. Eval: Esta función de evaluación devuelve un valor de utilidad esperada estimado para un agente en A, después de completar una acción de CA en algún estado del mundo w. Eval: A × CA × w → Definición 4. TrH - (Umbral) es una constante numérica en el rango [0,1] que representa un valor umbral de la función de evaluación (Eval). Una acción que produce una evaluación de utilidad estimada por encima del TrH se considera una acción altamente beneficiosa. El valor Eval es una estimación y no la función de utilidad real, la cual suele ser desconocida. Utilizar el valor real de utilidad para un agente racional fácilmente produciría el mejor resultado para ese agente. Sin embargo, los agentes generalmente no tienen las funciones de utilidad reales, sino más bien una estimación heurística de las mismas. Existen dos propiedades importantes que deben cumplir para la función de evaluación: Propiedad 1. La función de evaluación debería indicar que el estado del mundo más deseable es aquel en el que se logra el objetivo. Por lo tanto, una vez que se ha satisfecho el objetivo, no puede haber ninguna acción futura que pueda colocar al agente en un estado del mundo con un valor de Eval más alto. (∀Ai, G∗ Ai , α, β ∈ CAi , w ∈ W) Lograr(G∗ Ai , α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w) Propiedad 2. La función de evaluación debe proyectar una acción que cause la finalización de un objetivo o subobjetivo a un valor que sea mayor que TrH (una acción altamente beneficiosa). (∀Ai, G∗ Ai ∈ GAi , α ∈ CAi , w ∈ W, g∗ GAi ∈ gGAi ) Lograr(G∗ Ai , α, w) ∨ Lograr(g∗ GAi , α, w) ⇒ Eval(Ai, α, w) ≥ TrH. Definición 5. La acción de conjunto Definimos una acción de conjunto (SetAction) como un conjunto de operaciones de acción (ya sean acciones complejas o básicas) de algunos conjuntos de acción CAi y CAj que, según la creencia del agente Ai, están unidas por una relación temporal y consecuente, formando una cadena de eventos (acción y su acción consecuente siguiente). (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , w ∈ W) SetAction(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((Do(Ai, α1 , Tα1 , w) ⇒ Do(Aj, β1 , Tβ1 , w)) ⇒ Do(Ai, α2 , Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu , Tαu , w)) La relación consecuente podría existir debido a varias restricciones ambientales (cuando una acción obliga al adversario a responder con una acción específica) o debido al conocimiento de los agentes sobre el perfil de su adversario. Propiedad 3. A medida que aumenta el conocimiento que tenemos sobre nuestro adversario, tendremos creencias adicionales sobre su comportamiento en diferentes situaciones, lo que a su vez crea un nuevo conjunto de acciones. Formalmente, si nuestro AdvKnow en el tiempo Tn+1 es mayor que AdvKnow en el tiempo Tn, entonces cada SetAction conocido en el tiempo Tn también es conocido en el tiempo Tn+1. AdvKnow(P Aj Ai , Tn+1) > AdvKnow(P Aj Ai , Tn) ⇒ (∀α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ⇒ Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn+1) 2.3 La formulación del entorno Los siguientes axiomas proporcionan la definición formal para un Entorno Adversarial (AE) simple y de suma cero. La satisfacción de estos axiomas significa que el agente está situado en un entorno así. Proporciona especificaciones para que el agente Aag interactúe con su conjunto de adversarios A con respecto a los objetivos G∗ Aag y G∗ A en el tiempo TCo en algún estado del mundo w. AE(Aag, A, G∗ Aag, A1, . . . , Ak, G∗ A1, . . . , G∗ Ak, Tn, w) 1. Aag tiene un Int.Th su objetivo se completaría: (∃α ∈ CAag , Tα) Int.Th(Aag, Lograr(G∗ Aag , α), Tn, Tα, AE) 2. Aag cree que él y cada uno de sus adversarios Ao están persiguiendo metas completamente conflictivas: (∀Ao ∈ {A1, . . . , Ak}) Bel(Aag, FulConf(G∗ Aag , G∗ Ao ), Tn) 3. Aag cree que cada uno de sus adversarios en Ao tiene la Int.Th su objetivo en conflicto G∗ Aoi se completará: (∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ) Bel(Aag, Int.Th(Ao, Lograr(G∗ Ao , β), TCo, Tβ, AE), Tn) 4. Para construir un agente que pueda operar con éxito dentro de un entorno de adversarios estratégicos, debemos especificar pautas de comportamiento para sus interacciones. Utilizar una estrategia de maximización de Eval ingenua hasta cierta profundidad de búsqueda no siempre dará resultados satisfactorios por varias razones: (1) el problema del horizonte de búsqueda al buscar a una profundidad fija; (2) la fuerte suposición de un adversario óptimamente racional con recursos ilimitados; (3) utilizar una función de evaluación estimada que no dará resultados óptimos en todos los estados del mundo, y que puede ser explotada [9]. Los siguientes axiomas especifican los principios de comportamiento que se pueden utilizar para diferenciar entre agentes exitosos y menos exitosos en el entorno adversarial mencionado anteriormente. Esos axiomas deben ser utilizados como principios de especificación al diseñar e implementar agentes que deben ser capaces de desempeñarse bien en entornos adversarios. Los axiomas conductuales representan situaciones en las que el agente adoptará intenciones potenciales para (Pot.Int.To(...)) realizar una acción, lo que típicamente requerirá un razonamiento de medios-fines para seleccionar un posible curso de acción. Este razonamiento llevará a la adopción de un Int.To(...) (ver [4]). A1. Axioma para lograr metas. El primer axioma es el caso más simple; cuando el agente Aag cree que está a una acción (α) de lograr su objetivo en conflicto G∗ Aag, debería adoptar la intención potencial de hacer α y completar su objetivo. (∀Aag, α ∈ CAag, Tn, Tα, w ∈ W) (Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗ Aag, α, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este comportamiento algo trivial es el primer y más fuerte axioma. En cualquier situación, cuando el agente está a una acción de completar el objetivo, debería completar la acción. Cualquier función de evaluación justa clasificaría naturalmente a α como la acción de valor máximo (propiedad 1). Sin embargo, sin una axiomatización explícita de dicho comportamiento, podría haber situaciones en las que el agente decida tomar otra acción por diversas razones, debido a sus recursos de decisión limitados. A2. Axioma del Acto Preventivo. En una situación adversarial, el agente Aag podría decidir tomar acciones que dañen uno de los planes de su adversario para completar su objetivo, incluso si esas acciones no avanzan explícitamente a Aag hacia su objetivo en conflicto G∗ Aag. Tal acción preventiva tendrá lugar cuando el agente Aag tenga la creencia sobre la posibilidad de que su adversario Ao realice una acción β que le otorgará un alto 552 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) valor de evaluación de utilidad (> TrH). Creer que tomar la acción α evitará que el oponente realice su β, adoptará una intención potencial de hacer α. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W) (Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Este axioma es un componente básico de cualquier entorno adversarial. Por ejemplo, al observar un juego de ajedrez, un jugador podría darse cuenta de que está a punto de ser jaque mateado por su oponente, por lo tanto, realiza una jugada preventiva. Otro ejemplo es el juego de Cuatro en Línea: cuando un jugador tiene una fila de tres fichas, su oponente debe bloquearla o perder. Una instancia específica de A1 ocurre cuando el adversario está a una acción de lograr su objetivo, y la acción preventiva inmediata debe ser tomada por el agente. Formalmente, tenemos las mismas creencias que se mencionaron anteriormente, con la creencia modificada de que realizar la acción β hará que el agente Ao logre su objetivo. Proposición 1: Caso de prevención o pérdida. (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗ Ao , Tn, Tα, Tβ, w ∈ W) Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗ Ao , β, w), Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w)) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Esquema de prueba: La Proposición 1 puede derivarse fácilmente del axioma A1 y de la propiedad 2 de la función Eval, que establece que cualquier acción que cause la finalización de un objetivo es una acción altamente beneficiosa. El comportamiento del acto preventivo ocurrirá implícitamente cuando la función Eval sea igual a la función de utilidad del mundo real. Sin embargo, al ser agentes racionales limitados y al tratar con una función de evaluación estimada, necesitamos axiomatizar explícitamente dicho comportamiento, ya que no siempre ocurrirá implícitamente a partir de la función de evaluación. A3. Axioma de Movimiento Táctico Subóptimo. En muchos escenarios, puede ocurrir una situación en la que un agente decida no tomar la acción más beneficiosa actual que puede tomar (la acción con el valor de evaluación de utilidad máximo), porque cree que tomar otra acción (con un valor de evaluación de utilidad más bajo) podría generar (dependiendo de la respuesta del adversario) una posibilidad futura para una acción altamente beneficiosa. Esto ocurrirá con mayor frecuencia cuando la función Eval sea inexacta y difiera en gran medida de la función de Utilidad. De manera formal, el agente Aag cree en una cierta SetAction que evolucionará de acuerdo con su acción inicial y producirá un valor beneficioso alto (> TrH) únicamente para él. (∀Aag, Ao ∈ A, Tn, w ∈ W) (∃α1 , . . . , αu ∈ CAi , β1 , . . . , βv ∈ CAj , Tα1 ) Bel(Aag, SetAction(α1 , . . . , αu , β1 , . . . , βv ), Tn) ∧ Bel(Aag, Eval(Ao, βv , w) < TrH < Eval(Aag, αu , w), Tn) ⇒ Pot.Int.To(Aag, α1 , Tn, Tα1 , w) Un agente podría creer que una cadena de eventos ocurrirá por diversas razones debido a la naturaleza inevitable del dominio. Por ejemplo, en el ajedrez, a menudo observamos lo siguiente: una jugada provoca una posición de jaque, lo que a su vez limita los movimientos del oponente para evitar el jaque, a lo que el primer jugador podría reaccionar con otro jaque, y así sucesivamente. El agente también podría creer en una cadena de eventos basada en su conocimiento del perfil de su adversario, lo que le permite prever los movimientos del adversario con alta precisión. A4. Axioma de Detección de Perfiles. El agente puede ajustar los perfiles de sus adversarios mediante observaciones y estudio de patrones (específicamente, si hay encuentros repetidos con el mismo adversario). Sin embargo, en lugar de esperar a que se revele la información del perfil, un agente también puede iniciar acciones que obligarán a su adversario a reaccionar de una manera que revele conocimiento del perfil sobre él. Formalmente, el axioma establece que si todas las acciones (γ) no son acciones altamente beneficiosas (< TrH), el agente puede realizar la acción α en el tiempo Tα si cree que resultará en una acción no altamente beneficiosa β por parte de su adversario, lo que a su vez le enseña sobre el perfil del adversario, es decir, proporciona un mayor AdvKnow(P Aj Ai , Tβ). (∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W) Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧ Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧ Bel(Aag, Eval(Ao, β, w) < TrH) ∧ Bel(Aag, AdvKnow(P Aj Ai , Tβ) > AdvKnow(P Aj Ai , Tn), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) Por ejemplo, volviendo al escenario del juego de ajedrez, consideremos comenzar una partida contra un oponente del que no sabemos nada, ni siquiera si es humano o un oponente computarizado. Podríamos comenzar a jugar una estrategia que sea adecuada frente a un oponente promedio, y ajustar nuestro juego de acuerdo a su nivel de juego. A5. Axioma de Formación de Alianzas El siguiente axioma de comportamiento es relevante únicamente en una instancia multilateral del entorno adversarial (obviamente, una alianza no puede formarse en un encuentro bilateral de suma cero). En diferentes situaciones durante una interacción multilateral, un grupo de agentes podría creer que es de su interés formar una alianza temporal. Una alianza de este tipo es un acuerdo que limita el comportamiento de sus miembros, pero se cree por parte de estos que les permite alcanzar un valor de utilidad mayor al que podrían lograr fuera de la alianza. Como ejemplo, podemos observar el clásico juego de mesa Risk, donde cada jugador tiene como objetivo individual ser el único conquistador del mundo, un juego de suma cero. Sin embargo, para lograr este objetivo, podría ser estratégicamente sabio hacer acuerdos de alto el fuego a corto plazo con otros jugadores, o unirse y atacar a un oponente que sea más fuerte que el resto. Los términos de una alianza definen la forma en que sus miembros deben actuar. Es un conjunto de predicados, denominados Términos, que es acordado por los miembros de la alianza y debe mantenerse verdadero durante la duración de la alianza. Por ejemplo, el conjunto de términos en el escenario de riesgo podría contener los siguientes predicados: 1. Los miembros de la alianza no se atacarán entre sí en los territorios X, Y y Z; 2. Los miembros de la alianza contribuirán C unidades por turno para atacar al adversario Ao; 3. Los miembros están obligados a permanecer como parte de la alianza hasta el tiempo Tk o hasta que el ejército adversario Ao sea más pequeño que Q. El conjunto de términos especifica restricciones intergrupales en cada uno de los miembros de la alianza (∀Aal i ∈ Aal ⊆ A) conjunto de acciones Cal i ⊆ C. Definición 6. El valor total de evaluación que el agente Ai logrará al ser parte de Aal es la suma de Evali (Eval para Ai) de cada uno de los valores de evaluación de Aal j después de tomar sus propias acciones α (a través del predicado del agente(α)): Al val(Ai, Cal, Aal, w) = α∈Cal Evali(Aal j, agente(α), w) Definición 7. Al TrH - es un número que representa un valor de Al en la Sexta Conferencia Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 553 umbral; por encima de él, se puede decir que la alianza es una alianza altamente beneficiosa. El valor de Al TrH se calculará dinámicamente de acuerdo con el progreso de la interacción, como se puede ver en [7]. Después de formarse una alianza, sus miembros ahora están trabajando en su entorno adversarial normal, así como de acuerdo con los estados mentales y axiomas requeridos para sus interacciones como parte de la alianza. El siguiente modelo de Alianza (AL) especifica las condiciones bajo las cuales el grupo Aal puede considerarse en una alianza y trabajando con un nuevo y limitado conjunto de acciones Cal, en el tiempo Tn. AL(Aal , Cal , w, Tn) 1. 

AL(Aal , Cal , w, Tn) 1. Aal tiene un MB en el que todos los miembros son parte de Aal: MB(Aal, (∀Aal i ∈ Aal)miembro(Aal i, Aal), Tn) 2. Aal tiene una MB que el grupo se mantenga: MB(Aal , (∀Aal i ∈ Aal )Int.Th (Ai, miembro(Ai, Aal ), Tn, Tn+1, Co), Tn) 3. Aal tiene un beneficio marginal que al ser miembros les otorga un alto valor de utilidad: MB(Aal , (∀Aal i ∈ Aal )Al val(Aal i , Cal , Aal , w) ≥ Al TrH, Tn) Los perfiles de los miembros son una parte crucial de las alianzas exitosas. Suponemos que los agentes que tienen perfiles más precisos de sus adversarios serán más exitosos en tales entornos. Tales agentes podrán predecir cuándo un miembro está a punto de violar el contrato de alianzas (elemento 2 en el modelo anterior) y tomar medidas correctivas (cuando el elemento 3 se falsificará). La robustez de la alianza depende en parte de la medida de confianza de sus miembros, la estimación de su posición objetiva y otras propiedades de su perfil. Debemos tener en cuenta que un agente puede formar parte simultáneamente de más de una alianza. Una alianza temporal, donde los miembros del grupo no tienen un objetivo común pero actúan colaborativamente en interés de sus propias metas individuales, es clasificada como un Grupo de Tratamiento por los psicólogos modernos (en contraste con un Grupo de Tarea, donde sus miembros tienen un objetivo común). El modelo de Actividad Compartida presentado en [5] modeló el comportamiento del Grupo de Tratamiento utilizando la misma formalización de Planes Compartidos. Al comparar ambas definiciones de una alianza y un Grupo de Tratamiento, encontramos una similitud sorprendente entre ambos modelos: las definiciones de los modelos de entorno son casi idénticas (ver las definiciones de SAs en [5]), y sus axiomas de Acto Egoísta y Acto Cooperativo se ajustan al comportamiento de nuestros agentes adversarios. La principal distinción entre ambos modelos es la integración de un axioma de acto de comportamiento útil, en la Actividad Compartida que no puede formar parte de la nuestra. Este axioma establece que un agente considerará tomar una acción que disminuirá su valor de Evaluación (hasta un cierto límite inferior), si cree que un compañero del grupo obtendrá un beneficio significativo. Dicho comportamiento no puede ocurrir en un entorno puramente adversarial (como un juego de suma cero), donde los miembros de la alianza están constantemente alerta para manipular su alianza en su propio beneficio. A6. Axioma de Maximización de la Evaluación. En un caso en el que todos los demás axiomas no sean aplicables, procederemos con la acción que maximice el valor heurístico calculado en la función Eval. (∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W) Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn) ⇒ Pot.Int.To(Aag, α, Tn, Tα, w) T1. Optimalidad en Eval = Utilidad El modelo axiomático anterior aborda situaciones en las que la Utilidad es desconocida y los agentes son agentes racionales limitados. El siguiente teorema muestra que en interacciones bilaterales, donde los agentes tienen la función de Utilidad real (es decir, Eval = Utilidad) y son agentes racionales, los axiomas proporcionan el mismo resultado óptimo que la búsqueda adversarial clásica (por ejemplo, Min-Max). Teorema 1. Que Ae ag sea un agente AE racional ilimitado que utiliza la función de evaluación heurística Eval, Au ag sea el mismo agente utilizando la verdadera función de utilidad y Ao sea un único adversario racional ilimitado basado en utilidad. Dado que Eval = Utilidad: (∀α ∈ CAu ag , α ∈ CAe ag , Tn, w ∈ W) Pot.Int.To(Au ag, α, Tn, Tα, w) → Pot.Int.To(Ae ag, α , Tn, Tα, w) ∧ ((α = α ) ∨ (Utilidad(Au ag, α, w) = Eval(Ae ag, α , w))) Esquema de prueba - Dado que Au ag tiene la función de utilidad real y recursos ilimitados, puede generar el árbol de juego completo y ejecutar el algoritmo MinMax óptimo para elegir la acción de mayor valor de utilidad, que denotamos como α. La prueba mostrará que Ae ag, utilizando los axiomas AE, seleccionará la misma utilidad α o igual (cuando hay más de una acción con la misma utilidad máxima) cuando Eval = Utilidad. (A1) Axioma de logro de objetivo - supongamos que hay un α tal que su cumplimiento logrará el objetivo Au ags. Obtendrá la utilidad más alta mediante Min-Max para Au ag. El agente Ae seleccionará α u otra acción con el mismo valor de utilidad a través de A1. Si tal α no existe, Ae ag no puede aplicar este axioma y procede a A2. (A2) Axioma de la acción preventiva - (1) Observando el caso base (ver Prop1), si hay un β que lleva a Ao a alcanzar su objetivo, entonces una acción preventiva α producirá la mayor utilidad para Au ag. Au ag lo elegirá a través de la utilidad, mientras que Ae ag lo elegirá a través de A2. En el caso general, β es una acción altamente beneficiosa para Ao, lo que resulta en una baja utilidad para Au ag, lo que lo guiará a seleccionar un α que evitará β, mientras que Ae ag lo elegirá a través de A2.1 Si dicho β no existe para Ao, entonces A2 no es aplicable, y Ae ag puede proceder a A3. (A3) Axioma de movimiento táctico subóptimo: al usar una función de evaluación heurística, Ae ag tiene una creencia parcial en el perfil de su adversario (elemento 4 en el modelo AE), lo que puede llevarlo a creer en SetActions (Prop1). En nuestro caso, Ae ag tiene un perfil completo de su adversario óptimo y sabe que Ao se comportará de manera óptima según los valores reales de utilidad en el árbol de búsqueda completo, por lo tanto, cualquier creencia sobre una SetAction subóptima no puede existir, lo que hace que este axioma sea inaplicable. Ae ag procederá a A4. (A4) Axioma de detección de perfil - Dado que Ae ag tiene el perfil completo de Ao, ninguna de las acciones de Ae ag puede aumentar su conocimiento. Ese axioma no se aplicará, y el agente procederá con A6 (A5 será ignorado porque la interacción es bilateral). (A6) Axioma de maximización de la evaluación: este axioma seleccionará la máxima Evaluación para Ae ag. Dado que Eval = Utilidad, se seleccionará el mismo α que fue seleccionado por Au ag. 3. EVALUACIÓN El propósito principal de nuestro análisis experimental es evaluar el comportamiento y rendimiento de los modelos en un entorno adversarial real. Esta sección investiga si un caso acotado 1 en el que, tras la finalización de β, exista un γ que proporcione una alta utilidad para el Agente Au ag, no puede ocurrir porque Ao utiliza la misma utilidad, y la existencia de γ hará que clasifique a β como una acción de baja utilidad. 554 The Sixth Intl. En la Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07), los agentes racionales situados en entornos adversariales se beneficiarán más al aplicar nuestros axiomas de comportamiento sugeridos. 3.1 El Dominio Para explorar el uso del modelo anterior y sus axiomas de comportamiento, decidimos utilizar el juego de Conecta Cuatro como nuestro entorno adversarial. Connect-Four es un juego de 2 jugadores, de suma cero, que se juega utilizando un tablero similar a una matriz de 6x7. En cada turno, un jugador coloca un disco en una de las 7 columnas (el conjunto de 21 discos suele ser de color amarillo para el jugador 1 y rojo para el jugador 2; usaremos Blanco y Negro respectivamente para evitar confusiones). El ganador es el primer jugador en completar un conjunto de cuatro discos de su color en horizontal, vertical o diagonal. En ocasiones muy raras, el juego podría terminar en empate si todos los cuadros vacíos están llenos, pero ningún jugador logró crear un conjunto de 4 discos. El juego de Conecta Cuatro fue resuelto en [1], donde se muestra que el primer jugador (jugando con los discos blancos) puede forzar una victoria comenzando en la columna del medio (columna 4) y jugando de manera óptima. Sin embargo, la estrategia óptima es muy compleja y difícil de seguir incluso para agentes racionales limitados complejos, como los jugadores humanos. Antes de que podamos proceder a verificar el comportamiento del agente, primero debemos verificar que el dominio se ajusta a la definición de entornos adversarios mencionada anteriormente (en la que se basan los axiomas de comportamiento). Primero, al jugar al juego de Conecta Cuatro, el agente tiene la intención de ganar el juego (ítem 1). Segundo (ítem 2), nuestro agente cree que en Conecta Cuatro solo puede haber un ganador (o ningún ganador en la rara ocasión de un empate). Además, nuestro agente cree que su oponente en el juego intentará ganar (ítem 3), y esperamos que tenga algún conocimiento parcial (ítem 4) sobre su adversario (este conocimiento puede variar desde nada, pasando por hechos simples como la edad, hasta estrategias y debilidades). Por supuesto, no todos los encuentros de Conecta Cuatro son adversariales. Por ejemplo, cuando un padre está jugando el juego con su hijo, la siguiente situación podría ocurrir: el niño, con un fuerte incentivo para ganar, trata el entorno como adversarial (intenta ganar, entiende que solo puede haber un ganador y cree que su padre está tratando de vencerlo). Sin embargo, desde el punto de vista de los padres, el entorno podría ser visto como educativo, donde el objetivo no es ganar el juego, sino causar disfrute o practicar el razonamiento estratégico. En un entorno educativo como este, un nuevo conjunto de axiomas de comportamiento podría ser más beneficioso para los objetivos de los padres que nuestros axiomas de comportamiento adversarial sugeridos. 3.2 Análisis de Axiomas Después de demostrar que el juego de Conecta Cuatro es de hecho un entorno adversarial bilateral de suma cero, el siguiente paso es observar el comportamiento de los jugadores durante el juego y verificar si comportarse de acuerdo con nuestro modelo mejora el rendimiento. Para hacerlo, hemos recopilado archivos de registro de juegos de Conecta Cuatro completados que fueron jugados por jugadores humanos a través de Internet. Nuestros datos de archivo de registro recopilados provienen de sitios de Juego por Correo Electrónico (PBeM). Estos son sitios web que alojan juegos por correo electrónico, donde cada movimiento se realiza a través de un intercambio de correos electrónicos entre el servidor y los jugadores. Muchos de estos sitios de archivos contienen interacciones competitivas reales y también mantienen un sistema de clasificación para sus miembros. La mayoría de los datos que utilizamos se pueden encontrar en [6]. Como se puede aprender de [1], Conecta Cuatro tiene una estrategia óptima y una ventaja considerable para el jugador que comienza el juego (a quien llamamos jugador Blanco). Nos concentraremos en nuestro análisis en los movimientos de los segundos jugadores (a los que llamaremos Negras). El jugador Blanco, al ser el primero en actuar, tiene la llamada ventaja de la iniciativa. Tener la ventaja y una buena estrategia mantendrá al jugador Negro ocupado reaccionando a sus movimientos, en lugar de iniciar amenazas. Una amenaza es una combinación de tres discos del mismo color, con un espacio vacío para el cuarto disco ganador. Una amenaza abierta es una amenaza que puede materializarse en el próximo movimiento del oponente. Para que el jugador Negro gane, debe de alguna manera cambiar el rumbo, tomar la ventaja y comenzar a presentar amenazas al jugador Blanco. Exploraremos el comportamiento de los jugadores negros y su conformidad con nuestros axiomas. Para hacerlo, construimos una aplicación que lee archivos de registro y analiza los movimientos de los jugadores Negros. La aplicación contiene dos componentes principales: (1) un algoritmo Min-Max para la evaluación de movimientos; (2) un detector de amenazas abiertas para descubrir amenazas abiertas. El algoritmo Min-Max funcionará hasta una profundidad d dada y, para cada movimiento α, devolverá el valor heurístico para la próxima acción tomada por el jugador según lo escrito en el archivo de registro, h(α), junto con el valor heurístico máximo, maxh(α), que podría lograrse antes de realizar el movimiento (obviamente, si h(α) = maxh(α), entonces el jugador no realizó el movimiento óptimo desde el punto de vista heurístico). El trabajo de los detectores de amenazas es notificar si se tomó alguna acción para bloquear una amenaza abierta (no bloquear una amenaza abierta probablemente hará que el jugador pierda en el próximo movimiento del oponente). La función heurística utilizada por Min-Max para evaluar la utilidad de los jugadores es la siguiente función, que es simple de calcular, pero proporciona un desafío razonable a los oponentes humanos: Definición 8. Sea Grupo un conjunto adyacente de cuatro cuadrados que son horizontales, verticales o diagonales. El Grupo n (Grupo w) es un Grupo con n piezas de color negro (blanco) y 4−n casillas vacías. h = ((Grupo1 negro ∗α)+(Grupo2 negro ∗β)+(Grupo3 negro ∗γ)+(Grupo4 negro ∗∞)) − ((Grupo1 blanco ∗α)+(Grupo2 blanco ∗β)+(Grupo3 blanco ∗γ)+(Grupo4 blanco ∗∞)) Los valores de α, β y δ pueden variar para formar cualquier combinación lineal deseada; sin embargo, es importante asignarles valores teniendo en cuenta el orden α < β < δ (usamos 1, 4 y 8 como sus valores respectivos). Grupos de 4 discos del mismo color significan victoria, por lo tanto, el descubrimiento de dicho grupo resultará en ∞ para asegurar un valor extremo. Ahora utilizamos nuestra función de evaluación estimada para evaluar las acciones de los jugadores Negros durante la interacción adversarial de Conecta Cuatro. Cada juego del archivo de registro fue introducido en la aplicación, la cual procesó y generó un archivo de registro reformateado que contenía el valor h del movimiento actual, el valor maxh que podría ser alcanzado y una notificación si se detectaba una amenaza abierta. Un total de 123 partidas fueron analizadas (57 con victoria de las blancas y 66 con victoria de las negras). Algunos juegos adicionales fueron ignorados manualmente en el experimento, debido a estos problemas: un jugador abandonando el juego mientras el resultado no es definitivo, o una jugada irracional y contundente en las primeras etapas del juego (por ejemplo, no bloquear un grupo obviamente ganador en las primeras jugadas de apertura). Además, también se eliminó un empate. El simulador se ejecutó con una profundidad de búsqueda de 3 movimientos. Ahora procedemos a analizar los juegos con respecto a cada axioma de comportamiento. El Sexto Internacional. La siguiente sección presenta las evaluaciones heurísticas del algoritmo Min-Max para cada acción, y verifica la cantidad y el alcance de las acciones tácticas subóptimas y sus implicaciones en el rendimiento. La Tabla 1 muestra los resultados y las percepciones del análisis heurístico de los juegos, cuando la profundidad de búsqueda es igual a 3 (esta profundidad de búsqueda fue seleccionada para que los resultados sean comparables a [9], ver Sección 3.2.3). La información heurística de las tablas es la diferencia entre el valor heurístico máximo presente y el valor heurístico de la acción que finalmente tomó el jugador (es decir, cuanto más cercano sea el número a 0, más cercana estuvo la acción a la acción heurística máxima). La primera fila presenta los valores de diferencia de la acción que tuvo el valor de diferencia máximo entre todas las acciones de los jugadores Negros en un juego dado, promediado en todos los juegos ganados y perdidos por los Negros (ver columnas respectivas). En los juegos en los que el jugador Negro pierde, su valor promedio de diferencia fue de -17.62, mientras que en los juegos en los que el jugador Negro ganó, su promedio fue de -12.02. La segunda fila amplía el análisis al considerar las 3 acciones con mayor diferencia heurística, y promediarlas. En ese caso, notamos una diferencia heurística promedio de 5 puntos entre los juegos en los que el jugador Negro pierde y los juegos en los que gana. Sin embargo, la importancia de esos números es que nos permitieron hacer una suposición educada sobre un número umbral de 11.5, como el valor de la constante TrH, que diferencia entre acciones normales y altamente beneficiosas. Después de encontrar una constante TrH aproximada, podemos proceder con un análisis de la importancia de los movimientos subóptimos. Para hacerlo, tomamos el subconjunto de juegos en los que el valor mínimo de diferencia heurística para las acciones de las negras era de 11.5. Como se presenta en la Tabla 2, podemos ver el promedio de 3 horas mínimo de los 3 mayores rangos y el porcentaje respectivo de juegos ganados. La primera fila muestra que el jugador Negro ganó solo el 12% de los juegos en los que el promedio de sus 3 acciones con mayor diferencia heurística (min3 h) era menor que el umbral sugerido, TrH = 11.5. La segunda fila muestra un resultado sorprendente: parece que cuando min3 h > −4, el jugador Negro rara vez gana. La intuición sugeriría que los juegos en los que los valores de evaluación de la acción estén más cerca de los valores máximos resultarán en más juegos ganados para las piezas negras. Sin embargo, parece que en el dominio de Conecta Cuatro, simplemente responder con acciones algo fácilmente esperadas, sin iniciar algunos movimientos sorprendentes y subóptimos, no produce buenos resultados. La última fila resume las principales conclusiones del análisis; la mayoría de las victorias de Blacks (83%) se produjeron cuando su min3 h estaba en el rango de -11.5 a -4. Una inspección detallada de esos juegos ganados por las fichas negras muestra el siguiente patrón detrás de los números: después de los movimientos de apertura estándar, las fichas negras de repente colocan una ficha en una columna aislada, lo cual parece ser un desperdicio de un movimiento. El blanco sigue construyendo sus amenazas, generalmente ignorando el último movimiento de las negras, que a su vez utiliza el disco aislado como ancla para una futura amenaza ganadora. Los resultados muestran que fue beneficioso para el jugador Negro en la Tabla 2: Porcentajes de ganancias de los Negros % de juegos min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% tomar acciones subóptimas y no dar el valor heurístico más alto posible en ese momento, pero no será demasiado perjudicial para su posición (es decir, no dará un alto valor beneficioso a su adversario). Resultó que aprender el umbral es un aspecto importante del éxito: realizar movimientos extremadamente arriesgados (min3 h < −11.5) o intentar evitarlos (min3 h > −4) reduce significativamente las posibilidades de ganar de los jugadores negros. 3.2.2 Afirmación del Axioma de Monitoreo de Perfiles En la tarea de demostrar la importancia de monitorear los perfiles de los adversarios, nuestros archivos de registro no pudieron ser utilizados porque no contenían interacciones repetidas entre jugadores, que son necesarias para inferir el conocimiento de los jugadores sobre sus adversarios. Sin embargo, la importancia del modelado del oponente y su uso para obtener ventajas tácticas ya ha sido estudiada en varios ámbitos ([3, 9] son buenos ejemplos). En un artículo reciente, Markovitch y Reger [9] exploraron la noción de aprendizaje y explotación de la debilidad del oponente en interacciones competitivas. Aplican estrategias de aprendizaje simples analizando ejemplos de interacciones pasadas en un dominio específico. También utilizaron el dominio adversarial de Conecta Cuatro, que ahora se puede utilizar para comprender la importancia de monitorear el perfil del adversario. Tras la presentación de su modelo teórico, describen un extenso estudio empírico y verifican el rendimiento de los agentes después de aprender el modelo de debilidades con ejemplos pasados. Uno de los dominios utilizados como entorno competitivo fue el mismo juego de Conecta Cuatro (las Damas fueron el segundo dominio). Su función heurística era idéntica a la nuestra con tres variaciones diferentes (H1, H2 y H3) que se distinguen entre sí por los valores de sus coeficientes de combinación lineal. La profundidad de búsqueda para los jugadores fue de 3 (como en nuestro análisis). Sus extensos experimentos verifican y comparan diversas estrategias de aprendizaje, factores de riesgo, conjuntos de características predefinidas y métodos de uso. La conclusión es que el dominio de Conecta Cuatro muestra una mejora de una tasa de victoria de 0.556 antes de modelar a 0.69 después de modelar (página 22). Sus conclusiones, que muestran un rendimiento mejorado al sostener y utilizar el modelo del adversario, justifican el esfuerzo de monitorear el perfil del adversario para interacciones continuas y repetidas. Un punto adicional que surgió en sus experimentos es el siguiente: después de que se haya aprendido el modelo de debilidad del oponente, los autores describen diferentes métodos para integrar el modelo de debilidad del oponente en la estrategia de decisión de los agentes. Sin embargo, independientemente del método específico que elijan para trabajar, todos los métodos de integración podrían hacer que el agente tome decisiones subóptimas; podría hacer que el agente prefiera acciones que son subóptimas en el momento actual de la decisión, pero que podrían hacer que el oponente reaccione de acuerdo con su modelo de debilidad (como lo representa nuestro agente), lo cual a su vez será beneficioso para nosotros en el futuro. El comportamiento de los agentes, como se demuestra en [9], confirma y refuerza aún más nuestro Axioma Táctico Subóptimo discutido en la sección anterior. 556 The Sixth Intl. La necesidad de los axiomas de Logro de Objetivos, Acto Preventivo y Maximización de la Evaluación es evidente y no requiere más verificación. Sin embargo, incluso con respecto a esos axiomas, surgieron algunas ideas interesantes en el análisis de registros. Los axiomas de Logro y Prevención de Actos, aunque teóricamente triviales, parecen presentar un desafío para un jugador humano. En la inspección inicial de los registros, nos encontramos con algunos juegos donde un jugador, por razones inexplicables, no bloqueó al otro para que ganara o no logró ejecutar su propia jugada ganadora. Podemos atribuir esas fallas a la falta de atención de los humanos, o a un error de escritura en su respuesta al movimiento; sin embargo, esos errores podrían ocurrir en agentes racionales limitados, y el comportamiento apropiado debe ser axiomatizado. Un juego típico de Cuatro en Línea gira en torno a generar amenazas y bloquearlas. En nuestro análisis buscamos acciones preventivas explícitas, es decir, movimientos que bloqueen un grupo de 3 discos o que eliminen una amenaza futura (en nuestro horizonte de búsqueda limitado). Encontramos que en el 83% de los juegos totales, hubo al menos una acción preventiva tomada por el jugador Negro. También se encontró que Black promedió 2.8 acciones preventivas por juego en los juegos en los que perdió, mientras que promedió 1.5 acciones preventivas por juego al ganar. Parece que Black necesita 1 o 2 acciones preventivas para construir su posición inicial de toma, antes de comenzar a presentar amenazas. Si no logra ganar, generalmente evitará una amenaza adicional o dos antes de sucumbir ante el Blanco. 4. TRABAJO RELACIONADO Mucha investigación se ocupa de la axiomatización del trabajo en equipo y los estados mentales de los individuos: algunos modelos utilizan el conocimiento y la creencia [10], otros tienen modelos de metas e intenciones [8, 4]. Sin embargo, todas estas teorías formales tratan sobre el trabajo en equipo y la cooperación de agentes. Hasta donde sabemos, nuestro modelo es el primero en proporcionar un modelo formalizado para entornos adversarios explícitos y el comportamiento de los agentes en él. El algoritmo de búsqueda adversarial clásico Min-Max fue el primer intento de integrar al oponente en el espacio de búsqueda con la suposición débil de un oponente que juega de manera óptima. Desde entonces, se ha dedicado mucho esfuerzo a integrar el modelo del oponente en el procedimiento de toma de decisiones para predecir el comportamiento futuro. El algoritmo M∗ presentado por Carmel y Markovitch [2] mostró un método para incorporar modelos de oponentes en la búsqueda de adversarios, mientras que en [3] utilizaron el aprendizaje para proporcionar un modelo de oponente más preciso en un entorno de juego repetido de 2 jugadores, donde las estrategias de los agentes se modelaron como autómatas finitos. El trabajo adicional de planificación adversarial fue realizado por Willmott y colaboradores [13], quienes proporcionaron un enfoque de planificación adversarial para el juego de GO. La investigación mencionada anteriormente trató sobre la búsqueda adversarial y la integración de modelos de oponentes en métodos de búsqueda de utilidad clásicos. Ese trabajo muestra la importancia del modelado del oponente y la capacidad de explotarlo en beneficio de un agente. Sin embargo, las limitaciones básicas de esos métodos de búsqueda siguen aplicándose; nuestro modelo intenta superar esas limitaciones presentando un modelo formal para una nueva especificación adversarial basada en estados mentales. 5. CONCLUSIONES Presentamos un modelo de Entorno Adversarial para un agente racional acotado que está situado en un entorno de N jugadores, de suma cero. Estos fueron posteriormente eliminados del análisis final. Utilizamos la formalización de SharedPlans para definir el modelo y los axiomas que los agentes pueden aplicar como pautas de comportamiento. El modelo está destinado a ser utilizado como una guía para diseñar agentes que necesitan operar en entornos adversos. Presentamos resultados empíricos, basados en el análisis del archivo de registro de ConnectFour, que ejemplifican el modelo y los axiomas para una instancia bilateral del entorno. Los resultados que presentamos son un primer paso hacia un modelo ampliado que abarcará todos los tipos de entornos adversarios, por ejemplo, entornos que no son de suma cero y entornos que contienen agentes naturales que no forman parte del conflicto directo. Esos desafíos y más serán abordados en futuras investigaciones. 6. AGRADECIMIENTO Esta investigación fue apoyada en parte por las becas de la Fundación de Ciencia de Israel #1211/04 y #898/05. 7. REFERENCIAS [1] L. V. Allis. Un enfoque basado en el conocimiento de Conecta Cuatro: el juego está resuelto, gana el jugador Blanco. Tesis de maestría, Universidad Libre de Ámsterdam, Países Bajos, 1988. [2] D. Carmel y S. Markovitch. Incorporando modelos de oponentes en la búsqueda de adversarios. En Actas de la Decimotercera Conferencia Nacional de Inteligencia Artificial, páginas 120-125, Portland, Oregón, 1996. [3] D. Carmel y S. Markovitch. Modelado de oponentes en sistemas multiagente. En G. Weiß y S. Sen, editores, Adaptación y Aprendizaje en Sistemas Multiagente, páginas 40-52. Springer-Verlag, 1996. [4] B. J. Grosz y S. Kraus. Planes colaborativos para acciones grupales complejas. Inteligencia Artificial, 86(2):269-357, 1996. [5] M. Hadad, G. Kaminka, G. Armon y S. Kraus. Apoyando la actividad colaborativa. En Proc. de AAAI-2005, páginas 83-88, Pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] S. Kraus y D. Lehmann. Diseñando y construyendo un agente automatizado de negociación. Inteligencia Computacional, 11:132-171, 1995. [8] H. J. Levesque, P. R. Cohen y J. H. T. Nunes. Actuar juntos. En Proc. de AAAI-90, páginas 94-99, Boston, MA, 1990. [9] S. Markovitch y R. Reger. Aprender y aprovechar las debilidades relativas de los agentes oponentes. Agentes Autónomos y Sistemas Multiagente, 10(2):103-130, 2005. [10] Y. M. Ronald Fagin, Joseph Y. Halpern y M. Y. Vardi. Razonamiento sobre el conocimiento. MIT Press, Cambridge, Mass., 1995. [11] P. Thagard.
MIT Press, Cambridge, Massachusetts, 1995. [11] P. Thagard. Resolución de problemas adversariales: Modelado de un oponente utilizando coherencia explicativa. Ciencia Cognitiva, 16(1):123-149, 1992. [12] R. W. Toseland y R. F. Rivas. Una introducción a la práctica del trabajo en grupo. Prentice Hall, Englewood Cliffs, NJ, segunda edición, 1995. [13] S. Willmott, J. Richardson, A. Bundy y J. Levine. Un enfoque de planificación adversarial para el juego de Go. Notas de conferencia en Ciencias de la Computación, 1558:93-112, 1999. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 557