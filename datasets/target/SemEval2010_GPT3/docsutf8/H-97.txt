Representación de características para la detección efectiva de elementos de acción. Paul N. Bennett Departamento de Ciencias de la Computación Universidad Carnegie Mellon Pittsburgh, PA 15213 pbennett+@cs.cmu.edu Jaime Carbonell Instituto de Tecnologías del Lenguaje. Los usuarios de correo electrónico enfrentan un desafío cada vez mayor en la gestión de sus bandejas de entrada debido a la creciente centralidad del correo electrónico en el lugar de trabajo para la asignación de tareas, solicitudes de acción y otros roles más allá de la difusión de información. Mientras que las técnicas de Recuperación de Información y Aprendizaje Automático están ganando aceptación inicial en la filtración de spam y la asignación automática de carpetas, este documento informa sobre una nueva tarea: la detección automatizada de elementos de acción, con el fin de marcar correos electrónicos que requieren respuestas y resaltar el pasaje o pasajes específicos que indican las solicitudes de acción. A diferencia de la clasificación de texto estándar basada en temas, la detección de elementos de acción requiere inferir la intención del remitente, y como tal responde menos bien a la clasificación pura de bolsa de palabras. Sin embargo, el uso de conjuntos de características enriquecidas, como los n-gramas (hasta n=4) con selección de características chi-cuadrado, y pistas contextuales para la ubicación de elementos de acción, mejora el rendimiento hasta un 10% en comparación con los unigramas, utilizando en ambos casos clasificadores de vanguardia como SVM con selección de modelo automatizada a través de validación cruzada incrustada. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información; I.2.6 [Inteligencia Artificial]: Aprendizaje; I.5.4 [Reconocimiento de Patrones]: Aplicaciones Términos Generales Experimentación 1. Los usuarios de correo electrónico se enfrentan a una tarea cada vez más difícil de gestionar sus bandejas de entrada ante los crecientes desafíos que resultan del aumento del uso del correo electrónico. Esto incluye priorizar correos electrónicos sobre una variedad de fuentes, desde socios comerciales hasta miembros de la familia, filtrar y reducir correos no deseados, y gestionar rápidamente las solicitudes que requieren De: Henry Hutchins <hhutchins@innovative.company.com> Para: Sara Smith; Joe Johnson; William Woolings Asunto: reunión con clientes potenciales Enviado: vie 12/10/2005 8:08 AM Hola a todos, Me gustaría recordarles que el grupo de GRTY nos visitará el próximo viernes a las 4:30 p.m. El horario actual se ve así: + 9:30 a.m. Desayuno informal y discusión en la cafetería a las 10:30 a.m. Visión general de la empresa a las 11:00 a.m. Reuniones individuales (continúan durante el almuerzo) + 2:00 p.m. Recorrido de las instalaciones + 3:00 p.m. Para que todo salga sin problemas, me gustaría practicar la presentación con anticipación. Como resultado, necesitaré cada una de sus partes para el miércoles. ¡Sigue con el buen trabajo! -Henry Figura 1: Un correo electrónico con un elemento de acción enfatizado, una solicitud explícita que requiere la atención o acción de los destinatarios. La detección automatizada de elementos de acción se enfoca en el tercero de estos problemas al intentar detectar qué correos electrónicos requieren una acción o respuesta con información, y dentro de esos correos electrónicos, intenta resaltar la oración (o longitud de otro pasaje) que indica directamente la solicitud de acción. Un sistema de detección como este puede ser utilizado como una parte de un agente de correo electrónico que ayudaría a un usuario a procesar correos electrónicos importantes más rápido de lo que hubiera sido posible sin el agente. Consideramos la detección de elementos de acción como un componente necesario de un agente de correo electrónico exitoso que realizaría detección de spam, detección de elementos de acción, clasificación de temas y ranking de prioridades, entre otras funciones. La utilidad de un detector de este tipo puede manifestarse como un método para priorizar correos electrónicos según criterios orientados a la tarea, distintos de los estándares de tema y remitente, o como un medio para asegurar que el usuario de correo electrónico no haya dejado caer el balón proverbial al olvidar abordar una solicitud de acción. La detección de elementos de acción difiere de la clasificación de texto estándar en dos formas importantes. Primero, al usuario le interesa tanto detectar si un correo electrónico contiene elementos de acción como ubicar exactamente dónde se encuentran estas solicitudes de elementos de acción dentro del cuerpo del correo electrónico. Por el contrario, la categorización de texto estándar simplemente asigna una etiqueta de tema a cada texto, ya sea que esa etiqueta corresponda a una carpeta de correo electrónico o a un vocabulario de indexación controlado [12, 15, 22]. Segundo, la detección de elementos de acción intenta recuperar la intención del remitente del correo electrónico, ya sea que pretenda provocar una respuesta o una acción por parte del receptor; cabe destacar que para esta tarea, los clasificadores que utilizan solo unigramas como características no funcionan de manera óptima, como se evidencia en nuestros resultados a continuación. En cambio, descubrimos que necesitamos características con más información, como los n-gramos de orden superior. La categorización de texto por tema, por otro lado, funciona muy bien utilizando solo palabras individuales como características [2, 9, 13, 17]. De hecho, la clasificación de género, que uno pensaría que podría requerir más que un enfoque de bolsa de palabras, también funciona bastante bien utilizando solo características unigramas [14]. La detección y seguimiento de temas (TDT) también funciona bien con conjuntos de características de unigrama [1, 20]. Creemos que la detección de elementos de acción es uno de los primeros casos claros de una tarea relacionada con la recuperación de información en la que debemos ir más allá del modelo de bolsa de palabras para lograr un alto rendimiento, aunque no demasiado lejos, ya que los modelos de bolsa de n-gramos parecen ser suficientes. Primero revisamos trabajos relacionados para problemas de clasificación de texto similares, como la clasificación de prioridad de correos electrónicos y la identificación de actos de habla. Luego definimos de manera más formal el problema de detección de acciones, discutimos los aspectos que lo distinguen de problemas más comunes como la clasificación de temas, y destacamos los desafíos en la construcción de sistemas que puedan desempeñarse bien a nivel de oración y documento. A partir de ahí, pasamos a una discusión sobre las técnicas de representación y selección de características apropiadas para este problema y cómo los enfoques estándar de clasificación de texto pueden adaptarse para pasar sin problemas del problema de detección a nivel de oración al problema de clasificación a nivel de documento. Luego realizamos un análisis empírico que nos ayuda a determinar la efectividad de nuestros procedimientos de extracción de características, así como establecer líneas base para varios algoritmos de clasificación en esta tarea. Finalmente, resumimos las contribuciones de este artículo y consideramos direcciones interesantes para trabajos futuros. TRABAJO RELACIONADO Varios otros investigadores han considerado tareas de clasificación de texto muy similares. Cohen et al. [5] describen una ontología de actos de habla, como Proponer una Reunión, e intentan predecir cuándo un correo electrónico contiene uno de estos actos de habla. Consideramos que los elementos de acción son un tipo específico importante de acto de habla que se encuentra dentro de su clasificación más general. Si bien proporcionan resultados para varios métodos de clasificación, sus métodos solo hacen uso de juicios humanos a nivel de documento. Por el contrario, consideramos si la precisión puede aumentarse al utilizar juicios humanos más detallados que marquen las oraciones y frases específicas de interés. Corston-Oliver et al. [6] consideran detectar elementos en correos electrónicos para poner en una lista de tareas pendientes. Esta tarea de clasificación es muy similar a la nuestra, excepto que ellos no consideran que las preguntas simples de hecho pertenezcan a esta categoría. Incluimos preguntas, pero ten en cuenta que no todas las preguntas son tareas a realizar, algunas son retóricas o simplemente convenciones sociales, ¿Cómo estás? Desde una perspectiva de aprendizaje, aunque hacen uso de juicios a nivel de oración, no comparan explícitamente qué beneficios, si los hay, ofrecen los juicios más detallados. Además, no estudian opciones o enfoques alternativos para la tarea de clasificación. En cambio, simplemente aplican un SVM estándar a nivel de oración y se centran principalmente en un análisis lingüístico de cómo la oración puede ser reformulada lógicamente antes de agregarla a la lista de tareas. En este estudio, examinamos varios métodos de clasificación alternativos, comparamos enfoques a nivel de documento y a nivel de oración, y analizamos los problemas de aprendizaje automático implícitos en estos problemas. El interés en una variedad de tareas de aprendizaje relacionadas con el correo electrónico ha estado creciendo rápidamente en la literatura reciente. Por ejemplo, en un foro dedicado a tareas de aprendizaje por correo electrónico, Culotta et al. [7] presentaron métodos para aprender redes sociales a partir de correos electrónicos. En este trabajo, no nos enfocamos en las relaciones entre pares; sin embargo, tales métodos podrían complementar los presentes ya que las relaciones entre pares a menudo influyen en la elección de palabras al solicitar una acción. 3. DEFINICIÓN DEL PROBLEMA Y ENFOQUE A diferencia de trabajos anteriores, nos enfocamos explícitamente en los beneficios que ofrecen los juicios humanos a nivel de oración, más detallados y costosos, sobre los juicios a nivel de documento de grano grueso. Además, consideramos múltiples enfoques estándar de clasificación de texto y analizamos tanto las diferencias cuantitativas como cualitativas que surgen al tomar un enfoque a nivel de documento frente a un enfoque a nivel de oración para la clasificación. Finalmente, nos enfocamos en la representación necesaria para lograr el rendimiento más competitivo. 3.1 Definición del problema Para proporcionar el mayor beneficio al usuario, un sistema no solo detectaría el documento, sino que también indicaría las oraciones específicas en el correo electrónico que contienen las tareas pendientes. Por lo tanto, hay tres problemas básicos: 1. Detección de documentos: Clasificar un documento según si contiene o no un ítem de acción. 2. Clasificación de documentos: Clasifique los documentos de manera que todos los documentos que contengan elementos de acción aparezcan lo más alto posible en la clasificación. 3. Detectar oraciones: Clasificar cada oración en un documento como un ítem de acción o no. Como en la mayoría de las tareas de Recuperación de Información, el peso que la métrica de evaluación debe dar a la precisión y la exhaustividad depende de la naturaleza de la aplicación. En situaciones donde un usuario eventualmente leerá todos los mensajes recibidos, la clasificación (por ejemplo, a través de la precisión en la recuperación de 1) puede ser lo más importante, ya que esto ayudará a fomentar retrasos más cortos en las comunicaciones entre usuarios. Por el contrario, la detección de alta precisión con un bajo recuerdo será de importancia creciente cuando el usuario esté bajo una fuerte presión de tiempo y, por lo tanto, probablemente no leerá todo el correo. Esto puede ser el caso para los gestores de crisis durante la gestión de desastres. Finalmente, la detección de oraciones juega un papel tanto en situaciones de presión temporal como simplemente para aliviar el tiempo requerido por los usuarios para captar el mensaje. 3.2 Enfoque Como se mencionó anteriormente, los datos etiquetados pueden presentarse en una de dos formas: un etiquetado de documentos proporciona una etiqueta de sí/no para cada documento en cuanto a si contiene un elemento de acción; un etiquetado de frases proporciona solo una etiqueta de sí para los elementos específicos de interés. Llamamos a las valoraciones humanas "etiquetado de frases" ya que la percepción de los usuarios sobre el elemento de acción puede no corresponder con los límites reales de las oraciones o los límites de oraciones predichos. Obviamente, es sencillo generar una etiqueta de documento coherente con una etiqueta de frase al etiquetar un documento como sí solo si contiene al menos una frase etiquetada como sí. Para entrenar clasificadores para esta tarea, podemos tomar varios puntos de vista relacionados tanto con los problemas básicos que hemos enumerado como con la forma de los datos etiquetados. La vista a nivel de documento trata cada correo electrónico como una instancia de aprendizaje con una etiqueta de clase asociada. Entonces, el documento puede ser convertido en un vector de características y valores, y el aprendizaje progresa como de costumbre. Aplicar un clasificador a nivel de documento para la detección y clasificación de documentos es sencillo. Para aplicarlo a la detección de oraciones, se deben seguir pasos adicionales. Por ejemplo, si el clasificador predice que un documento contiene un ítem de acción, entonces se pueden indicar las áreas del documento que contienen una alta concentración de palabras que el modelo pondera fuertemente a favor de los ítems de acción. El beneficio obvio del enfoque a nivel de documento es que los costos de recopilación del conjunto de entrenamiento son más bajos, ya que el usuario solo tiene que especificar si un correo electrónico contiene o no un elemento de acción y no las frases específicas. En la vista a nivel de oración, cada correo electrónico se segmenta automáticamente en oraciones, y cada oración se trata como una instancia de aprendizaje con una etiqueta de clase asociada. Dado que la etiquetación de frases proporcionada por el usuario puede no coincidir con la segmentación automática, debemos determinar qué etiqueta asignar a una oración parcialmente superpuesta al convertirla en una instancia de aprendizaje. Una vez entrenados, aplicar los clasificadores resultantes a la detección de oraciones es ahora sencillo, pero para aplicar los clasificadores a la detección de documentos y la clasificación de documentos, las predicciones individuales sobre cada oración deben ser agregadas para hacer una predicción a nivel de documento. Este enfoque tiene el potencial de beneficiarse de etiquetas más específicas que permitan al aprendiz centrar la atención en las oraciones clave en lugar de tener que aprender basándose en datos que la mayoría de las palabras en el correo electrónico no proporcionan o proporcionan poca información sobre la pertenencia a una clase. 3.2.1 Características Considera algunas de las frases que podrían constituir parte de un elemento de acción: me gustaría saber, házmelo saber, lo antes posible, ¿tienes? Cada una de estas frases consiste en palabras comunes que aparecen en muchos correos electrónicos. Sin embargo, cuando ocurren en la misma oración, son mucho más indicativos de una tarea a realizar. Además, el orden puede ser importante: considera tienes tú versus tú tienes. Debido a esto, postulamos que los n-gramos juegan un papel más importante en este problema de lo que es típico en problemas como la clasificación de temas. Por lo tanto, consideramos todos los n-gramos de tamaño hasta 4. Al utilizar n-gramas, si encontramos un n-grama de tamaño 4 en un segmento de texto, podemos representar el texto como solo una ocurrencia del n-grama o como una ocurrencia del n-grama y una ocurrencia de cada n-grama más pequeño contenido en él. Elegimos la segunda de estas alternativas ya que esto permitirá que el algoritmo mismo retroceda suavemente en términos de recuperación. Métodos como el Bayes ingenuo pueden resultar perjudicados por esta representación debido al doble conteo. Dado que la puntuación al final de una oración puede proporcionar información, conservamos el token de puntuación de terminación cuando es identificable. Además, agregamos un token de inicio de oración y un token de fin de oración para capturar patrones que a menudo son indicadores al principio o al final de una oración. Suponiendo una puntuación adecuada, estos tokens adicionales son innecesarios, pero a menudo los correos electrónicos carecen de una puntuación adecuada. Además, para los clasificadores a nivel de oración que utilizan ngramas, codificamos adicionalmente para cada oración una codificación binaria de la posición de la oración en relación al documento. Este codificación tiene ocho características asociadas que representan en qué octil (el primer octavo, segundo octavo, etc.) se encuentra la oración. 3.2.2 Detalles de Implementación Para comparar el enfoque a nivel de documento con el enfoque a nivel de oración, comparamos las predicciones a nivel de documento. No abordamos cómo usar un clasificador a nivel de documento para hacer predicciones a nivel de oración. Para segmentar automáticamente el texto del correo electrónico, utilizamos el analizador estadístico RASP [4]. Dado que las oraciones segmentadas automáticamente pueden no corresponder directamente con los límites a nivel de frase, tratamos cualquier oración que contenga al menos el 30% de un segmento de elemento de acción marcado como un elemento de acción. Al evaluar la detección de oraciones para el sistema a nivel de oración, utilizamos estas etiquetas de clase como verdad absoluta. Dado que no estamos evaluando múltiples enfoques de segmentación, esto no sesga ninguno de los métodos. Si se estuvieran evaluando varios sistemas de segmentación, se necesitaría utilizar una métrica que emparejara las oraciones positivas predichas con las frases etiquetadas como positivas. La métrica tendría que penalizar tanto las predicciones verdaderas excesivamente largas como las predicciones demasiado cortas. Nuestro criterio para convertir a instancias etiquetadas incluye implícitamente ambos criterios. Dado que la segmentación está fija, una predicción excesivamente larga estaría prediciendo sí para muchas instancias negativas, ya que presumiblemente la longitud adicional corresponde a oraciones segmentadas adicionales que no contienen el 30% de elementos de acción. Asimismo, una predicción demasiado corta debe corresponder a una pequeña oración incluida en la tarea de acción pero no constituir toda la tarea de acción. Por lo tanto, para considerar que la predicción es demasiado corta, habrá una oración adicional anterior/posterior que sea una tarea de acción donde incorrectamente predijimos que no. Una vez que un clasificador a nivel de oración ha hecho una predicción para cada oración, debemos combinar estas predicciones para hacer tanto una predicción a nivel de documento como un puntaje a nivel de documento. Utilizamos la política simple de predecir positivo cuando cualquiera de las oraciones se predice positiva. Para producir una puntuación de documento para clasificación, la confianza de que el documento contiene un ítem de acción es: ψ(d) = 1 n(d) s∈d|π(s)=1 ψ(s) si para cualquier s ∈ d, π(s) = 1 1 n(d) maxs∈d ψ(s) o.w. donde s es una oración en el documento d, π es la predicción de los clasificadores 1/0, ψ es la puntuación que el clasificador asigna como su confianza de que π(s) = 1, y n(d) es el mayor de 1 y el número de tokens (unigramas) en el documento. En otras palabras, cuando se predice que una oración es positiva, la puntuación del documento es la suma normalizada por longitud de las puntuaciones de las oraciones por encima del umbral. Cuando ninguna oración se predice como positiva, la puntuación del documento es la puntuación máxima de la oración normalizada por la longitud. Como en otros problemas de texto, es más probable que emitamos falsos positivos para documentos con más palabras u oraciones. Así que incluimos un factor de normalización de longitud. 4. ANÁLISIS EXPERIMENTAL 4.1 Los Datos Nuestro corpus consiste en correos electrónicos obtenidos de voluntarios de una institución educativa y abarcan temas como: organizar un taller de investigación, coordinar entrevistas con candidatos a puestos de trabajo, publicar actas y anuncios de charlas. Los mensajes fueron anonimizados reemplazando los nombres de cada individuo e institución con un seudónimo. Después de intentar identificar y eliminar correos electrónicos duplicados, el corpus contiene 744 mensajes de correo electrónico. Después de la anonimización de la identidad, el corpus tiene tres versiones básicas. El material citado se refiere al texto de un correo electrónico anterior que un autor suele dejar en un mensaje de correo electrónico al responder al mismo. El material citado puede actuar como ruido al aprender, ya que puede incluir elementos de acción de mensajes anteriores que ya no son relevantes. Para aislar los efectos del material citado, tenemos tres versiones del corpus. La forma cruda contiene los mensajes básicos. La versión auto-eliminada contiene los mensajes después de que el material citado ha sido eliminado automáticamente. La versión editada a mano contiene los mensajes después de que el material citado ha sido eliminado por un humano. Además, la versión despojada a mano ha eliminado cualquier contenido xml y firmas de correo electrónico, dejando solo el contenido esencial del mensaje. Los estudios reportados aquí se realizaron con la versión despojada a mano. Esto nos permite equilibrar la carga cognitiva en términos del número de tokens que deben ser leídos en los estudios de usuario que reportamos, ya que incluir material citado complicaría los estudios de usuario, dado que algunos usuarios podrían saltarse el material mientras que otros lo leen. Además, asegurando que todo el material citado sea eliminado, tenemos una versión aún más altamente anonimizada del corpus que puede estar disponible para experimentación externa. Por favor, contacta a los autores para obtener más información sobre la obtención de estos datos. Esto evita contaminar la validación cruzada, ya que de lo contrario, un elemento de prueba podría aparecer como material citado en un documento de entrenamiento. 4.1.1 Etiquetado de Datos Dos anotadores humanos etiquetaron cada mensaje según si contenía o no un elemento de acción. Además, identificaron cada segmento del correo electrónico que contenía una tarea pendiente. Un segmento es una sección contigua de texto seleccionada por los anotadores humanos y puede abarcar varias oraciones o una frase completa contenida en una oración. Se les indicó que un elemento de acción es una solicitud explícita de información que requiere la atención de los destinatarios o una acción requerida, y se les dijo que resaltaran las frases o oraciones que conforman la solicitud. El Anotador 1 No Sí Anotador 2 No 391 26 Sí 29 298 Tabla 1: Acuerdo de Anotadores Humanos a Nivel de Documento El Anotador Uno etiquetó 324 mensajes como conteniendo elementos de acción. El Anotador Dos etiquetó 327 mensajes como conteniendo elementos de acción. El acuerdo de los anotadores humanos se muestra en las Tablas 1 y 2. Se dice que los anotadores están de acuerdo a nivel de documento cuando ambos marcaron el mismo documento como no conteniendo elementos de acción o ambos marcaron al menos un elemento de acción, independientemente de si los segmentos de texto eran los mismos. A nivel de documento, los anotadores estuvieron de acuerdo el 93% del tiempo. El estadístico kappa [3, 5] se utiliza frecuentemente para evaluar la concordancia entre anotadores: κ = A − R 1 − R, donde A es la estimación empírica de la probabilidad de acuerdo. R es la estimación empírica de la probabilidad de acuerdo aleatorio dada las probabilidades empíricas de clase. Un valor cercano a −1 implica que los anotadores están de acuerdo mucho menos a menudo de lo que se esperaría al azar, mientras que un valor cercano a 1 significa que están de acuerdo más a menudo de lo que se esperaría al azar. A nivel de documento, la estadística kappa para la concordancia entre anotadores es de 0.85. Este valor es lo suficientemente fuerte como para esperar que el problema sea aprendible y es comparable con los resultados de tareas similares [5, 6]. Para determinar el acuerdo a nivel de oración, utilizamos cada juicio para crear un corpus de oraciones con etiquetas según se describe en la Sección 3.2.2, luego consideramos el acuerdo sobre estas oraciones. Esto nos permite comparar el acuerdo sin juicios. Realizamos esta comparación sobre el corpus despojado a mano ya que elimina juicios no válidos que podrían surgir al incluir material citado, etc. Ambos anotadores tenían libertad para etiquetar el sujeto como una tarea de acción, pero como ninguno lo hizo, también omitimos la línea del sujeto del mensaje. Esto solo reduce la cantidad de desacuerdos. Esto deja 6301 oraciones segmentadas automáticamente. A nivel de oración, los anotadores estuvieron de acuerdo el 98% del tiempo, y la estadística kappa para el acuerdo entre anotadores es de 0.82. Para producir un único conjunto de juicios, los anotadores humanos revisaron cada anotación en la que hubo desacuerdo y llegaron a una opinión de consenso. Los anotadores no recopilaron estadísticas durante este proceso, pero informaron anecdóticamente que la mayoría de las discrepancias fueron casos de descuido claro por parte del anotador o diferentes interpretaciones de afirmaciones condicionales. Por ejemplo, si deseas conservar tu trabajo, venir a la reunión de mañana implica una acción requerida, mientras que si deseas unirte al grupo de apuestas de fútbol, venir a la reunión de mañana no lo hace. El primero sería una tarea de acción en la mayoría de los contextos, mientras que el segundo no lo sería. Por supuesto, muchas afirmaciones condicionales no son tan claramente interpretables. Después de conciliar los juicios, hay 416 correos electrónicos sin elementos de acción y 328 correos electrónicos que contienen elementos de acción. De los 328 correos electrónicos que contienen elementos de acción, 259 mensajes tienen un segmento de elemento de acción; 55 mensajes tienen dos segmentos de elementos de acción; 11 mensajes tienen tres segmentos de elementos de acción. Dos mensajes tienen cuatro segmentos de elementos de acción, y un mensaje tiene seis segmentos de elementos de acción. Calcular el acuerdo a nivel de oración utilizando las evaluaciones del estándar de oro reconciliado con cada una de las evaluaciones individuales de los anotadores da como resultado un kappa de 0.89 para el Anotador Uno y un kappa de 0.92 para el Anotador Dos. En cuanto a las características del mensaje, en promedio había 132 tokens de contenido en el cuerpo después de eliminarlos. Para los mensajes de acciones pendientes, hubo 115. Sin embargo, al examinar la Figura 2 vemos que las distribuciones de longitud son casi idénticas. Como era de esperar para el correo electrónico, se trata de una distribución de cola larga con aproximadamente la mitad de los mensajes que tienen más de 60 tokens en el cuerpo (este párrafo tiene 65 tokens). 4.2 Clasificadores Para este experimento, hemos seleccionado una variedad de algoritmos estándar de clasificación de texto. Al seleccionar algoritmos, hemos elegido algoritmos que no solo se sabe que funcionan bien, sino que también difieren en líneas como discriminativo vs. generativo y perezoso vs. ansioso. Hemos hecho esto con el fin de proporcionar tanto una muestra competitiva como exhaustiva de métodos de aprendizaje para la tarea en cuestión. Esto es importante ya que es fácil mejorar un clasificador de hombre de paja al introducir una nueva representación. Al muestrear exhaustivamente opciones de clasificadores alternativos, demostramos que las mejoras en la representación sobre el modelo de bolsa de palabras no se deben a utilizar la información de la bolsa de palabras de manera deficiente. 4.2.1 kNN Empleamos una variante estándar del algoritmo de vecinos más cercanos k utilizado en la clasificación de texto, kNN con umbral de puntuación de corte s [19]. Utilizamos una ponderación tfidf de los términos con un voto ponderado por la distancia de los vecinos para calcular la puntuación antes de aplicar un umbral. Para elegir el valor de s para la segmentación, realizamos validación cruzada de dejar uno fuera sobre el conjunto de entrenamiento. El valor de k se establece en 2( log2 N + 1) donde N es el número de puntos de entrenamiento. Esta regla para elegir k está teóricamente motivada por resultados que muestran que dicha regla converge al clasificador óptimo a medida que aumenta el número de puntos de entrenamiento [8]. En la práctica, también hemos encontrado que es una conveniencia computacional que a menudo conduce a resultados comparables al optimizar numéricamente k a través de un procedimiento de validación cruzada. 4.2.2 Naïve Bayes Utilizamos un clasificador multinomial naïve Bayes estándar [16]. Al utilizar este clasificador, suavizamos las probabilidades de palabras y clases utilizando una estimación bayesiana (con la prioridad de la palabra) y una estimación m de Laplace, respectivamente. 0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 Número de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 Porcentaje de Mensajes Número de Tokens Todos los Mensajes Mensajes de Acción Figura 2: El Histograma (izquierda) y Distribución (derecha) de la Longitud de los Mensajes. Se utilizó un tamaño de contenedor de 20 palabras. Solo se contaron los tokens en el cuerpo después de la extracción manual. Después de eliminar, la mayoría de las palabras restantes suelen ser el contenido real del mensaje. Clasificadores Documento Unigram Documento Ngram Oración Unigram Oración Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 Naïve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Perceptrón Votado 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Precisión kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 Naïve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Perceptrón Votado 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Tabla 3: Rendimiento Promedio de Detección de Documentos durante la Validación Cruzada para Cada Método y la Desviación Estándar de la Muestra (Sn−1) en cursiva. El mejor rendimiento para cada clasificador se muestra en negrita. 4.2.3 SVM Hemos utilizado un SVM lineal con una representación de características tfidf y norma L2 implementada en el paquete SVMlight v6.01 [11]. Se utilizaron todas las configuraciones predeterminadas. 4.2.4 Perceptrón Votado Al igual que el SVM, el Perceptrón Votado es un método de aprendizaje basado en núcleos. Utilizamos la misma representación de características y núcleo que tenemos para la SVM, un núcleo lineal con ponderación tfidf y una norma L2. El perceptrón votado es un método de aprendizaje en línea que mantiene un historial de los perceptrones utilizados anteriormente, así como un peso que indica con qué frecuencia ese perceptrón fue correcto. Con cada nuevo ejemplo de entrenamiento, una clasificación correcta aumenta el peso en el perceptrón actual y una clasificación incorrecta actualiza el perceptrón. La salida del clasificador utiliza los pesos en el perceptrón para hacer una clasificación final votada. Cuando se utiliza de forma offline, se pueden realizar múltiples pasadas a través de los datos de entrenamiento. Tanto el perceptrón votado como la SVM dan una solución desde el mismo espacio de hipótesis, en este caso, un clasificador lineal. Además, es bien sabido que el Perceptrón Votado aumenta el margen de la solución después de cada paso a través de los datos de entrenamiento [10]. Dado que Cohen et al. [5] obtienen resultados peores utilizando una SVM que un Perceptrón Votado con una iteración de entrenamiento, concluyen que la mejor solución para detectar actos de habla puede no encontrarse en un área con un margen grande. Debido a que sus tareas son muy similares a las nuestras, empleamos ambos clasificadores para asegurarnos de que no estamos pasando por alto un clasificador alternativo competitivo al SVM para la representación básica de bolsa de palabras. 4.3 Medidas de rendimiento Para comparar el rendimiento de los métodos de clasificación, observamos dos medidas de rendimiento estándar, F1 y precisión. El valor F1 [18, 21] es la media armónica de precisión y exhaustividad, donde Precisión = Positivos Correctos / Positivos Predichos y Exhaustividad = Positivos Correctos / Positivos Reales. 4.4 Metodología Experimental Realizamos validación cruzada estándar de 10 pliegues en el conjunto de documentos. Para el enfoque a nivel de oración, todas las oraciones en un documento están completamente en el conjunto de entrenamiento o completamente en el conjunto de prueba para cada pliegue. Para las pruebas de significancia, utilizamos una prueba t de dos colas [21] para comparar los valores obtenidos durante cada iteración de validación cruzada con un valor p de 0.05. La selección de características se realizó utilizando la estadística de chi-cuadrado. Se consideraron diferentes niveles de selección de características para cada clasificador. Se probaron cada uno de los siguientes números de características: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000. Hay aproximadamente 4700 tokens unigram sin selección de características. Para elegir el número de características a utilizar para cada clasificador, realizamos una validación cruzada anidada y elegimos la configuración que produce el mejor F1 a nivel de documento para ese clasificador. Para este estudio, solo se utilizó el cuerpo de cada mensaje de correo electrónico. La selección de características siempre se aplica a todas las características candidatas. Es decir, para la representación de n-gramos, los n-gramos y las características de posición también están sujetos a eliminación por el método de selección de características. 4.5 Resultados Los resultados para la clasificación a nivel de documento se muestran en la Tabla 3. La hipótesis principal con la que estamos preocupados es que los n-gramos son críticos para esta tarea; si esto es cierto, esperamos ver una brecha significativa en el rendimiento entre los clasificadores a nivel de documento que utilizan n-gramos (denominados Ngrama de Documento) y aquellos que utilizan solo características unigramas (denominados Unigrama de Documento). Examinando la Tabla 3, observamos que este es realmente el caso para cada clasificador excepto para Naïve Bayes. Esta diferencia en el rendimiento producida por la representación de n-gramos es estadísticamente significativa para cada clasificador, excepto para el clasificador de Bayes ingenuo y la métrica de precisión para kNN (ver Tabla 4). El bajo rendimiento del Naïve Bayes con la representación de n-gramas no es sorprendente, ya que la bolsa de n-gramas causa un exceso de conteo doble como se menciona en la Sección 3.2.1; sin embargo, el Naïve Bayes no se ve afectado a nivel de oración porque los ejemplos dispersos proporcionan pocas oportunidades para los efectos aglomerativos del conteo doble. En cualquier caso, cuando se desea un enfoque de modelado de lenguaje, modelar directamente los n-gramos sería preferible a Naïve Bayes. Más importante para la hipótesis de los n-gramos, los n-gramos también conducen al mejor rendimiento del clasificador a nivel de documento. Como era de esperar, la diferencia entre la representación de n-gramas a nivel de oración y la representación unigrama es pequeña. Esto se debe a que la ventana de texto es tan pequeña que la representación unigrama, cuando se realiza a nivel de oración, recoge implícitamente el poder de los n-gramos. Un mayor mejoramiento significaría que el orden de las palabras importa incluso al considerar solo una ventana de tamaño de oración pequeña. Por lo tanto, los juicios a nivel de oración más detallados permiten que una representación unigrama tenga éxito, pero solo cuando se realiza en una ventana pequeña, comportándose como una representación de n-grama para todos los propósitos prácticos. Tabla 4: Resultados de significancia para n-gramas versus unigramas para la detección de documentos utilizando clasificadores a nivel de documento y a nivel de oración. Cuando el resultado de F1 es estadísticamente significativo, se muestra en negrita. Cuando el resultado de precisión es significativo, se muestra con un †. Ganador de F1 Precisión Ganador kNN Oración Oración Naïve Bayes Oración Oración SVM Oración Oración Perceptrón Votado Oración Tabla de Documentos 5: Resultados de significancia para clasificadores a nivel de oración vs. clasificadores a nivel de documento para el problema de detección de documentos. Cuando el resultado es estadísticamente significativo, se muestra en negrita. Destacando aún más la mejora a partir de juicios más detallados y n-gramas, la Figura 3 representa gráficamente la ventaja que tiene el clasificador SVM a nivel de oraciones sobre el enfoque estándar de bolsa de palabras con una curva de precisión-recuperación. En la zona de alta precisión del gráfico, el borde consistente del clasificador a nivel de oraciones es bastante impresionante, manteniendo una precisión de 1 hasta un recall de 0.1. Esto significaría que una décima parte de los elementos de acción de los usuarios se colocarían en la parte superior de su bandeja de entrada de elementos de acción ordenados. Además, la gran separación en la parte superior derecha de las curvas corresponde al área donde se produce el F1 óptimo para cada clasificador, coincidiendo con la gran mejora de 0.6904 a 0.7682 en la puntuación de F1. Dado el carácter relativamente inexplorado de la clasificación a nivel de oración, esto brinda grandes esperanzas para futuros aumentos en el rendimiento. La precisión F1 de los clasificadores de nivel de oración en la detección de oraciones es la siguiente: Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686, Naïve Bayes 0.9419 0.9550 0.6176 0.6676, SVM 0.9559 0.9579 0.6271 0.6672, Voted Perceptron 0.8895 0.9247 0.3744 0.5164. Aunque Cohen et al. [5] observaron que el Voted Perceptron con una sola iteración de entrenamiento superó al SVM en un conjunto de tareas similares, no observamos tal comportamiento aquí. Esto refuerza aún más la evidencia de que un clasificador alternativo con la representación de bolsa de palabras no podría alcanzar el mismo nivel de rendimiento. El clasificador Voted Perceptron mejora cuando se aumenta el número de iteraciones de entrenamiento, pero sigue siendo inferior al clasificador SVM. Los resultados de detección de oraciones se presentan en la Tabla 6. En cuanto al problema de detección de oraciones, observamos que la medida F1 proporciona una mejor idea del espacio restante para mejorar en este problema difícil. Es decir, a diferencia de la detección de documentos donde los documentos de elementos de acción son bastante comunes, las frases de elementos de acción son muy raras. Por lo tanto, al igual que en otros problemas de texto, los números de precisión son engañosamente altos simplemente debido a la precisión predeterminada alcanzable al predecir siempre negativo. Aunque los resultados aquí son significativamente superiores a lo aleatorio, no está claro qué nivel de rendimiento es necesario para que la detección de oraciones sea útil en sí misma y no simplemente como un medio para documentar la clasificación y el ranking. Figura 4: Los usuarios encuentran los elementos de acción más rápidamente cuando son asistidos por un sistema de clasificación. Finalmente, al considerar un nuevo tipo de tarea de clasificación, una de las preguntas más básicas es si un clasificador preciso construido para la tarea puede tener un impacto en el usuario final. Para demostrar el impacto que esta tarea puede tener en los usuarios de correo electrónico, realizamos un estudio de usuarios utilizando una versión anterior menos precisa del clasificador de oraciones, donde en lugar de usar solo una oración, se utilizó un enfoque de ventana de tres oraciones. Había tres conjuntos distintos de correos electrónicos en los que los usuarios tenían que encontrar elementos de acción. Estos conjuntos fueron presentados en un orden aleatorio (Desordenado), ordenados por el clasificador (Ordenado), o ordenados por el clasificador y con la precisión de 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recuperación de Acciones de Detección de Elementos SVM Rendimiento (Post Selección de Modelo) Unigrama de Documento N-grama de Oración Figura 3: Tanto los n-gramas como una pequeña ventana de predicción conducen a mejoras consistentes sobre el enfoque estándar. la oración central en la ventana de mayor confianza resaltada (Orden+ayuda). Para realizar comparaciones justas entre condiciones, el número total de tokens en cada conjunto de mensajes debe ser aproximadamente igual; es decir, la carga cognitiva de lectura debe ser aproximadamente la misma antes de reordenar los clasificadores. Además, los usuarios suelen mostrar efectos de práctica al mejorar en la tarea general y, por lo tanto, desempeñarse mejor en los conjuntos de mensajes posteriores. Esto suele ser manejado variando el orden de los conjuntos entre usuarios para que las medias sean comparables. Sin entrar en más detalles, señalamos que los conjuntos se equilibraron en cuanto al número total de fichas y se utilizó un diseño de cuadrado latino para equilibrar los efectos de práctica. La Figura 4 muestra que en intervalos de 5, 10 y 15 minutos, los usuarios encontraron consistentemente significativamente más elementos de acción cuando fueron asistidos por el clasificador, pero fueron ayudados de manera más crítica en los primeros cinco minutos. Aunque el clasificador ayuda consistentemente a los usuarios, no obtuvimos un impacto adicional en los usuarios finales al resaltar. Como se mencionó anteriormente, esto podría ser resultado del amplio margen de mejora que aún existe para la detección de oraciones, pero la evidencia anecdótica sugiere que esto también podría ser resultado de cómo se presenta la información al usuario en lugar de la precisión de la detección de oraciones. Por ejemplo, resaltar la oración incorrecta cerca de una acción real perjudica la confianza de los usuarios, pero si un indicador vago (por ejemplo, una flecha) señala el área aproximada de la que el usuario no está al tanto, se evita el error por poco. Dado que el usuario estudia utilizó una ventana de tres oraciones, creemos que esto también jugó un papel en la precisión de la detección de oraciones. 4.6 Discusión En contraste con problemas donde los n-gramas han mostrado poca diferencia, creemos que su poder aquí se deriva del hecho de que muchos de los n-gramas significativos para las acciones consisten en palabras comunes, por ejemplo, házmelo saber. Por lo tanto, el enfoque de unigrama a nivel de documento no puede obtener mucha ventaja, incluso al modelar correctamente su probabilidad conjunta, ya que estas palabras a menudo co-ocurrirán en el documento pero no necesariamente en una frase. Además, la detección de elementos de acción es distinta de muchas tareas de clasificación de texto en que una sola oración puede cambiar la etiqueta de clase del documento. Como resultado, los buenos clasificadores no pueden depender de la agregación de evidencia de un gran número de indicadores débiles en todo el documento. A pesar de haber descartado la información del encabezado, al examinar las características mejor clasificadas a nivel de documento, se revela que muchas de las características son nombres o partes de direcciones de correo electrónico que aparecieron en el cuerpo y están altamente asociadas con correos electrónicos que tienden a contener muchos o ningún elemento de acción. Algunos ejemplos son términos como org, bob y gov. Observamos que estas características serán sensibles a la distribución particular (emisores/receptores) y, por lo tanto, el enfoque a nivel de documento puede producir clasificadores que se transfieran menos fácilmente a contextos y usuarios alternativos en diferentes instituciones. Esto señala que parte del problema de ir más allá del modelo de bolsa de palabras puede ser la metodología, y al investigar propiedades como las curvas de aprendizaje y qué tan bien un modelo se transfiere, se pueden resaltar diferencias en modelos que parecen tener un rendimiento similar cuando se prueban en las distribuciones en las que fueron entrenados. Actualmente estamos investigando si los clasificadores a nivel de oración funcionan mejor en diferentes corpus de prueba sin necesidad de volver a entrenarlos. 5. TRABAJO FUTURO Si bien la aplicación de clasificadores de texto a nivel de documento está bastante bien entendida, existe el potencial de aumentar significativamente el rendimiento de los clasificadores a nivel de oración. Tales métodos incluyen formas alternativas de combinar las predicciones sobre cada oración, ponderaciones distintas a tfidf, que pueden no ser apropiadas dado que las oraciones son pequeñas, una mejor segmentación de oraciones y otros tipos de análisis frásico. Además, el etiquetado de entidades nombradas, las expresiones de tiempo, etc., parecen ser candidatos probables para características que pueden mejorar aún más esta tarea. Actualmente estamos explorando algunas de estas vías para ver qué beneficios adicionales ofrecen. Finalmente, sería interesante investigar los mejores métodos para combinar los clasificadores a nivel de documento y a nivel de oración. Dado que la representación simple de bolsa de palabras a nivel de documento conduce a un modelo aprendido que se comporta de alguna manera como un prior dependiente del contexto específico del emisor/receptor y del tema general, la primera opción sería tratarlo como tal al combinar las estimaciones de probabilidad con el clasificador a nivel de oración. Un modelo así podría servir como un ejemplo general para otros problemas donde el enfoque de bolsa de palabras puede establecer un modelo base, pero se necesitan enfoques más complejos para lograr un rendimiento más allá de ese modelo base. RESUMEN Y CONCLUSIONES La efectividad de la detección a nivel de oración argumenta que etiquetar a nivel de oración proporciona un valor significativo. Se necesitan más experimentos para ver cómo esto interactúa con la cantidad de datos de entrenamiento disponibles. La detección de oraciones que luego se aglomera a nivel de documento funciona sorprendentemente mejor con un bajo índice de recuperación de lo que se esperaría con elementos a nivel de oración. Esto, a su vez, indica que métodos mejorados de segmentación de oraciones podrían generar más mejoras en la clasificación. En este trabajo, examinamos cómo se pueden detectar de manera efectiva los elementos de acción en correos electrónicos. Nuestro análisis empírico ha demostrado que los n-gramos son de vital importancia para aprovechar al máximo las evaluaciones a nivel de documento. Cuando están disponibles juicios más detallados, entonces un enfoque estándar de bolsa de palabras utilizando un tamaño de ventana pequeño (de oración) y técnicas de segmentación automática puede producir resultados casi tan buenos como los enfoques basados en n-gramos. Agradecimientos: Este material se basa en trabajo respaldado por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autor(es) y no reflejan necesariamente las opiniones de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) o el Centro Nacional de Negocios del Departamento del Interior (DOI-NBC). Nos gustaría extender nuestro más sincero agradecimiento a Jill Lehman, cuyos esfuerzos en la recolección de datos fueron esenciales para la construcción del corpus, y tanto a Jill como a Aaron Steinfeld por su dirección de los experimentos de HCI. También nos gustaría agradecer a Django Wexler por construir y apoyar las herramientas de etiquetado de corpus y a Curtis Huttenhower por su apoyo al paquete de preprocesamiento de texto. Finalmente, agradecemos sinceramente a Scott Fahlman por su apoyo y las útiles discusiones sobre este tema. 7. REFERENCIAS [1] J. Allan, J. Carbonell, G. Doddington, J. Yamron y Y. Yang. Estudio piloto de detección y seguimiento de temas: Informe final. En Actas del Taller de Transcripción y Comprensión de Noticias de Radiodifusión de DARPA, Washington, D.C., 1998. [2] C. Apte, F. Damerau y S. M. Weiss. Aprendizaje automatizado de reglas de decisión para la categorización de texto. ACM Transactions on Information Systems, 12(3):233-251, julio de 1994. [3] J. Carletta. Evaluación del acuerdo en tareas de clasificación: La estadística kappa. Lingüística Computacional, 22(2):249-254, 1996. [4] J. Carroll. Extracción de relaciones gramaticales de alta precisión. En Actas de la 19ª Conferencia Internacional sobre Lingüística Computacional (COLING), páginas 134-140, 2002. [5] W. W. Cohen, V. R. Carvalho y T. M. Mitchell. Aprendiendo a clasificar correos electrónicos en actos de habla. En EMNLP-2004 (Conferencia sobre Métodos Empíricos en el Procesamiento del Lenguaje Natural), páginas 309-316, 2004. [6] S. Corston-Oliver, E. Ringger, M. Gamon y R. Campbell. Resumen centrado en la tarea del correo electrónico. En "La ramificación de la síntesis de texto: Actas del taller ACL-04", páginas 43-50, 2004. [7] A. Culotta, R. Bekkerman y A. McCallum. Extrayendo redes sociales e información de contacto de correos electrónicos y la web. En CEAS-2004 (Conferencia sobre Correo Electrónico y Anti-Spam), Mountain View, CA, julio de 2004. [8] L. Devroye, L. Gy¨orfi y G. Lugosi. Una teoría probabilística del reconocimiento de patrones. Springer-Verlag, Nueva York, NY, 1996. [9] S. T. Dumais, J. Platt, D. Heckerman y M. Sahami. Algoritmos de aprendizaje inductivo y representaciones para la categorización de texto. En CIKM 98, Actas de la 7ª Conferencia de la ACM sobre Gestión de la Información y el Conocimiento, páginas 148-155, 1998. [10] Y. Freund y R. Schapire. Clasificación de márgen amplio utilizando el algoritmo del perceptrón. Aprendizaje automático, 37(3):277-296, 1999. [11] T. Joachims. Haciendo que el aprendizaje svm a gran escala sea práctico. En B. Sch¨olkopf, C. J. Burges y A. J. Smola, editores, Avances en Métodos de Núcleo - Aprendizaje de Vectores de Soporte, páginas 41-56. MIT Press, 1999. [12] L. S. Larkey. 

MIT Press, 1999. [12] L. S. Larkey. Un sistema de búsqueda y clasificación de patentes. En Actas de la Cuarta Conferencia de Bibliotecas Digitales de la ACM, páginas 179 - 187, 1999. [13] D. D. Lewis. Una evaluación de representaciones frasales y agrupadas en una tarea de categorización de texto. En SIGIR 92, Actas de la 15ª Conferencia Anual Internacional de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 37-50, 1992. [14] Y. Liu, J. Carbonell y R. Jin. Un enfoque de conjunto por pares para una clasificación precisa de géneros. En Actas de la Conferencia Europea sobre Aprendizaje Automático (ECML), 2003. [15] Y. Liu, R. Yan, R. Jin y J. Carbonell. Un estudio comparativo de núcleos para la clasificación de texto multi-etiqueta utilizando asociación de categorías. En la Vigésimo-primera Conferencia Internacional sobre Aprendizaje Automático (ICML), 2004. [16] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Notas de Trabajo de AAAI 98 (La 15ª Conferencia Nacional sobre Inteligencia Artificial), Taller sobre Aprendizaje para la Categorización de Textos, páginas 41-48, 1998. TR WS-98-05. [17] F. Sebastiani. 

TR WS-98-05. [17] F. Sebastiani. Aprendizaje automático en la categorización automatizada de textos. ACM Computing Surveys, 34(1):1-47, marzo de 2002. [18] C. J. van Rijsbergen. Recuperación de información. Butterworths, Londres, 1979. [19] Y. Yang. Una evaluación de enfoques estadísticos para la categorización de texto. Recuperación de información, 1(1/2):67-88, 1999. [20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. T. Archibald y X. Liu. Enfoques de aprendizaje para la detección y seguimiento de temas. IEEE EXPERT, Número Especial sobre Aplicaciones de la Recuperación de Información Inteligente, 1999. [21] Y. Yang y X. Liu. Una reevaluación de los métodos de categorización de textos. En SIGIR 99, Actas de la 22ª Conferencia Internacional Anual de la ACM sobre Investigación y Desarrollo en Recuperación de Información, páginas 42-49, 1999. [22] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada por el tema. En Actas de la Conferencia Internacional de la ACM SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, julio de 2002.