La programación de recursos combinatoria para MDPs multiagentes. Dmitri A. Dolgov, Michael R. James y Michael E. Samples. Grupo de Investigación en IA y Robótica, Centro Técnico de Toyota en EE. UU. {ddolgov, michael.r.james, michael.samples}@gmail.com. RESUMEN La programación óptima de recursos en sistemas multiagentes es una tarea computacionalmente desafiante, especialmente cuando los valores de los recursos no son aditivos. Consideramos el problema combinatorio de programar el uso de múltiples recursos entre agentes que operan en entornos estocásticos, modelados como procesos de decisión de Markov (MDPs). En los últimos años, se han desarrollado algoritmos eficientes de asignación de recursos para agentes con valores de recursos inducidos por MDPs. Sin embargo, este trabajo previo se ha centrado en problemas de asignación de recursos estáticos donde los recursos se distribuyen una vez y luego se utilizan en MDPs de horizonte infinito. Extendemos esos modelos existentes al problema de la programación de recursos combinatoria, donde los agentes persisten solo por períodos finitos entre sus tiempos de llegada y salida (predefinidos), requiriendo recursos solo durante esos períodos de tiempo. Proporcionamos un procedimiento computacionalmente eficiente para calcular asignaciones de recursos óptimas a nivel global a agentes a lo largo del tiempo. Ilustramos y analizamos empíricamente el método en el contexto de un dominio de programación de trabajos estocásticos. Categorías y Descriptores de Asignaturas I.2.8 [Inteligencia Artificial]: Resolución de Problemas, Métodos de Control y Búsqueda; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente Términos Generales Algoritmos, Rendimiento, Diseño 1. INTRODUCCIÓN Las tareas de asignación óptima de recursos y programación son ubicuas en sistemas multiagentes, pero resolver tales problemas de optimización puede ser computacionalmente difícil, debido a una serie de factores. En particular, cuando el valor de un conjunto de recursos para un agente no es aditivo (como suele ser el caso con recursos que son sustitutos o complementos), la función de utilidad podría tener que definirse en un espacio exponencialmente grande de combinaciones de recursos, lo que rápidamente se vuelve computacionalmente intratable. Además, incluso cuando cada agente tiene una función de utilidad que es distinta de cero solo en un pequeño subconjunto de los posibles conjuntos de recursos, obtener una asignación óptima sigue siendo computacionalmente prohibitivo, ya que el problema se vuelve NP-completo [14]. Tales problemas computacionales han dado lugar recientemente a varias líneas de trabajo en el uso de modelos compactos de las preferencias de los agentes. Una idea es utilizar cualquier estructura presente en las funciones de utilidad para representarlas de forma compacta, por ejemplo, a través de fórmulas lógicas [15, 10, 4, 3]. Una alternativa es modelar directamente los mecanismos que definen las funciones de utilidad de los agentes y realizar la asignación de recursos directamente con estos modelos [9]. Una forma de lograr esto es modelar los procesos mediante los cuales un agente podría utilizar los recursos y definir la función de utilidad como la ganancia de estos procesos. En particular, si un agente utiliza recursos para actuar en un entorno estocástico, su función de utilidad puede ser modelada de forma natural con un proceso de decisión de Markov, cuyo conjunto de acciones está parametrizado por los recursos disponibles. Esta representación puede ser utilizada para construir algoritmos de asignación de recursos muy eficientes que resultan en una aceleración exponencial sobre un problema de optimización directa con representaciones planas de preferencias combinatorias [6, 7, 8]. Sin embargo, este trabajo existente sobre asignación de recursos con preferencias inducidas por MDPs parametrizados por recursos hace una suposición de que los recursos se asignan solo una vez y luego son utilizados por los agentes de forma independiente dentro de sus MDPs de horizonte infinito. Esta suposición de que no es posible la reasignación de recursos puede ser limitante en dominios donde los agentes llegan y se van dinámicamente. En este artículo, ampliamos el trabajo sobre asignación de recursos bajo preferencias inducidas por MDP a problemas de programación en tiempo discreto, donde los agentes están presentes en el sistema durante intervalos de tiempo finitos y solo pueden utilizar recursos dentro de estos intervalos. En particular, los agentes llegan y parten en momentos arbitrarios (predefinidos) y dentro de estos intervalos utilizan recursos para ejecutar tareas en MDPs de horizonte finito. Abordamos el problema de la programación de recursos óptima a nivel global, donde el objetivo es encontrar una asignación de recursos a los agentes a lo largo del tiempo que maximice la suma de las recompensas esperadas que obtienen. En este contexto, nuestra principal contribución es una formulación de programación entera mixta del problema de programación que elige asignaciones de recursos globalmente óptimas, tiempos de inicio y horizontes de ejecución para todos los agentes (dentro de sus intervalos de llegada y salida). Analizamos y comparamos empíricamente dos variantes del problema de programación: uno, donde los agentes tienen asignaciones de recursos estáticas dentro de sus MDPs de horizonte finito, y otro, donde los recursos pueden ser realocados dinámicamente entre agentes en cada paso de tiempo. En el resto del documento, primero establecemos las bases necesarias en la Sección 2 y luego presentamos nuestro modelo y declaración formal del problema en la Sección 3. En la Sección 4.2, describimos nuestro resultado principal, el programa de optimización para la programación de recursos globalmente óptima. Tras la discusión de nuestros resultados experimentales sobre un problema de programación de trabajos en la Sección 5, concluimos en la Sección 6 con una discusión de posibles extensiones y generalizaciones de nuestro método. De manera similar al modelo utilizado en trabajos anteriores sobre asignación de recursos con preferencias inducidas por MDP [6, 7], definimos el valor de un conjunto de recursos para un agente como el valor de la mejor política MDP que es realizable, dadas esos recursos. Sin embargo, dado que el enfoque de nuestro trabajo se centra en problemas de programación, y una gran parte del problema de optimización consiste en decidir cómo se asignan los recursos en el tiempo entre agentes con tiempos finitos de llegada y salida, modelamos los problemas de planificación de los agentes como MDPs de horizonte finito, en contraste con trabajos anteriores que utilizaron MDPs descontados de horizonte infinito. En el resto de esta sección, primero presentamos algo de antecedentes necesarios sobre MDPs de horizonte finito y presentamos una formulación de programación lineal que sirve como base para nuestro algoritmo de solución desarrollado en la Sección 4. También describimos los métodos estándar para la programación de recursos combinatorios con valores de recursos planos, que sirven como punto de referencia de comparación para el nuevo modelo desarrollado aquí. 2.1 Procesos de Decisión de Markov Un MDP (Proceso de Decisión de Markov) estacionario, de dominio finito y discreto en el tiempo (ver, por ejemplo, [13] para un desarrollo detallado y exhaustivo) se puede describir como S, A, p, r, donde: S es un conjunto finito de estados del sistema; A es un conjunto finito de acciones disponibles para el agente; p es una función de transición estocástica estacionaria, donde p(σ|s, a) es la probabilidad de transicionar al estado σ al ejecutar la acción a en el estado s; r es una función de recompensa estacionaria, donde r(s, a) especifica la recompensa obtenida al ejecutar la acción a en el estado s. Dado un MDP de este tipo, un problema de decisión bajo un horizonte finito T consiste en elegir una acción óptima en cada paso de tiempo para maximizar el valor esperado de la recompensa total acumulada durante la vida (finita) de los agentes. La política óptima del agente es entonces una función del estado actual s y el tiempo hasta el horizonte. Una política óptima para tal problema es actuar de forma codiciosa con respecto a la función de valor óptimo, definida de forma recursiva por el siguiente sistema de ecuaciones de Bellman de tiempo finito [2]: v(s, t) = max a r(s, a) + X σ p(σ|s, a)v(σ, t + 1), ∀s ∈ S, t ∈ [1, T − 1]; v(s, T) = 0, ∀s ∈ S; donde v(s, t) es el valor óptimo de estar en el estado s en el tiempo t ∈ [1, T]. Esta función de valor óptimo se puede calcular fácilmente utilizando programación dinámica, lo que conduce a la siguiente política óptima π, donde π(s, a, t) es la probabilidad de ejecutar la acción a en el estado s en el tiempo t: π(s, a, t) = (1, a = argmaxa r(s, a) + P σ p(σ|s, a)v(σ, t + 1), 0, en caso contrario. La anterior es la forma más común de calcular la función de valor óptimo (y por lo tanto una política óptima) para un MDP de horizonte finito. Sin embargo, también podemos formular el problema como el siguiente programa lineal (similarmente al LP dual para MDPs descontados de horizonte infinito [13, 6, 7]): max X s X a r(s, a) X t x(s, a, t) sujeto a: X a x(σ, a, t + 1) = X s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1]; X a x(s, a, 1) = α(s), ∀s ∈ S; (1) donde α(s) es la distribución inicial sobre el espacio de estados, y x es la medida de ocupación (no estacionaria) (x(s, a, t) ∈ [0, 1] es el número total esperado de veces que se ejecuta la acción a en el estado s en el tiempo t). Una política óptima (no estacionaria) se obtiene a partir de la medida de ocupación de la siguiente manera: π(s, a, t) = x(s, a, t)/ X a x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2) Tenga en cuenta que el MDP estándar de horizonte finito sin restricciones, como se describe anteriormente, siempre tiene una solución uniformemente óptima (óptima para cualquier distribución inicial α(s)). Por lo tanto, una política óptima se puede obtener utilizando una constante arbitraria α(s) > 0 (en particular, α(s) = 1 resultará en x(s, a, t) = π(s, a, t)). Sin embargo, para los MDPs con restricciones de recursos (como se definen a continuación en la Sección 3), las políticas óptimas uniformemente no existen en general. En tales casos, α se convierte en parte de la entrada del problema, y una política resultante solo es óptima para ese α en particular. Este resultado es bien conocido para MDPs de horizonte infinito con varios tipos de restricciones [1, 6], y también se cumple para nuestro modelo de horizonte finito, el cual puede establecerse fácilmente a través de una línea de razonamiento completamente análoga a los argumentos en [6]. 2.2 Programación de Recursos Combinatoria Un enfoque directo para la programación de recursos para un conjunto de agentes M, cuyos valores para los recursos son inducidos por problemas de planificación estocástica (en nuestro caso, MDPs de horizonte finito) sería que cada agente enumerara todas las asignaciones de recursos posibles a lo largo del tiempo y, para cada una, calcular su valor resolviendo el MDP correspondiente. Entonces, cada agente proporcionaría valoraciones para cada posible conjunto de recursos a lo largo del tiempo a un coordinador centralizado, quien calcularía las asignaciones óptimas de recursos a lo largo del tiempo basándose en estas valoraciones. Cuando los recursos pueden asignarse en diferentes momentos a diferentes agentes, cada agente debe presentar valoraciones para cada combinación de posibles horizontes temporales. Que cada agente m ∈ M ejecute su MDP dentro del intervalo de tiempo de llegada-salida τ ∈ [τa m, τd m]. Por lo tanto, el agente m ejecutará un MDP con un horizonte temporal no mayor que Tm = τd m−τa m+1. Que bτ sea el horizonte temporal global para el problema, antes del cual todos los MDP de los agentes deben finalizar. Suponemos que τd m < bτ, ∀m ∈ M. El Sexto Congreso Internacional. En el problema de programación donde los agentes tienen requisitos de recursos estáticos dentro de sus MDPs de horizonte finito, los agentes proporcionan una valoración para cada conjunto de recursos para cada horizonte de tiempo posible (desde [1, Tm]) que puedan utilizar. Sea Ω el conjunto de recursos a ser asignados entre los agentes. Un agente recibirá como máximo un paquete de recursos para uno de los horizontes temporales. Que la variable ψ ∈ Ψm enumere todos los posibles pares de conjuntos de recursos y horizontes temporales para el agente m, por lo que hay 2|Ω| × Tm valores para ψ (el espacio de conjuntos es exponencial en el número de tipos de recursos |Ω|). El agente m debe proporcionar un valor vψ m para cada ψ, y el coordinador asignará como máximo un par (recurso, horizonte temporal) ψ a cada agente. Esta asignación se expresa como una variable indicadora zψ m ∈ {0, 1} que muestra si ψ está asignado al agente m. Para el tiempo τ y el recurso ω, la función nm(ψ, τ, ω) ∈ {0, 1} indica si el paquete en ψ utiliza el recurso ω en el tiempo τ (hacemos la suposición de que los agentes tienen requisitos de recursos binarios). Este problema de asignación es NP-completo, incluso al considerar solo un paso de tiempo, y su dificultad aumenta significativamente con múltiples pasos de tiempo debido al creciente número de valores de ψ. El problema de encontrar una asignación óptima que satisfaga la restricción global de que la cantidad de cada recurso ω asignada a todos los agentes no exceda la cantidad disponible bϕ(ω) se puede expresar como el siguiente programa entero: max X m∈M X ψ∈Ψm zψ mvψ m sujeto a: X ψ∈Ψm zψ m ≤ 1, ∀m ∈ M; X m∈M X ψ∈Ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω; (3) La primera restricción en la ecuación 3 dice que ningún agente puede recibir más de un paquete, y la segunda restricción asegura que la asignación total del recurso ω no exceda, en ningún momento, el límite del recurso. Para el problema de programación en el que los agentes pueden reasignar dinámicamente recursos, cada agente debe especificar un valor para cada combinación de paquetes y pasos de tiempo dentro de su horizonte temporal. Que la variable ψ ∈ Ψm en este caso enumere todos los posibles conjuntos de recursos para los cuales a lo sumo un conjunto puede ser asignado al agente m en cada paso de tiempo. Por lo tanto, en este caso hay P t∈[1,Tm](2|Ω| )t ∼ 2|Ω|Tm posibilidades de combinaciones de recursos asignadas a diferentes intervalos de tiempo, para los Tm horizontes de tiempo diferentes. El mismo conjunto de ecuaciones (3) se puede utilizar para resolver este problema de programación dinámica, pero el programa entero es diferente debido a la diferencia en cómo se define ψ. En este caso, el número de valores de ψ es exponencial en el horizonte de planificación Tm de cada agente, lo que resulta en un programa mucho más grande. Este enfoque directo para resolver ambos problemas de programación requiere una enumeración y solución de 2|Ω| Tm (asignación estática) o P t∈[1,Tm] 2|Ω|t (reubicación dinámica) MDPs para cada agente, lo cual rápidamente se vuelve intratable con el crecimiento del número de recursos |Ω| o el horizonte de tiempo Tm. MODELO Y DECLARACIÓN DEL PROBLEMA Ahora presentamos formalmente nuestro modelo del problema de programación de recursos. La entrada del problema consiste en los siguientes componentes: • M, Ω, bϕ, τa m, τd m, bτ están definidos arriba en la Sección 2.2. • {Θm} = {S, A, pm, rm, αm} son los MDPs de todos los agentes m ∈ M. Sin pérdida de generalidad, asumimos que los espacios de estados y acciones de todos los agentes son iguales, pero cada uno tiene su propia función de transición pm, función de recompensa rm, y condiciones iniciales αm. • ϕm : A×Ω → {0, 1} es la asignación de acciones a recursos para el agente m. ϕm(a, ω) indica si la acción a del agente m necesita el recurso ω. Un agente m que recibe un conjunto de recursos que no incluye el recurso ω no puede ejecutar en su política MDP ninguna acción a para la cual ϕm(a, ω) = 0. Suponemos que todos los requisitos de recursos son binarios; como se discute más adelante en la Sección 6, esta suposición no es limitante. Dado el input anterior, el problema de optimización que consideramos es encontrar la asignación globalmente óptima que maximice la suma de recompensas esperadas de recursos a agentes para todos los pasos de tiempo: Δ : τ × M × Ω → {0, 1}. Una solución es factible si la asignación correspondiente de recursos a los agentes no viola la restricción global de recursos: X m Δm(τ, ω) ≤ bϕ(ω), ∀ω ∈ Ω, τ ∈ [1, bτ]. Consideramos dos variantes del problema de programación de recursos. La primera formulación restringe las asignaciones de recursos al espacio donde la asignación de recursos a cada agente es estática durante la vida de los agentes. La segunda formulación permite la reasignación de recursos entre agentes en cada paso de tiempo durante sus vidas. La Figura 1 representa un problema de programación de recursos con tres agentes M = {m1, m2, m3}, tres recursos Ω = {ω1, ω2, ω3}, y un horizonte de problema global de bτ = 11. Los tiempos de llegada y salida de los agentes se muestran como cajas grises y son {1, 6}, {3, 7} y {2, 11}, respectivamente. Una solución a este problema se muestra a través de barras horizontales dentro de cada casilla de agentes, donde las barras corresponden a la asignación de los tres tipos de recursos. La Figura 1a muestra una solución a un problema de programación estática. Según la solución mostrada, el agente m1 comienza la ejecución de su MDP en el tiempo τ = 1 y tiene un bloqueo en los tres recursos hasta que finaliza la ejecución en el tiempo τ = 3. Se debe tener en cuenta que el agente m1 renuncia a su control sobre los recursos antes de su hora de salida anunciada de τd m1 = 6, aparentemente porque otros agentes pueden utilizar los recursos de manera más efectiva. Por lo tanto, en el tiempo τ = 4, los recursos ω1 y ω3 se asignan al agente m2, quien luego los utiliza para ejecutar su MDP (utilizando solo acciones admitidas por los recursos ω1 y ω3) hasta el tiempo τ = 7. El agente m3 posee el recurso ω3 durante el intervalo τ ∈ [4, 10]. La Figura 1b muestra una posible solución a la versión dinámica del mismo problema. Allí, los recursos pueden ser reasignados entre agentes en cada paso de tiempo. Por ejemplo, el agente m1 renuncia a su uso del recurso ω2 en el tiempo τ = 2, aunque continúa la ejecución de su MDP hasta el tiempo τ = 6. Ten en cuenta que a un agente no se le permite detener y reiniciar su MDP, por lo que el agente m1 solo puede continuar ejecutando en el intervalo τ ∈ [3, 4] si tiene acciones que no requieren recursos (ϕm(a, ω) = 0). Claramente, el modelo y la declaración del problema descritos anteriormente hacen una serie de suposiciones sobre el problema y las propiedades de la solución deseada. Discutimos algunas de esas suposiciones y sus implicaciones en la Sección 6. 1222 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) (a) (b) Figura 1: Ilustración de una solución a un problema de programación de recursos con tres agentes y tres recursos: a) asignaciones de recursos estáticas (las asignaciones de recursos son constantes durante la vida de los agentes); b) asignación dinámica (las asignaciones de recursos pueden cambiar en cada paso de tiempo). 4. PROGRAMACIÓN DE RECURSOS Nuestro algoritmo de programación de recursos se lleva a cabo en dos etapas. Primero, realizamos un paso de preprocesamiento que aumenta los MDP de los agentes; este proceso se describe en la Sección 4.1. Segundo, utilizando estos MDPs aumentados construimos un problema de optimización global, el cual se describe en la Sección 4.2. 4.1 Aumentando los MDPs de los Agentes En el modelo descrito en la sección anterior, asumimos que si un agente no posee los recursos necesarios para realizar acciones en su MDP, su ejecución se detiene y el agente abandona el sistema. En otras palabras, los MDPs no pueden ser pausados y reanudados. Por ejemplo, en el problema mostrado en la Figura 1a, el agente m1 libera todos los recursos después del tiempo τ = 3, momento en el cual la ejecución de su MDP se detiene. De manera similar, los agentes m2 y m3 solo ejecutan sus MDPs en los intervalos τ ∈ [4, 6] y τ ∈ [4, 10], respectivamente. Por lo tanto, una parte importante del problema de toma de decisiones global es decidir la ventana de tiempo durante la cual cada uno de los agentes está activo (es decir, ejecutando su MDP). Para lograr esto, ampliamos el MDP de cada agente con dos nuevos estados (estados de inicio y final sb, sf, respectivamente) y una nueva acción de inicio/parada a∗, como se ilustra en la Figura 2. La idea es que un agente permanezca en el estado inicial sb hasta que esté listo para ejecutar su MDP, momento en el que realiza la acción de inicio/parada a∗ y transita al espacio de estados del MDP original con la probabilidad de transición que corresponde a la distribución inicial original α(s). Por ejemplo, en la Figura 1a, para el agente m2 esto ocurriría en el tiempo τ = 4. Una vez que el agente llega al final de su ventana de actividad (tiempo τ = 6 para el agente m2 en la Figura 1a), realiza la acción de inicio/parada, lo que lo lleva al estado final de finalización del sumidero sf en el tiempo τ = 7. Más precisamente, dado un MDP S, A, pm, rm, αm, definimos un MDP aumentado S, A, pm, rm, αm de la siguiente manera: S = S ∪ sb ∪ sf; A = A ∪ a∗; p(s|sb, a∗) = α(s), ∀s ∈ S; p(sb|sb, a) = 1.0, ∀a ∈ A; p(sf|s, a∗) = 1.0, ∀s ∈ S; p(σ|s, a) = p(σ|s, a), ∀s, σ ∈ S, a ∈ A; r(sb, a) = r(sf, a) = 0, ∀a ∈ A; r(s, a) = r(s, a), ∀s ∈ S, a ∈ A; α(sb) = 1; α(s) = 0, ∀s ∈ S; donde se asume que todas las probabilidades de transición no especificadas son cero. Además, para tener en cuenta el nuevo estado inicial, comenzamos el MDP un paso de tiempo antes, estableciendo τa m ← τa m − 1. Esto no afectará la asignación de recursos debido a que las restricciones de recursos solo se aplican a los estados originales del MDP, como se discutirá en la siguiente sección. Por ejemplo, los MDPs aumentados mostrados en la Figura 2b (que comienza en el estado sb en el tiempo τ = 2) se construirían a partir de un MDP con tiempo de llegada original τ = 3. La Figura 2b también muestra una trayectoria de muestra a través del espacio de estados: el agente comienza en el estado sb, transita al espacio de estados S del MDP original y finalmente sale al estado de sumidero sf. Cabe destacar que si quisiéramos modelar un problema en el que los agentes pudieran pausar sus MDP en pasos de tiempo arbitrarios (lo cual podría ser útil para dominios donde la reasignación dinámica es posible), podríamos lograrlo fácilmente incluyendo una acción adicional que transiciona de cada estado a sí mismo con una recompensa de cero. 4.2 MILP para la programación de recursos Dado un conjunto de MDPs aumentados, como se define arriba, el objetivo de esta sección es formular un programa de optimización global que resuelva el problema de programación de recursos. En esta sección y a continuación, se asume que todos los MDPs son los MDPs aumentados tal como se definen en la Sección 4.1. Nuestro enfoque es similar a la idea utilizada en [6]: comenzamos con la formulación del programa lineal de los MDP de los agentes (1) y lo complementamos con restricciones que aseguran que la asignación de recursos correspondiente entre agentes y tiempo sea válida. El problema de optimización resultante resuelve simultáneamente los MDP de los agentes y los problemas de programación de recursos. En el resto de esta sección, desarrollamos de forma incremental un programa de enteros mixtos (MILP) que logra esto. En ausencia de restricciones de recursos, los MDPs de horizonte finito de los agentes son completamente independientes, y la solución óptima global puede obtenerse trivialmente a través del siguiente LP, que es simplemente una agregación de LPs de horizonte finito de un solo agente: max X m X s X a rm(s, a) X t xm(s, a, t) sujeto a: X a xm(σ, a, t + 1) = X s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ M, σ ∈ S, t ∈ [1, Tm − 1]; X a xm(s, a, 1) = αm(s), ∀m ∈ M, s ∈ S; (12) donde xm(s, a, t) es la medida de ocupación del agente m, y The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1223 (a) (b) Figura 2: Ilustración de la ampliación de un MDP para permitir tiempos de inicio y finalización variables: a) (izquierda) el MDP original de dos estados con una sola acción; (derecha) el MDP ampliado con nuevos estados sb y sf y la nueva acción a∗ (nota que las transiciones originales no se modifican en el proceso de ampliación); b) el MDP ampl augmentado mostrado como una trayectoria a través del tiempo (las líneas grises indican todas las transiciones, mientras que las líneas negras indican una trayectoria dada). Función objetivo (suma de recompensas esperadas de todos los agentes) máx X m X s X a rm(s, a) X t xm(s, a, t) (5) Implicación de significado Restricciones lineales Atar x a θ. El agente solo está activo cuando la medida de ocupación es distinta de cero en los estados originales del MDP. θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ A X s/∈{sb,sf } X a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ M, ∀t ∈ [1, Tm] (6) El agente solo puede estar activo en τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ M, τ /∈ (τa m, τd m) (7) No se pueden utilizar recursos cuando no está activo θm(τ) = 0 =⇒ Δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ Ω Δm(τ, ω) ≤ θm(τ) ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω (8) Vincular x a Δ (x distinto de cero obliga a que el correspondiente Δ sea distinto de cero). Δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|A| X a ϕm(a, ω) X s/∈{sb,sf } xm(s, a, t) ≤ Δm(t + τa m − 1, ω) ∀m ∈ M, ω ∈ Ω, t ∈ [1, Tm] (9) Límites de recursos X m Δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ Ω, τ ∈ [0, bτ] (10) El agente no puede cambiar los recursos mientras está activo. Solo habilitado para programación con asignaciones estáticas. θm(τ) = 1 y θm(τ + 1) = 1 =⇒ Δm(τ, ω) = Δm(τ + 1, ω) Δm(τ, ω) − Z(1 − θm(τ + 1)) ≤ Δm(τ + 1, ω) + Z(1 − θm(τ)) Δm(τ, ω) + Z(1 − θm(τ + 1)) ≥ Δm(τ + 1, ω) − Z(1 − θm(τ)) ∀m ∈ M, ω ∈ Ω, τ ∈ [0, bτ] (11) Tabla 1: MILP para programación óptima global de recursos. Tm = τd m − τa m + 1 es el horizonte temporal para los agentes MDP. Usando este LP como base, lo ampliamos con restricciones que aseguran que el uso de recursos implicado por las medidas de ocupación de los agentes {xm} no viola los requisitos globales de recursos bϕ en ningún paso de tiempo τ ∈ [0, bτ]. Para formular estas restricciones de recursos, utilizamos las siguientes variables binarias: • Δm(τ, ω) = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ], ω ∈ Ω, que sirven como variables indicadoras que definen si el agente m posee el recurso ω en el tiempo τ. Estos son análogos a las variables indicadoras estáticas utilizadas en el problema de asignación de recursos estáticos de una sola vez en [6]. • θm = {0, 1}, ∀m ∈ M, τ ∈ [0, bτ] son variables indicadoras que especifican si el agente m está activo (es decir, ejecutando su MDP) en el tiempo τ. El significado de las variables de uso de recursos Δ se ilustra en la Figura 1: Δm(τ, ω) = 1 solo si el recurso ω se asigna al agente m en el tiempo τ. El significado de los indicadores de actividad θ se ilustra en la Figura 2b: cuando el agente m se encuentra en el estado inicial sb o en el estado final sf, el θm correspondiente es igual a 0, pero una vez que el agente se vuelve activo y entra en uno de los otros estados, establecemos θm = 1. Este significado de θ puede ser impuesto con una restricción lineal que sincroniza los valores de las medidas de ocupación de los agentes xm y la actividad 1224 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) indicadores θ, como se muestra en (6) en la Tabla 1. Otra restricción que debemos agregar, debido a que los indicadores de actividad θ están definidos en la línea de tiempo global τ, es hacer cumplir el hecho de que el agente está inactivo fuera de su ventana de llegada y salida. Esto se logra mediante la restricción (7) en la Tabla 1. Además, los agentes no deben estar utilizando recursos mientras están inactivos. Esta restricción también puede ser impuesta a través de una desigualdad lineal en θ y Δ, como se muestra en (8). La restricción (6) establece el valor de θ para que coincida con la política definida por la medida de ocupación xm. De manera similar, debemos asegurarnos de que las variables de uso de recursos Δ estén también sincronizadas con la medida de ocupación xm. Esto se realiza a través de la restricción (9) en la Tabla 1, que es casi idéntica a la restricción análoga de [6]. Después de implementar la restricción anterior, que refuerza el significado de Δ, agregamos una restricción que garantiza que el uso de recursos de los agentes nunca exceda los montos de recursos disponibles. Esta condición también se expresa trivialmente como una desigualdad lineal (10) en la Tabla 1. Finalmente, para la formulación del problema en la que las asignaciones de recursos son estáticas durante la vida de un agente, agregamos una restricción que garantiza que las variables de uso de recursos Δ no cambien su valor mientras el agente está activo (θ = 1). Esto se logra a través de la restricción lineal (11), donde Z ≥ 2 es una constante que se utiliza para desactivar las restricciones cuando θm(τ) = 0 o θm(τ + 1) = 0. Esta restricción no se utiliza para la formulación del problema dinámico, donde los recursos pueden ser reasignados entre agentes en cada paso de tiempo. En resumen, la Tabla 1 junto con las restricciones de conservación de flujo de (12) define el MILP que calcula simultáneamente una asignación óptima de recursos para todos los agentes a lo largo del tiempo, así como políticas óptimas de MDP de horizonte finito que son válidas bajo esa asignación de recursos. Como una medida aproximada de la complejidad de este MILP, consideremos el número de variables de optimización y restricciones. Sea TM = P Tm = P m(τa m − τd m + 1) la suma de las longitudes de las ventanas de llegada-salida a través de todos los agentes. Entonces, el número de variables de optimización es: TM + bτ|M||Ω| + bτ|M|, de las cuales TM son continuas (xm), y bτ|M||Ω| + bτ|M| son binarias (Δ y θ). Sin embargo, hay que tener en cuenta que todos menos TM|M| de los θ se establecen en cero por la restricción (7), lo que también obliga inmediatamente a que todos menos TM|M||Ω| de los Δ sean cero a través de las restricciones (8). El número de restricciones (sin incluir las restricciones degeneradas en (7)) en el MILP es: TM + TM|Ω| + bτ|Ω| + bτ|M||Ω|. A pesar de que la complejidad del MILP es, en el peor de los casos, exponencial en el número de variables binarias, la complejidad de este MILP es significativamente (exponencialmente) menor que la del MILP con funciones de utilidad planas, descrito en la Sección 2.2. Este resultado refleja las ganancias de eficiencia reportadas en [6] para problemas de asignación de recursos de un solo disparo, pero es mucho más pronunciado, debido a la explosión de la representación de utilidad plana debido al aspecto temporal del problema (recuerde la complejidad prohibitiva de la optimización combinatoria en la Sección 2.2). Analizamos empíricamente el rendimiento de este método en la Sección 5.1. Estrictamente hablando, resolver MILPs hasta la optimalidad es NP-completo en el número de variables enteras. RESULTADOS EXPERIMENTALES Aunque la complejidad de resolver MILPs es en el peor de los casos exponencial en el número de variables enteras, existen muchos métodos eficientes para resolver MILPs que permiten que nuestro algoritmo se adapte bien a parámetros comunes en problemas de asignación de recursos y programación. En particular, esta sección introduce un dominio de problema: el problema del taller de reparación, que se utiliza para evaluar empíricamente la escalabilidad de nuestros algoritmos en términos del número de agentes |M|, el número de recursos compartidos |Ω| y las diferentes longitudes de tiempo global bτ durante las cuales los agentes pueden entrar y salir del sistema. El problema del taller de reparaciones es un MDP parametrizado simple que adopta la metáfora de un taller de reparaciones de vehículos. Los agentes en el taller de reparación son mecánicos con una serie de tareas independientes que solo generan recompensa cuando se completan. En nuestro modelo MDP de este sistema, las acciones tomadas para avanzar a través del espacio de estados solo están permitidas si el agente posee ciertos recursos que están públicamente disponibles para la tienda. Estos recursos son de suministro finito, y las políticas óptimas para la tienda determinarán cuándo cada agente puede retener los recursos limitados para tomar acciones y obtener recompensas individuales. Cada tarea a completar está asociada con una sola acción, aunque se requiere que el agente repita la acción numerosas veces antes de completar la tarea y obtener una recompensa. Este modelo fue parametrizado en términos del número de agentes en el sistema, el número de diferentes tipos de recursos que podrían estar vinculados a acciones necesarias, un tiempo global durante el cual se permite a los agentes llegar y partir, y una longitud máxima para el número de pasos de tiempo que un agente puede permanecer en el sistema. Todos los puntos de datos en nuestros experimentos se obtuvieron con 20 evaluaciones utilizando CPLEX para resolver los MILPs en una computadora Pentium4 con 2Gb de RAM. Se realizaron pruebas tanto en la versión estática como en la dinámica del problema de programación de recursos, tal como se definió anteriormente. La Figura 3 muestra el tiempo de ejecución y el valor de la política para modificaciones independientes en el conjunto de parámetros. La fila superior muestra cómo el tiempo de solución para el MILP escala a medida que aumentamos el número de agentes |M|, el horizonte de tiempo global bτ y el número de recursos |Ω|. Aumentar el número de agentes conduce a una escalabilidad exponencial en complejidad, lo cual es de esperarse para un problema NP-completo. Sin embargo, aumentar el límite de tiempo global bτ o el número total de tipos de recursos |Ω|, manteniendo constante el número de agentes, no conduce a una disminución en el rendimiento. Esto ocurre porque los problemas se vuelven más fáciles a medida que se vuelven sub-determinados, lo cual también es un fenómeno común en problemas NP-completos. También observamos que la solución a la versión dinámica del problema a menudo se puede calcular mucho más rápido que la versión estática. La fila inferior de la Figura 3 muestra el valor de la política conjunta de las políticas que corresponden a los horarios óptimos de asignación de recursos calculados. Podemos observar que la versión dinámica produce una recompensa más alta (como era de esperar, ya que la recompensa de la versión dinámica siempre es igual o mayor que la recompensa de la versión estática). Deberíamos señalar que estos gráficos no deben ser vistos como una medida del rendimiento de dos algoritmos diferentes (ambos algoritmos producen soluciones óptimas pero para problemas distintos), sino más bien como observaciones sobre cómo cambia la calidad de las soluciones óptimas a medida que se permite más flexibilidad en la reasignación de recursos. La Figura 4 muestra el tiempo de ejecución y el valor de la política para las pruebas en las que las variables de entrada comunes se escalan juntas. Esto permite la Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1225 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 10 −2 10 −1 10 0 10 1 10 2 10 3 Límite de Tiempo Global τ Tiempo de CPU, seg |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 10 −2 10 −1 10 0 10 1 10 2 Número de Recursos |Ω| Tiempo de CPU, seg |M| = 5, τ = 50 estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 Número de Agentes |M| Valor |Ω| = 5, τ = 50 estático dinámico 50 100 150 200 400 500 600 700 800 900 1000 1100 1200 1300 1400 Límite de Tiempo Global τ Valor |M| = 5, |Ω| = 5 estático dinámico 10 20 30 40 50 500 600 700 800 900 1000 1100 1200 1300 1400 Número de Recursos |Ω| Valor |M| = 5, τ = 50 estático dinámico Figura 3: Evaluación de nuestro MILP para números variables de agentes (columna 1), longitudes de ventana de tiempo global (columna 2) y números de tipos de recursos (columna 3). La fila superior muestra el tiempo de CPU, y la fila inferior muestra la recompensa conjunta de las políticas MDP de los agentes. Las barras de error muestran los cuartiles 1ro y 3ro (25% y 75%). 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 Número de Agentes |M| Tiempo de CPU, seg τ = 10|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 2|M| estático dinámico 2 4 6 8 10 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 Número de Agentes |M| Tiempo de CPU, seg |Ω| = 5|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 Número de Agentes |M| Valor τ = 10|M| estático dinámico 2 4 6 8 10 200 400 600 800 1000 1200 1400 1600 1800 2000 Número de Agentes |M| Valor |Ω| = 2|M| estático dinámico 2 4 6 8 10 0 500 1000 1500 2000 2500 Número de Agentes |M| Valor |Ω| = 5|M| estático dinámico Figura 4: Evaluación de nuestro MILP utilizando variables de entrada correlacionadas. La columna izquierda sigue el rendimiento y el tiempo de CPU a medida que aumenta el número de agentes y la ventana de tiempo global juntos (bτ = 10|M|). La columna del medio y la columna de la derecha siguen el rendimiento y el tiempo de CPU a medida que aumenta el número de recursos y el número de agentes juntos, con |Ω| = 2|M| y |Ω| = 5|M|, respectivamente. Las barras de error muestran el primer y tercer cuartil (25% y 75%). 1226 The Sixth Intl. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) nos permite explorar dominios donde el número total de agentes escala proporcionalmente al número total de tipos de recursos o al horizonte temporal global, manteniendo constante la densidad promedio de agentes (por unidad de tiempo global) o el número promedio de recursos por agente (lo cual comúnmente ocurre en aplicaciones de la vida real). En general, creemos que estos resultados experimentales indican que nuestra formulación MILP puede ser utilizada para resolver de manera efectiva problemas de programación de recursos de tamaño no trivial. DISCUSIÓN Y CONCLUSIONES A lo largo del documento, hemos realizado una serie de suposiciones en nuestro modelo y algoritmo de solución; discutimos sus implicaciones a continuación. • Ejecución continua. Suponemos que una vez que un agente deja de ejecutar su MDP (transiciona al estado sf), sale del sistema y no puede regresar. Es fácil relajar esta suposición para dominios donde los agentes de MDP pueden ser pausados y reiniciados. Todo lo que se requiere es incluir una acción de pausa adicional que haga la transición desde un estado dado de regreso a sí mismo, y que tenga una recompensa de cero. • Indiferencia al tiempo de inicio. Utilizamos un modelo de recompensa donde las recompensas de los agentes dependen solo del horizonte temporal de sus MDP y no del tiempo de inicio global. Esta es una consecuencia de nuestro procedimiento de aumento de MDP de la Sección 4.1. Es fácil extender el modelo para que los agentes incurran en una penalización explícita por inactividad al asignar una recompensa negativa no nula al estado inicial sb. • Requisitos de recursos binarios. Para simplificar, hemos asumido que los costos de recursos son binarios: ϕm(a, ω) = {0, 1}, pero nuestros resultados se generalizan de manera directa a asignaciones de recursos no binarias, de manera análoga al procedimiento utilizado en [5]. • Agentes cooperativos. El procedimiento de optimización discutido en este artículo fue desarrollado en el contexto de agentes cooperativos, pero también puede ser utilizado para diseñar un mecanismo para programar recursos entre agentes egoístas. Este procedimiento de optimización puede ser incorporado en una subasta de Vickrey-Clarke-Groves, de manera completamente análoga a como se hizo en [7]. De hecho, todos los resultados de [7] sobre las propiedades de la subasta y la privacidad de la información se aplican directamente al dominio de programación discutido en este documento, requiriendo solo ligeras modificaciones para tratar con MDPs de horizonte finito. • Tiempos de llegada y salida conocidos y deterministas. Finalmente, hemos asumido que los tiempos de llegada y salida de los agentes (τa m y τd m) son deterministas y conocidos de antemano. Esta suposición es fundamental para nuestro método de solución. Si bien hay muchos dominios donde esta suposición es válida, en muchos casos los agentes llegan y se van de forma dinámica y sus tiempos de llegada y salida solo pueden predecirse de manera probabilística, lo que lleva a problemas de asignación de recursos en línea. En particular, en el caso de agentes con interés propio, esto se convierte en una versión interesante de un problema de diseño de mecanismos en línea [11, 12]. En resumen, hemos presentado una formulación MILP para el problema combinatorio de programación de recursos donde los valores de los agentes para posibles asignaciones de recursos están definidos por MDPs de horizonte finito. Este resultado amplía el trabajo previo ([6, 7]) sobre asignación estática de recursos de una sola vez bajo preferencias inducidas por MDP a problemas de programación de recursos con un aspecto temporal. Por lo tanto, este trabajo da un paso en la dirección de diseñar un mecanismo en línea para agentes con preferencias de recursos combinatorios inducidas por problemas de planificación estocástica. Relajar la suposición sobre los tiempos deterministas de llegada y salida de los agentes es un enfoque de nuestro trabajo futuro. Nos gustaría agradecer a los revisores anónimos por sus comentarios y sugerencias perspicaces. 7. REFERENCIAS [1] E. Altman y A. Shwartz. Control adaptativo de cadenas de Markov restringidas: Criterios y políticas. Anales de Investigación de Operaciones, número especial sobre Procesos de Decisión de Markov, 28:101-134, 1991. [2] R. Bellman. Programación dinámica. Princeton University Press, 1957. [3] C. Boutilier. 

Prensa de la Universidad de Princeton, 1957. [3] C. Boutilier. Resolviendo problemas de subasta combinatoria expresados de manera concisa. En Proc. de AAAI-02, páginas 359-366, 2002. [4] C. Boutilier y H. H. Hoos. Subastando idiomas para subastas combinatorias. En Proc. de IJCAI-01, páginas 1211-1217, 2001. [5] D. Dolgov. Asignación integrada de recursos y planificación en entornos estocásticos de múltiples agentes. Tesis doctoral, Departamento de Ciencias de la Computación, Universidad de Michigan, febrero de 2006. [6] D. A. Dolgov y E. H. Durfee. Asignación óptima de recursos y formulación de políticas en procesos de decisión de Markov débilmente acoplados. En Proc. de ICAPS-04, páginas 315-324, junio de 2004. [7] D. A. Dolgov y E. H. Durfee. Subastas combinatorias computacionalmente eficientes para asignación de recursos en MDPs débilmente acoplados. En Proc. de AAMAS-05, Nueva York, NY, EE. UU., 2005. ACM Press. [8] D. A. Dolgov y E. H. Durfee. Asignación de recursos entre agentes con preferencias inducidas por MDP factorizados. En Proc. de AAMAS-06, 2006. [9] K. Larson y T. Sandholm. Diseño de mecanismos y agentes deliberativos. En Proc. de AAMAS-05, páginas 650-656, Nueva York, NY, EE. UU., 2005. ACM Press. [10] N. Nisan. 

ACM Press. [10] N. Nisan. Subasta y asignación en subastas combinatorias. En Comercio Electrónico, 2000. [11] D. C. Parkes y S. Singh. Un enfoque basado en MDP para el Diseño de Mecanismos en Línea. En Proc. de la Decimoséptima Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-03), 2003. [12] D. C. Parkes, S. Singh y D. Yanovsky. Diseño de mecanismos en línea aproximadamente eficiente. En Actas de la Decimoctava Conferencia Anual sobre Sistemas de Procesamiento de Información Neural (NIPS-04), 2004. [13] M. L. Puterman. Procesos de Decisión de Markov. John Wiley & Sons, Nueva York, 1994. [14] M. H. Rothkopf, A. Pekec y R. M. Harstad. Subastas combinatorias manejables computacionalmente. Ciencia de la Gestión, 44(8):1131-1147, 1998. [15] T. Sandholm. Un algoritmo para la determinación óptima del ganador en subastas combinatorias. En Proc. de IJCAI-99, páginas 542-547, San Francisco, CA, EE. UU., 1999. Morgan Kaufmann Publishers Inc. - Editores Morgan Kaufmann Inc. El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 1227