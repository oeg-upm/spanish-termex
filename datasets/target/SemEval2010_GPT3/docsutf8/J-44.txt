Exploradores, promotores y conectores: Los roles de las calificaciones en el filtrado colaborativo de vecinos más cercanos. Departamento de Bharath Kumar Mohan. Del CSA Instituto Indio de Ciencias Bangalore 560 012, India mbk@csa.iisc.ernet.in Benjamin J. Keller Dept. de Ciencias de la Computación Universidad del Este de Michigan Ypsilanti, MI 48917, EE. UU. bkeller@emich.edu Naren Ramakrishnan Dept. de Ciencias de la Computación Virginia Tech, Blacksburg VA 24061, EE. UU. naren@cs.vt.edu RESUMEN Los sistemas de recomendación agregan las calificaciones individuales de los usuarios en predicciones de productos o servicios que podrían interesar a los visitantes. La calidad de este proceso de agregación afecta crucialmente la experiencia del usuario y, por lo tanto, la efectividad de los recomendadores en el comercio electrónico. Presentamos un estudio novedoso que desagrega las métricas de rendimiento de recomendaciones globales en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. En particular, formulamos tres roles - exploradores, promotores y conectores - que capturan cómo los usuarios reciben recomendaciones, cómo se recomiendan los elementos y cómo las calificaciones de estos dos tipos están conectadas entre sí. Estos roles encuentran usos directos en mejorar las recomendaciones para los usuarios, en apuntar mejor a los elementos y, lo más importante, en ayudar a monitorear la salud del sistema en su totalidad. Por ejemplo, se pueden utilizar para rastrear la evolución de los vecindarios, identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, enumerar usuarios que están en peligro de irse y evaluar la susceptibilidad del sistema a ataques como el shilling. Sostenemos que los tres roles de calificación presentados aquí proporcionan primitivas amplias para gestionar un sistema de recomendación y su comunidad. Categorías y Descriptores de Asignaturas H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas-Apoyo a la toma de decisiones; J.4 [Aplicaciones Informáticas]: Ciencias Sociales y del Comportamiento Términos Generales Algoritmos, Factores Humanos 1. INTRODUCCIÓN Los sistemas de recomendación se han vuelto fundamentales para el comercio electrónico, proporcionando tecnología que sugiere productos a un visitante basándose en compras anteriores o historial de calificaciones. El filtrado colaborativo, una forma común de recomendación, predice la calificación de un usuario para un artículo al combinar las calificaciones de ese usuario con las calificaciones de otros usuarios. Se ha realizado una investigación significativa en la implementación de algoritmos de filtrado colaborativo rápidos y precisos [2, 7], en el diseño de interfaces para presentar recomendaciones a los usuarios [1], y en el estudio de la robustez de estos algoritmos [8]. Sin embargo, con la excepción de algunos estudios sobre la influencia de los usuarios [10], se ha prestado poca atención a desentrañar el funcionamiento interno de un sistema de recomendación en términos de las calificaciones individuales y los roles que desempeñan en hacer recomendaciones (buenas). Tal comprensión proporcionará un importante recurso para monitorear y gestionar un sistema de recomendación, para diseñar mecanismos que mantengan el sistema de recomendación y, de esta manera, garantizar su éxito continuo. Nuestra motivación aquí es desagregar las métricas de rendimiento globales del recomendador en contribuciones realizadas por cada calificación individual, lo que nos permite caracterizar los diversos roles desempeñados por las calificaciones en el filtrado colaborativo de vecinos más cercanos. Identificamos tres posibles roles: (exploradores) para conectar al usuario al sistema y recibir recomendaciones, (promotores) para conectar un elemento al sistema y ser recomendado, y (conectores) para conectar las valoraciones de estos dos tipos. Al ver las calificaciones de esta manera, podemos definir la contribución de una calificación en cada rol, tanto en términos de permitir que se realicen recomendaciones, como en términos de influencia en la calidad de las recomendaciones. A su vez, esta capacidad ayuda a respaldar escenarios como: 1. Ubicar a los usuarios en vecindarios mejores: Las calificaciones de un usuario pueden conectar inadvertidamente al usuario con un vecindario cuyos gustos no sean una combinación perfecta para el usuario. Identificar las calificaciones responsables de tales malas recomendaciones y sugerir nuevos elementos para calificar puede ayudar a situar al usuario en un vecindario mejor. Objetivo de los elementos: Los sistemas de recomendación sufren de falta de participación de los usuarios, especialmente en escenarios de inicio en frío [13] que involucran elementos recién llegados. Identificar a los usuarios que pueden ser alentados a calificar elementos específicos ayuda a garantizar la cobertura del sistema de recomendaciones. 3. Monitoreando la evolución del sistema de recomendación y sus partes interesadas: Un sistema de recomendación está constantemente en cambio: creciendo con nuevos usuarios y 250 elementos, disminuyendo con usuarios que abandonan el sistema, elementos que se vuelven irrelevantes y partes del sistema bajo ataque. Seguir los roles de una calificación y su evolución a lo largo del tiempo proporciona muchas ideas sobre la salud del sistema y cómo podría ser gestionado y mejorado. Estos incluyen la capacidad de identificar subespacios de calificación que no contribuyen (o contribuyen negativamente) al rendimiento del sistema, y podrían ser eliminados; enumerar usuarios que están en peligro de abandonar, o que han abandonado el sistema; y evaluar la susceptibilidad del sistema a ataques como el shilling [5]. Como mostramos, la caracterización de roles de calificación presentada aquí proporciona primitivas amplias para gestionar un sistema de recomendación y su comunidad. El resto del documento está organizado de la siguiente manera. El fondo sobre el filtrado colaborativo de vecinos más cercanos y la evaluación de algoritmos se discute en la Sección 2. La sección 3 define y discute los roles de una calificación, y la sección 4 define las medidas de la contribución de una calificación en cada uno de estos roles. En la Sección 5, ilustramos el uso de estos roles para abordar los objetivos mencionados anteriormente. 2. FONDO 2.1 Algoritmos Los algoritmos de filtrado colaborativo de vecinos más cercanos utilizan vecindarios de usuarios o vecindarios de elementos para calcular una predicción. Un algoritmo del primer tipo se llama basado en el usuario, y uno del segundo tipo se llama basado en el artículo [12]. En ambas familias de algoritmos, los vecindarios se forman primero calculando la similitud entre todos los pares de usuarios (para basados en usuarios) o elementos (para basados en elementos). Las predicciones se calculan luego mediante la agregación de calificaciones, que en un algoritmo basado en usuarios implica la agregación de las calificaciones del ítem objetivo por los vecinos del usuario y, en un algoritmo basado en ítems, implica la agregación de las calificaciones de los usuarios de ítems que son vecinos del ítem objetivo. Los algoritmos dentro de estas familias difieren en la definición de similitud, formación de vecindarios y cálculo de predicciones. Consideramos un algoritmo basado en el usuario basado en el definido para GroupLens [11] con variaciones de Herlocker et al. [2], y un algoritmo basado en el ítem similar al de Sarwar et al. [12]. El algoritmo utilizado por Resnick et al. [11] define la similitud de dos usuarios u y v como la correlación de Pearson de sus calificaciones comunes: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2, donde Iu es el conjunto de elementos calificados por el usuario u, ru,i es la calificación del usuario u para el elemento i, y ¯ru es la calificación promedio del usuario u (de manera similar para v). La similitud calculada de esta manera suele ser escalada por un factor proporcional al número de calificaciones comunes, para reducir la posibilidad de hacer una recomendación basada en conexiones débiles: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), donde γ ≈ 5 es una constante utilizada como límite inferior en el escalado [2]. Estas nuevas similitudes se utilizan luego para definir un vecindario estático Nu para cada usuario u, que consiste en los K usuarios más similares al usuario u. Una predicción para el usuario u y el ítem i se calcula mediante un promedio ponderado de las calificaciones de los vecinos pu,i = ¯ru + Σ v∈V sim (u, v)(rv,i − ¯rv) / Σ v∈V sim (u, v) (1) donde V = Nu ∩ Ui es el conjunto de usuarios más similares a u que han calificado i. El algoritmo basado en elementos que utilizamos es el definido por Sarwar et al. [12]. En este algoritmo, la similitud se define como la medida del coseno ajustado sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) donde Ui es el conjunto de usuarios que han calificado el ítem i. En cuanto al algoritmo basado en usuarios, los pesos de similitud se ajustan proporcionalmente al número de usuarios que han calificado los elementos en común sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). Dadas las similitudes, el vecindario Ni de un elemento i se define como los K elementos más similares para ese elemento. Una predicción para el usuario u y el ítem i se calcula como el promedio ponderado pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) donde J = Ni ∩ Iu es el conjunto de ítems calificados por u que son más similares a i. 2.2 Evaluación Los algoritmos recomendadores típicamente han sido evaluados utilizando medidas de precisión predictiva y cobertura [3]. Los estudios sobre algoritmos recomendadores, notablemente Herlocker et al. [2] y Sarwar et al. [12], típicamente calculan la precisión predictiva dividiendo un conjunto de calificaciones en conjuntos de entrenamiento y prueba, y calculan la predicción para un ítem en el conjunto de prueba utilizando las calificaciones en el conjunto de entrenamiento. Una medida estándar de precisión predictiva es el error absoluto medio (MAE), que para un conjunto de pruebas T = {(u, i)} se define como, MAE = P (u,i)∈T |pu,i − ru,i| |T | . La cobertura tiene varias definiciones, pero generalmente se refiere a la proporción de elementos que pueden ser predichos por el algoritmo [3]. Un problema práctico con la precisión predictiva es que los usuarios suelen recibir listas de recomendaciones, y no predicciones numéricas individuales. Las listas de recomendaciones son listas de elementos en orden decreciente de predicción (a veces expresadas en términos de calificaciones por estrellas), por lo que la precisión predictiva puede no reflejar la precisión de la lista. Por lo tanto, en lugar de eso podemos medir la precisión de la recomendación o del rango, lo cual indica en qué medida la lista está en el orden correcto. Herlocker et al. [3] discuten una serie de medidas de precisión de rango, que van desde el Tau de Kendall hasta medidas que consideran el hecho de que los usuarios tienden a mirar solo un prefijo de la lista [5]. El coeficiente de correlación de Kendall mide el número de inversiones al comparar pares ordenados en el verdadero orden de usuario de los elementos 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figura 1: Calificaciones en un recomendador de películas simple. y el orden recomendado, y se define como τ = C − D p (C + D + TR)(C + D + TP) (6) donde C es el número de pares que el sistema predice en el orden correcto, D es el número de pares que el sistema predice en el orden incorrecto, TR es el número de pares en el orden verdadero que tienen las mismas calificaciones, y TP es el número de pares en el orden predicho que tienen las mismas calificaciones [3]. Una desventaja de la métrica Tau es que es ajena a la posición en la lista ordenada donde ocurre la inversión [3]. Por ejemplo, una inversión hacia el final de la lista se le da el mismo peso que una al principio. Una solución es considerar inversiones solo en los primeros elementos de la lista recomendada o ponderar las inversiones según su posición en la lista. ROLES DE UNA CALIFICACIÓN Nuestra observación básica es que cada calificación desempeña un papel diferente en cada predicción en la que se utiliza. Considera un sistema de recomendación de películas simplificado con tres usuarios, Jim, Jeff y Tom, y sus calificaciones para algunas películas, como se muestra en la Fig. 1. (Para esta discusión inicial no consideraremos los valores de calificación involucrados). El recomendador predice si a Tom le gustará La Máscara utilizando las calificaciones ya disponibles. Cómo se hace esto depende del algoritmo: 1. Un algoritmo de filtrado colaborativo basado en elementos construye un vecindario de películas alrededor de The Mask utilizando las calificaciones de usuarios que calificaron The Mask y otras películas de manera similar (por ejemplo, las calificaciones de Jim de The Matrix y The Mask; y las calificaciones de Jeff de Star Wars y The Mask). Las calificaciones de Tom de esas películas se utilizan luego para hacer una predicción para La Máscara. 2. Un algoritmo de filtrado colaborativo basado en usuarios construiría un vecindario alrededor de Tom rastreando a otros usuarios cuyos comportamientos de calificación son similares a los de Tom (por ejemplo, Tom y Jeff han calificado Star Wars; Tom y Jim han calificado The Matrix). La predicción de la calificación de Tom para The Mask se basa en las calificaciones de Jeff y Tim. Aunque los algoritmos de vecinos más cercanos agregan las calificaciones para formar vecindarios utilizados para calcular predicciones, podemos desagregar las similitudes para ver la computación de una predicción como siguiendo simultáneamente caminos paralelos de calificaciones. Por lo tanto, independientemente del algoritmo de filtrado colaborativo utilizado, podemos visualizar la predicción de la calificación de Tom para "The Mask" como caminar a través de una secuencia de calificaciones. En Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figura 2: Calificaciones utilizadas para predecir The Mask para Tom. Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figura 3: Predicción de The Mask para Tom en la que se utiliza una calificación más de una vez. En este ejemplo, se utilizaron dos caminos para esta predicción como se muestra en la Fig. 2: (p1, p2, p3) y (q1, q2, q3). Ten en cuenta que estos caminos son no dirigidos y todos tienen una longitud de 3. Solo el orden en el que se recorren las calificaciones es diferente entre el algoritmo basado en elementos (por ejemplo, (p3, p2, p1), (q3, q2, q1)) y el algoritmo basado en usuarios (por ejemplo, (p1, p2, p3), (q1, q2, q3)). Una calificación puede formar parte de muchos caminos para una única predicción, como se muestra en la Fig. 3, donde se utilizan tres caminos para una predicción, dos de los cuales siguen a p1: (p1, p2, p3) y (p1, r2, r3). Las predicciones en algoritmos de filtrado colaborativo pueden implicar miles de caminos en paralelo, cada uno jugando un papel en influir en el valor predicho. Cada camino de predicción consta de tres calificaciones, desempeñando roles que llamamos exploradores, promotores y conectores. Para ilustrar estos roles, considera el camino (p1, p2, p3) en la Fig. 2 utilizado para hacer una predicción de The Mask para Tom: 1. La calificación p1 (Tom → Star Wars) establece una conexión desde Tom hacia otras calificaciones que pueden ser utilizadas para predecir la calificación de Tom para The Mask. Esta calificación sirve como un explorador en el grafo bipartito de calificaciones para encontrar un camino que lleve a The Mask. 2. La calificación p2 (Jeff → Star Wars) ayuda al sistema a recomendarle a Tom la película The Mask al conectar al explorador con el promotor. 3. La calificación p3 (Jeff → La Máscara) permite conexiones con La Máscara y, por lo tanto, promociona esta película a Tom. Formalmente, dado un pronóstico pu,a de un ítem objetivo a para el usuario u, un explorador para pu,a es una calificación ru,i tal que existe un usuario v con calificaciones rv,a y rv,i para algún ítem i; un promotor para pu,a es una calificación rv,a para algún usuario v, tal que existen calificaciones rv,i y ru,i para un ítem i, y; un conector para pu,a es una calificación rv,i de algún usuario v y calificación i, tal que existen calificaciones ru,i y rv,a. Los exploradores, conectores y promotores para la predicción de la calificación de Tom de "The Mask" son p1 y q1, p2 y q2, y p3 y q3 (respectivamente). Cada uno de estos roles tiene un valor en el recomendador para el usuario, el vecindario de usuarios y el sistema en términos de permitir que se realicen recomendaciones. 3.1 Roles en Detalle Las calificaciones que actúan como exploradores tienden a ayudar al sistema recomendador a sugerir más películas al usuario, aunque el grado en que esto es cierto depende del comportamiento de calificación de otros usuarios. Por ejemplo, en la Fig. 4, la calificación de Tom → Star Wars ayuda al sistema a recomendarle solo The Mask, mientras que Tom → The Matrix ayuda a recomendar The Mask, Jurassic Park y My Cousin Vinny. Tom establece una conexión con Jim, quien es un usuario prolífico del sistema, al calificar The Matrix. Sin embargo, esto no convierte a The Matrix en la mejor película para calificar para todos. Por ejemplo, Jim se beneficia por igual de La Máscara y Matrix, lo que permite al sistema recomendarle Star Wars. Su calificación de "The Mask" es la mejor referencia para Jeff, y la única referencia de Jerry es su calificación de "Star Wars". Esto sugiere que los buenos exploradores permiten a un usuario construir similitud con usuarios prolíficos, y de esta manera aseguran que obtengan más del sistema. Mientras que los exploradores representan calificaciones beneficiosas desde la perspectiva de un usuario, los promotores son sus opuestos y benefician a los productos. En la Fig. 4, Mi Primo Vinny se beneficia de la calificación de Jim, ya que permite recomendaciones a Jeff y Tom. La Máscara no depende tanto de una sola calificación, ya que las calificaciones de Jim y Jeff la ayudan. Por otro lado, la calificación de Jerry de Star Wars no ayuda a promocionarla a ningún otro usuario. Concluimos que un buen promotor conecta un artículo con una comunidad más amplia de otros artículos, y de esta manera garantiza que sea recomendado a más usuarios. Los conectores desempeñan un papel crucial en un sistema de recomendación que no es tan obvio. Las películas Mi Primo Vinny y Parque Jurásico tienen el mayor potencial de recomendación ya que pueden ser recomendadas a Jeff, Jerry y Tom basándose en la estructura de enlace ilustrada en la Fig. 4. Además del hecho de que Jim calificó estas películas, estas recomendaciones son posibles solo gracias a las calificaciones Jim → The Matrix y Jim → The Mask, que son los mejores conectores. Un conector mejora la capacidad del sistema para hacer recomendaciones sin obtener un beneficio explícito para el usuario. Ten en cuenta que cada calificación puede ser de beneficio variado en cada uno de estos roles. La calificación de Jim → Mi primo Vinny es un mal explorador y conector, pero es un muy buen promotor. La calificación de Jim → The Mask es un explorador razonablemente bueno, un conector muy bueno y un promotor bueno. Finalmente, la calificación de Jerry → Star Wars es muy buena como explorador, pero no tiene valor como conector o promotor. Como se ilustra aquí, una calificación puede tener un valor diferente en cada uno de los tres roles en términos de si se puede hacer una recomendación o no. Podríamos medir este valor simplemente contando el número de veces que se utiliza una calificación en cada rol, lo cual por sí solo sería útil para caracterizar el comportamiento de un sistema. Pero también podemos medir la contribución de cada calificación a la calidad de las recomendaciones o la salud del sistema. Dado que cada predicción es un esfuerzo combinado de varios caminos de recomendación, estamos interesados en discernir la influencia de cada calificación (y, por lo tanto, de cada camino) en el sistema hacia el error general del sistema. Podemos entender la dinámica del sistema con mayor detalle al rastrear la influencia de una calificación según el rol desempeñado. La siguiente sección describe el enfoque para medir los valores de una calificación en cada rol. 4. CONTRIBUCIONES DE LAS CALIFICACIONES Como hemos visto, una calificación puede desempeñar diferentes roles en diferentes predicciones y, en cada predicción, contribuir a la calidad de una predicción de diferentes maneras. Nuestro enfoque puede utilizar cualquier medida numérica de una propiedad de la salud del sistema, y asigna crédito (o culpa) a cada calificación proporcional a su influencia en la predicción. Al rastrear el papel de cada calificación en una predicción, podemos acumular el crédito para una calificación en cada uno de los tres roles, y también seguir la evolución de los roles de la calificación a lo largo del tiempo en el sistema. Esta sección define la metodología para calcular la contribución de las calificaciones al definir primero la influencia de una calificación, y luego implementar el enfoque para la precisión predictiva y la precisión de rango. También demostramos cómo estas contribuciones pueden ser agregadas para estudiar el vecindario de calificaciones involucrado en el cálculo de las recomendaciones de un usuario. Tenga en cuenta que, aunque nuestra formulación general para evaluar la influencia es independiente del algoritmo, debido a limitaciones de espacio, presentamos el enfoque solo para el filtrado colaborativo basado en elementos. La definición de algoritmos basados en el usuario es similar y se presentará en una versión ampliada de este documento. 4.1 Influencia de las calificaciones. Recuerde que un enfoque basado en el artículo para el filtrado colaborativo se basa en la construcción de vecindarios de artículos utilizando la similitud de las calificaciones por el mismo usuario. Como se describió anteriormente, la similitud se define por la medida del coseno ajustado (Ecuaciones (2) y (3)). Se mantiene un conjunto de los K vecinos principales para todos los elementos para eficiencia espacial y computacional. Una predicción del ítem i para un usuario u se calcula como la desviación ponderada de la calificación media de los ítems, como se muestra en la Ecuación (4). La lista de recomendaciones para un usuario es entonces la lista de elementos ordenados en orden descendente de sus valores predichos. Primero definimos impacto(a, i, j), el impacto que un usuario a tiene en determinar la similitud entre dos elementos i y j. Esta es la variación en la similitud entre i y j cuando se elimina una calificación, y se define como impacto(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| donde Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} es el conjunto de corredores de artículos i y j (usuarios que califican tanto i como j), R(u) es el conjunto de calificaciones proporcionadas por el usuario u, y sim¯a(i, j) es la similitud de i y j cuando se eliminan las calificaciones de usuario a sim¯a(i, j) = P v∈U\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\{a}(ru,i − ¯ru)2 qP u∈U\{a}(ru,j − ¯ru)2 , y ajustado por el número de calificadores sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j). Si todos los coraters de i y j los califican de manera idéntica, definimos el impacto como impacto(a, i, j) = 1 |Cij| ya que P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0. La influencia de cada camino (u, j, v, i) = [ru,j, rv,j, rv,i] en la predicción de pu,i está dada por la influencia(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impacto(v, i, j). Se sigue que la suma de las influencias sobre todos esos caminos, para un conjunto dado de puntos finales, es 1. 4.2 Valores de Rol para Precisión Predictiva El valor de una calificación en cada rol se calcula a partir de la influencia dependiendo de la medida de evaluación empleada. Aquí ilustramos el enfoque utilizando la precisión predictiva como métrica de evaluación. En general, la bondad de una predicción determina si las calificaciones involucradas deben ser acreditadas o desacreditadas por su papel. Para precisión predictiva, el error en la predicción e = |pu,i − ru,i| se mapea a un nivel de comodidad utilizando una función de mapeo M(e). La evidencia anecdótica sugiere que los usuarios no pueden discernir errores menores a 1.0 (en una escala de calificación de 1 a 5) [4], por lo que un error menor a 1.0 se considera aceptable, pero cualquier cosa mayor no lo es. Por lo tanto, definimos M(e) como (1 − e) agrupado en un valor apropiado en [−1, −0.5, 0.5, 1]. Para cada predicción pu,i, M(e) se atribuye a todos los caminos que ayudaron en el cálculo de pu,i, proporcional a sus influencias. Este tributo, M(e)∗influence(u, j, v, i), es a su vez heredado por cada una de las calificaciones en el camino [ru,j, rv,j, rv,i], con el crédito/culpa acumulándose en los roles respectivos de ru,j como explorador, rv,j como conector y rv,i como promotor. En otras palabras, el valor de explorador SF(ru,j), el valor de conector CF(rv,j) y el valor de promotor PF(rv,i) se incrementan todos por la cantidad del tributo. Durante un gran número de predicciones, los exploradores que han dado repetidamente grandes tasas de error tienen un gran valor negativo de explorador, y viceversa (de manera similar con los otros roles). Cada calificación se resume así por su triple [SF, CF, PF]. 4.3 Valores de Rol para la Precisión de Rango Definimos ahora el cálculo de la contribución de las calificaciones a la precisión del rango observado. Para este cálculo, debemos conocer el orden de preferencia de los usuarios para un conjunto de elementos para los cuales se pueden realizar predicciones. Suponemos que tenemos un conjunto de pruebas de las calificaciones de los usuarios de los elementos presentados en la lista de recomendaciones. Para cada par de elementos calificados por un usuario en los datos de prueba, verificamos si el orden predicho es concordante con su preferencia. Decimos que un par (i, j) es concordante (con error ε) siempre que se cumpla una de las siguientes condiciones: • si (ru,i < ru,j) entonces (pu,i − pu,j < ε); • si (ru,i > ru,j) entonces (pu,i − pu,j > ε); o • si (ru,i = ru,j) entonces (|pu,i − pu,j| ≤ ε). De manera similar, un par (i, j) es discordante (con error ) si no es concordante. Nuestros experimentos descritos a continuación utilizan una tolerancia de error de = 0.1. Todos los caminos involucrados en la predicción de los dos elementos en un par concordante son acreditados, y los caminos involucrados en un par discordante son desacreditados. El crédito asignado a un par de elementos (i, j) en la lista de recomendaciones para el usuario u se calcula como c(i, j) = ( t T · 1 C+D si (i, j) son concordantes − t T · 1 C+D si (i, j) son discordantes (7) donde t es el número de elementos en el conjunto de prueba de los usuarios cuyas calificaciones podrían predecirse, T es el número de elementos calificados por el usuario u en el conjunto de prueba, C es el número de concordancias y D es el número de discordancias. El crédito c se divide luego entre todos los caminos responsables de predecir pu,i y pu,j proporcional a sus influencias. Nuevamente agregamos los valores de rol obtenidos de todos los experimentos para formar un triple [SF, CF, PF] para cada calificación. 4.4 Agregando roles de calificación. Después de calcular los valores de rol para las calificaciones individuales, también podemos utilizar estos valores para estudiar vecindarios y el sistema. Aquí consideramos cómo podemos utilizar los valores del rol para caracterizar la salud de un vecindario. Considera la lista de recomendaciones principales presentada a un usuario en un momento específico. El algoritmo de filtrado colaborativo recorrió muchos caminos en su vecindario a través de sus exploradores y otros conectores y promotores para hacer estas recomendaciones. Llamamos a estas calificaciones el vecindario recomendador del usuario. El usuario elige implícitamente este vecindario de calificaciones a través de los elementos que califica. Además del algoritmo de filtrado colaborativo, la salud de este vecindario influye completamente en la satisfacción de un usuario con el sistema. Podemos caracterizar el vecindario recomendador de un usuario mediante la agregación de los valores de rol individuales de las calificaciones involucradas, ponderados por la influencia de las calificaciones individuales en la determinación de su lista recomendada. Diferentes secciones del vecindario de los usuarios ejercen una influencia variada en su lista de recomendaciones. Por ejemplo, las calificaciones accesibles a través de artículos altamente calificados tienen más peso en los artículos recomendados. Nuestro objetivo es estudiar el sistema y clasificar a los usuarios según su ubicación en un vecindario saludable o no saludable. Un usuario puede tener un buen conjunto de exploradores, pero puede estar expuesto a un vecindario con malos conectores y promotores. Él puede tener un buen vecindario, pero sus malos exploradores pueden asegurarse de que el potencial del vecindario sea inútil. Esperamos que los usuarios con buenos exploradores y buenas vecindades estén más satisfechos con el sistema en el futuro. El vecindario de un usuario se caracteriza por un triple que representa la suma ponderada de los valores de rol de las calificaciones individuales involucradas en hacer recomendaciones. Considera a un usuario u y su lista ordenada de recomendaciones L. Un elemento i 254 en la lista tiene un peso inverso, como K(i), dependiendo de su posición en la lista. En nuestros estudios utilizamos K(i) = posición(i) multiplicado por p. Varios caminos de calificaciones [ru,j, rv,j, rv,i] están involucrados en predecir pu,i, lo cual finalmente decide su posición en L, cada uno con influencia(u, j, v, i). El vecindario recomendador de un usuario u está caracterizado por el triple, [SFN(u), CFN(u), PFN(u)] donde SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) y PFN(u) están definidos de manera similar. Este triple estima la calidad de nuestras recomendaciones basándose en el historial pasado de las calificaciones involucradas en sus respectivos roles. 5. Como hemos visto, podemos asignar valores de rol a cada calificación al evaluar un sistema de filtrado colaborativo. En esta sección, demostramos el uso de este enfoque para nuestro objetivo general de definir un enfoque para monitorear y gestionar la salud de un sistema de recomendación a través de experimentos realizados en el conjunto de datos de un millón de calificaciones de MovieLens. En particular, discutimos resultados relacionados con la identificación de buenos exploradores, promotores y conectores; la evolución de roles de calificación; y la caracterización de vecindarios de usuarios. 5.1 Metodología Nuestros experimentos utilizan el conjunto de datos de calificaciones de un millón de MovieLens, que consiste en calificaciones de 6040 usuarios de 3952 películas. Las calificaciones están en el rango de 1 a 5, y están etiquetadas con la hora en que se dieron las calificaciones. Como se discutió anteriormente, consideramos únicamente el algoritmo basado en ítems aquí (con vecindarios de ítems de tamaño 30) y, debido a limitaciones de espacio, solo presentamos los resultados de valor de rol para la precisión de rango. Dado que estamos interesados en la evolución de los valores del rol de calificación con el tiempo, el modelo del sistema de recomendación se construye procesando las calificaciones en su orden de llegada. La marca de tiempo proporcionada por MovieLens es crucial para los análisis presentados aquí. Realizamos evaluaciones de roles de calificación en intervalos de 10,000 calificaciones y procesamos las primeras 200,000 calificaciones en el conjunto de datos (dando lugar a 20 instantáneas). Actualizamos incrementalmente los valores de los roles a medida que las calificaciones ordenadas por tiempo se fusionan en el modelo. Para mantener el experimento manejable computacionalmente, definimos un conjunto de datos de prueba para cada usuario. A medida que las calificaciones ordenadas por tiempo se fusionan en el modelo, etiquetamos un pequeño porcentaje seleccionado al azar (20%) como datos de prueba. En épocas discretas, es decir, después de procesar cada 10,000 valoraciones, calculamos las predicciones para las valoraciones en los datos de prueba, y luego calculamos los valores de rol para las valoraciones utilizadas en las predicciones. Una crítica potencial de esta metodología es que las calificaciones en el conjunto de pruebas nunca se evalúan por sus roles. Superamos esta preocupación repitiendo el experimento, utilizando diferentes semillas aleatorias. La probabilidad de que se considere cada calificación para la evaluación es entonces considerablemente alta: 1 − 0.2n, donde n es el número de veces que se repite el experimento con diferentes semillas aleatorias. Los resultados aquí se basan en n = 4 repeticiones. El rendimiento de los algoritmos de filtrado colaborativo basado en elementos fue ordinario en cuanto a la precisión de la clasificación. La Fig. 5 muestra un gráfico de la precisión y la exhaustividad a medida que las calificaciones se fusionaron en orden temporal en el modelo. El recuerdo siempre fue alto, pero la precisión promedio fue de aproximadamente el 53%. 0 0.2 0.4 0.6 0.8 1 1.2 10000 30000 50000 70000 90000110000130000150000 Calificaciones fusionadas en el modelo Valor Precisión Recuerdo Figura 5: Precisión y recuerdo para el algoritmo de filtrado colaborativo basado en elementos. 5.2 Induciendo buenos exploradores Las calificaciones de un usuario que actúan como exploradores son aquellas que permiten al usuario recibir recomendaciones. Sostenemos que los usuarios con calificaciones que tienen valores de explorador respetables estarán más contentos con el sistema que aquellos con calificaciones con valores de explorador bajos. Ten en cuenta que el algoritmo basado en elementos discutido aquí produce listas de recomendaciones con casi la mitad de los pares en la lista discordantes con las preferencias de los usuarios. Si todos estos pares discordantes son observables por el usuario no está claro, sin embargo, ciertamente esto sugiere que hay una necesidad de poder dirigir a los usuarios hacia elementos cuyas calificaciones mejorarían las listas. La distribución de los valores de los exploradores para la mayoría de las calificaciones de los usuarios es gaussiana con media cero. La Fig. 6 muestra la distribución de frecuencia de los valores de exploración para un usuario de muestra en un momento dado. Observamos que un gran número de calificaciones nunca sirven como exploradores para sus usuarios. Un escenario relevante es cuando el recomendador de Amazon sugiere libros o artículos basados en otros artículos que fueron comprados como regalos. Con la retroalimentación simple relevante del usuario, dichas calificaciones pueden ser identificadas como malas y descartadas de las predicciones futuras. Eliminar a los exploradores malos resultó ser beneficioso para los usuarios individuales, pero la mejora en el rendimiento general fue solo marginal. Una pregunta obvia es si se pueden formar buenos exploradores simplemente calificando películas populares, como sugieren Rashid et al. [9]. Muestran que una combinación de popularidad y entropía de calificaciones identifica los mejores elementos para sugerir a nuevos usuarios cuando se evalúa utilizando MAE. Siguiendo su intuición, esperaríamos ver una correlación más alta entre la entropía de popularidad y los buenos exploradores. Medimos el coeficiente de correlación de Pearson entre los valores de exploración agregados para una película con la popularidad de la película (número de veces que es calificada); y con su medida de popularidad*varianza en diferentes instantáneas del sistema. Se observó que los valores de los exploradores estaban inicialmente en una correlación negativa con la popularidad (Fig. 7), pero se volvieron moderadamente correlacionados a medida que el sistema evolucionaba. Tanto la popularidad como la popularidad*varianza tuvieron un rendimiento similar. Una posible explicación es que no ha habido suficiente tiempo para que las películas populares acumulen calificaciones. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Valor de exploración Frecuencia Figura 6: Distribución de los valores de exploración para un usuario de muestra. -0.4 -0.2 0 0.2 0.4 0.6 0.8 30000 60000 90000 120000 150000 180000 Popularidad Pop*Var Figura 7: Correlación entre el valor de exploración agregado y la popularidad del ítem (calculado en diferentes intervalos). -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 30000 60000 90000 120000 150000 180000 Figura 8: Correlación entre el valor de promotor agregado y la prolificidad del usuario (calculado en diferentes intervalos). Tabla 1: Películas que forman a los mejores exploradores. Mejor Conferencia de Exploradores. Plop. Siendo John Malkovich (1999) 1.00 445 La guerra de las galaxias: Episodio IV - Una nueva esperanza (1977) 0.92 623 La princesa prometida (1987) 0.85 477 El sexto sentido (1999) 0.85 617 Matrix (1999) 0.77 522 Los cazafantasmas (1984) 0.77 441 Casablanca (1942) 0.77 384 El informante (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: El juicio final (1991) 0.69 503 El club de la pelea (1999) 0.69 235 Sueño de fuga (1994) 0.69 445 Corre Lola, corre (1998) 0.69 220 Terminator (1984) 0.62 450 Sospechosos comunes (1995) 0.62 326 Aliens: El regreso (1986) 0.62 385 Con la muerte en los talones (1959) 0.62 245 El fugitivo (1993) 0.62 402 El fin de los días (1999) 0.62 132 En busca del arca perdida (1981) 0.54 540 La lista de Schindler (1993) 0.54 453 Volver al futuro (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien, el octavo pasajero (1979) 0.54 415 El abismo (1989) 0.54 345 2001: Una odisea del espacio (1968) 0.54 358 Dogma (1999) 0.54 228 La sirenita (1989) 0.54 203 Tabla 2: Películas que forman los peores exploradores. Peores exploradores Conf. Plop. Al estudiar la evolución de los valores de los exploradores, podemos identificar películas que aparecen consistentemente en buenos exploradores a lo largo del tiempo. Afirmamos que estas películas serán exploraciones viables para otros usuarios. Encontramos los valores de exploración agregados para todas las películas en intervalos de 10,000 calificaciones cada una. Se dice que una película induce a un buen explorador si la película estaba en el top 100 de la lista ordenada, y a un mal explorador si estaba en el fondo 100 de la misma lista. Se espera que las películas que mantienen consistentemente altas calificaciones a lo largo del tiempo permanezcan en la cima en el futuro. La confianza efectiva en una película m es Cm = Tm − Bm N (8) donde Tm es el número de veces que apareció en el top 100, Bm el número de veces que apareció en el bottom 100, y N es el número de intervalos considerados. Usando esta medida, las pocas películas principales que se espera que atraigan a los mejores exploradores se muestran en la Tabla 1. Las películas que serían malas opciones para los exploradores se muestran en la Tabla 2 con sus respectivas confianzas asociadas. Las popularidades de las películas también se muestran. Aunque las películas más populares aparecen en la lista de buenos exploradores, estas tablas muestran que una elección a ciegas de explorador basada únicamente en la popularidad puede ser potencialmente peligrosa. Curiosamente, el mejor explorador-Siendo John Malkovich-trata sobre un titiritero que descubre un portal hacia una estrella de cine, una película que ha sido descrita de diversas maneras en amazon.com como que te hace sentir mareado, seriamente extraña, comedia con profundidad, tonta, extraña e inventiva. Indicar si a alguien le gusta esta película o no ayuda mucho a situar al usuario en un vecindario adecuado, con preferencias similares. Por otro lado, varios factores pueden haber hecho que una película sea considerada una mala elección, como la marcada variación en las preferencias de los usuarios en el entorno de una película. Dos usuarios pueden tener la misma opinión sobre Lawrence de Arabia, pero pueden diferir notablemente en cómo se sintieron acerca de las otras películas que vieron. Los malos exploradores surgen cuando hay desviación en el comportamiento alrededor de un punto de sincronización común. 5.3 Inducir buenos promotores Las calificaciones que sirven para promover elementos en un sistema de filtrado colaborativo son críticas para permitir que un nuevo elemento sea recomendado a los usuarios. Por lo tanto, inducir buenos promotores es importante para la recomendación en frío. Observamos que la distribución de frecuencia de los valores del promotor para una muestra de calificaciones de películas también es gaussiana (similar a la Figura 6). Esto indica que la promoción de una película se beneficia principalmente de las calificaciones de unos pocos usuarios y no se ve afectada por las calificaciones de la mayoría de los usuarios. Encontramos una fuerte correlación entre el número de calificaciones de un usuario y su valor de promotor agregado. La Fig. 8 muestra la evolución del coeficiente de correlación de Pearson entre la prolificidad de un usuario (número de calificaciones) y su valor promotor agregado. Esperamos que los vendedores falsos conspicuos, al recomendar películas incorrectas a los usuarios, sean desacreditados con valores negativos de promotores agregados y sean fácilmente identificables. Dada esta observación, la regla obvia a seguir al presentar una nueva película es que sea calificada directamente por usuarios prolíficos que posean altos valores promotores agregados. Una nueva película es así lanzada al vecindario de muchas otras películas, mejorando su visibilidad. Ten en cuenta, sin embargo, que un usuario puede haber dejado de usar el sistema hace mucho tiempo. El seguimiento constante de los valores de los promotores permite considerar solo a los usuarios recientes más activos. 5.4 Inducir buenos conectores Dado cómo se caracterizan los exploradores, conectores y promotores, se deduce que las películas que forman parte de los mejores exploradores también forman parte de los mejores conectores. De igual manera, los usuarios que conforman los mejores promotores también forman parte de los mejores conectores. Buenos conectores se inducen al asegurar que un usuario con un alto valor promotor califique una película con un alto valor de explorador. En nuestros experimentos, encontramos que el papel más duradero de una calificación suele ser como conector. Una calificación con un valor de conexión pobre suele verse debido a que su usuario es un mal promotor, o su película es un mal explorador. Tales calificaciones pueden ser eliminadas del proceso de predicción para aportar mejoras marginales a las recomendaciones. En algunos experimentos seleccionados, observamos que al eliminar un conjunto de conectores que se comportaban mal, se logró mejorar el rendimiento general del sistema en un 1.5%. El efecto fue aún mayor en unos pocos usuarios seleccionados que observaron una mejora de más del 10% en precisión sin mucha pérdida en recuperación. 5.5 Monitoreo de la evolución de los roles de calificación. Una de las contribuciones más significativas de nuestro trabajo es la capacidad de modelar la evolución de los sistemas de recomendación, estudiando los roles cambiantes de las calificaciones con el tiempo. El papel y el valor de una calificación pueden cambiar dependiendo de muchos factores como el comportamiento del usuario, la redundancia, los efectos de manipulación o las propiedades del algoritmo de filtrado colaborativo utilizado. Estudiar la dinámica de los roles de calificación en términos de transiciones entre valores buenos, malos y negligibles puede proporcionar información sobre el funcionamiento del sistema de recomendación. Creemos que una visualización continua de estas transiciones mejorará la capacidad de gestionar un sistema de recomendaciones. Clasificamos diferentes estados de calificación como buenos, malos o insignificantes. Considera a un usuario que ha calificado 100 películas en un intervalo particular, de las cuales 20 forman parte del conjunto de pruebas. Si un explorador tiene un valor mayor a 0.005, indica que está involucrado de manera única en al menos 2 predicciones concordantes, lo cual consideraremos como bueno. Por lo tanto, se elige un umbral de 0.005 para clasificar una calificación como buena, mala o insignificante en términos de su valor de explorador, conector y promotor. Por ejemplo, una calificación r, en el tiempo t con un valor de rol triple [0.1, 0.001, −0.01] se clasifica como [explorador +, conector 0, promotor −], donde + indica bueno, 0 indica insignificante, y − indica malo. El crédito positivo que tiene una calificación es una medida de su contribución a la mejora del sistema, y el descrédito es una medida de su contribución al detrimento del sistema. Aunque los roles positivos (y los roles negativos) representan un porcentaje muy pequeño de todas las calificaciones, su contribución supera su tamaño. Por ejemplo, aunque solo el 1.7% de todas las calificaciones fueron clasificadas como buenas, ¡representan el 79% de todo el crédito positivo en el sistema! De manera similar, los exploradores malos representaban solo el 1.4% de todas las calificaciones pero acumulan el 82% de todo el descrédito. Ten en cuenta que los exploradores buenos y malos, juntos, representan solo el 1.4% + 1.7% = 3.1% de las calificaciones, lo que implica que la mayoría de las calificaciones son jugadores de roles insignificantes como exploradores (más sobre esto más adelante). Asimismo, los buenos conectores representaban el 1.2% del sistema y poseían el 30% de todo el crédito positivo. Los conectores defectuosos (0.8% del sistema) representan el 36% de todo el descredito. Los buenos promotores (3% del sistema) poseen el 46% de todo el crédito, mientras que los malos promotores (2%) poseen el 50% de todo el descrédito. Esto reitera que algunas calificaciones influyen en la mayor parte del rendimiento de los sistemas. Por lo tanto, es importante seguir las transiciones entre ellos independientemente de sus pequeños números. A lo largo de diferentes instantáneas, una calificación puede permanecer en el mismo estado o cambiar. Un buen explorador puede convertirse en un mal explorador, un buen promotor puede convertirse en un buen conector, exploradores buenos y malos pueden volverse vestigiales, y así sucesivamente. No es práctico esperar que un sistema de recomendación no tenga calificaciones en roles malos. Sin embargo, basta con ver que las calificaciones en roles malos se conviertan en roles buenos o se conviertan en roles vestigiales. De igual manera, observar que una gran cantidad de roles buenos se convierten en malos es un signo de un inminente fracaso del sistema. Empleamos el principio de episodios no superpuestos [6] para contar tales transiciones. Una secuencia como: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] se interpreta como las transiciones [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 en lugar de [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1. Consulte [6] para obtener más detalles sobre este procedimiento de conteo. Por lo tanto, una calificación puede estar en uno de los 27 estados posibles, y hay alrededor de 272 transiciones posibles. Realizamos una simplificación adicional y utilizamos solo 9 estados, indicando si la calificación es de explorador, promotor o conector, y si tiene un papel positivo, negativo o insignificante. Las calificaciones que sirven para múltiples propósitos se cuentan utilizando múltiples instanciaciones de episodios, pero los estados en sí no se duplican más allá de los 9 estados restringidos. En este modelo, una transición como [+, 0, +] ; [0, +, 0] : 1 se cuenta como [explorador+] ; [explorador0] : 1 [explorador+] ; [conector+] : 1 [explorador+] ; [promotor0] : 1 [conector0] ; [explorador0] : 1 [conector0] ; [explorador+] : 1 [conector0] ; [promotor0] : 1 [promotor+] ; [explorador0] : 1 [promotor+] ; [conector+] : 1 [promotor+] ; [promotor0] : 1 De estas, las transiciones como [pX] ; [q0] donde p = q, X ∈ {+, 0, −} se consideran no interesantes, y solo se cuentan las demás. La Fig. 9 muestra las transiciones principales contadas durante el procesamiento de las primeras 200,000 calificaciones del conjunto de datos de MovieLens. Solo se muestran las transiciones con una frecuencia mayor o igual al 3%. Los porcentajes para cada estado indican la cantidad de calificaciones que se encontraron en esos estados. Consideramos las transiciones de cualquier estado a un estado bueno como saludables, de cualquier estado a un estado malo como no saludables, y de cualquier estado a un estado vestigial como en decadencia. A partir de la Fig. 9, podemos observar: • La mayoría de las calificaciones tienen valores insignificantes, independientemente de su rol. La mayoría de las transiciones implican que tanto las calificaciones buenas como malas se vuelven insignificantes. Scout + (2%) Scout(1.5%) Scout 0 (96.5%) Conector + (1.2%) Conector(0.8%) Conector 0 (98%) Promotor + (3%) Promotor(2%) Promotor 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Saludable No saludable En descomposición Figura 9: Transiciones entre roles de calificación. • El número de calificaciones buenas es comparable con las malas, y las calificaciones cambian de estado con frecuencia, excepto en el caso de los scouts (ver abajo). • Los estados negativos y positivos de los scouts no son alcanzables a través de ninguna transición, lo que indica que estas calificaciones deben comenzar así y no pueden ser forzadas a asumir estos roles. • Los buenos promotores y buenos conectores tienen un período de supervivencia mucho más largo que los scouts. Las transiciones que recurren a estos estados tienen frecuencias del 10% y 15% en comparación con solo el 4% para los exploradores. Los buenos conectores son los más lentos en decaer, mientras que los buenos exploradores decaen más rápido. • Se observan porcentajes saludables en las transiciones entre promotores y conectores. Como se indicó anteriormente, apenas hay transiciones de promotores/conectores a exploradores. Esto indica que, a largo plazo, la calificación de un usuario es más útil para otros (películas u otros usuarios) que para el propio usuario. • Los porcentajes de transiciones saludables superan a las poco saludables, lo que sugiere que el sistema está saludable, aunque solo marginalmente. Ten en cuenta que estos resultados están condicionados por la naturaleza estática del conjunto de datos, que es un conjunto de calificaciones en una ventana de tiempo fija. Sin embargo, un diagrama como el de la Figura 9 es claramente útil para monitorear la salud de un sistema de recomendación. Por ejemplo, se pueden imponer límites aceptables a diferentes tipos de transiciones y, si una transición no cumple con el umbral, el sistema de recomendación o una parte de él puede ser sometido a un escrutinio más detenido. Además, el diagrama de transición de estados del rol sería también el lugar ideal para estudiar los efectos del shilling, un tema que consideraremos en futuras investigaciones. 5.6 Caracterización de vecindarios. Anteriormente vimos que podemos caracterizar el vecindario de calificaciones involucrado en la creación de una lista de recomendaciones L para un usuario. En nuestro experimento, consideramos listas de longitud 30, y muestreamos las listas de aproximadamente el 5% de los usuarios a través de la evolución del modelo (a intervalos de 10,000 valoraciones cada uno) y calculamos sus características de vecindario. Para simplificar nuestra presentación, consideramos el porcentaje de la muestra que cae en una de las siguientes categorías: 1. Usuario inactivo: (SFN(u) = 0) 2. Buenos exploradores, buen vecindario: (SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 3. Buenos exploradores, mal vecindario: (SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) 4. Malos exploradores, buen vecindario: (SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0) 5. Malos exploradores, mal vecindario: (SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) De nuestro conjunto de muestra de 561 usuarios, encontramos que 476 usuarios estaban inactivos. De los 85 usuarios restantes, encontramos que 26 usuarios tenían buenos exploradores y un buen vecindario, 6 tenían malos exploradores y un buen vecindario, 29 tenían buenos exploradores y un mal vecindario, y 24 tenían malos exploradores y un mal vecindario. Por lo tanto, conjeturamos que 59 usuarios (29+24+6) están en peligro de abandonar el sistema. Como remedio, se puede pedir a los usuarios con malos exploradores y un buen vecindario que reconsideren la calificación de algunas películas con la esperanza de mejorar las recomendaciones del sistema. Se espera que el sistema entregue más si diseñan algunos buenos exploradores. Los usuarios con buenos exploradores y un vecindario malo son más difíciles de abordar; esta situación podría implicar la eliminación selectiva de algunas parejas conector-promotor que están causando el daño. Manejar usuarios con malos exploradores y malos vecindarios es un desafío más difícil. Una clasificación de este tipo permite el uso de diferentes estrategias para mejorar la experiencia de un usuario con el sistema dependiendo de su contexto. En trabajos futuros, tenemos la intención de llevar a cabo estudios de campo y estudiar la mejora en el rendimiento de diferentes estrategias para diferentes contextos. 6. CONCLUSIONES Para fomentar la aceptación y la implementación de sistemas de recomendación, necesitamos nuevas herramientas y metodologías para gestionar un sistema de recomendación instalado y desarrollar ideas sobre los roles desempeñados por las calificaciones. Una caracterización detallada en términos de roles de calificación como exploradores, promotores y conectores, como se hace aquí, ayuda en tal esfuerzo. Aunque hemos presentado resultados solo sobre el algoritmo basado en elementos con precisión de rango de lista como métrica, el mismo enfoque descrito aquí se aplica a algoritmos basados en usuarios y otras métricas. En futuras investigaciones, planeamos estudiar sistemáticamente los numerosos parámetros algorítmicos, tolerancias y umbrales de corte empleados aquí y razonar sobre sus efectos en las conclusiones posteriores. También tenemos como objetivo extender nuestra formulación a otros algoritmos de filtrado colaborativo, estudiar el efecto del shilling en la alteración de roles de calificación, realizar estudios de campo y evaluar mejoras en la experiencia del usuario ajustando las calificaciones basadas en sus valores de rol. Finalmente, planeamos desarrollar la idea de extraer la evolución de los patrones de roles de calificación en un sistema de informes y seguimiento para todos los aspectos de la salud del sistema de recomendaciones. 7. REFERENCIAS [1] Cosley, D., Lam, S., Albert, I., Konstan, J., y Riedl, J. ¿Ver es creer? : Cómo las interfaces de los sistemas de recomendación afectan las opiniones de los usuarios. En Proc. CHI (2001), pp. 585-592. [2] Herlocker, J. L., Konstan, J. A., Borchers, A., y Riedl, J. Un marco algorítmico para realizar filtrado colaborativo. En Proc. SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. 

SIGIR (1999), pp. 230-237. [3] Herlocker, J. L., Konstan, J. A., Terveen, L. G., y Riedl, J. T. Evaluación de Sistemas de Recomendación de Filtrado Colaborativo. ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. 

ACM Transactions on Information Systems Vol. 22, 1 (2004), pp. 5-53. [4] Konstan, J. I'm sorry, but you did not provide a sentence to translate. Could you please provide the sentence you would like me to translate to Spanish? Comunicación personal. 2003. [5] Lam, S. K. y Riedl, J. Sistemas de recomendación de shilling para diversión y beneficio. En Actas de la 13ª Conferencia Internacional World Wide Web (2004), ACM Press, pp. 393-402. [6] Laxman, S., Sastry, P. S. y Unnikrishnan, K. P. Descubriendo Episodios Frecuentes y Aprendiendo Modelos Ocultos de Markov: Una Conexión Formal. IEEE Transactions on Knowledge and Data Engineering Vol. 17, 11 (2005), 1505-1517. [7] McLaughlin, M. R., y Herlocker, J. L. Un algoritmo de filtrado colaborativo y una métrica de evaluación que modelan con precisión la experiencia del usuario. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información (2004), pp. 329 - 336. [8] OMahony, M., Hurley, N. J., Kushmerick, N., y Silvestre, G. Recomendación Colaborativa: Un Análisis de Robustez. ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. 

ACM Transactions on Internet Technology Vol. 4, 4 (Nov 2004), pp. 344-377. [9] Rashid, A. M., Albert, I., Cosley, D., Lam, S., McNee, S., Konstan, J. A., y Riedl, J. Conociéndote: Aprendiendo nuevas preferencias de usuario en sistemas de recomendación. En Actas de la Conferencia de Interfaces de Usuario Inteligentes (IUI 2002) (2002), pp. 127-134. [10] Rashid, A. M., Karypis, G., y Riedl, J. Influencia en Sistemas de Recomendación Basados en Calificaciones: Un Enfoque Independiente del Algoritmo. En Proc. de la Conferencia Internacional de Minería de Datos de SIAM (2005). [11] Resnick, P., Iacovou, N., Sushak, M., Bergstrom, P., y Riedl, J. GroupLens: Una Arquitectura Abierta para el Filtrado Colaborativo de Netnews. En Actas de la Conferencia sobre Trabajo Colaborativo con Soporte Informático (CSCW94) (1994), ACM Press, pp. 175-186. [12] Sarwar, B., Karypis, G., Konstan, J., y Reidl, J. Algoritmos de Recomendación de Filtrado Colaborativo Basados en Elementos. En Actas de la Décima Conferencia Internacional de la World Wide Web (WWW10) (2001), pp. 285-295. [13] Schein, A., Popescu, A., Ungar, L., y Pennock, D. Métodos y Métricas para Recomendaciones de Inicio en Frío. En Proc. SIGIR (2002), pp. 253-260. 259

SIGIR (2002), pp. 253-260. 259