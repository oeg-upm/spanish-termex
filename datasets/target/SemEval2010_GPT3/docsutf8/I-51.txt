Aprendizaje y deliberación conjunta a través de la argumentación en sistemas multiagente. Santi Ontañón CCL, Laboratorio de Computación Cognitiva, Instituto de Tecnología de Georgia, Atlanta, GA 303322/0280 santi@cc.gatech.edu Enric Plaza IIIA, Instituto de Investigación en Inteligencia Artificial, CSIC, Consejo Superior de Investigaciones Científicas, Campus UAB, 08193 Bellaterra, Cataluña (España) enric@iiia.csic.es RESUMEN En este artículo presentaremos un marco de argumentación para agentes de aprendizaje (AMAL) diseñado con dos propósitos: (1) para deliberación conjunta, y (2) para aprender a través de la comunicación. El marco AMAL se basa completamente en el aprendizaje a partir de ejemplos: la relación de preferencia de argumentos, la política de generación de argumentos y la política de generación de contraargumentos son técnicas basadas en casos. Para unirse en la deliberación, los agentes de aprendizaje comparten su experiencia formando un comité para decidir sobre alguna decisión conjunta. Experimentalmente demostramos que la argumentación entre comités de agentes mejora tanto el rendimiento individual como el conjunto. Para aprender a través de la comunicación, un agente se involucra en discusiones con otros agentes para contrastar sus hipótesis individuales y recibir contraejemplos; el proceso de argumentación mejora su alcance de aprendizaje y rendimiento individual. Categorías y Descriptores de Asignaturas I.2.6 [Inteligencia Artificial]: Aprendizaje; I.2.11 [Inteligencia Artificial]: Inteligencia Artificial Distribuida-Sistemas Multiagente, Agentes Inteligentes 1. Los marcos de argumentación para sistemas multiagente pueden ser utilizados para diferentes propósitos como la deliberación conjunta, la persuasión, la negociación y la resolución de conflictos. En este artículo presentaremos un marco de argumentación para agentes de aprendizaje, y mostraremos que puede ser utilizado para dos propósitos: (1) deliberación conjunta, y (2) aprendizaje a partir de la comunicación. La deliberación conjunta basada en argumentos implica discutir sobre el resultado de una situación particular o la acción apropiada a tomar en una situación específica. Los agentes de aprendizaje son capaces de aprender de la experiencia, en el sentido de que ejemplos pasados (situaciones y sus resultados) se utilizan para predecir el resultado para la situación actual. Sin embargo, dado que la experiencia de los agentes individuales puede ser limitada, el conocimiento individual y la precisión de la predicción también son limitados. Por lo tanto, los agentes de aprendizaje que son capaces de argumentar sus predicciones individuales con otros agentes pueden alcanzar una mayor precisión en las predicciones después de dicho proceso de argumentación. La mayoría de los marcos de argumentación existentes para sistemas multiagente se basan en lógica deductiva u otra formalización de lógica deductiva diseñada específicamente para respaldar la argumentación, como la lógica por defecto [3]. Por lo general, un argumento se ve como una declaración lógica, mientras que un contraargumento es un argumento ofrecido en oposición a otro argumento; los agentes utilizan una relación de preferencia para resolver argumentos conflictivos. Sin embargo, los marcos de argumentación basados en lógica asumen agentes con conocimiento predefinido y relación de preferencia. En este artículo, nos enfocamos en un marco de Aprendizaje Multiagente basado en Argumentación (AMAL) donde tanto el conocimiento como la relación de preferencia se aprenden a partir de la experiencia. Por lo tanto, consideramos un escenario con agentes que (1) trabajan en el mismo dominio utilizando una ontología compartida, (2) son capaces de aprender a partir de ejemplos y (3) se comunican utilizando un marco argumentativo. Tener capacidades de aprendizaje permite a los agentes utilizar de manera efectiva una forma específica de contraargumento, a saber, el uso de contraejemplos. Los contraejemplos ofrecen la posibilidad de que los agentes aprendan durante el proceso de argumentación. Además, los agentes de aprendizaje permiten técnicas que utilizan la experiencia aprendida para generar argumentos y contraargumentos adecuados. Específicamente, necesitaremos abordar dos problemas: (1) cómo definir una técnica para generar argumentos y contraargumentos a partir de ejemplos, y (2) cómo definir una relación de preferencia sobre dos argumentos en conflicto que han sido inducidos a partir de ejemplos. Este documento presenta un enfoque basado en casos para abordar ambas cuestiones. Los agentes utilizan el razonamiento basado en casos (CBR) [1] para aprender de casos pasados (donde un caso es una situación y su resultado) con el fin de predecir el resultado de una nueva situación. Proponemos un protocolo de argumentación dentro del marco de AMAL que apoya a los agentes en llegar a una predicción conjunta sobre una situación o problema específico; además, el razonamiento necesario para respaldar el proceso de argumentación también se basará en casos. En particular, presentamos dos medidas basadas en casos, una para generar los argumentos y contraargumentos adecuados para una situación particular y otra para determinar la relación de preferencia entre los argumentos. Finalmente, evaluamos (1) si la argumentación entre agentes de aprendizaje puede producir una predicción conjunta que mejore el rendimiento de aprendizaje individual y (2) si el aprendizaje a partir de los contraejemplos transmitidos durante el proceso de argumentación aumenta el rendimiento individual con precisamente aquellos casos que se utilizan mientras discuten entre ellos. El documento está estructurado de la siguiente manera. La sección 2 discute la relación entre la argumentación, la colaboración y el aprendizaje. Luego, la Sección 3 introduce nuestro marco de trabajo de CBR multiagente (MAC) y el concepto de predicción justificada. Después de eso, la Sección 4 define formalmente nuestro marco de argumentación. Las secciones 5 y 6 presentan nuestra relación de preferencia basada en casos y las políticas de generación de argumentos, respectivamente. Más adelante, la Sección 7 presenta el protocolo de argumentación en nuestro marco AMAL. Después de eso, la Sección 8 presenta una ejemplificación del marco de argumentación. Finalmente, la Sección 9 presenta una evaluación empírica de nuestras dos hipótesis principales. El artículo concluye con secciones de trabajo relacionado y conclusiones. 2. ARGUMENTACIÓN, COLABORACIÓN Y APRENDIZAJE Tanto el aprendizaje como la colaboración son formas en las que un agente puede mejorar el rendimiento individual. De hecho, existe un claro paralelismo entre el aprendizaje y la colaboración en sistemas multiagentes, ya que ambos son formas en las que los agentes pueden lidiar con sus deficiencias. Mostremos cuáles son las principales motivaciones que un agente puede tener para aprender o colaborar. • Motivaciones para aprender: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles. • Motivaciones para colaborar: - Aumentar la calidad de la predicción, - Aumentar la eficiencia, - Ampliar el rango de problemas resolubles, - Ampliar el rango de recursos accesibles. Al observar las listas de motivación anteriores, podemos ver fácilmente que el aprendizaje y la colaboración están muy relacionados en los sistemas multiagente. De hecho, con la excepción del último elemento en la lista de motivaciones para colaborar, son dos extremos de un continuo de estrategias para mejorar el rendimiento. Un agente puede optar por aumentar el rendimiento mediante el aprendizaje, la colaboración o encontrando un punto intermedio que combine el aprendizaje y la colaboración para mejorar el rendimiento. En este artículo propondremos AMAL, un marco de argumentación para agentes de aprendizaje, y también mostraremos cómo AMAL puede ser utilizado tanto para aprender a través de la comunicación como para resolver problemas de manera colaborativa: • Los agentes pueden resolver problemas de manera colaborativa al participar en un proceso de argumentación sobre la predicción para la situación actual. Utilizando esta colaboración, la predicción puede realizarse de una manera más informada, ya que se ha tenido en cuenta la información conocida por varios agentes. Los agentes también pueden aprender de la comunicación con otros agentes al participar en un proceso de argumentación. Los agentes que participan en estos procesos de argumentación pueden aprender de los argumentos y contraejemplos recibidos de otros agentes, y utilizar esta información para predecir los resultados de situaciones futuras. En el resto de este documento propondremos un marco de argumentación y mostraremos cómo puede ser utilizado tanto para el aprendizaje como para la resolución de problemas de manera colaborativa. SISTEMAS CBR MULTIAGENTE Un Sistema de Razonamiento Basado en Casos Multiagente (MAC) M = {(A1, C1), ..., (An, Cn)} es un sistema multiagente compuesto por A = {Ai, ..., An}, un conjunto de agentes CBR, donde cada agente Ai ∈ A posee una base de casos individual Ci. Cada agente individual Ai en un MAC es completamente autónomo y cada agente Ai solo tiene acceso a su base de casos individual y privada Ci. Un conjunto de casos Ci = {c1, ..., cm} es una colección de casos. Los agentes en un sistema MAC pueden resolver problemas de forma individual, pero también pueden colaborar con otros agentes para resolver problemas. En este marco, nos limitaremos a tareas analíticas, es decir, tareas como la clasificación, donde la solución de un problema se logra seleccionando una clase de solución de un conjunto enumerado de clases de solución. En lo siguiente, notaremos el conjunto de todas las clases de soluciones por S = {S1, ..., SK}. Por lo tanto, un caso c = P, S es una tupla que contiene una descripción del caso P y una clase de solución S ∈ S. En lo siguiente, utilizaremos indistintamente los términos problema y descripción del caso. Además, utilizaremos la notación de punto para referirnos a elementos dentro de una tupla; por ejemplo, para referirnos a la clase de solución de un caso c, escribiremos c.S. Por lo tanto, decimos que un grupo de agentes realiza una deliberación conjunta cuando colaboran para encontrar una solución conjunta mediante un proceso de argumentación. Sin embargo, para hacerlo, un agente debe ser capaz de justificar su predicción a los otros agentes (es decir, generar un argumento para su solución predicha que pueda ser examinado y criticado por los otros agentes). La siguiente sección aborda este tema. 3.1 Predicciones Justificadas Tanto los sistemas expertos como los sistemas CBR pueden tener un componente de explicación [14] encargado de justificar por qué el sistema ha proporcionado una respuesta específica al usuario. La línea de razonamiento del sistema puede ser examinada por un experto humano, aumentando así la fiabilidad del sistema. La mayoría de los trabajos existentes sobre generación de explicaciones se centran en generar explicaciones para ser proporcionadas al usuario. Sin embargo, en nuestro enfoque utilizamos explicaciones (o justificaciones) como una herramienta para mejorar la comunicación y coordinación entre agentes. Estamos interesados en las justificaciones ya que pueden ser utilizadas como argumentos. Para ese propósito, nos beneficiaremos de la capacidad de algunos métodos de aprendizaje automático para proporcionar justificaciones. Una justificación construida por un método CBR después de determinar que la solución de un problema particular P fue Sk es una descripción que contiene la información relevante del problema P que el método CBR ha considerado para predecir Sk como la solución de P. En particular, los métodos CBR funcionan recuperando casos similares al problema en cuestión, y luego reutilizando sus soluciones para el problema actual, esperando que dado que el problema y los casos son similares, las soluciones también serán similares. Por lo tanto, si un método CBR ha recuperado un conjunto de casos C1, ..., Cn para resolver un problema particular P, la justificación construida contendrá la información relevante del problema P que hizo que el sistema CBR recuperara ese conjunto particular de casos, es decir, contendrá la información relevante que P y C1, ..., Cn tienen en común. Por ejemplo, la Figura 1 muestra una justificación construida por un sistema CBR para un problema de juguete (en las siguientes secciones mostraremos justificaciones para problemas reales). En la figura, un problema tiene dos atributos (Semáforo y Coches_pasando), el mecanismo de recuperación del sistema CBR nota que al considerar solo el atributo Semáforo, puede recuperar dos casos que predicen la misma solución: esperar. Por lo tanto, dado que solo se ha utilizado este atributo, es el único que aparece en la justificación. Los valores del resto de atributos son irrelevantes, ya que, sea cual sea su valor, la clase de solución habría sido la misma. 976 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Problema Semáforo: rojo Pasan coches: no Caso 1 Semáforo: rojo Pasan coches: no Solución: esperar Caso 3 Semáforo: rojo Pasan coches: sí Solución: esperar Caso 4 Semáforo: verde Pasan coches: sí Solución: esperar Caso 2 Semáforo: verde Pasan coches: no Solución: cruzar Casos recuperados Solución: esperar Justificación Semáforo: rojo Figura 1: Un ejemplo de generación de justificación en un sistema de RCB. Observa que, dado que la única característica relevante para decidir es Traffic_light (la única utilizada para recuperar casos), es la única que aparece en la justificación. En general, el significado de una justificación es que todos (o la mayoría) de los casos en la base de casos de un agente que satisfacen la justificación (es decir, todos los casos que están subsumidos por la justificación) pertenecen a la clase de solución predicha. En el resto del documento, usaremos para denotar la relación de subsumpción. En nuestro trabajo, utilizamos LID [2], un método CBR capaz de construir justificaciones simbólicas como la ejemplificada en la Figura 1. Cuando un agente proporciona una justificación para una predicción, el agente genera una predicción justificada: DEFINICIÓN 3.1. Una Predicción Justificada es una tupla J = A, P, S, D donde el agente A considera que S es la solución correcta para el problema P, y esa predicción está justificada por una descripción simbólica D tal que J.D J.P. Las justificaciones pueden tener muchos usos para los sistemas de RCB [8, 9]. En este documento, vamos a utilizar justificaciones como argumentos, con el fin de permitir que los agentes de aprendizaje participen en procesos de argumentación. 4. ARGUMENTOS Y CONTRAARGUMENTOS Para nuestros propósitos, un argumento α generado por un agente A está compuesto por una afirmación S y alguna evidencia D que respalda a S como correcta. En el resto de esta sección veremos cómo esta definición general de argumento puede ser concretada en un tipo específico de argumentos que los agentes pueden generar. En el contexto de los sistemas MAC, los agentes discuten sobre predicciones para nuevos problemas y pueden proporcionar dos tipos de información: a) casos específicos P, S, y b) predicciones justificadas: A, P, S, D. Usando esta información, podemos definir tres tipos de argumentos: predicciones justificadas, contraargumentos y contraejemplos. Una predicción justificada α es generada por un agente Ai para argumentar que Ai cree que la solución correcta para un problema dado P es α.S, y la evidencia proporcionada es la justificación α.D. En el ejemplo representado en la Figura 1, un agente Ai puede generar el argumento α = Ai, P, Esperar, (Semáforo = rojo), lo que significa que el agente Ai cree que la solución correcta para P es Esperar porque el atributo Semáforo es igual a rojo. Un contraargumento β es un argumento ofrecido en oposición a otro argumento α. En nuestro marco de trabajo, un contraargumento consiste en una predicción justificada Aj, P, S, D generada por un agente Aj con la intención de refutar un argumento α generado por otro agente Ai, que respalda una clase de solución S diferente a la de α.S para el problema en cuestión y justifica esto con una justificación D. En el ejemplo de la Figura 1, si un agente genera el argumento α = Ai, P, Caminar, (Pasando_autos = no), un agente que piensa que la solución correcta es Esperar podría responder con el contraargumento β = Aj, P, Esperar, (Pasando_autos = no ∧ Semáforo = rojo), lo que significa que, aunque no hay autos pasando, el semáforo está en rojo y la calle no se puede cruzar. Un contraejemplo c es un caso que contradice un argumento α. Por lo tanto, un contraejemplo también es un contraargumento, uno que afirma que un argumento específico α no siempre es verdadero, y la evidencia proporcionada es el caso c. Específicamente, para que un caso c sea un contraejemplo de un argumento α, se deben cumplir las siguientes condiciones: α.D c y α.S = c.S, es decir, el caso debe satisfacer la justificación α.D y la solución de c debe ser diferente a la predicha por α. Al intercambiar argumentos y contraargumentos (incluyendo contraejemplos), los agentes pueden discutir sobre la solución correcta de un problema dado, es decir, pueden participar en un proceso de deliberación conjunto. Sin embargo, para hacerlo, necesitan un protocolo de interacción específico, una relación de preferencia entre argumentos contradictorios y una política de decisión para generar contraargumentos (incluidos contraejemplos). En las siguientes secciones presentaremos estos elementos. 5. RELACIÓN DE PREFERENCIA Un argumento específico proporcionado por un agente podría no ser consistente con la información conocida por otros agentes (o incluso con parte de la información conocida por el agente que ha generado la justificación debido al ruido en los datos de entrenamiento). Por esa razón, vamos a definir una relación de preferencia sobre predicciones justificadas contradictorias basadas en casos. Básicamente, definiremos una medida de confianza para cada predicción justificada (que tenga en cuenta los casos poseídos por cada agente), y la predicción justificada con la mayor confianza será la preferida. La idea detrás de la confianza basada en casos es contar cuántos de los casos en una base de casos individual respaldan una predicción justificada, y cuántos de ellos son ejemplos en contra de ella. Cuanto más casos de respaldo, mayor es la confianza; y cuanto más contrajemplos, menor es la confianza. Específicamente, para evaluar la confianza de una predicción justificada α, un agente obtiene el conjunto de casos en su base de casos individuales que están subsumidos por α.D. Con ellos, un agente Ai obtiene los valores Y (sí) y N (no): • Y Ai α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que pertenecen a la clase de solución α.S, • NAi α = |{c ∈ Ci| α.D c.P ∧ α.S = c.S}| es el número de casos en la base de casos del agente subsumidos por la justificación α.D que no pertenecen a esa clase de solución. El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 977 + + + + + +  - - + Figura 2: La confianza de los argumentos se evalúa contrastándolos con las bases de casos de los agentes. Un agente estima la confianza de un argumento como: CAi (α) = Y Ai α 1 + Y Ai α + NAi α es decir, la confianza en una predicción justificada es el número de casos que la respaldan dividido por el número de casos que la respaldan más los contraejemplos. Observa que sumamos 1 al denominador, esto es para evitar otorgar confianzas excesivamente altas a predicciones justificadas cuya confianza se ha calculado utilizando un número pequeño de casos. Ten en cuenta que esta corrección sigue la misma idea que la corrección de Laplace para estimar probabilidades. La Figura 2 ilustra la evaluación individual de la confianza de un argumento, en particular, se encuentran tres casos de respaldo y un contraejemplo en la base de casos de los agentes Ai, lo que da una confianza estimada de 0.6. Además, también podemos definir la confianza conjunta de un argumento α como la confianza calculada utilizando los casos presentes en las bases de casos de todos los agentes en el grupo: C(α) = Π Ai α / (1 + Σ Ai α + NAi α). Nótese que, para calcular conjuntamente la confianza conjunta, los agentes solo tienen que hacer públicos los valores de aprobación y rechazo calculados localmente para un argumento dado. En nuestro marco de trabajo, los agentes utilizan esta confianza conjunta como la relación de preferencia: una predicción justificada α se prefiere sobre otra β si C(α) ≥ C(β). GENERACIÓN DE ARGUMENTOS En nuestro marco de trabajo, los argumentos son generados por los agentes a partir de casos, utilizando métodos de aprendizaje. Cualquier método de aprendizaje capaz de proporcionar una predicción justificada puede ser utilizado para generar argumentos. Por ejemplo, los árboles de decisión y LID [2] son métodos de aprendizaje adecuados. Específicamente, en los experimentos reportados en este artículo, los agentes utilizan LID. Por lo tanto, cuando un agente desea generar un argumento que respalde que una clase de solución específica es la solución correcta para un problema P, genera una predicción justificada como se explica en la Sección 3.1. Por ejemplo, la Figura 3 muestra una justificación real generada por LID después de resolver un problema P en el dominio de la identificación de esponjas marinas. En particular, la Figura 3 muestra cómo cuando un agente recibe un nuevo problema para resolver (en este caso, un nuevo esponja para determinar su orden), el agente utiliza LID para generar un argumento (que consiste en una predicción justificada) utilizando los casos en la base de casos del agente. La justificación mostrada en la Figura 3 se puede interpretar diciendo que la solución predicha es hadromerida porque la forma lisa de las megascleras del esqueleto espicular de la esponja es de tipo tilostilo, el esqueleto espicular de la esponja no tiene una longitud uniforme y no hay gemas en las características externas de la esponja. Por lo tanto, el argumento generado será α = A1, P, hadromerida, D1. 6.1 Generación de Contraargumentos Como se mencionó anteriormente, los agentes pueden intentar refutar argumentos generando contraargumentos o encontrando contraejemplos. Permítanos explicar cómo pueden ser generados. Un agente Ai quiere generar un contraargumento β para rebatir un argumento α cuando α está en contradicción con la base de casos locales de Ai. Además, al generar dicho contraargumento β, Ai espera que β sea preferido sobre α. Con ese propósito, presentaremos una política específica para generar contraargumentos basados en el criterio de especificidad [10]. El criterio de especificidad se utiliza ampliamente en marcos deductivos para la argumentación, y establece que entre dos argumentos en conflicto, se debe preferir el más específico ya que, en principio, está más informado. Por lo tanto, se espera que los contraargumentos generados basados en el criterio de especificidad sean preferibles (ya que están más informados) que los argumentos que intentan refutar. Sin embargo, no hay garantía de que tales contraargumentos siempre ganen, ya que, como hemos indicado en la Sección 5, los agentes en nuestro marco utilizan una relación de preferencia basada en la confianza conjunta. Además, uno podría pensar que sería mejor que los agentes generaran contraargumentos basados en la relación de preferencia de confianza conjunta; sin embargo, no es obvio cómo generar contraargumentos basados en la confianza conjunta de manera eficiente, ya que se requiere colaboración para evaluar la confianza conjunta. Por lo tanto, el agente que genera el contraargumento debe comunicarse constantemente con los otros agentes en cada paso del algoritmo de inducción utilizado para generar contraargumentos (actualmente una de nuestras líneas de investigación futuras). Por lo tanto, en nuestro marco de trabajo, cuando un agente quiere generar un contraargumento β a un argumento α, β tiene que ser más específico que α (es decir, α.D < β.D). La generación de contraargumentos utilizando el criterio de especificidad impone algunas restricciones sobre el método de aprendizaje, aunque LID o ID3 pueden adaptarse fácilmente para esta tarea. Por ejemplo, LID es un algoritmo que genera una descripción desde cero y añade heurísticamente características a ese término. Por lo tanto, en cada paso, la descripción se vuelve más específica que en el paso anterior, y el número de casos que se incluyen en esa descripción se reduce. Cuando la descripción cubre solo (o casi solo) casos de una única clase de solución, LID termina y predice esa clase de solución. Para generar un contraargumento a un argumento α, LID solo tiene que usar como punto de partida la descripción α.D en lugar de empezar desde cero. De esta manera, la justificación proporcionada por LID siempre estará subsumida por α.D, y por lo tanto, el contraargumento resultante será más específico que α. Sin embargo, hay que tener en cuenta que LID a veces puede no ser capaz de generar contraargumentos, ya que LID puede no ser capaz de especializar aún más la descripción α.D, o porque el agente Ai no tiene ningún caso en Ci que esté subsumido por α.D. La Figura 4 muestra cómo un agente A2 que no estaba de acuerdo con el argumento mostrado en la Figura 3, genera un contraargumento utilizando LID. Además, la Figura 4 muestra la generación de un contraargumento β1 2 para el argumento α0 1 (en la Figura 3) que es una especialización de α0 1. 978 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Solución: hadromerida Justificación: D1 Esponja Esqueleto espiculado Características externas Características externas Gemmulas: no Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Caso Base de A1 LID Nueva esponja P Figura 3: Ejemplo de una justificación real generada por LID en el conjunto de datos de esponjas marinas. Específicamente, en nuestros experimentos, cuando un agente Ai quiere refutar un argumento α, utiliza la siguiente política: 1. El Agente Ai utiliza LID para intentar encontrar un contraargumento β más específico que α; si se encuentra, β se envía al otro agente como un contraargumento de α. 2. Si no se encuentra, entonces Ai busca un contraejemplo c ∈ Ci de α. Si se encuentra un caso c, entonces c se envía al otro agente como un contraejemplo de α. Si no se encuentran contraejemplos, entonces Ai no puede refutar el argumento α. 7. APRENDIZAJE MULTIAGENTE BASADO EN ARGUMENTACIÓN El protocolo de interacción de AMAL permite que un grupo de agentes A1, ..., An deliberen sobre la solución correcta de un problema P mediante un proceso de argumentación. Si el proceso de argumentación llega a una solución consensuada, la deliberación conjunta termina; de lo contrario, se utiliza una votación ponderada para determinar la solución conjunta. Además, AMAL también permite a los agentes aprender de los contraejemplos recibidos de otros agentes. El protocolo AMAL consiste en una serie de rondas. En la ronda inicial, cada agente declara cuál es su predicción individual para P. Luego, en cada ronda, un agente puede intentar refutar la predicción hecha por cualquiera de los otros agentes. El protocolo utiliza un mecanismo de paso de token para que los agentes (uno a la vez) puedan enviar contraargumentos o contraejemplos si no están de acuerdo con la predicción hecha por cualquier otro agente. Específicamente, a cada agente se le permite enviar un contraargumento o contraejemplo cada vez que recibe el token (nota que esta restricción es solo para simplificar el protocolo, y que no restringe la cantidad de contraargumentos que un agente puede enviar, ya que pueden ser retrasados para rondas posteriores). Cuando un agente recibe un contraargumento o contraejemplo, informa a los otros agentes si acepta el contraargumento (y cambia su predicción) o no. Además, los agentes también tienen la oportunidad de responder a los contraargumentos cuando reciben el token, intentando generar un contraargumento al contraargumento. Cuando todos los agentes hayan tenido el token una vez, el token regresa al primer agente, y así sucesivamente. Si en algún momento del protocolo, todos los agentes están de acuerdo o durante las últimas n rondas ningún agente ha generado ninguna contra-argumentación, el protocolo termina. Además, si al final de la argumentación los agentes no han llegado a un acuerdo, entonces se utiliza un mecanismo de votación que utiliza la confianza de cada predicción como pesos para decidir la solución final (Así, AMAL sigue el mismo mecanismo que los comités humanos, primero cada miembro individual de un comité expone sus argumentos y discute los de los otros miembros (deliberación conjunta), y si no se alcanza un consenso, entonces se requiere un mecanismo de votación). En cada iteración, los agentes pueden usar los siguientes performativos: • afirmar(α): la predicción justificada que se mantendrá durante la próxima ronda será α. Un agente solo puede tener una única predicción en cada ronda, por lo tanto, si se envían múltiples afirmaciones, solo se considera la última como la predicción actualmente mantenida. • rebut(β, α): el agente ha encontrado un contraargumento β a la predicción α. Definiremos Ht = αt 1, ..., αt n como las predicciones que cada uno de los n agentes tiene en una ronda t. Además, también definiremos contradict(αt i) = {α ∈ Ht|α.S = αt i.S} como el conjunto de argumentos contradictorios para un agente Ai en una ronda t, es decir, el conjunto de argumentos en la ronda t que respaldan una clase de solución diferente a la de αt i. El protocolo se inicia porque uno de los agentes recibe un problema P que debe ser resuelto. Después de eso, el agente informa a todos los demás agentes sobre el problema P a resolver, y el protocolo comienza: 1. En la ronda t = 0, cada uno de los agentes resuelve individualmente P y construye una predicción justificada utilizando su propio método de RBC. Entonces, cada agente Ai envía el assertivo performativo assert(α0 i) a los otros agentes. Por lo tanto, los agentes conocen H0 = α0 i , ..., α0 n. Una vez que todas las predicciones hayan sido enviadas, el token se le da al primer agente A1. 2. En cada ronda t (distinta de 0), los agentes verifican si sus argumentos en Ht coinciden. Si lo hacen, el protocolo avanza al paso 5. Además, si durante las últimas n rondas ningún agente ha enviado ningún contraejemplo o contraargumento, el protocolo también avanza al paso 5. De lo contrario, el agente Ai propietario del token intenta generar un contraargumento para cada uno de los argumentos opuestos en contradict(αt i) ⊆ Ht (ver Sección 6.1). Entonces, se selecciona el contraargumento βt i contra la predicción αt j con la menor confianza C(αt j) (ya que αt j es la predicción más probable de ser refutada con éxito). • Si βt i es un contraargumento, entonces, Ai compara localmente αt i con βt i evaluando su confianza contra su base de casos individuales Ci (ver Sección 5) (nótese que Ai está comparando su argumento previo con el contraargumento que Ai mismo acaba de generar y que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 979 Esponja Esqueleto espiculado Características externas Características externas Gemas: no Crecimiento: Esqueleto espiculado Megascleres Longitud uniforme: no Megascleres Forma lisa: tilostilo Crecimiento Crecimiento: masivo Caso Base de A2 LID Solución: astrophorida Justificación: D2 Figura 4: Generación de un contraargumento utilizando LID en el conjunto de datos de esponjas. para enviar a Aj. Si CAi (βt i) > CAi (αt i), entonces Ai considera que βt i es más fuerte que su argumento anterior, cambia su argumento a βt i enviando assert(βt i) al resto de los agentes (la intuición detrás de esto es que dado que un contraargumento también es un argumento, Ai verifica si el nuevo contraargumento es un mejor argumento que el que sostenía anteriormente) y rebut(βt i, αt j) a Aj. De lo contrario (es decir, Si CAi (βt i ) ≤ CAi (αt i), Ai solo enviará rebut(βt i , αt j) a Aj. En cualquiera de las dos situaciones, el protocolo avanza al paso 3. • Si βt i es un contraejemplo c, entonces Ai envía una refutación(c, αt j) a Aj. El protocolo avanza al paso 4. • Si Ai no puede generar ningún contraargumento o contraejemplo, el token se envía al siguiente agente, comienza una nueva ronda t + 1, y el protocolo avanza al estado 2. 3. El agente Aj que ha recibido el contraargumento βt i, lo compara localmente con su propio argumento, αt j, evaluando localmente su confianza. Si CAj (βt i ) > CAj (αt j), entonces Aj aceptará el contraargumento como más fuerte que su propio argumento, y enviará assert(βt i ) a los otros agentes. De lo contrario (es decir, Si CAj (βt i ) ≤ CAj (αt j)), Aj no aceptará el contraargumento e informará a los otros agentes en consecuencia. Cualquiera de las dos situaciones inicia una nueva ronda t + 1, Ai envía el token al siguiente agente y el protocolo vuelve al estado 2. 4. El agente Aj que ha recibido el contraejemplo c lo retiene en su base de casos y genera un nuevo argumento αt+1 j que tiene en cuenta c, e informa al resto de los agentes enviando assert(αt+1 j) a todos ellos. Entonces, Ai envía el token al siguiente agente, comienza una nueva ronda t + 1 y el protocolo vuelve al paso 2. 5. El protocolo finaliza generando una predicción conjunta, de la siguiente manera: si los argumentos en Ht están de acuerdo, entonces su predicción es la predicción conjunta; de lo contrario, se utiliza un mecanismo de votación para decidir la predicción conjunta. El mecanismo de votación utiliza la medida de confianza conjunta como los pesos de votación, de la siguiente manera: S = arg max Sk∈S αi∈Ht|αi.S=Sk C(αi) Además, para evitar iteraciones infinitas, si un agente envía dos veces el mismo argumento o contraargumento al mismo agente, el mensaje no se considera. 8. EJEMPLIFICACIÓN Consideremos un sistema compuesto por tres agentes A1, A2 y A3. Uno de los agentes, A1, recibe un problema P para resolver y decide usar AMAL para resolverlo. Por esa razón, invita a A2 y A3 a participar en el proceso de argumentación. Aceptan la invitación y comienza el protocolo de argumentación. Inicialmente, cada agente genera su predicción individual para P y la transmite a los otros agentes. Por lo tanto, todos ellos pueden calcular H0 = α0 1, α0 2, α0 3. En particular, en este ejemplo: • α0 1 = A1, P, hadromerida, D1 • α0 2 = A2, P, astrophorida, D2 • α0 3 = A3, P, axinellida, D3 A1 comienza poseyendo el token e intenta generar argumentos en contra de α0 2 y α0 3, pero no tiene éxito; sin embargo, tiene un contraejemplo c13 para α0 3. Por lo tanto, A1 envía el mensaje rebut(c13, α03) a A3. A3 incorpora c13 en su base de casos e intenta resolver el problema P nuevamente, ahora teniendo en cuenta c13. A3 elabora la predicción justificada α1 3 = A3, P, hadromerida, D4, y la transmite al resto de los agentes con el mensaje assert(α1 3). Por lo tanto, todos ellos conocen el nuevo H1 = α0 1, α0 2, α1 3. La ronda 1 comienza y A2 obtiene el token. A2 intenta generar contraargumentos para α0 1 y α1 3 y solo logra generar un contraargumento β1 2 = A2, P, astrophorida, D5 contra α1 3. El contraargumento se envía a A3 con el mensaje rebut(β1 2 , α1 3). El agente A3 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza individual del contraargumento β1 2 es menor que la confianza local de α1 3. Por lo tanto, A3 no acepta el contraargumento, y así H2 = α0 1, α0 2, α1 3. La ronda 2 comienza y A3 recibe el token. A3 genera un contraargumento β2 3 = A3, P, hadromerida, D6 para α0 2 y lo envía a A2 con el mensaje refutar(β2 3 , α0 2). El agente A2 recibe el contraargumento y evalúa su confianza local. El resultado es que la confianza local del contraargumento β2 3 es mayor que la confianza local de α0 2. Por lo tanto, A2 acepta el contraargumento e informa al resto de los agentes con el mensaje assert(β2 3). Después de eso, H3 = α0 1, β2 3 , α1 3. En la Ronda 3, dado que todos los agentes están de acuerdo (todas las predicciones justificadas en H3 predicen hadromerida como la clase de solución), el protocolo finaliza, y A1 (el agente que recibió el problema) considera hadromerida como la solución conjunta para el problema P. 9. EVALUACIÓN EXPERIMENTAL 980 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) SPONGE 75 77 79 81 83 85 87 89 91 2 3 4 5 AMAL Votación Individual SOYBEAN 55 60 65 70 75 80 85 90 2 3 4 5 AMAL Votación Individual Figura 5: Precisión individual y conjunta para 2 a 5 agentes. En esta sección evaluamos empíricamente el marco de argumentación AMAL. Hemos realizado experimentos en dos conjuntos de datos diferentes: soja (del repositorio de aprendizaje automático de UCI) y esponja (un conjunto de datos relacional). El conjunto de datos de soja tiene 307 ejemplos y 19 clases de solución, mientras que el conjunto de datos de esponja tiene 280 ejemplos y 3 clases de solución. En una ejecución experimental, el conjunto de datos se divide en 2 conjuntos: el conjunto de entrenamiento y el conjunto de prueba. Los ejemplos del conjunto de entrenamiento están distribuidos entre 5 agentes diferentes sin replicación, es decir, no hay ningún ejemplo compartido por dos agentes. En la etapa de pruebas, los problemas en el conjunto de pruebas llegan aleatoriamente a uno de los agentes, y su objetivo es predecir la solución correcta. Los experimentos están diseñados para probar dos hipótesis: (H1) que la argumentación es un marco útil para la deliberación conjunta y puede mejorar sobre otros métodos típicos como la votación; y (H2) que el aprendizaje a través de la comunicación mejora el rendimiento individual de un agente de aprendizaje que participa en un proceso de argumentación. Además, también esperamos que la mejora lograda a través de la argumentación aumente a medida que aumente el número de agentes que participan en la argumentación (ya que se tomará en cuenta más información). En relación con H1 (la argumentación es un marco útil para la deliberación conjunta), realizamos 4 experimentos, utilizando 2, 3, 4 y 5 agentes respectivamente (en todos los experimentos cada agente tiene un 20% de los datos de entrenamiento, ya que el entrenamiento siempre se distribuye entre 5 agentes). La Figura 5 muestra el resultado de esos experimentos en los conjuntos de datos de esponjas y soja. La precisión de la clasificación se representa en el eje vertical, y en el eje horizontal se muestra el número de agentes que participaron en los procesos de argumentación. Para cada número de agentes, se muestran tres barras: individual, Voting y AMAL. La barra individual muestra la precisión promedio de las predicciones de los agentes individuales; la barra de votación muestra la precisión promedio de la predicción conjunta lograda mediante votación pero sin argumentación; y finalmente la barra de AMAL muestra la precisión promedio de la predicción conjunta utilizando argumentación. Los resultados mostrados son el promedio de 5 ejecuciones de validación cruzada de 10 pliegues. La Figura 5 muestra que la colaboración (votación y AMAL) supera el rendimiento de la resolución individual de problemas. Además, como esperábamos, la precisión mejora a medida que más agentes colaboran, ya que se tiene en cuenta más información. También podemos ver que AMAL siempre supera al voto estándar, demostrando que las decisiones conjuntas se basan en una mejor información proporcionada por el proceso de argumentación. Por ejemplo, la precisión conjunta para 2 agentes en el conjunto de datos de esponjas es del 87.57% para AMAL y del 86.57% para la votación (mientras que la precisión individual es solo del 80.07%). Además, la mejora lograda por AMAL sobre Voting es aún mayor en el conjunto de datos de soja. La razón es que el conjunto de datos de soja es más difícil (en el sentido de que los agentes necesitan más datos para producir buenas predicciones). Estos resultados experimentales muestran que AMAL aprovecha de manera efectiva la oportunidad de mejora: la precisión es mayor solo porque más agentes han cambiado su opinión durante la argumentación (de lo contrario, lograrían el mismo resultado que la Votación). En relación con H2 (el aprendizaje a partir de la comunicación en procesos de argumentación mejora la predicción individual), llevamos a cabo el siguiente experimento: inicialmente, distribuimos un 25% del conjunto de entrenamiento entre los cinco agentes; después, el resto de los casos en el conjunto de entrenamiento se envía a los agentes uno por uno; cuando un agente recibe un nuevo caso de entrenamiento, tiene varias opciones: el agente puede descartarlo, el agente puede retenerlo, o el agente puede usarlo para participar en un proceso de argumentación. La Figura 6 muestra el resultado de ese experimento para los dos conjuntos de datos. La Figura 6 contiene tres gráficos, donde NL (no aprendizaje) muestra la precisión de un agente sin aprendizaje alguno; L (aprendizaje) muestra la evolución de la precisión de clasificación individual cuando los agentes aprenden reteniendo los casos de entrenamiento que reciben individualmente (notar que cuando todos los casos de entrenamiento han sido retenidos al 100%, la precisión debería ser igual a la de la Figura 5 para los agentes individuales); y finalmente LFC (aprendizaje a partir de la comunicación) muestra la evolución de la precisión de clasificación individual de los agentes de aprendizaje que también aprenden reteniendo los contraejemplos recibidos durante la argumentación (es decir, aprenden tanto de ejemplos de entrenamiento como de contraejemplos). La Figura 6 muestra que si un agente Ai también aprende de la comunicación, Ai puede mejorar significativamente su rendimiento individual con solo un pequeño número de casos adicionales (seleccionados como contraejemplos relevantes para Ai durante la argumentación). Por ejemplo, en el conjunto de datos de soja, los agentes individuales han logrado una precisión del 70.62% cuando también aprenden de la comunicación, en comparación con una precisión del 59.93% cuando solo aprenden de su experiencia individual. El número de casos aprendidos a través de la comunicación depende de las propiedades del conjunto de datos: en el conjunto de datos de esponjas, los agentes solo han retenido muy pocos casos adicionales y han mejorado significativamente la precisión individual; es decir, retienen en promedio 59.96 casos (en comparación con los 50.4 casos retenidos si no aprenden de la comunicación). En el conjunto de datos de soja se aprenden más contraejemplos para mejorar significativamente la precisión individual, es decir, retienen en promedio 87.16 casos (en comparación con 55.27 casos retenidos si no aprenden de la comunicación). Finalmente, el hecho de que ambos conjuntos de datos muestren una mejora significativa señala la naturaleza adaptativa del enfoque basado en la argumentación para el aprendizaje a partir de la comunicación: los casos útiles se seleccionan como contraejemplos (y no más de los necesarios), y tienen el efecto deseado. 10. TRABAJO RELACIONADO En cuanto al RCB en un entorno multiagente, la primera investigación se centró en la recuperación de casos negociada [11] entre grupos de agentes. Nuestro trabajo sobre el aprendizaje basado en casos multiagente comenzó en 1999; más tarde Mc Ginty y Smyth presentaron un enfoque colaborativo de CBR multiagente (CCBR) para la planificación. Finalmente, otro enfoque interesante es el razonamiento de múltiples bases de casos (MCBR) [5], que trata sobre The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 981 ESPONJA 60 65 70 75 80 85 25% 40% 55% 70% 85% 100% LFC L NL SOJA 20 30 40 50 60 70 80 90 25% 40% 55% 70% 85% 100% LFC L NL Figura 6: Aprendizaje a partir de la comunicación resultante de la argumentación en un sistema compuesto por 5 agentes. sistemas distribuidos donde hay varias bases de casos disponibles para la misma tarea y aborda los problemas de adaptación entre bases de casos. La principal diferencia es que nuestro enfoque MAC es una forma de distribuir el proceso de Reutilización de CBR (utilizando un sistema de votación) mientras que la Recuperación se realiza individualmente por cada agente; sin embargo, los otros enfoques de CBR multiagente se centran en distribuir el proceso de Recuperación. La investigación sobre la argumentación en MAS se centra en varios temas como a) lógicas, protocolos y lenguajes que respaldan la argumentación, b) selección de argumentos y c) interpretación de argumentos. Los enfoques para la lógica y los lenguajes que respaldan la argumentación incluyen la lógica no definitiva [4] y los modelos BDI [13]. Aunque la selección de argumentos es un aspecto clave de la argumentación automatizada (ver [12] y [13]), la mayoría de las investigaciones se han centrado en las relaciones de preferencia entre argumentos. En nuestro marco de trabajo hemos abordado tanto la selección de argumentos como las relaciones de preferencia utilizando un enfoque basado en casos. 11. CONCLUSIONES Y TRABAJO FUTURO En este artículo hemos presentado un marco basado en argumentación para el aprendizaje multiagente. Específicamente, hemos presentado AMAL, un marco que permite a un grupo de agentes de aprendizaje discutir sobre la solución de un problema dado y hemos demostrado cómo las capacidades de aprendizaje pueden ser utilizadas para generar argumentos y contraargumentos. La evaluación experimental muestra que la cantidad aumentada de información proporcionada a los agentes por el proceso de argumentación aumenta su precisión predictiva, especialmente cuando un número adecuado de agentes participa en la argumentación. Las principales contribuciones de este trabajo son: a) un marco de argumentación para agentes de aprendizaje; b) una relación de preferencia basada en casos sobre argumentos, basada en el cálculo de una estimación general de confianza de los argumentos; c) una política basada en casos para generar contraargumentos y seleccionar contraejemplos; y d) un enfoque basado en la argumentación para aprender a partir de la comunicación. Finalmente, en los experimentos presentados aquí, un agente de aprendizaje retendría todos los contraejemplos presentados por el otro agente; sin embargo, esta es una política de retención de casos muy simple, y nos gustaría experimentar con políticas más informadas, con el objetivo de que los agentes de aprendizaje individuales puedan mejorar significativamente utilizando solo un pequeño conjunto de casos propuestos por otros agentes. Finalmente, nuestro enfoque se centra en el aprendizaje perezoso, y los trabajos futuros tienen como objetivo incorporar el aprendizaje inductivo ansioso dentro del marco argumentativo para el aprendizaje a partir de la comunicación. 12. REFERENCIAS [1] Agnar Aamodt y Enric Plaza. Razonamiento basado en casos: Aspectos fundamentales, variaciones metodológicas y enfoques de sistemas. Comunicaciones de Inteligencia Artificial, 7(1):39-59, 1994. [2] E. Armengol y E. Plaza. Inducción perezosa de descripciones para el aprendizaje basado en casos relacionales. En ECML2001, páginas 13-24, 2001. [3] Gerhard Brewka. Sistemas de argumentación dinámica: Un modelo formal de procesos de argumentación basado en el cálculo de situaciones. Revista de Lógica y Computación, 11(2):257-282, 2001. [4] Carlos I. Chesñevar y Guillermo R. Simari. Formalizando la argumentación no definitiva utilizando sistemas deductivos etiquetados. Revista de Ciencias de la Computación y Tecnología, 1(4):18-33, 2000. [5] D. Leake y R. Sooriamurthi. Seleccionando estrategias automáticamente para el razonamiento multi-base de casos. En S. Craw y A. Preece, editores, ECCBR2002, páginas 204-219, Berlín, 2002. Springer Verlag. [6] Francisco J. Martín, Enric Plaza y Josep-Lluis Arcos. Reutilización de conocimiento y experiencia a través de comunicaciones entre agentes competentes (pares). Revista Internacional de Ingeniería de Software e Ingeniería del Conocimiento, 9(3):319-341, 1999. [7] Lorraine McGinty y Barry Smyth. Razonamiento basado en casos colaborativo: Aplicaciones en la planificación de rutas personalizadas. En I. Watson y Q. Yang, editores, ICCBR, número 2080 en LNAI, páginas 362-376. Springer-Verlag, 2001. [8] Santi Ontañón y Enric Plaza. Aprendizaje multiagente basado en justificaciones. En ICML2003, páginas 576-583. Morgan Kaufmann, 2003. [9] Enric Plaza, Eva Armengol y Santiago Ontañón. El poder explicativo de la similitud simbólica en el razonamiento basado en casos. Revisión de Inteligencia Artificial, 24(2):145-161, 2005. [10] David Poole. En la comparación de teorías: prefiriendo la explicación más específica. En IJCAI-85, páginas 144-147, 1985. [11] M V Nagendra Prassad, Victor R Lesser y Susan Lander. Recuperación y razonamiento en bases de casos distribuidas. Informe técnico, Departamento de Ciencias de la Computación de UMass, 1995. [12] K. Sycara S. Kraus y A. Evenchik. Alcanzando acuerdos a través de la argumentación: un modelo lógico y su implementación. Revista de Inteligencia Artificial, 104:1-69, 1998. [13] N. R. Jennings, S. Parsons, C. Sierra. Agentes que razonan y negocian mediante argumentos. Revista de Lógica y Computación, 8:261-292, 1998. [14] Bruce A. Wooley. Componente de explicación de sistemas de software. ACM CrossRoads, 5.1, 1998. 982 The Sixth Intl. 

ACM CrossRoads, 5.1, 1998. 982 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)