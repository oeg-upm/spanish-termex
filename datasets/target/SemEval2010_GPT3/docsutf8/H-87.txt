Robustez de los Métodos de Filtrado Adaptativo en una Evaluación Cruzada de Referencia Yiming Yang, Shinjae Yoo, Jian Zhang, Bryan Kisiel Escuela de Ciencias de la Computación, Universidad Carnegie Mellon 5000 Forbes Avenue, Pittsburgh, PA 15213, EE. UU. RESUMEN Este artículo informa sobre una evaluación cruzada de referencia de la regresión logística regularizada (LR) y Rocchio incremental para el filtrado adaptativo. Utilizando cuatro corpus del foro de Detección y Seguimiento de Temas (TDT) y de las Conferencias de Recuperación de Texto (TREC), evaluamos estos métodos con temas no estacionarios en varios niveles de granularidad, y medimos el rendimiento con diferentes configuraciones de utilidad. Encontramos que LR tiene un rendimiento sólido y robusto en la optimización de T11SU (una función de utilidad de TREC) mientras que Rocchio es mejor para optimizar Ctrk (el costo de seguimiento de TDT), una función objetivo orientada a un alto recuerdo. Utilizando la optimización sistemática de parámetros entre corpus con ambos métodos, obtuvimos los mejores resultados jamás reportados en TDT5, TREC10 y TREC11. El feedback de relevancia en una pequeña porción (0.05~0.2%) de los documentos de prueba TDT5 produjo mejoras significativas en el rendimiento, alcanzando una reducción de hasta un 54% en Ctrk y un aumento del 20.9% en T11SU (con β=0.1), en comparación con los resultados del sistema de mejor rendimiento en TDT2004 sin información de feedback de relevancia. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 [Metodología de Diseño]: Diseño y evaluación de clasificadores Términos Generales Algoritmos, Medición, Rendimiento, Experimentación 1. La filtración adaptativa (AF) ha sido un tema de investigación desafiante en la recuperación de información. La tarea consiste en que el sistema tome una decisión de membresía en línea (sí o no) para cada documento, tan pronto como llegue, con respecto a cada tema de interés predefinido. A partir de 1997 en el área de Detección y Seguimiento de Temas (TDT) y 1998 en las Conferencias de Recuperación de Texto (TREC), se han llevado a cabo evaluaciones de referencia por parte del NIST bajo las siguientes condiciones: • Se proporcionó un número muy pequeño (1 a 4) de ejemplos de entrenamiento positivos para cada tema en el punto de partida. • La retroalimentación de relevancia estaba disponible, pero solo para los documentos aceptados por el sistema (con una decisión afirmativa) en las evaluaciones de TREC para AF. • La retroalimentación de relevancia (RF) no estaba permitida en las evaluaciones de TDT para AF (o seguimiento de temas en la terminología de TDT) hasta 2004. • TDT2004 fue la primera vez que las métricas de TREC y TDT se utilizaron conjuntamente para evaluar métodos de AF en la misma referencia (el corpus TDT5) donde predominan los temas no estacionarios. Las condiciones anteriores intentan imitar situaciones realistas donde se usaría un sistema de enfoque automático. Es decir, el usuario estaría dispuesto a proporcionar algunos ejemplos positivos para cada tema de interés al principio, y podría o no ser capaz de proporcionar etiquetado adicional en una pequeña parte de los documentos entrantes a través de retroalimentación de relevancia. Además, los temas de interés pueden cambiar con el tiempo, con nuevos temas que aparecen y crecen, y antiguos temas que se reducen y disminuyen. Estas condiciones hacen que el filtrado adaptativo sea una tarea difícil en el aprendizaje estadístico (clasificación en línea), por las siguientes razones: 1) es difícil aprender modelos precisos para la predicción basados en datos de entrenamiento extremadamente dispersos; 2) no es obvio cómo corregir el sesgo de muestreo (es decir, retroalimentación de relevancia solo en documentos aceptados por el sistema) durante el proceso de adaptación; 3) no se comprende bien cómo ajustar efectivamente los parámetros en los métodos de FA utilizando validación cruzada entre corpus donde los temas de validación y evaluación no se superponen, y los documentos pueden ser de diferentes fuentes o épocas diferentes. Ninguno de estos problemas se aborda en la literatura de aprendizaje estadístico para clasificación por lotes donde todos los datos de entrenamiento se proporcionan de una vez. Los dos primeros problemas han sido estudiados en la literatura de filtrado adaptativo, incluyendo la adaptación del perfil del tema utilizando Rocchio incremental, modelos de densidad gaussiana-exponencial, regresión logística en un marco bayesiano, etc., y estrategias de optimización de umbrales utilizando calibración probabilística o técnicas de ajuste local. Aunque estas obras proporcionan ideas valiosas para comprender los problemas y posibles soluciones, es difícil sacar conclusiones sobre la efectividad y solidez de los métodos actuales porque el tercer problema no ha sido investigado a fondo. Abordar el tercer problema es el enfoque principal de este documento. Sostenemos que la robustez es una medida importante para evaluar y comparar los métodos de AF. Por robusto nos referimos a un rendimiento consistente y fuerte en los corpus de referencia con un método sistemático para ajustar parámetros en múltiples corpus. La mayoría de los métodos de AF tienen parámetros predefinidos que pueden influir significativamente en el rendimiento y que deben determinarse antes de que comience el proceso de prueba. Sin embargo, los ejemplos de entrenamiento disponibles a menudo son insuficientes para ajustar los parámetros. En TDT5, por ejemplo, solo hay un ejemplo de entrenamiento etiquetado por tema al principio; la optimización de parámetros en tales datos de entrenamiento está condenada a ser ineficaz. Esto deja solo una opción (asumiendo que ajustar en el conjunto de pruebas no es una alternativa), es decir, elegir un corpus externo como conjunto de validación. Ten en cuenta que los temas del conjunto de validación a menudo no se superponen con los temas del conjunto de pruebas, por lo tanto, la optimización de parámetros se realiza bajo la condición difícil de que los datos de validación y los datos de prueba pueden ser bastante diferentes entre sí. Ahora la pregunta importante es: ¿qué métodos (si los hay) son robustos bajo la condición de utilizar validación cruzada de corpus para ajustar parámetros? La literatura actual no ofrece una respuesta porque no se ha informado de una investigación exhaustiva sobre la robustez de los métodos de AF. En este artículo abordamos la pregunta anterior realizando una evaluación de referencia cruzada con dos enfoques efectivos en la FA: Rocchio incremental y regresión logística regularizada (LR). Los clasificadores de estilo Rocchio han sido populares en AF, con un buen rendimiento en evaluaciones de referencia (TREC y TDT) si se utilizaban parámetros apropiados y si se combinaban con una estrategia efectiva de calibración de umbrales [2][4][7][8][9][11][13]. La regresión logística es un método clásico en el aprendizaje estadístico, y uno de los mejores en la categorización de texto en modo por lotes [15][14]. Recientemente fue evaluado en filtrado adaptativo y se encontró que tiene un rendimiento relativamente fuerte (Sección 5.1). Además, un artículo reciente [13] informó que el uso conjunto de Rocchio y LR en un marco bayesiano superó los resultados de utilizar cada método por separado en el corpus TREC11. Estimulados por esos hallazgos, decidimos incluir Rocchio y LR en nuestra evaluación de referencia cruzada para pruebas de robustez. Específicamente, nos enfocamos en cuánto depende el rendimiento de estos métodos de la sintonización de parámetros, cuáles son los parámetros más influyentes en estos métodos, qué tan difícil (o fácil) es optimizar estos parámetros influyentes utilizando validación cruzada de corpus, qué tan bien funcionan estos métodos en múltiples pruebas con la sintonización sistemática de parámetros en otros corpus, y qué tan eficientes son estos métodos al ejecutar AF en grandes corpus de referencia. La organización del documento es la siguiente: La Sección 2 presenta los cuatro corpus de referencia (TREC10 y TREC11, TDT3 y TDT5) utilizados en este estudio. La sección 3 analiza las diferencias entre las métricas TREC y TDT (utilidades y costos de seguimiento) y las posibles implicaciones de esas diferencias. La sección 4 describe respectivamente los enfoques de Rocchio y LR para AF. La sección 5 informa sobre los experimentos y resultados. La sección 6 concluye los hallazgos principales de este estudio. 2. CORPUS DE REFERENCIA Utilizamos cuatro corpus de referencia en nuestro estudio. La Tabla 1 muestra las estadísticas sobre estos conjuntos de datos. TREC10 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2001, que consistió aproximadamente de 806,791 noticias de Reuters desde agosto de 1996 hasta agosto de 1997 con 84 etiquetas de temas (categorías de sujetos). Las primeras dos semanas (del 20 al 31 de agosto de 1996) de documentos conforman el conjunto de entrenamiento, y los 11 meses y medio restantes (del 1 de septiembre de 1996 al 19 de agosto de 1997) conforman el conjunto de pruebas. TREC11 fue el punto de referencia de evaluación para el filtrado adaptativo en TREC 2002, compuesto por el mismo conjunto de documentos que los de TREC10 pero con un punto de división ligeramente diferente para los conjuntos de entrenamiento y prueba. Los temas de TREC11 (50) son bastante diferentes de los de TREC10; son consultas para recuperación con juicios de relevancia por evaluadores de NIST [8]. TDT3 fue el punto de referencia de evaluación en la prueba piloto1 de TDT2001. La parte de seguimiento del corpus consta de 71,388 noticias de múltiples fuentes en inglés y mandarín (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America y PRI the World) en el período de octubre a diciembre de 1998. Se proporcionan versiones traducidas por máquina de las historias no escritas en inglés (Xinhua, Zaobao y VOA Mandarín). El punto de división para los conjuntos de entrenamiento y prueba es diferente para cada tema en TDT. TDT5 fue el punto de referencia de evaluación en TDT2004 [4]. La parte de seguimiento del corpus consta de 407,459 noticias en el período de abril a septiembre de 2003 de 15 agentes de noticias o fuentes de transmisión en inglés, árabe y mandarín, con versiones traducidas por máquina de las historias en otros idiomas. Solo utilizamos las versiones en inglés de esos documentos en nuestros experimentos para este artículo. Los temas de TDT difieren de los temas de TREC tanto conceptual como estadísticamente. En lugar de categorías de temas genéricas y duraderas (como las de TREC), los temas de TDT se definen a un nivel de granularidad más fino, para eventos que ocurren en momentos y lugares específicos, y que nacen y mueren, típicamente asociados con una distribución explosiva sobre historias de noticias ordenadas cronológicamente. El tamaño promedio de los temas de TDT (eventos) es dos órdenes de magnitud más pequeño que el de los temas de TREC10. La Figura 1 compara las densidades de documentos de un tema de TREC (Guerras Civiles) y dos temas de TDT (Disparos y Reunión de la Cumbre de la APEC, respectivamente) durante un período de tres meses, donde el área bajo cada curva está normalizada a uno. Las diferencias de granularidad entre los temas y las distribuciones no estacionarias correspondientes hacen interesante la evaluación cruzada de referencia. Por ejemplo, los algoritmos que favorecen temas grandes y estables pueden no funcionar bien para temas de corta duración y no estacionarios, y viceversa. Las evaluaciones de referencia cruzada nos permiten probar esta hipótesis y posiblemente identificar las debilidades en los enfoques actuales de filtrado adaptativo en el seguimiento de las tendencias cambiantes de los temas. 1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Tabla 1: Estadísticas de corpora de referencia para evaluaciones de filtrado adaptativo N(tr) es el número de documentos de entrenamiento inicial; N(ts) es el número de documentos de prueba; n+ es el número de ejemplos positivos de un tema predefinido; * es un promedio sobre todos los temas. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Semana P(tema|semana) Disparo (TDT5) Reunión de la Cumbre de APEC (TDT3) Guerra Civil (TREC10) Figura 1: La naturaleza temporal de los temas 3. MÉTRICAS Para hacer nuestros resultados comparables con la literatura, decidimos utilizar tanto métricas convencionales de TREC como de TDT en nuestra evaluación. 3.1 Métricas de TREC11 Sea A, B, C y D, respectivamente, los números de verdaderos positivos, falsas alarmas, omisiones y verdaderos negativos para un tema específico, y DCBAN +++= sea el número total de documentos de prueba. Las métricas convencionales de TREC se definen como: Precisión )/( BAA += , Recuperación )/( CAA += )(2 )21( CABA A F +++ + = β β β ( ) η ηηβ ηβ − −+− = 1 ),/()(max 11 , CABA SUT donde los parámetros β y η se establecieron en 0.5 y -0.5 respectivamente en TREC10 (2001) y TREC11 (2002). Para evaluar el rendimiento de un sistema, primero se calculan las puntuaciones de rendimiento para cada tema individual y luego se promedian entre los temas (macroaveraging). 3.2 Métricas TDT La métrica TDT convencional para el seguimiento de temas se define como: famisstrk PTPwPTPwTC ))(1()()( 21 −+= donde P(T) es el porcentaje de documentos sobre el tema T, missP es la tasa de error del sistema en ese tema, faP es la tasa de falsa alarma, y 1w y 2w son los costos (constantes predefinidas) para un error y una falsa alarma, respectivamente. Las evaluaciones de referencia de TDT (desde 1997) han utilizado la configuración de 11 =w, 1.02 =w y 02.0)( =TP para todos los temas. Para evaluar el rendimiento de un sistema, se calcula Ctrk para cada tema primero y luego se promedian las puntuaciones resultantes para obtener una medida única (el Ctrk ponderado por tema). Para hacer transparente la intuición detrás de esta medida, sustituimos los términos en la definición de Ctrk de la siguiente manera: N CA TP + =)( , N DB TP + =− )(1 , CA C Pmiss + = , DB B Pfa + = , )( 1 )( 21 21 BwCw N DB B N DB w CA C N CA wTCtrk +⋅= + ⋅ + ⋅+ + ⋅ + ⋅= Claramente, trkC es el costo promedio por error en el tema T, con 1w y 2w controlando la proporción de penalización para omisiones vs. falsas alarmas. Además de trkC, TDT2004 también empleó 1.011 =βSUT como métrica de utilidad. Para distinguir esto del 5.011 =βSUT en TREC11, lo llamamos el antiguo TDT5SU en el resto de este artículo. Corpus #Temas N(tr) N(ts) Prom n+ (tr) Prom n+ (ts) Máx n+ (ts) Mín n+ (ts) #Temas por doc (ts) TREC10 84 20,307 783,484 2 9795.3 39,448 38 1.57 TREC11 50 80.664 726,419 3 378.0 597 198 1.12 TDT3 53 18,738* 37,770* 4 79.3 520 1 1.06 TDT5 111 199,419* 207,991* 1 71.3 710 1 1.01 3.3 Las correlaciones y las diferencias Desde un punto de vista de optimización, TDT5SU y T11SU son ambas funciones de utilidad mientras que Ctrk es una función de costo. Nuestro objetivo es maximizar lo primero o minimizar lo segundo en los documentos de prueba. Las diferencias y correlaciones entre estas funciones objetivas pueden ser analizadas a través de los recuentos compartidos de A, B, C y D en sus definiciones. Por ejemplo, tanto TDT5SU como T11SU están correlacionados positivamente con los valores de A y D, y correlacionados negativamente con los valores de B y C; la única diferencia entre ellos está en sus ratios de penalización por omisiones vs. falsas alarmas, es decir, 10:1 en TDT5SU y 2:1 en T11SU. La función Ctrk, por otro lado, está positivamente correlacionada con los valores de C y B, y negativamente correlacionada con los valores de A y D; por lo tanto, está negativamente correlacionada con T11SU y TDT5SU. Más importante aún, hay una diferencia sutil y significativa entre Ctrk y las funciones de utilidad: T11SU y TDT5SU. Es decir, Ctrk tiene una proporción de penalización muy diferente para los fallos en comparación con las alarmas falsas: favorece a los sistemas orientados al recuerdo de manera extrema. A primera vista, uno pensaría que la proporción de penalización en Ctrk es de 10:1 ya que 11 =w y 1.02 =w. Sin embargo, esto no es cierto si 02.0)( =TP es una estimación inexacta de los documentos relevantes en promedio para el corpus de prueba. Usando TDT3 como ejemplo, el porcentaje real es: 002.0 37770 3.79 )( ≈= + = N n TP donde N es el tamaño promedio de los conjuntos de pruebas en TDT3, y n+ es el número promedio de ejemplos positivos por tema en los conjuntos de pruebas. Usar 02.0)(ˆ =TP como una estimación (inexacta) de 0.002 amplía la proporción de penalización prevista de 10:1 a 100:1, hablando en términos generales. A saber: )1.010( 1 1.010 )3.7937770(37770 3.79 1011.0101 1011.0101 ))(1(2)(1 )02.01(202.01)( BC NN B N C B N C DB B N CA N C faPTPwmissPTPw faPwmissPwTtrkC ×+×=×+×≈ − ×−×+××= + + ×−×+××= ×−⋅+××= −×+××= ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ⎟ ⎠ ⎞ ⎜ ⎝ ⎛ ρρ donde 10 002.0 02.0 )( )(ˆ === TP TP ρ es el factor de ampliación en la estimación de P(T) en comparación con la verdad. Comparando el resultado anterior con la fórmula 2, podemos ver que la proporción real de penalización por omisiones frente a alarmas falsas fue de 100:1 en las evaluaciones en TDT3 utilizando Ctrk. De manera similar, podemos calcular el factor de ampliación para TDT5 utilizando las estadísticas en la Tabla 1 de la siguiente manera: 3.58 991,207/3.71 02.0 )( )(ˆ === TP TP ρ lo que significa que la proporción real de penalización por omisiones frente a falsas alarmas en la evaluación en TDT5 utilizando Ctrk fue aproximadamente de 583:1. Las implicaciones del análisis anterior son bastante significativas: • Ctrk definido en la misma fórmula no necesariamente significa la misma función objetivo en la evaluación; en cambio, el criterio de optimización depende del corpus de prueba. • Los sistemas optimizados para Ctrk no optimizarían TDT5SU (y T11SU) porque el primero favorece una alta recuperación orientada a un extremo mientras que el segundo no. • Los parámetros ajustados en un corpus (por ejemplo, TDT3) podrían no funcionar para una evaluación en otro corpus (digamos, TDT5) a menos que tengamos en cuenta la sutil dependencia previamente desconocida de Ctrk en los datos. • Los resultados en Ctrk en los últimos años de las evaluaciones de TDT pueden no ser directamente comparables entre sí porque las colecciones de evaluación cambiaron la mayoría de los años y, por lo tanto, la proporción de penalización en Ctrk varió. Aunque estos problemas con Ctrk no fueron anticipados originalmente, ofrecieron la oportunidad de examinar la capacidad de los sistemas para intercambiar precisión por un alto nivel de recuperación. Esta fue una parte desafiante de la evaluación TDT2004 para AF. Comparar las métricas en TDT y TREC desde un punto de vista de optimización de utilidad o coste es importante para comprender los resultados de evaluación de los métodos de filtrado adaptativo. Esta es la primera vez que este tema se analiza explícitamente, según nuestro conocimiento. 4. MÉTODOS 4.1 Rocchio Incremental para AF Empleamos una versión común de clasificadores de estilo Rocchio que calcula un vector prototipo por tema (T) de la siguiente manera: |)(| |)(| )()( )()( TD d TD d TqTp TDdTDd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα El primer término en el lado derecho es la representación vectorial ponderada de la descripción del tema cuyos elementos son los pesos de los términos. El segundo término es el centroide ponderado del conjunto )(TD+ de ejemplos de entrenamiento positivos, cada uno de los cuales es un vector de pesos de términos dentro del documento. El tercer término es el centroide ponderado del conjunto )(TD− de ejemplos de entrenamiento negativos que son los vecinos más cercanos del centroide positivo. Los tres términos se les asignan pesos preespecificados de β, α y γ, controlando la influencia relativa de estos componentes en el prototipo. El prototipo de un tema se actualiza cada vez que el sistema toma una decisión afirmativa sobre un nuevo documento para ese tema. Si se dispone de retroalimentación de relevancia (como es el caso en el filtrado adaptativo de TREC), el nuevo documento se agrega al grupo de )(TD+ o )(TD−, y el prototipo se recalcula en consecuencia; si no se dispone de retroalimentación de relevancia (como es el caso en el seguimiento de eventos de TDT), la predicción del sistema (sí) se trata como la verdad, y el nuevo documento se agrega a )(TD+ para actualizar el prototipo. Ambos casos forman parte de nuestros experimentos en este artículo (y de las evaluaciones TDT 2004 para AF). Para distinguir los dos, llamamos al primer caso simplemente Rocchio y al segundo caso Rocchio PRF donde PRF significa retroalimentación de pseudo relevancia. Las predicciones en un nuevo documento se realizan calculando la similitud del coseno entre cada prototipo de tema y el vector del documento, y luego comparando los puntajes resultantes con un umbral: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dTpsign new θ rr La calibración del umbral en Rocchio incremental es un desafiante tema de investigación. Se han desarrollado múltiples enfoques. Lo más sencillo es utilizar un umbral universal para todos los temas, ajustado en un conjunto de validación y fijo durante la fase de pruebas. Métodos más elaborados incluyen la calibración de umbrales probabilísticos que convierte las puntuaciones de similitud no probabilísticas en probabilidades (es decir, )|( dTP r ) para la optimización de la utilidad [9][13], y la regresión local basada en márgenes para la reducción del riesgo [11]. Está fuera del alcance de este documento comparar todas las diferentes formas de adaptar los métodos de estilo Rocchio para AF. En cambio, nuestro enfoque aquí es investigar la robustez de los métodos de estilo Rocchio en cuanto a cuánto depende su rendimiento de una ajuste del sistema elaborado, y qué tan difícil (o fácil) es obtener un buen rendimiento a través de la optimización de parámetros entre corpus. Por lo tanto, decidimos utilizar una versión relativamente simple de Rocchio como referencia, es decir, con un umbral universal ajustado en un corpus de validación y fijo para todos los temas en la fase de prueba. Esta versión simplificada de Rocchio ha sido comúnmente utilizada en las evaluaciones de referencia TDT pasadas para el seguimiento de temas, y tuvo un rendimiento destacado en las evaluaciones TDT2004 para el filtrado adaptativo con y sin retroalimentación de relevancia (Sección 5.1). También se discuten los resultados de variantes más complejas de Rocchio cuando sea relevante. 4.2 Regresión Logística para AF La regresión logística (LR) estima la probabilidad posterior de un tema dado un documento utilizando una función sigmoide )1/(1),|1( xw ewxyP rrrr ⋅− +== donde x r es el vector del documento cuyos elementos son los pesos de los términos, w r es el vector de coeficientes de regresión, y }1,1{ −+∈y es la variable de salida correspondiente a sí o no con respecto a un tema particular. Dado un conjunto de entrenamiento de documentos etiquetados { },),(,),,( 11 nn yxyxD r L r = , el problema estándar de regresión se define como encontrar las estimaciones de máxima verosimilitud de los coeficientes de regresión (los parámetros del modelo): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wDP w wDP w mlw rr r r r r r r ⋅−+∑ == == Este es un problema de optimización convexa que se puede resolver utilizando un algoritmo estándar de gradiente conjugado en tiempo O(INF) para el entrenamiento por tema, donde I es el número promedio de iteraciones necesarias para la convergencia, y N y F son el número de documentos de entrenamiento y el número de características respectivamente [14]. Una vez que los coeficientes de regresión se optimizan en los datos de entrenamiento, la predicción de filtrado en cada documento entrante se realiza como: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no sí wxyPsign optnew θ rr Cabe destacar que w r se actualiza constantemente cada vez que se dispone de un nuevo juicio de relevancia en la fase de prueba de AF, mientras que el umbral óptimo optθ es constante, dependiendo únicamente de la función de utilidad (o costo) predefinida para la evaluación. Si T11SU es la métrica, por ejemplo, con una proporción de penalización de 2:1 para omisiones y alarmas falsas (Sección 3.1), el umbral óptimo para LR es 33.0)12/(1 =+ para todos los temas. Modificamos la versión estándar (anterior) de LR para permitir criterios de optimización más flexibles de la siguiente manera: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii donde )( iys se toma como α , β y γ para consultas, documentos positivos y documentos negativos respectivamente, que son similares a los de Rocchio, asignando diferentes pesos a los tres tipos de ejemplos de entrenamiento: descripciones de temas (consultas), documentos relevantes y documentos no relevantes. El segundo término en la función objetivo es para regularización, equivalente a agregar una priori gaussiana a los coeficientes de regresión con media μ r y matriz de varianza de covarianza Ι⋅λ2/1, donde Ι es la matriz identidad. Ajustar λ (≥0) está justificado teóricamente para reducir la complejidad del modelo (el grado efectivo de libertad) y evitar el sobreajuste en los datos de entrenamiento [5]. Cómo encontrar un μ r efectivo es un tema abierto para la investigación, dependiendo de la creencia del usuario sobre el espacio de parámetros y el rango óptimo. La solución de la función objetivo modificada se llama estimación del Máximo a Posteriori (MAP), que se reduce a la solución de máxima verosimilitud para la regresión logística estándar si λ = 0.5. EVALUACIONES Informamos nuestros hallazgos empíricos en cuatro partes: los resultados de la evaluación oficial TDT2004, los resultados de optimización de parámetros entre corpus, y los resultados correspondientes a las cantidades de retroalimentación de relevancia. 5.1 Resultados de referencia TDT2004 Las evaluaciones TDT2004 para el filtrado adaptativo fueron realizadas por NIST en noviembre de 2004. Varios equipos de investigación participaron y se permitieron múltiples ejecuciones de cada equipo. Se utilizaron Ctrk y TDT5SU como métricas. La Figura 2 y la Figura 3 muestran los resultados; se seleccionó la mejor ejecución de cada equipo con respecto a Ctrk o TDT5SU, respectivamente. Nuestro Rocchio (con perfiles adaptativos pero umbral universal fijo para todos los temas) obtuvo el mejor resultado en Ctrk, y nuestra regresión logística obtuvo el mejor resultado en TDT5SU. Todos los parámetros de nuestras ejecuciones fueron ajustados en el corpus TDT3. Los resultados de otros sitios también se enumeran de forma anónima para su comparación. Ctrk Nuestro 0.0324 Sitio2 0.0467 Sitio3 0.1366 Sitio4 0.2438 Métrica = Ctrk (cuanto menor, mejor) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Nuestro Sitio2 Sitio3 Sitio4 Figura 2: Resultados de TDT2004 en Ctrk de sistemas que utilizan retroalimentación de relevancia verdadera. (El nuestro es el método de Rocchio.) También colocamos los cuartiles 1 y 3 como palos para cada sitio. 2 T11SU Nuestro 0.7328 Sitio3 0.7281 Sitio2 0.6672 Sitio4 0.382 Métrica = TDT5SU (cuanto mayor, mejor) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Nuestro Sitio3 Sitio2 Sitio4 Figura 3: Resultados de TDT2004 en TDT5SU de sistemas que utilizan retroalimentación de relevancia verdadera. (Nuestro es LR con 0=μ r y 005.0=λ ). CTrk Nuestro 0.0707 Sitio2 0.1545 Sitio5 0.5669 Sitio4 0.6507 Sitio6 0.8973 Resultados de Seguimiento de Tema Principal en TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Nuestro Sitio2 Sitio5 Sitio4 Sitio6 Ctrk Figura 4: Resultados de TDT2004 en Ctrk de sistemas sin utilizar retroalimentación de relevancia verdadera. (Nuestro es PRF Rocchio.) El filtrado adaptativo sin utilizar retroalimentación de relevancia real también fue parte de las evaluaciones. En este caso, los sistemas solo tenían un ejemplo de entrenamiento etiquetado por tema durante todo el proceso de entrenamiento y pruebas, aunque los documentos de prueba no etiquetados podrían ser utilizados tan pronto como se hicieran predicciones sobre ellos. Un entorno así ha sido convencional para la tarea de seguimiento de temas en TDT hasta 2004. La Figura 4 muestra los resúmenes de las presentaciones oficiales de cada equipo. Nuestro PRF Rocchio (con un umbral fijo para todos los temas) tuvo el mejor rendimiento. Utilizamos cuartiles en lugar de desviaciones estándar ya que los primeros son más resistentes a valores atípicos. La optimización de parámetros entre corpus cruzados. Cuánto depende el buen rendimiento de nuestros sistemas de la sintonización de parámetros es una pregunta importante. Tanto Rocchio como LR tienen parámetros que deben ser especificados previamente antes del proceso de AF. Los parámetros compartidos incluyen los pesos de muestra α, β y γ, el tamaño de la muestra de los documentos de entrenamiento negativos (es decir, TD-), el esquema de ponderación de términos y el número máximo de elementos no nulos en cada vector de documento. Los parámetros específicos del método incluyen el umbral de decisión en Rocchio, y μ r , λ y MI (el número máximo de iteraciones en el entrenamiento) en LR. Dado que solo tenemos un ejemplo etiquetado por tema en los conjuntos de entrenamiento de TDT5, es imposible optimizar eficazmente estos parámetros en los datos de entrenamiento, por lo que tuvimos que elegir un corpus externo para la validación. Entre las opciones de TREC10, TREC11 y TDT3, elegimos TDT3 (c.f. Sección 2) porque es la más similar a TDT5 en cuanto a la naturaleza de los temas (Sección 2). Optimizamos los parámetros de nuestros sistemas en TDT3 y fijamos esos parámetros en las ejecuciones en TDT5 para nuestras presentaciones en TDT2004. También probamos nuestros métodos en TREC10 y TREC11 para un análisis más detallado. Dado que realizar pruebas exhaustivas de todas las posibles configuraciones de parámetros es computacionalmente intratable, seguimos en su lugar un procedimiento de encadenamiento hacia adelante paso a paso: especificamos previamente un orden de los parámetros en un método (Rocchio o LR), y luego ajustamos un parámetro a la vez mientras manteníamos fijas las configuraciones de los parámetros restantes. Repetimos este procedimiento varias veces según lo permitía el tiempo. 0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Umbral TDT5SU TDT3 TDT5 TREC10 TREC11 Figura 5: Curvas de rendimiento de Rocchio adaptativo La Figura 5 compara las curvas de rendimiento en TDT5SU para Rocchio en TDT3, TDT5, TREC10 y TREC11 cuando variaba el umbral de decisión. Estas curvas alcanzan su punto máximo en diferentes ubicaciones: la óptima para TDT3 está más cerca de la óptima para TDT5, mientras que las óptimas para TREC10 y TREC1 están bastante lejos de la óptima para TDT5. Si estuviéramos utilizando TREC10 o TREC11 en lugar de TDT3 como corpus de validación para TDT5, o si el corpus TDT3 no estuviera disponible, tendríamos dificultades para obtener un rendimiento sólido para Rocchio en TDT2004. La dificultad proviene de los puntajes ad-hoc (no probabilísticos) generados por el método de Rocchio: la distribución de los puntajes depende del corpus, lo que hace que la optimización del umbral entre corpus sea un problema complicado. La regresión logística tiene menos dificultad con respecto a la ajuste del umbral porque produce puntuaciones probabilísticas de )|1Pr( xy = sobre las cuales el umbral óptimo puede ser calculado directamente si la estimación de probabilidad es precisa. Dado que la proporción de penalización por errores de omisión frente a falsas alarmas es de 2:1 en T11SU, 10:1 en TDT5SU y 583:1 en Ctrk (Sección 3.3), los umbrales óptimos correspondientes (t) son 0.33, 0.091 y 0.0017 respectivamente. Aunque el umbral teórico podría ser inexacto, aún sugiere el rango de configuraciones cercanas a óptimas. Con estos ajustes de umbral en nuestros experimentos para LR, nos enfocamos en la validación cruzada de los parámetros previos bayesianos, es decir, μ r y λ. La Tabla 2 resume los resultados 3. Medimos el rendimiento de las ejecuciones en TREC10 y TREC11 utilizando T11SU, y el rendimiento de las ejecuciones en TDT3 y TDT5 utilizando TDT5SU. Para la comparación también incluimos los mejores resultados de los métodos basados en Rocchio en estos corpus, que son nuestros propios resultados de Rocchio en TDT3 y TDT5, y los mejores resultados reportados por NIST para TREC10 y TREC11. De este conjunto de resultados, vemos que LR superó significativamente a Rocchio en todos los corpus, incluso en las ejecuciones de LR estándar sin ningún ajuste, es decir, λ=0. Este hallazgo empírico es consistente con un informe previo [13] para LR en TREC11, aunque nuestros resultados de LR (0.585~0.608 en T11SU) son más sólidos que los resultados (0.49 para LR estándar y 0.54 para LR utilizando el prototipo de Rocchio como prior) en ese informe. Más importante aún, nuestra evaluación de referencia cruzada proporciona evidencia sólida de la robustez de LR. La robustez, creemos, proviene de la naturaleza probabilística de las puntuaciones generadas por el sistema. Es decir, en comparación con las puntuaciones ad-hoc en Rocchio, las probabilidades posteriores normalizadas hacen que la optimización del umbral en LR sea un problema mucho más fácil. Además, se sabe que la regresión logística converge hacia el clasificador de Bayes asintóticamente, mientras que los parámetros de los clasificadores de Rocchio no lo hacen. Otra observación interesante en estos resultados es que el rendimiento de LR no mejoró al usar un prototipo de Rocchio como la media en el anterior; en cambio, el rendimiento disminuyó en algunos casos. Esta observación no respalda el informe anterior de [13], pero no nos sorprende porque no estamos convencidos de que los prototipos de Rocchio sean más precisos que los modelos LR para los temas en la etapa inicial del proceso de AF, y creemos que usar un prototipo de Rocchio como la media en la distribución gaussiana introduciría un sesgo no deseado en LR. También creemos que la reducción de la varianza (en la fase de pruebas) debería ser controlada por la elección de λ (pero no de μ r ), para lo cual realizamos los experimentos como se muestra en la Figura 6. Tabla 2: Resultados de LR con diferentes priors bayesianos Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Mejor Rocchio 0.6628 0.6917 0.4964 0.475 3 Los resultados de LR (0.77~0.78) en TDT5 en esta tabla son mejores que nuestro resultado oficial de TDT2004 (0.73) porque la optimización de parámetros ha sido mejorada posteriormente. 4 El mejor resultado de TREC10 (0.496 por Oracle) solo está disponible en T10U, lo cual no es directamente comparable con las puntuaciones en T11SU, solo indicativo. *: μ r se estableció en el prototipo de Rocchio Figura 6: LR con lambda variable. El rendimiento de LR se resume con respecto a la sintonización de λ en los corpus de TREC10, TREC11 y TDT3. El rendimiento en cada corpus se midió utilizando las métricas correspondientes, es decir, T11SU para las ejecuciones en TREC10 y TREC11, y TDT5SU y Ctrk para las ejecuciones en TDT3. En el caso de maximizar las utilidades, el intervalo seguro para λ está entre 0 y 0.01, lo que significa que el rendimiento de la regresión logística regularizada es estable, igual o ligeramente mejorado en comparación con el rendimiento de la regresión logística estándar. En el caso de minimizar Ctrk, el rango seguro para λ está entre 0 y 0.1, y establecer λ entre 0.005 y 0.05 produjo mejoras relativamente grandes en el rendimiento del LR estándar porque entrenar un modelo para un recuerdo extremadamente alto es estadísticamente más complicado, y por lo tanto se necesita más regularización. En cualquier caso, ajustar λ es relativamente seguro y fácil de hacer con éxito mediante el ajuste de corpus cruzado. Otra elección influyente en la configuración de nuestro experimento es la ponderación de términos: examinamos las opciones de los esquemas binario, TF y TF-IDF (versión ltc). Encontramos que TF-IDF fue el más efectivo tanto para Rocchio como para LR, y utilizamos esta configuración en todos nuestros experimentos. El 5.3 por ciento de datos etiquetados. Cuánta retroalimentación de relevancia (RF) sería necesaria durante el proceso de AF es una pregunta significativa en aplicaciones del mundo real. Para responderlo, evaluamos Rocchio y LR en TDT con la siguiente configuración: • Rocchio básico, sin adaptación alguna • Rocchio PRF, actualizando perfiles de temas sin utilizar retroalimentación de relevancia verdadera; • Rocchio adaptativo, actualizando perfiles de temas utilizando retroalimentación de relevancia en documentos aceptados por el sistema más 10 documentos seleccionados al azar de la lista de documentos rechazados por el sistema; • LR con 0 rr =μ , 01.0=λ y umbral = 0.004; • Todos los parámetros en Rocchio ajustados en TDT3. La Tabla 3 resume los resultados en Ctrk: Rocchio Adaptativo con retroalimentación de relevancia en el 0.6% de los documentos de prueba redujo el costo de seguimiento en un 54% en comparación con el resultado del Rocchio PRF, el mejor sistema en la evaluación TDT2004 para el seguimiento de temas sin información de retroalimentación de relevancia. El LR incremental, por otro lado, era más débil pero aún impresionante. Recuerde que Ctrk es una métrica orientada a un alto nivel de recuperación, lo que provoca actualizaciones frecuentes de perfiles y, por lo tanto, un problema de eficiencia en LR. Por esta razón establecimos un umbral más alto (0.004) en lugar del umbral teóricamente óptimo (0.0017) en LR para evitar un costo de computación inaceptable. El tiempo de computación en horas de máquina fue de 0.33 para la ejecución de Rocchio adaptativo y de 14 para la ejecución de LR en TDT5 al optimizar Ctrk. La Tabla 4 resume los resultados en TDT5SU; LR adaptativo fue el ganador en este caso, con retroalimentación de relevancia en el 0.05% de los documentos de prueba mejorando la utilidad en un 20.9% sobre los resultados de PRF Rocchio. Tabla 3: Métodos de AF en TDT5 (Rendimiento en Ctrk) Base Roc PRF Roc Adp Roc LR % de RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (línea base) -54% -46% Tabla 4: Métodos de AF en TDT5 (Rendimiento en TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % de RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (línea base) +6.9% +20.9% Evidentemente, tanto Rocchio como LR son altamente efectivos en el filtrado adaptativo, en términos de utilizar una pequeña cantidad de datos etiquetados para mejorar significativamente la precisión del modelo en el aprendizaje estadístico, que es el principal objetivo de AF. Resumen del Proceso de Adaptación Después de decidir la configuración de parámetros utilizando validación, realizamos el filtrado adaptativo en los siguientes pasos para cada tema: 1) Entrenar el modelo LR/Rocchio utilizando los ejemplos de entrenamiento positivos proporcionados y 30 ejemplos negativos muestreados al azar; 2) Para cada documento en el corpus de prueba: primero hacemos una predicción sobre la relevancia, y luego obtenemos retroalimentación de relevancia para aquellos documentos positivos (predichos). 3) El modelo y las estadísticas de IDF se actualizarán incrementalmente si obtenemos su verdadera retroalimentación de relevancia. CONCLUSIONES FINALES Presentamos una evaluación de referencia cruzada de Rocchio incremental y LR incremental en filtrado adaptativo, centrándonos en su robustez en términos de consistencia de rendimiento con respecto a la optimización de parámetros entre corpus cruzados. Nuestras principales conclusiones de este estudio son las siguientes: • La optimización de parámetros en AF es un desafío abierto pero no ha sido estudiada a fondo en el pasado. • La robustez en la sintonización de parámetros entre corpus es importante para la evaluación y comparación de métodos. • Encontramos que LR es más robusto que Rocchio; obtuvo los mejores resultados (en T11SU) reportados en TDT5, TREC10 y TREC11 sin una sintonización extensa. • Encontramos que Rocchio tiene un rendimiento sólido cuando se dispone de un buen corpus de validación, y es la opción preferida al optimizar Ctrk como objetivo, favoreciendo la recuperación sobre la precisión de manera extrema. Para futuras investigaciones queremos estudiar el modelado explícito de las tendencias temporales en las distribuciones de temas y el desplazamiento de contenido. Agradecimientos Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias (NSF) bajo el subsidio IIS-0434035, por el Departamento de Defensa (DoD) bajo el premio 114008-N66001992891808 y por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el Contrato No. NBCHD030010. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. En SIGIR-96, 1996. [2] J. Callan. Aprendiendo mientras se filtran documentos. En SIGIR-98, 224-231, 1998. [3] J. Fiscus y G. Duddington. Resumen de detección y seguimiento de temas. En Detección y seguimiento de temas: organización de información basada en eventos, 17-31, 2002. [4] J. Fiscus y B. Wheatley. Resumen de la Evaluación y Resultados de TDT 2004. En TDT-04, 2004. [5] T. Hastie, R. Tibshirani y J. Friedman. Elementos del Aprendizaje Estadístico. Springer, 2001. [6] S. Robertson y D. Hull. El informe final de la pista de filtrado TREC-9. En TREC-9, 2000. [7] S. Robertson e I. Soboroff. El informe final de la pista de filtrado TREC-10. En TREC-10, 2001. [8] S. Robertson e I. Soboroff. El informe de la pista de filtrado TREC 2002. En TREC-11, 2002. [9] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9. En TREC-9, 2000. [10] R. Schapire, Y. Cantante y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. En SIGIR-98, 215-223, 1998. [11] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. En CIKM-03, 2003. [12] Y. Zhang y J. Callan. Estimación de máxima verosimilitud para umbrales de filtrado. En SIGIR-01, 2001. [13] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. En SIGIR-04, 2004. [14] J. Zhang y Y. Yang. Robustez de los métodos de clasificación lineal regularizados en la categorización de texto. En SIGIR-03: 190-197, 2003. [15] T. Zhang, F. J. Oles. Categorización de texto basada en métodos de clasificación lineal regularizados. I'm sorry, but the sentence "Inf." is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Rev. 4(1): 5-31 (2001).