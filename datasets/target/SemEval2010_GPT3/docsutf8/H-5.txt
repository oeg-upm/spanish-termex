Destilación de información basada en utilidad sobre documentos secuenciados temporalmente en el Instituto de Tecnologías del Lenguaje de Yiming Yang. Universidad Carnegie Mellon, Pittsburgh, EE. UU. yiming@cs.cmu.edu Abhimanyu Lad Instituto de Tecnologías del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. alad@cs.cmu.edu Instituto de Tecnologías del Lenguaje Ni Lao. Universidad Carnegie Mellon, Pittsburgh, EE. UU. nlao@cs.cmu.edu Abhay Harpale Instituto de Tecnologías del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. aharpale@cs.cmu.edu Bryan Kisiel Instituto de Tecnologías del Lenguaje. Universidad Carnegie Mellon, Pittsburgh, EE. UU. bkisiel@cs.cmu.edu Mónica Rogati Instituto de Tecnologías del Lenguaje. La Universidad Carnegie Mellon en Pittsburgh, EE. UU. mrogati@cs.cmu.edu RESUMEN Este artículo examina un nuevo enfoque para la destilación de información en documentos ordenados temporalmente, y propone un esquema de evaluación novedoso para dicho marco. Combina las fortalezas y se extiende más allá del filtrado adaptativo convencional, la detección de novedades y la clasificación de pasajes no redundantes con respecto a las necesidades de información duraderas (tareas con múltiples consultas). Nuestro enfoque respalda la retroalimentación detallada del usuario a través del resaltado de fragmentos arbitrarios de texto, y aprovecha dicha información para la optimización de la utilidad en entornos adaptativos. Para nuestros experimentos, definimos tareas hipotéticas basadas en eventos de noticias en el corpus TDT4, con múltiples consultas por tarea. Se generaron claves de respuestas (nuggets) para cada consulta y se utilizó un procedimiento semiautomático para adquirir reglas que permiten emparejar automáticamente los nuggets con las respuestas del sistema. También proponemos una extensión de la métrica NDCG para evaluar la utilidad de los pasajes clasificados como una combinación de relevancia y novedad. Nuestros resultados muestran mejoras alentadoras en la utilidad utilizando el nuevo enfoque, en comparación con los sistemas base sin aprendizaje incremental o componentes de detección de novedades. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Filtrado de información, Retroalimentación de relevancia, Modelos de recuperación, Proceso de selección; I.5.2 Términos Generales Diseño, Medición, Rendimiento, Experimentación. 1. INTRODUCCIÓN Seguir información nueva y relevante de flujos de datos temporales para usuarios con necesidades duraderas ha sido un tema de investigación desafiante en la recuperación de información. El filtrado adaptativo (AF) es una tarea de predicción en línea de la relevancia de cada nuevo documento con respecto a temas predefinidos. Basándose en la consulta inicial y algunos ejemplos positivos (si están disponibles), un sistema de AF mantiene un perfil para cada tema de interés, y lo actualiza constantemente según la retroalimentación del usuario. La naturaleza de aprendizaje incremental de los sistemas de AF los hace más poderosos que los motores de búsqueda estándar que admiten recuperación ad-hoc (por ejemplo. Google y Yahoo) en términos de encontrar información relevante con respecto a temas de interés duraderos, y más atractivo para los usuarios que están dispuestos a proporcionar retroalimentación para adaptar el sistema a sus necesidades de información específicas, sin tener que modificar sus consultas manualmente. Se han estudiado una variedad de algoritmos de aprendizaje supervisado (clasificadores de estilo Rocchio, modelos exponenciales-gaussianos, regresión local y enfoques de regresión logística) para entornos adaptativos, examinados con retroalimentación de relevancia explícita e implícita, y evaluados con respecto a la optimización de la utilidad en grandes colecciones de datos de referencia en los foros de TREC (Conferencias de Recuperación de Texto) y TDT (Detección y Seguimiento de Temas) [1, 4, 7, 15, 16, 20, 24, 23]. La regresión logística regularizada [21] se ha encontrado representativa para los enfoques de vanguardia, y altamente eficiente para adaptaciones frecuentes del modelo en grandes colecciones de documentos como el corpus TREC-10 (más de 800,000 documentos y 84 temas). A pesar de los logros sustanciales en la investigación reciente de filtrado adaptativo, aún quedan problemas significativos sin resolver sobre cómo aprovechar de manera efectiva y eficiente la retroalimentación del usuario. Específicamente, los siguientes problemas pueden limitar seriamente la verdadera utilidad de los sistemas de AF en aplicaciones del mundo real: 1. El usuario tiene un papel bastante pasivo en la configuración convencional de filtrado adaptativo: solo reacciona ante el sistema cuando este toma una decisión afirmativa sobre un documento, confirmando o rechazando esa decisión. Una alternativa más activa sería permitir al usuario emitir múltiples consultas sobre un tema, revisar una lista clasificada de documentos candidatos (o pasajes) por consulta, y proporcionar retroalimentación sobre la lista clasificada, refinando así su necesidad de información y solicitando listas clasificadas actualizadas. La última forma de interacción del usuario ha sido altamente efectiva en la recuperación estándar para consultas ad-hoc. Cómo implementar una estrategia para necesidades de información duraderas en entornos de FA es una pregunta abierta para la investigación. 2. La unidad para recibir un juicio de relevancia (sí o no) está restringida al nivel del documento en AF convencional. Sin embargo, un usuario real puede estar dispuesto a proporcionar comentarios más informativos y detallados resaltando algunas partes de texto en un documento recuperado como relevantes, en lugar de etiquetar todo el documento como relevante. Aprovechar de manera efectiva este tipo de retroalimentación detallada podría mejorar sustancialmente la calidad de un sistema de AF. Para esto, necesitamos habilitar el aprendizaje supervisado a partir de fragmentos de texto etiquetados de longitud arbitraria en lugar de solo permitir documentos etiquetados. 3. Los documentos seleccionados por el sistema suelen ser altamente redundantes. Un evento importante de noticias, por ejemplo, sería reportado por múltiples fuentes repetidamente durante un tiempo, haciendo que la mayor parte del contenido informativo en esos artículos sea redundante entre sí. Un sistema de AF convencional seleccionaría todas estas historias de noticias redundantes para la retroalimentación del usuario, desperdiciando el tiempo de los usuarios mientras ofrece poco beneficio. Claramente, las técnicas de detección de novedades pueden ayudar en principio [25, 2, 22] a mejorar la utilidad de los sistemas AF. Sin embargo, la efectividad de tales técnicas a nivel de pasaje para detectar novedad con respecto a la retroalimentación de los usuarios a nivel detallado y para detectar redundancia en listas clasificadas aún debe ser evaluada utilizando una medida de utilidad que imite las necesidades de un usuario real. Para abordar las limitaciones actuales de los sistemas de AF mencionadas anteriormente, proponemos y examinamos un nuevo enfoque en este artículo, que combina las fortalezas del AF convencional (aprendizaje incremental de modelos de temas), recuperación de pasajes multipase para consultas de larga duración condicionadas al tema, y detección de novedades para eliminar la redundancia de las interacciones del usuario con el sistema. Llamamos al nuevo proceso destilación de información basada en utilidad. Ten en cuenta que los corpus de referencia convencionales para evaluaciones de AF, que tienen juicios de relevancia a nivel de documento y no definen tareas con múltiples consultas, son insuficientes para evaluar el nuevo enfoque. Por lo tanto, ampliamos un corpus de referencia, la colección TDT4 de noticias y transmisiones de televisión, con definiciones de tareas, múltiples consultas por tarea y claves de respuestas por consulta. Hemos realizado nuestros experimentos en este corpus extendido TDT4 y hemos puesto a disposición del público los datos generados adicionalmente para futuras evaluaciones comparativas. Para evaluar automáticamente los fragmentos de texto arbitrarios devueltos por el sistema utilizando nuestras claves de respuestas, desarrollamos un esquema de evaluación con un procedimiento semiautomático para 1 URL: http://nyc.lti.cs.cmu.edu/downloads adquiriendo reglas que pueden comparar fragmentos con las respuestas del sistema. Además, proponemos una extensión de NDCG (Normalized Discounted Cumulated Gain) [9] para evaluar la utilidad de los pasajes clasificados como una función tanto de la relevancia como de la novedad. El resto de este documento está organizado de la siguiente manera. La sección 2 describe el proceso de destilación de información con un ejemplo concreto. La sección 3 describe los núcleos técnicos de nuestro sistema llamado CAF´E - CMU Adaptive Filtering Engine. La sección 4 discute problemas con respecto a la metodología de evaluación y propone un nuevo esquema. La sección 5 describe el corpus extendido TDT4. La sección 6 presenta nuestros experimentos y resultados. La sección 7 concluye el estudio y ofrece perspectivas futuras. 2. Una TAREA DE EJEMPLO Considera un evento noticioso: la fuga de siete convictos de una prisión de Texas en diciembre de 2000 y su captura un mes después. Suponiendo que un usuario estuviera interesado en este evento desde sus primeras etapas, la necesidad de información podría ser: Encontrar información sobre la fuga de convictos de una prisión de Texas, y la información relacionada con su recaptura. Las preguntas de nivel inferior asociadas podrían ser: 1. ¿Cuántos prisioneros escaparon? 2. ¿Dónde y cuándo fueron avistados? 3. ¿Quiénes son sus contactos conocidos dentro y fuera de la prisión? 4. ¿Cómo están armados? 5. ¿Tienen algún vehículo? 6. ¿Qué pasos se han tomado hasta ahora? Llamamos a esta necesidad de información una tarea, y a las preguntas asociadas como las consultas en esta tarea. Un sistema de destilación debe monitorear los documentos entrantes, procesarlos por fragmentos en un orden temporal, seleccionar pasajes potencialmente relevantes y novedosos de cada fragmento con respecto a cada consulta, y presentar una lista clasificada de pasajes al usuario. La clasificación de pasajes aquí se basa en cuán relevante es un pasaje con respecto a la consulta actual, cuán novedoso es con respecto al historial actual del usuario (de sus interacciones con el sistema) y cuán redundante es en comparación con otros pasajes con un rango más alto en la lista. Cuando se le presenta una lista de pasajes, el usuario puede proporcionar retroalimentación resaltando tramos de texto arbitrarios que considere relevantes. Estos fragmentos de texto se toman como ejemplos positivos en la adaptación del perfil de consulta, y también se añaden al historial de los usuarios. Los pasajes no marcados por el usuario se consideran ejemplos negativos. Tan pronto como se actualiza el perfil de consulta, el sistema vuelve a realizar una búsqueda y devuelve otra lista clasificada de pasajes donde los pasajes previamente vistos son eliminados o clasificados como bajos, según la preferencia del usuario. Por ejemplo, si el usuario resalta ...los funcionarios han ofrecido una recompensa de $100,000 por su captura... como respuesta relevante a la consulta ¿Qué pasos se han tomado hasta ahora?, entonces la pieza resaltada se utiliza como un ejemplo de entrenamiento positivo adicional en la adaptación del perfil de la consulta. Este comentario también se añade al historial del usuario como un ejemplo visto, para que en el futuro, el sistema no coloque otro pasaje mencionando una recompensa de $100,000 en la parte superior de la lista clasificada. Sin embargo, un artículo que mencione que los funcionarios han duplicado el dinero de recompensa a $200,000 podría clasificarse alto ya que es relevante tanto para el perfil de consulta (actualizado) como novedoso con respecto a la historia del usuario (actualizada). El usuario puede modificar las consultas originales o agregar una nueva consulta durante el proceso; los perfiles de consulta se cambiarán en consecuencia. Claramente, la detección de novedades es muy importante para la utilidad de dicho sistema debido a la búsqueda iterativa. Sin detección de novedad, los antiguos pasajes relevantes se mostrarían al usuario repetidamente en cada lista clasificada. A través del ejemplo anterior, podemos ver las principales propiedades de nuestro nuevo marco de trabajo para la destilación de información basada en utilidad sobre documentos ordenados temporalmente. Nuestro marco de trabajo combina y amplía el poder del filtrado adaptativo (AF), la recuperación ad-hoc (IR) y la detección de novedades (ND). En comparación con la IR estándar, nuestro enfoque tiene la capacidad de aprender de forma incremental las necesidades de información a largo plazo y modelar una secuencia de consultas dentro de una tarea. En comparación con el AF convencional, permite un papel más activo del usuario en refinar sus necesidades de información y solicitar nuevos resultados al permitir comentarios sobre relevancia y novedad mediante el resaltado de fragmentos arbitrarios de texto en los pasajes devueltos por el sistema. Comparado con trabajos anteriores, esta es la primera evaluación de detección de novedad integrada con filtrado adaptativo para consultas secuenciadas que permite retroalimentación flexible del usuario sobre pasajes clasificados. La combinación de AF, IR y ND con las nuevas extensiones plantea una importante pregunta de investigación sobre la metodología de evaluación: ¿cómo podemos medir la utilidad de un sistema de destilación de información como este? Las métricas existentes en IR estándar, AF y ND son insuficientes, y se deben explorar nuevas soluciones, como discutiremos en la Sección 4, después de describir los núcleos técnicos de nuestro sistema en la próxima sección. 3. NÚCLEOS TÉCNICOS Los componentes principales de CAFÉ son: 1) AF para el aprendizaje incremental de perfiles de consulta, 2) IR para estimar la relevancia de pasajes con respecto a los perfiles de consulta, 3) ND para evaluar la novedad de pasajes con respecto al historial de usuarios, y 4) un componente anti-redundancia para eliminar la redundancia de las listas clasificadas. 3.1 Componente de Filtrado Adaptativo Utilizamos un algoritmo de vanguardia en el campo, el método de regresión logística regularizada que obtuvo los mejores resultados en varios corpus de evaluación de referencia para AF [21]. La regresión logística (LR) es un algoritmo de aprendizaje supervisado para clasificación estadística. Basándose en un conjunto de entrenamiento de instancias etiquetadas, aprende un modelo de clase que luego puede ser utilizado para predecir las etiquetas de instancias no vistas. Su rendimiento, así como su eficiencia en términos de tiempo de entrenamiento, lo convierten en un buen candidato cuando se requieren actualizaciones frecuentes del modelo de clase, como es el caso en el filtrado adaptativo, donde el sistema debe aprender de cada nueva retroalimentación proporcionada por el usuario. (Consulte [21] y [23] para complejidad computacional y problemas de implementación). En el filtrado adaptativo, cada consulta se considera como una clase y la probabilidad de que un pasaje pertenezca a esta clase corresponde al grado de relevancia del pasaje con respecto a la consulta. Para entrenar el modelo, utilizamos la propia consulta como ejemplo inicial de entrenamiento positivo de la clase, y las piezas de texto resaltadas por el usuario (marcadas como Relevante o No relevante) durante la retroalimentación como ejemplos de entrenamiento adicionales. Para abordar el problema de inicio en frío en la etapa inicial antes de obtener cualquier retroalimentación del usuario, el sistema utiliza una pequeña muestra de un corpus retrospectivo como ejemplos negativos iniciales en el conjunto de entrenamiento. Los detalles de cómo utilizar la regresión logística para el filtrado adaptativo (asignando diferentes pesos a las instancias de entrenamiento positivas y negativas, y regularizando la función objetivo para prevenir el sobreajuste en los datos de entrenamiento) se presentan en [21]. El modelo de clase w∗ aprendido por Regresión Logística, o el perfil de consulta, es un vector cuyas dimensiones son términos individuales y cuyos elementos son los coeficientes de regresión, indicando cuán influyente es cada término en el perfil de consulta. El perfil de la consulta se actualiza cada vez que se recibe una nueva pieza de retroalimentación del usuario. Un peso decaído temporalmente se puede aplicar a cada ejemplo de entrenamiento, como una opción, para enfatizar la retroalimentación más reciente del usuario. 3.2 Componente de Recuperación de Pasajes Utilizamos técnicas estándar de IR en esta parte de nuestro sistema. Los documentos entrantes se procesan por bloques, donde cada bloque puede definirse como un intervalo de tiempo fijo o como un número fijo de documentos, según lo prefiera el usuario. Para cada documento entrante, se actualizan las estadísticas del corpus como el IDF (Frecuencia Inversa de Documento) de cada término. Utilizamos un identificador y rastreador de entidades con tecnología de punta [8, 12] para identificar nombres de personas y lugares, y fusionarlos con entidades con referencia cruzada vistas en el pasado. Luego, los documentos se dividen en pasajes, que pueden ser un documento completo, un párrafo, una oración, o cualquier otro fragmento continuo de texto, según se prefiera. Cada pasaje se representa utilizando un vector de pesos TF-IDF (Frecuencia de Término - Frecuencia Inversa de Documento), donde el término puede ser una palabra o una entidad nombrada. Dado un perfil de consulta, es decir, la solución de regresión logística w∗ como se describe en la Sección 3.1, el sistema calcula la probabilidad posterior de relevancia para cada pasaje x como fRL(x) ≡ P(y = 1|x, w∗ ) = 1 (1 + e−w∗·x) (1). Los pasajes se ordenan según sus puntajes de relevancia, y aquellos con puntajes por encima de un umbral (ajustado en un conjunto de entrenamiento) conforman la lista de relevancia que se pasa al paso de detección de novedad. 3.3 Componente de Detección de Novedad CAF´E mantiene un historial de usuario H(t), que contiene todos los fragmentos de texto hi que el usuario resaltó (como retroalimentación) durante sus interacciones pasadas con el sistema, hasta el tiempo actual t. Denotando el historial como H(t) = n h1, h2, ..., ht o , (2), el puntaje de novedad de un nuevo pasaje candidato x se calcula como: fND(x) = 1 − max i∈1..t {cos(x, hi)} (3), donde tanto el pasaje candidato x como los fragmentos de texto resaltados hi se representan como vectores TF-IDF. El puntaje de novedad de cada pasaje se compara con un umbral predefinido (ajustado también en un conjunto de entrenamiento), y cualquier pasaje con un puntaje por debajo de este umbral se elimina de la lista de relevancia. Componente de clasificación anti-redundante Aunque el componente de detección de novedad garantiza que solo la información novedosa (no vista previamente) permanezca en la lista de relevancia, esta lista aún podría contener la misma información novedosa en múltiples posiciones en la lista clasificada. Supongamos, por ejemplo, que el usuario ya ha leído sobre una recompensa de $100,000 por información sobre los convictos fugados. Una nueva noticia de que el premio ha sido aumentado a $200,000 es novedosa ya que el usuario aún no la ha leído. Sin embargo, múltiples fuentes de noticias podrían informar sobre esta noticia y podríamos terminar mostrando artículos (redundantes) de todas estas fuentes en una lista clasificada. Por lo tanto, una lista clasificada también debe ser no redundante con respecto a su propio contenido. Utilizamos una versión simplificada del método de Máxima Relevancia Marginal [5], originalmente desarrollado para combinar relevancia y novedad en la recuperación de texto y resúmenes. Nuestro procedimiento comienza con la lista actual de pasajes ordenados por relevancia (sección 3.2), filtrados por el componente de Detección de Novedades (sección 3.3), y genera una nueva lista no redundante de la siguiente manera: 1. Toma el pasaje superior en la lista actual como el primero en la nueva lista. 2. Agrega el siguiente pasaje x en la lista actual a la nueva lista solo si fAR(x) > t donde fAR(x) = 1 − max pi∈Lnew {cos(x, pi)} y Lnew es el conjunto de pasajes ya seleccionados en la nueva lista. Repetir el paso 2 hasta que se hayan examinado todos los pasajes en la lista actual. Después de aplicar el algoritmo mencionado anteriormente, cada pasaje en la nueva lista es lo suficientemente diferente a los demás, favoreciendo así la diversidad en lugar de la redundancia en la nueva lista clasificada. El umbral anti-redundancia t se ajusta en un conjunto de entrenamiento. 4. METODOLOGÍA DE EVALUACIÓN El enfoque que propusimos anteriormente para la destilación de información plantea importantes cuestiones sobre la metodología de evaluación. En primer lugar, dado que nuestro marco de trabajo permite que la salida sean fragmentos en diferentes niveles de granularidad (por ejemplo, ventanas de k oraciones donde k puede variar) en lugar de una longitud fija, no es posible tener juicios de relevancia preanotados en todos esos niveles de granularidad. En segundo lugar, dado que deseamos medir la utilidad de la salida del sistema como una combinación de relevancia y novedad, las medidas tradicionales basadas únicamente en la relevancia deben ser reemplazadas por medidas que penalicen la repetición de la misma información en la salida del sistema a lo largo del tiempo. En tercer lugar, dado que la salida del sistema son listas clasificadas, debemos recompensar a aquellos sistemas que presenten información útil (tanto relevante como no vista previamente) utilizando listas clasificadas más cortas, y penalizar a aquellos que presenten la misma información utilizando listas clasificadas más largas. Ninguna de las medidas existentes en la recuperación ad-hoc, filtrado adaptativo, detección de novedades u otras áreas relacionadas (resumen de texto y respuesta a preguntas) tiene propiedades deseables en los tres aspectos. Por lo tanto, debemos desarrollar una nueva metodología de evaluación. 4.1 Claves de Respuesta Para permitir la evaluación de un sistema cuya salida consiste en pasajes de longitud arbitraria, tomamos prestado el concepto de claves de respuesta de la comunidad de Preguntas y Respuestas (QA), donde los sistemas pueden devolver fragmentos de texto arbitrarios como respuestas. Las claves de respuestas definen lo que debe estar presente en la respuesta de un sistema para recibir crédito, y están compuestas por una colección de "nuggets" de información, es decir, unidades de datos sobre las cuales los evaluadores humanos pueden tomar decisiones binarias sobre si una respuesta del sistema las contiene o no. La definición de claves de respuestas y la toma de decisiones binarias asociadas son tareas conceptuales que requieren un mapeo semántico [19], ya que los pasajes devueltos por el sistema pueden contener la misma información expresada de muchas formas diferentes. Por lo tanto, las evaluaciones de QA han dependido de evaluadores humanos para la correspondencia entre diversas expresiones, lo que hace que el proceso sea costoso, consume mucho tiempo y no sea escalable para grandes colecciones de consultas y documentos, y evaluaciones extensas de sistemas con diversos ajustes de parámetros. 4.1.1 Automatización de la evaluación basada en claves de respuestas Los métodos de evaluación automática permitirían una construcción y ajuste más rápidos del sistema, así como proporcionar una forma objetiva y asequible de comparar diversos sistemas. Recientemente, se han propuesto métodos basados más o menos en la idea de las co-ocurrencias de n-gramos. Pourpre [10] asigna una puntuación de recuerdo fraccional a una respuesta del sistema basada en su superposición de unigramas con una descripción de pepitas dada. Por ejemplo, una respuesta del sistema A B C tiene una recuperación de 3/4 con respecto a un fragmento con la descripción A B C D. Sin embargo, este enfoque es injusto para los sistemas que presentan la misma información pero utilizando palabras distintas a A, B, C y D. Otro problema abierto es cómo ponderar las palabras individuales al medir la cercanía de una coincidencia. Por ejemplo, considera la pregunta ¿Cuántos prisioneros escaparon? En el fragmento Siete prisioneros escaparon de una prisión de Texas, no hay indicación de que siete sea la palabra clave y que debe coincidir para obtener algún crédito de relevancia. El uso de los valores de IDF no ayuda, ya que la palabra "seven" generalmente no tendrá un IDF más alto que palabras como "texas" y "prison". Además, redefinir el fragmento como simplemente siete no resuelve el problema, ya que ahora podría coincidir erróneamente con cualquier mención de siete fuera de contexto. Nuggeteer [13] trabaja sobre principios similares pero toma decisiones binarias sobre si hay un fragmento en una respuesta de sistema dada ajustando un umbral. Sin embargo, también está plagado de relevancia espuria ya que no todas las palabras contenidas en la descripción de la pepita (o respuestas correctas conocidas) son centrales para la pepita. 4.1.2 Reglas de Coincidencia de Pepitas Proponemos un método automático confiable para determinar si un fragmento de texto contiene una pepita dada, basado en reglas de coincidencia de pepitas, que se generan utilizando un procedimiento semiautomático explicado a continuación. Estas reglas son básicamente consultas booleanas que solo coincidirán con fragmentos que contengan la pepita. Por ejemplo, una regla candidata para emparejar respuestas a ¿Cuántos prisioneros escaparon? es (Texas Y siete Y escapar Y (convictos O prisioneros)), posiblemente con otros sinónimos y variantes en la regla. Para un corpus de artículos de noticias, que suelen seguir una prosa formal típica, es bastante fácil escribir reglas simples para coincidir con respuestas esperadas utilizando un enfoque de arranque, como se describe a continuación. Proponemos un enfoque de dos etapas, inspirado en Autoslog [14], que combina la capacidad de los humanos para identificar expresiones semánticamente equivalentes y la capacidad del sistema para recopilar evidencia estadística de un corpus de documentos anotados por humanos. En la primera etapa, los sujetos humanos anotaron (utilizando una herramienta de resaltado) porciones de documentos relevantes que contenían respuestas a cada fragmento 2. En la segunda etapa, los sujetos utilizaron nuestra herramienta de generación de reglas para crear reglas que coincidieran con las anotaciones de cada fragmento. La herramienta permite a los usuarios ingresar una regla booleana como una disyunción de conjunciones (por ejemplo, ((a Y b) O (a Y c Y d) O (e))). Dada una regla candidata, nuestra herramienta la utiliza como una consulta booleana sobre el conjunto completo de documentos relevantes y calcula su recall y precisión con respecto a las anotaciones que se espera que coincida. Por lo tanto, los sujetos pueden comenzar con una regla simple y refinarla iterativamente hasta que estén satisfechos con su precisión y recuperación. Observamos que era muy fácil para los humanos mejorar la precisión de una regla ajustando sus conjunciones existentes (agregando más Y) y mejorar la recuperación agregando más conjunciones a la disyunción (agregando más O). Como ejemplo, intentemos crear una regla para el acertijo que dice que siete prisioneros escaparon de la prisión de Texas. Empezamos con una regla simple - (siete). Cuando introducimos esto en la herramienta de generación de reglas, nos damos cuenta de que esta regla coincide con muchas ocurrencias espurias de siete (por ejemplo, ...siete estados...) y, por lo tanto, obtiene una puntuación baja de precisión. Podemos calificar aún más nuestra regla - Texas Y siete Y convictos. A continuación, al revisar las anotaciones omitidas, nos damos cuenta de que algunos artículos de noticias mencionaban ...siete prisioneros escaparon.... Luego reemplazamos convictos por la disyunción (convictos O prisioneros). Continuamos ajustando la regla de esta manera hasta lograr un nivel de recordación y precisión suficientemente alto, es decir, que los (pocos) errores y alarmas falsas se puedan ignorar de forma segura. Así podemos crear reglas de coincidencia de pepitas que capturen de manera sucinta varias formas de expresar una pepita, evitando al mismo tiempo emparejar respuestas incorrectas (o fuera de contexto). La participación humana en el proceso de creación de reglas garantiza reglas genéricas de alta calidad que luego se pueden utilizar para evaluar de manera confiable las respuestas del sistema arbitrario. 4.2 Evaluación de la Utilidad de una Secuencia de Listas Clasificadas La utilidad de un sistema de recuperación se puede definir como la diferencia entre cuánta información útil obtuvo el usuario y cuánto tiempo y energía perdió. Calculamos esta utilidad a partir de las utilidades de los pasajes individuales de la siguiente manera. Después de leer cada pasaje devuelto por el sistema, el usuario obtiene cierta ganancia dependiendo de la presencia de información relevante y novedosa, y incurre en una pérdida en términos del tiempo y la energía invertidos en revisar el pasaje. Sin embargo, la probabilidad de que el usuario realmente lea un pasaje depende de su posición en la lista clasificada. Por lo tanto, para una consulta q, la utilidad esperada 2 LDC [18] ya proporciona juicios de relevancia para 100 temas en el corpus TDT4. Además, nos aseguramos de que estos juicios sean exhaustivos en todo el corpus utilizando la agrupación. El valor de un pasaje pi en el rango i se puede definir como U(pi, q) = P(i) ∗ (Ganancia(pi, q) - Pérdida(pi, q)) (4) donde P(i) es la probabilidad de que el usuario pase por un pasaje en el rango i. La utilidad esperada para una lista clasificada completa de longitud n se puede calcular simplemente sumando la utilidad esperada de cada pasaje: U(q) = Σ i=1 P(i) ∗ (Ganancia(pi, q) − Pérdida(pi, q)) (5) Nótese que si ignoramos el término de pérdida y definimos P(i) como P(i) ∝ 1/ logb(b + i − 1) (6) entonces obtenemos la métrica recientemente popularizada llamada Ganancia Acumulada Descontada (DCG) [9], donde Ganancia(pi, q) se define como la relevancia graduada del pasaje pi. Sin embargo, sin el término de pérdida, DCG es una métrica puramente orientada al recuerdo y no es adecuada para un entorno de filtrado adaptativo, donde la utilidad del sistema depende en parte de su capacidad para limitar el número de elementos mostrados al usuario. Aunque P(i) podría definirse en base a estudios empíricos del comportamiento del usuario, por simplicidad, utilizamos P(i) tal como se define exactamente en la ecuación 6. La ganancia G(pi, q) del pasaje pi con respecto a la consulta q es una función de - 1) el número de pepitas relevantes presentes en pi, y 2) la novedad de cada una de estas pepitas. Combinamos estos dos factores de la siguiente manera. Para cada pepita Nj, asignamos un peso inicial wj, y también mantenemos un conteo nj del número de veces que esta pepita ha sido vista por el usuario en el pasado. La ganancia derivada de cada ocurrencia posterior del mismo hallazgo se asume que se reduce por un factor de amortiguamiento γ. Por lo tanto, G(pi, q) se define como G(pi, q) = X Nj ∈C(pi,q) wj ∗ γnj (7) donde C(pi, q) es el conjunto de todas las pepitas que aparecen en el pasaje pi y también pertenecen a la clave de respuesta de la consulta q. Los pesos iniciales wj se establecen todos en 1.0 en nuestros experimentos, pero también pueden establecerse según un enfoque de pirámide [11]. La elección del factor de amortiguación γ determina la tolerancia del usuario a la redundancia. Cuando γ = 0, un pepita solo recibirá crédito por su primera ocurrencia, es decir, cuando nj sea cero. Para 0 < γ < 1, un nugget recibe menos crédito por cada ocurrencia sucesiva. Cuando γ = 1, no se produce amortiguamiento y las repeticiones de un valor reciben el mismo crédito. Se debe tener en cuenta que los recuentos de ocurrencia de las pepitas se conservan entre la evaluación de las listas clasificadas sucesivas devueltas por el sistema, ya que se espera que los usuarios recuerden lo que el sistema les mostró en el pasado. Definimos la pérdida L(pi, q) como un costo constante c (usamos 0.1) incurrido al leer un pasaje devuelto por el sistema. Por lo tanto, nuestra métrica puede ser reescrita como U(q) = nX i=1 Ganancia(pi, q) logb(b + i − 1) − L(n) (8) donde L(n) es la pérdida asociada con una lista clasificada de longitud n: L(n) = c · nX i=1 1 logb(b + i − 1) (9) 3 Nótese que 00 = 1 Debido a la similitud con la Ganancia Acumulada Descontada (DCG), llamamos a nuestra métrica Utilidad Acumulada Descontada (DCU). El puntaje de DCU obtenido por el sistema se convierte en un puntaje de DCU normalizado (NDCU) dividiéndolo por el puntaje de DCU de la lista ideal clasificada, que se crea ordenando los pasajes por sus puntajes de utilidad decrecientes U(pi, q) y deteniéndose cuando U(pi, q) ≤ 0, es decir, cuando la ganancia es menor o igual al costo de leer el pasaje. El conjunto de datos TDT4 fue el corpus de referencia utilizado en las evaluaciones TDT2002 y TDT2003. El corpus consiste en más de 90,000 artículos de noticias de múltiples fuentes (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America, PRI the World, etc.) publicados entre octubre de 2000 y enero de 2001, en los idiomas árabe, inglés y mandarín. Las versiones de los artículos en otros idiomas reconocidas por voz y traducidas por máquina también fueron proporcionadas. LDC [18] ha anotado el corpus con 100 temas, que corresponden a varios eventos de noticias en este período de tiempo. De entre estos, seleccionamos un subconjunto de 12 eventos accionables y definimos tareas correspondientes para ellos. Para cada tarea, definimos manualmente un perfil que consiste en un conjunto inicial de (5 a 10) consultas, una descripción de texto libre del historial del usuario, es decir, lo que el usuario ya sabe sobre el evento, y una lista de documentos conocidos relevantes y no relevantes (si están disponibles) como ejemplos de entrenamiento. Para cada consulta, generamos claves de respuestas y reglas de coincidencia de pepitas correspondientes utilizando el procedimiento descrito en la sección 4.1.2, y produjimos un total de 120 consultas, con un promedio de 7 pepitas por consulta. EXPERIMENTOS Y RESULTADOS 6.1 Baselines Utilizamos Indri [17], un motor de búsqueda basado en modelos de lenguaje popular, como referencia para comparar con CAFÉ. Indri admite la funcionalidad estándar de los motores de búsqueda, incluido el feedback de pseudo-relevancia (PRF) [3, 6], y es representativo de un sistema de recuperación basado en consultas típico. Indri no admite ningún tipo de detección de novedades. Comparamos Indri con PRF activado y desactivado, frente a CAFÉ con retroalimentación del usuario, detección de novedades y clasificación antirredundante activados y desactivados. 6.2 Configuración Experimental Dividimos el corpus TDT4 que abarca 4 meses en 10 fragmentos, cada uno definido como un período de 12 días consecutivos. En cualquier momento dado del proceso de destilación, cada sistema accedía a los datos pasados hasta el punto actual, y devolvía una lista clasificada de hasta 50 pasajes por consulta. Las 12 tareas definidas en el corpus se dividieron en un conjunto de entrenamiento y prueba con 6 tareas cada uno. Cada sistema pudo utilizar el conjunto de entrenamiento para ajustar sus parámetros con el fin de optimizar NDCU (ecuación 8), incluyendo el umbral de relevancia tanto para Indri como para CAF´E, y los umbrales de novedad y antirredundancia para CAF´E. La NDCU para cada ejecución del sistema se calcula automáticamente. También se simuló la retroalimentación de los usuarios: los juicios de relevancia para cada pasaje devuelto por el sistema (según las reglas de coincidencia de pepitas descritas en la sección 4.1.2) fueron utilizados como retroalimentación de los usuarios en la adaptación de perfiles de consulta e historiales de usuario. En la Tabla 1, mostramos los puntajes NDCU de los dos sistemas en diferentes configuraciones. Estas puntuaciones se promedian en las seis tareas del conjunto de pruebas, y se calculan con dos factores de amortiguación (ver sección 4.2): γ = 0 y 0.1, para simular ninguna tolerancia y poca tolerancia para la redundancia, respectivamente. Usar γ = 0 crea una métrica mucho más estricta ya que no otorga ningún crédito a un pasaje que contenga información relevante pero redundante. Por lo tanto, la mejora obtenida al habilitar la retroalimentación del usuario es menor con γ = 0 que la mejora obtenida de la retroalimentación con γ = 0.1. Esto revela una deficiencia de los sistemas de recuperación contemporáneos: cuando el usuario da retroalimentación positiva sobre un pasaje, los sistemas otorgan mayor peso a los términos presentes en ese pasaje y tienden a recuperar otros pasajes que contienen los mismos términos, y por lo tanto, generalmente la misma información. Sin embargo, el usuario no se beneficia al ver pasajes redundantes y generalmente está interesado en otros pasajes que contengan información relacionada. Es informativo evaluar los sistemas de recuperación utilizando nuestra medida de utilidad (con γ = 0) que tiene en cuenta la novedad y, por lo tanto, ofrece una imagen más realista de cuán bien un sistema puede generalizar a partir de la retroalimentación del usuario, en lugar de utilizar medidas tradicionales de RI como la recuperación y la precisión, que ofrecen una imagen incompleta de la mejora obtenida a partir de la retroalimentación del usuario. A veces, sin embargo, los usuarios podrían estar interesados en ver la misma información de múltiples fuentes, como un indicador de su importancia o fiabilidad. En tal caso, simplemente podemos elegir un valor más alto para γ que corresponda a una mayor tolerancia para la redundancia, y así permitir que el sistema ajuste sus parámetros en consecuencia. Dado que los documentos fueron procesados por fragmentos, sería interesante ver cómo mejora el rendimiento de los sistemas con el tiempo. Las figuras 1 y 2 muestran las tendencias de rendimiento de ambos sistemas a lo largo de los fragmentos. Si bien se espera que el rendimiento con y sin retroalimentación en los primeros fragmentos sea similar, para los fragmentos posteriores, la curva de rendimiento con la retroalimentación habilitada supera a la del ajuste sin retroalimentación. Las tendencias de rendimiento no son consistentes en todos los fragmentos porque los documentos relevantes no están distribuidos uniformemente en todos los fragmentos, lo que hace que algunas consultas sean más fáciles que otras en ciertos fragmentos. Además, dado que Indri utiliza retroalimentación de pseudo-relevancia mientras que CAF´E utiliza retroalimentación basada en juicios de relevancia reales, la mejora en el caso de Indri es menos dramática que la de CAF´E. 7. CONCLUSIONES FINALES Este artículo presenta la primera investigación sobre la destilación de información basada en la utilidad con un sistema que aprende las necesidades de información duraderas a partir de la retroalimentación detallada del usuario sobre una secuencia de pasajes clasificados. Nuestro sistema, llamado CAF´E, combina filtrado adaptativo, detección de novedades y clasificación de pasajes antirredundantes en un marco unificado para la optimización de la utilidad. Desarrollamos un nuevo esquema para la evaluación y retroalimentación automatizadas basado en un procedimiento semiautomático para adquirir reglas que permitan emparejar automáticamente fragmentos con las respuestas del sistema. También propusimos una extensión de la métrica NDCG para evaluar la utilidad de los pasajes clasificados como una combinación ponderada de relevancia y novedad. Nuestros experimentos en el recién anotado corpus de referencia TDT4 muestran un alentador aumento de utilidad sobre Indri, y también sobre nuestro propio sistema con aprendizaje incremental y detección de novedades desactivados. 8. AGRADECIMIENTOS Nos gustaría agradecer a Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng y al Programa de Análisis de Datos Cualitativos de la Universidad de Pittsburgh liderado por el Dr. Stuart Shulman por su ayuda en la recolección y procesamiento de las anotaciones extendidas de TDT4 utilizadas en nuestros experimentos. Este trabajo está parcialmente respaldado por la Fundación Nacional de Ciencias (NSF) bajo la subvención IIS0434035, y la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo los contratos NBCHD030010 y W0550432. Cualquier opinión, hallazgo, conclusión o recomendación expresada en este material son responsabilidad de los autores y no reflejan necesariamente los puntos de vista de los patrocinadores. AUTORES ADICIONALES Jian Zhang (jianzhan@stat.purdue.edu)∗ , Jaime Carbonell (jgc@cs.cmu.edu)† , Peter Brusilovsky (peterb+@pitt.edu)‡ , Daqing He(dah44@pitt.edu)‡ 10. REFERENCIAS [1] J. Allan. Retroalimentación incremental de relevancia para la filtración de información. Actas de la 19ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 270-278, 1996. [2] J. Allan, C. Wade y A. Bolivar. Recuperación y Detección de Novedad a Nivel de Oración. Actas de la conferencia ACM SIGIR sobre investigación y desarrollo en recuperación de información, 2003. [3] C. Buckley, G. Salton y J. Allan. Recuperación automática con información de localidad utilizando SMART. Publicación especial del NIST, (500207):59-72, 1993. [4] J. Callan. Aprendiendo mientras se filtran documentos. Actas de la 21ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 224-231, 1998. [5] J. Carbonell y J. Goldstein. El uso de MMR, Reordenamiento basado en Diversidad para Reorganizar Documentos y Producir Resúmenes. Actas de la 21ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 335-336, 1998. [6] E. Efthimiadis. Expansión de consulta. Revisión Anual de Ciencia de la Información y Tecnología (ARIST), 31:p121-87, 1996. [7] J. Fiscus y G. Duddington. Resumen de Detección y Seguimiento de Temas. Detección y seguimiento de temas: Organización de la información basada en eventos, páginas 17-31. [8] R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov y S. Roukos. Un modelo estadístico para la detección y seguimiento de entidades multilingües. NAACL/HLT, 2004. [9] K. Järvelin y J. Kekäläinen. Evaluación basada en la Ganancia Acumulada de Técnicas de Recuperación de Información. ACM Transactions on Information Systems (TOIS), 20(4):422-446, 2002. [10] J. Lin y D. Demner-Fushman. Evaluación automática de respuestas a preguntas de definición. Actas de la Conferencia de Tecnología del Lenguaje Humano y Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural (HLT/EMNLP 2005), 2005. ∗ Departamento de Estadística, Universidad de Purdue, West Lafayette, EE. UU. † Instituto de Tecnologías del Lenguaje, Universidad Carnegie Mellon, Pittsburgh, EE. UU. ‡ Escuela de Ciencias de la Información, Universidad de Pittsburgh, Pittsburgh, EE. UU. [11] J. Lin y D. Demner-Fushman. ¿Se derrumbarán las pirámides construidas con pepitas? Actas de HLT-NAACL, 2006. [12] X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla y S. Roukos. Un algoritmo de resolución de correferencia síncrona basado en el árbol de Bell. Actas de ACL, 4:136-143, 2004. [13] G. Marton. Nuggeteer: Evaluación Automática Basada en Pepitas Utilizando Descripciones y Juicios. HLT/NAACL, 2006. [14] E. Riloff. Construcción automática de un diccionario para tareas de extracción de información. Actas de la Undécima Conferencia Nacional sobre Inteligencia Artificial, páginas 811-816, 1993. [15] S. Robertson y S. Walker. Microsoft Cambridge en TREC-9: Pista de filtrado. La Novena Conferencia de Recuperación de Texto (TREC-9), páginas 361-368. [16] R. Schapire, Y. Cantante, y A. Singhal. Boosting y Rocchio aplicados a la filtración de texto. Actas de la 21ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 215-223, 1998. [17] T. Strohman, D. Metzler, H. Turtle y W. Croft. Indri: Un motor de búsqueda basado en modelos de lenguaje para consultas complejas. Actas de la Conferencia Internacional sobre Análisis de Inteligencia, 2004. [18] El Consorcio de Datos Lingüísticos. http://www.ldc.upenn.edu/. [19] E. Voorhees. Resumen de la pista de preguntas y respuestas TREC 2003. Actas de la Duodécima Conferencia de Recuperación de Texto (TREC 2003), 2003. [20] Y. Yang y B. Kisiel. Regresión local basada en márgenes para filtrado adaptativo. Actas de la duodécima conferencia internacional sobre gestión de la información y el conocimiento, páginas 191-198, 2003. [21] Y. Yang, S. Yoo, J. Zhang y B. Kisiel. Robustez de los Métodos de Filtrado Adaptativo en una Evaluación de Referencia Cruzada. Actas de la 28ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 98-105, 2005. [22] C. Zhai, W. Cohen y J. Lafferty. Más allá de la relevancia independiente: Métodos y métricas de evaluación para la recuperación de subtemas. Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 10-17, 2003. [23] J. Zhang y Y. Yang. Robustez de los Métodos de Clasificación Lineal Regularizados en la Categorización de Textos. Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre investigación y desarrollo en recuperación de información, páginas 190-197, 2003. [24] Y. Zhang. Utilizando priors bayesianos para combinar clasificadores para filtrado adaptativo. Actas de la 27ª conferencia internacional anual sobre Investigación y desarrollo en recuperación de información, páginas 345-352, 2004. [25] Y. Zhang, J. Callan y T. Minka. Detección de novedad y redundancia en el filtrado adaptativo. Actas de la 25ª Conferencia Internacional Anual de ACM SIGIR sobre Investigación y Desarrollo en Recuperación de Información, 2002.