La gestión distribuida del flujo de tráfico aéreo basada en agentes es uno de los desafíos fundamentales que enfrenta la Administración Federal de Aviación (FAA) en la actualidad. La FAA estima que solo en 2005 hubo más de 322,000 horas de retrasos con un costo para la industria superior a los tres mil millones de dólares. Encontrar soluciones confiables y adaptables al problema de gestión del flujo es de suma importancia si se quiere lograr el objetivo declarado de los Sistemas de Transporte Aéreo de la Próxima Generación de acomodar tres veces el volumen de tráfico actual. Este problema es particularmente complejo ya que requiere la integración y/o coordinación de muchos factores, incluyendo: nuevos datos (por ejemplo, información meteorológica cambiante), prioridades potencialmente conflictivas (por ejemplo, diferentes aerolíneas), recursos limitados (por ejemplo, controladores de tráfico aéreo) y un volumen de tráfico muy alto (por ejemplo, más de 40,000 vuelos sobre el espacio aéreo de los Estados Unidos). En este artículo utilizamos FACET, un simulador de flujo de tráfico aéreo desarrollado en la NASA y ampliamente utilizado por la FAA y la industria, para probar un algoritmo multiagente para la gestión del flujo de tráfico. Un agente está asociado con un punto fijo (una ubicación específica en el espacio 2D) y su acción consiste en establecer la separación requerida entre los aviones que pasan por ese punto fijo. Los agentes utilizan el aprendizaje por refuerzo para establecer esta separación y sus acciones aceleran o ralentizan el tráfico para gestionar la congestión. Nuestros resultados basados en FACET muestran que los agentes que reciben recompensas personalizadas reducen la congestión hasta en un 45% en comparación con los agentes que reciben una recompensa global y hasta en un 67% en comparación con un enfoque actual de la industria (estimación de Monte Carlo). Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial Sistemas Multiagentes Términos Generales Algoritmos, Rendimiento 1. INTRODUCCIÓN La gestión eficiente, segura y confiable de nuestro tráfico aéreo en constante aumento es uno de los desafíos fundamentales que enfrenta la industria aeroespacial en la actualidad. En un día típico, más de 40,000 vuelos comerciales operan dentro del espacio aéreo de los Estados Unidos [14]. Para dirigir de manera eficiente y segura este tráfico aéreo, el control actual del flujo de tráfico se basa en una estrategia de enrutamiento centralizada y jerárquica que realiza proyecciones de flujo que van desde una hasta seis horas. Como consecuencia, el sistema tarda en responder a las condiciones climáticas o del aeropuerto en desarrollo, lo que puede hacer que los retrasos locales potencialmente menores se conviertan en grandes congestiones regionales. En 2005, el clima, las decisiones de enrutamiento y las condiciones del aeropuerto causaron 437,667 retrasos, lo que representó 322,272 horas de retraso. El costo total de estos retrasos se estimó en más de tres mil millones de dólares por la industria [7]. Además, a medida que aumenta el flujo de tráfico, los procedimientos actuales aumentan la carga en el sistema, los aeropuertos y los controladores de tráfico aéreo (más aeronaves por región) sin proporcionarles medios para dar forma a los patrones de tráfico más allá de desvíos menores. La iniciativa de Sistemas de Transporte Aéreo de la Próxima Generación (NGATS) tiene como objetivo abordar estos problemas y no solo tener en cuenta un aumento triple en el tráfico, sino también la creciente heterogeneidad de las aeronaves y la disminución de las restricciones en las rutas de vuelo. A diferencia de muchos otros problemas de flujo en los que el aumento del tráfico es absorbido en cierta medida por hardware mejorado (por ejemplo, más servidores con memorias más grandes y CPUs más rápidas para el enrutamiento de internet), el dominio del tráfico aéreo necesita encontrar principalmente soluciones algorítmicas, ya que la infraestructura (por ejemplo, el número de aeropuertos) no cambiará significativamente para afectar el problema de flujo. Por lo tanto, existe una fuerte necesidad de explorar nuevas soluciones distribuidas y adaptativas al problema de control del flujo de aire. Un enfoque adaptativo y multiagente es ideal para este problema naturalmente distribuido, donde la compleja interacción entre las aeronaves, aeropuertos y controladores de tráfico hace que una solución centralizada predefinida sea severamente subóptima ante la primera desviación del plan esperado. Aunque una solución verdaderamente distribuida y adaptable (por ejemplo, vuelo libre donde las aeronaves pueden elegir casi cualquier ruta) ofrece el mayor potencial en términos de optimizar el flujo, también supone el cambio más radical con respecto al sistema actual. Como consecuencia, un cambio a dicho sistema presenta enormes dificultades tanto en términos de implementación (por ejemplo, programación y capacidad aeroportuaria) como en las repercusiones políticas (por ejemplo, impacto en los controladores de tráfico aéreo). En este artículo, nos enfocamos en un sistema basado en agentes que puede ser implementado fácilmente. En este enfoque, asignamos un agente de IFAAMAS con el código 342 978-81-904262-7-5 (RPS) del año 2007 a una ubicación fija, un lugar específico en 2D. Debido a que los planes de vuelo de las aeronaves consisten en una secuencia de puntos fijos, esta representación permite que los puntos fijos localizados (o agentes) tengan un impacto directo en el flujo del tráfico aéreo. En este enfoque, las acciones de los agentes son establecer la separación que las aeronaves en aproximación deben mantener. Este simple par de agente-acción permite a los agentes reducir la velocidad o acelerar el tráfico local y les permite tener un impacto significativo en el flujo general del tráfico aéreo. Los agentes aprenden la separación más apropiada para su ubicación utilizando un algoritmo de aprendizaje por refuerzo (RL) [15]. En un enfoque de aprendizaje por refuerzo, la selección de la recompensa del agente tiene un gran impacto en el rendimiento del sistema. En este trabajo, exploramos cuatro funciones de recompensa de agente diferentes y las comparamos con la simulación de varios cambios en el sistema y la selección de la mejor solución (por ejemplo, equivalente a una búsqueda de Monte Carlo). La primera recompensa explorada consistió en la recompensa del sistema. La segunda recompensa fue una recompensa de agente personalizada basada en colectivos [3, 17, 18]. Las dos últimas recompensas fueron recompensas personalizadas basadas en estimaciones para reducir la carga computacional del cálculo de recompensas. Los tres premios personalizados tienen como objetivo alinear las recompensas del agente con la recompensa del sistema y garantizar que las recompensas sigan siendo sensibles a las acciones de los agentes. El trabajo previo en este dominio se dividió en dos categorías distintas: los enfoques de modelado basados en principios fundamentales utilizados por expertos del dominio [5, 8, 10, 13] y los enfoques algorítmicos explorados por la comunidad de aprendizaje y/o agentes [6, 9, 12]. Aunque nuestro enfoque proviene de la segunda categoría, nuestro objetivo es cerrar la brecha utilizando FACET para probar nuestros algoritmos, un simulador introducido y ampliamente utilizado (es decir, por más de 40 organizaciones y 5000 usuarios) por trabajos en la primera categoría [4, 11]. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET. En la Sección 2, describimos el problema del flujo de tráfico aéreo y la herramienta de simulación, FACET. En la Sección 3, presentamos el enfoque basado en agentes, centrándonos en la selección de los agentes y su espacio de acción, junto con los algoritmos de aprendizaje de los agentes y las estructuras de recompensa. En la Sección 4 presentamos resultados en dominios con una y dos congestiones, exploramos diferentes compensaciones de la función objetivo del sistema, discutimos las propiedades de escalado de las recompensas de los diferentes agentes y analizamos el costo computacional de lograr ciertos niveles de rendimiento. Finalmente, en la Sección 5, discutimos las implicaciones de estos resultados y proporcionamos un mapa del trabajo requerido para permitir que la FAA alcance su objetivo declarado de aumentar el volumen de tráfico en tres veces. GESTIÓN DEL FLUJO DE TRÁFICO AÉREO Con más de 40,000 vuelos operando dentro del espacio aéreo de los Estados Unidos en un día promedio, la gestión del flujo de tráfico es un problema complejo y exigente. No solo hay preocupaciones por la eficiencia del sistema, sino también por la equidad (por ejemplo, diferentes aerolíneas), adaptabilidad (por ejemplo, patrones climáticos cambiantes), fiabilidad y seguridad (por ejemplo, gestión aeroportuaria). Para abordar tales problemas, la gestión de este flujo de tráfico se lleva a cabo en cuatro niveles jerárquicos: 1. Garantía de separación (decisiones de 2 a 30 minutos); 1. Discutimos cómo pueden manejarse con más detalle los planes de vuelo con pocos puntos de referencia en la Sección 2. 2. Flujo regional (20 minutos a 2 horas); 3. Flujo nacional (1-8 horas); y 4. Configuración dinámica del espacio aéreo (6 horas a 1 año). Debido a las estrictas pautas y preocupaciones de seguridad que rodean la separación de aeronaves, no abordaremos ese nivel de control en este documento. Del mismo modo, debido al impacto empresarial y político de la configuración dinámica del espacio aéreo, tampoco abordaremos el nivel de control de flujo más externo. En cambio, nos enfocaremos en los problemas de gestión del flujo regional y nacional, restringiendo nuestro impacto a decisiones con horizontes temporales entre veinte minutos y ocho horas. El algoritmo propuesto se ajustará entre la planificación a largo plazo de la FAA y las decisiones muy a corto plazo de los controladores de tráfico aéreo. El espacio aéreo de los Estados Unidos continentales consta de 20 centros regionales (que manejan de 200 a 300 vuelos en un día dado) y 830 sectores (que manejan de 10 a 40 vuelos). El problema de control de flujo debe abordar la integración de políticas en estos sectores y centros, tener en cuenta la complejidad del sistema (por ejemplo, más de 5200 aeropuertos de uso público y 16,000 controladores de tráfico aéreo) y manejar los cambios en las políticas causados por los patrones climáticos. Dos de los problemas fundamentales al abordar el problema del flujo son: (i) modelar y simular un sistema tan grande y complejo como la fidelidad requerida para obtener resultados confiables es difícil de lograr; y (ii) establecer el método mediante el cual se evalúa la gestión del flujo, ya que minimizar directamente el retraso total puede llevar a desigualdades hacia regiones particulares o entidades comerciales. A continuación, discutimos cómo abordamos ambos problemas, a saber, presentamos FACET, una herramienta de simulación ampliamente utilizada, y discutimos nuestra función de evaluación del sistema. Figura 1: Captura de pantalla de FACET que muestra rutas de tráfico y estadísticas de flujo de aire. 2.1 FACET FACET (Future ATM Concepts Evaluation Tool), un modelo basado en física del espacio aéreo de los Estados Unidos, fue desarrollado para modelar con precisión el complejo problema de flujo de tráfico aéreo [4]. Se basa en propagar hacia adelante en el tiempo las trayectorias de los vuelos propuestos. FACET se puede utilizar tanto para simular y mostrar el tráfico aéreo (una muestra de 24 horas con 60,000 vuelos tarda 15 minutos en simularse en una computadora de 3 GHz y 1 GB de RAM) o para proporcionar estadísticas rápidas sobre datos grabados (trayectorias 4D para 10,000 vuelos, incluyendo sectores, aeropuertos y estadísticas de puntos fijos en 10 segundos en la misma computadora) [11]. FACET es ampliamente utilizado por The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 343 la FAA, la NASA y la industria (más de 40 organizaciones y 5000 usuarios) [11]. FACET simula el tráfico aéreo basado en planes de vuelo y a través de una interfaz gráfica que permite al usuario analizar los patrones de congestión de diferentes sectores y centros (Figura 1). FACET también permite al usuario cambiar los patrones de flujo de la aeronave a través de varios mecanismos, incluyendo el control de la aeronave a través de puntos fijos. El usuario puede entonces observar los efectos de estos cambios en la congestión. En este documento, los agentes utilizan FACET directamente a través del modo por lotes, donde los agentes envían scripts a FACET pidiéndole que simule el tráfico aéreo basado en las órdenes de control de tráfico aéreo impuestas por los agentes. Los agentes luego producen sus recompensas basadas en recibir retroalimentación de FACET sobre el impacto de estas mediciones. 2.2 Evaluación del Sistema La función de evaluación del rendimiento del sistema que seleccionamos se centra en el retraso y la congestión, pero no tiene en cuenta el impacto en la equidad de diferentes entidades comerciales. En cambio, se centra en la cantidad de congestión en un sector específico y en la cantidad de retraso medido del tráfico aéreo. La combinación lineal de estos dos términos da como resultado la función de evaluación del sistema completo, G(z), como función del estado completo del sistema z. Más precisamente, tenemos: G(z) = −((1 − α)B(z) + αC(z)) , (1) donde B(z) es la penalización total por retraso para todas las aeronaves en el sistema, y C(z) es la penalización total por congestión. La importancia relativa de estas dos penalizaciones se determina por el valor de α, y exploramos varios compromisos basados en α en la Sección 4. La demora total, B, es la suma de las demoras sobre un conjunto de sectores S y se da por: B(z) = X s∈S Bs(z) (2) donde Bs(z) = X t Θ(t − τs)kt,s(t − τs) , (3) donde ks,t es el número de aeronaves en el sector s en un momento particular, τs es un tiempo predeterminado, y Θ(·) es la función escalón que es igual a 1 cuando su argumento es mayor o igual a cero, y tiene un valor de cero de lo contrario. De manera intuitiva, Bs(z) proporciona el número total de aeronaves que permanecen en un sector s después de un tiempo predeterminado τs, y ajusta su contribución al recuento según la cantidad por la que llegan tarde. De esta manera, Bs(z) proporciona un factor de retraso que no solo tiene en cuenta todas las aeronaves que llegan tarde, sino que también proporciona una escala para medir su tardanza. Esta definición se basa en la suposición de que la mayoría de las aeronaves deberían haber llegado al sector para el tiempo τs y que las aeronaves que llegan después de este tiempo se consideran tardías. En este documento, el valor de τs se determina evaluando el recuento de aeronaves en el sector en ausencia de cualquier intervención o desviación de las rutas predichas. De manera similar, la penalización total por congestión es una suma de las penalizaciones por congestión sobre los sectores de observación, S: C(z) = Σ s∈S Cs(z) (4) donde Cs(z) = a Σ t Θ(ks,t − cs)eb(ks,t−cs) , (5) donde a y b son constantes de normalización, y cs es la capacidad del sector s definida por la FAA. De manera intuitiva, Cs(z) penaliza un estado del sistema en el que el número de aeronaves en un sector excede la capacidad oficial del sector de la FAA. La capacidad de cada sector se calcula utilizando varios indicadores que incluyen el número de controladores de tráfico aéreo disponibles. La penalización exponencial tiene como objetivo proporcionar una retroalimentación fuerte para devolver el número de aeronaves en un sector por debajo de las capacidades exigidas por la FAA. El enfoque multiagente para la gestión del flujo de tráfico aéreo que presentamos se basa en agentes adaptativos que toman acciones independientes que maximizan la función de evaluación del sistema discutida anteriormente. Con ese fin, hay cuatro decisiones críticas que deben tomarse: selección del agente, selección del conjunto de acciones del agente, selección del algoritmo de aprendizaje del agente y selección de la estructura de recompensa del agente. 3.1 Selección del Agente Seleccionar la aeronave como agentes es quizás la elección más obvia para definir un agente. Esa selección tiene la ventaja de que las acciones de los agentes pueden ser intuitivas (por ejemplo, cambio de plan de vuelo, aumento o disminución de velocidad y altitud) y ofrecer un alto nivel de granularidad, ya que cada agente puede tener su propia política. Sin embargo, hay varios problemas con ese enfoque. Primero, hay más de 40,000 aeronaves en un día determinado, lo que da lugar a un sistema multiagente enormemente grande. Segundo, dado que los agentes no podrían muestrear suficientemente su espacio de estados, el aprendizaje sería excesivamente lento. Como alternativa, asignamos agentes a ubicaciones individuales en tierra a lo largo del espacio aéreo llamadas fijos. Cada agente es entonces responsable de cualquier aeronave que pase por su punto de referencia. Las reparaciones ofrecen muchas ventajas como agentes: 1. Su número puede variar dependiendo de la necesidad. El sistema puede tener tantos agentes como se requieran para una situación dada (por ejemplo, agentes que se conectan en vivo alrededor de un área con condiciones climáticas en desarrollo). 2. Debido a que las correcciones son estacionarias, recopilar datos y emparejar el comportamiento con la recompensa es más fácil. 3. Debido a que los planes de vuelo de las aeronaves consisten en puntos fijos, el agente tendrá la capacidad de afectar los patrones de flujo de tráfico. 4. Pueden ser desplegados dentro de los procedimientos actuales de enrutamiento del tráfico aéreo, y pueden ser utilizados como herramientas para ayudar a los controladores de tráfico aéreo en lugar de competir con ellos o reemplazarlos. La Figura 2 muestra un esquema de este sistema basado en agentes. Los agentes que rodean una congestión o condición climática afectan el flujo del tráfico para reducir la carga en regiones particulares. 3.2 Acciones de los Agentes El segundo problema que debe abordarse es determinar el conjunto de acciones de los agentes. Una opción obvia puede ser que las reparaciones oferten por aviones, afectando sus planes de vuelo. Aunque atractivo desde la perspectiva de vuelo libre, ese enfoque hace que los planes de vuelo sean demasiado poco confiables y complica significativamente el problema de programación (por ejemplo, la llegada a los aeropuertos y el proceso de asignación de puertas subsiguiente). En cambio, establecemos las acciones de un agente para determinar la separación (distancia entre aeronaves) que las aeronaves tienen 344 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) para mantener, al pasar por los agentes, arreglar. Esto se conoce como establecer las Millas en Rastro o MIT. Cuando un agente establece el valor de MIT en d, se instruye a las aeronaves que se dirigen hacia su punto fijo a alinearse y mantener una separación de d millas (aunque las aeronaves siempre mantendrán una distancia segura entre sí independientemente del valor de d). Cuando hay muchos aviones pasando por un punto fijo, el efecto de emitir valores MIT más altos es reducir la velocidad a la que los aviones pasan por el punto fijo. Al aumentar el valor de d, un agente puede limitar la cantidad de tráfico aéreo aguas abajo de su punto fijo, reduciendo la congestión a expensas de aumentar los retrasos aguas arriba. Figura 2: Esquema de la arquitectura del agente. Los agentes correspondientes a las correcciones en torno a una posible congestión se activan y comienzan a establecer nuevos tiempos de separación. 3.3 Aprendizaje de Agentes El objetivo de cada agente es aprender los mejores valores de d que conducirán al mejor rendimiento del sistema, G. En este documento asumimos que cada agente tendrá una función de recompensa y buscará maximizar su recompensa utilizando su propio aprendiz reforzado [15] (aunque alternativas como la evolución de neurocontroladores también son efectivas [1]). Para problemas complejos de recompensa retrasada, puede ser necesario utilizar sistemas de aprendizaje por refuerzo relativamente sofisticados como la diferencia temporal. Sin embargo, debido a nuestra selección de agentes y conjunto de acciones de agentes, el dominio de congestión del tráfico aéreo modelado en este documento solo necesita utilizar recompensas inmediatas. Como consecuencia, se utiliza el aprendizaje por refuerzo de recompensa inmediata basado en tablas simples. Nuestro aprendiz reforzado es equivalente a un aprendiz Q-ε con una tasa de descuento de 0 [15]. En cada episodio, un agente toma una acción y luego recibe una recompensa evaluando esa acción. Después de tomar la acción a y recibir la recompensa R, un agente actualiza su tabla Q (que contiene su estimación del valor de tomar esa acción [15]) de la siguiente manera: Q(a) = (1 − l)Q(a) + l(R), (6), donde l es la tasa de aprendizaje. En cada paso de tiempo, el agente elige la acción con el valor de tabla más alto con probabilidad 1 − y elige una acción al azar con probabilidad . En los experimentos descritos en este artículo, α es igual a 0.5 y es igual a 0.25. Los parámetros fueron elegidos experimentalmente, aunque el rendimiento del sistema no era excesivamente sensible a estos parámetros. 3.4 Estructura de Recompensa del Agente El último problema que debe abordarse es seleccionar la estructura de recompensa para los agentes de aprendizaje. El enfoque primero y más directo es permitir que cada agente reciba el rendimiento del sistema como su recompensa. Sin embargo, en muchos ámbitos, una estructura de recompensa como esta conduce a un aprendizaje lento. Por lo tanto, también estableceremos un segundo conjunto de estructuras de recompensa basadas en recompensas específicas del agente. Dado que los agentes buscan maximizar sus propias recompensas, una tarea crítica es crear buenas recompensas para los agentes, es decir, recompensas que, al ser perseguidas por los agentes, conduzcan a un buen rendimiento general del sistema. En este trabajo nos enfocamos en recompensas de diferencia que tienen como objetivo proporcionar una recompensa que sea sensible a las acciones de los agentes y alineada con la recompensa general del sistema [2, 17, 18]. 3.4.1 Recompensas de Diferencia Consideremos recompensas de diferencia de la forma [2, 17, 18]: Di ≡ G(z) − G(z − zi + ci), (7) donde zi es la acción del agente i. Todos los componentes de z que son afectados por el agente i son reemplazados por la constante fija ci 2. En muchas situaciones es posible usar un ci que es equivalente a sacar al agente i del sistema. De manera intuitiva, esto hace que el segundo término de la recompensa de diferencia evalúe el rendimiento del sistema sin i y, por lo tanto, D evalúa la contribución del agente al rendimiento del sistema. Hay dos ventajas de usar D: Primero, debido a que el segundo término elimina una parte significativa del impacto de otros agentes en el sistema, proporciona a un agente una señal más limpia que G. Este beneficio ha sido denominado aprendibilidad (los agentes tienen más facilidad para aprender) en trabajos anteriores [2, 17]. Segundo, dado que el segundo término no depende de las acciones del agente i, cualquier acción del agente i que mejore D, también mejora G. Este término, que mide el grado de alineación entre dos recompensas, ha sido denominado factorización en trabajos anteriores [2, 17]. 3.4.2 Estimaciones de Diferencia de Recompensas. Aunque proporciona un buen compromiso entre apuntar al rendimiento del sistema y eliminar el impacto de otros agentes de la recompensa de un agente, un problema que puede afectar a D es el costo computacional. Debido a que se basa en el cálculo del término contrafactual G(z − zi + ci) (es decir, el rendimiento del sistema sin el agente i), puede ser difícil o imposible de calcular, especialmente cuando no se conoce la forma matemática exacta de G. Enfoquémonos en las funciones G en la siguiente forma: G(z) = Gf (f(z)), donde Gf () es no lineal con una forma funcional conocida y, f(z) = X i fi(zi), donde cada fi es una función no lineal desconocida. Suponemos que podemos muestrear valores de f(z), lo que nos permite calcular G, pero que no podemos muestrear de cada fi(zi). Esta notación utiliza relleno con ceros y suma de vectores en lugar de concatenación para formar vectores de estado completos a partir de vectores de estado parciales. El vector zi en nuestra notación sería ziei en la notación estándar de vectores, donde ei es un vector con un valor de 1 en el componente i-ésimo y es cero en todas partes. El Sexto Internacional. Además, asumimos que Gf es mucho más fácil de calcular que f(z), o que incluso puede que no podamos calcular f(z) directamente y debamos muestrearlo de una computación en caja negra. Esta forma de G coincide con nuestra evaluación del sistema en el dominio del tráfico aéreo. Cuando organizamos los agentes de manera que cada aeronave sea típicamente afectada solo por un agente, el impacto de cada agente en el recuento del número de aeronaves en un sector, kt,s, será en su mayoría independiente de los otros agentes. Estos valores de kt,s son los f(z) en nuestra formulación y las funciones de penalización forman Gf. Se debe tener en cuenta que, dada la cantidad de aeronaves, las funciones de penalización (Gf) se pueden calcular fácilmente en microsegundos, mientras que la cantidad de aeronaves (f) solo se puede calcular ejecutando FACET, lo que lleva unos segundos. Para calcular nuestro contrafactual G(z − zi + ci) necesitamos calcular: Gf (f(z − zi + ci)) = Gf 0 @ X j=i fj(zj) + fi(ci) 1 A (10) = Gf (f(z) − fi(zi) + fi(ci)) . (11) Desafortunadamente, no podemos calcular esto directamente ya que los valores de fi(zi) son desconocidos. Sin embargo, si los agentes toman acciones de forma independiente (no observan cómo actúan otros agentes antes de tomar su propia acción), podemos aprovechar la forma lineal de f(z) en el fis con la siguiente igualdad: E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12) donde E(f−i(z−i)|zi) es el valor esperado de todos los fs excepto fi dado el valor de zi y E(f−i(z−i)|ci) es el valor esperado de todos los fs excepto fi dado que el valor de zi se cambia a ci. Podemos entonces estimar f(z − zi + ci): f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(fi(zi)|zi) + E(fi(ci)|ci) + E(f−i(z−i)|ci) − E(f−i(z−i)|zi) = f(z) − E(f(z)|zi) + E(f(z)|ci). Por lo tanto, podemos evaluar Di = G(z) − G(z − zi + ci) como: Dest1 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13), lo que nos deja con la tarea de estimar los valores de E(f(z)|zi) y E(f(z)|ci)). Estas estimaciones pueden ser calculadas manteniendo una tabla de promedios donde promediamos los valores de f(z) observados para cada valor de zi que hemos visto. Esta estimación debería mejorar a medida que aumenta el número de muestras. Para mejorar nuestras estimaciones, podemos establecer ci = E(z) y si hacemos la aproximación del cuadrado medio de la media de f(E(z)) ≈ E(f(z)), entonces podemos estimar G(z) − G(z − zi + ci) como: Dest2 i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14) Esta formulación tiene la ventaja de que tenemos más muestras a nuestra disposición para estimar E(f(z)) que para estimar E(f(z)|ci)). RESULTADOS DE LA SIMULACIÓN En este artículo probamos el rendimiento de nuestro método de optimización del tráfico aéreo basado en agentes en una serie de simulaciones utilizando el simulador de tráfico aéreo FACET. En todos los experimentos probamos el rendimiento de cinco métodos diferentes. El primer método es la estimación de Monte Carlo, donde se crean políticas aleatorias y se elige la mejor política. Los otros cuatro métodos son métodos basados en agentes donde los agentes están maximizando una de las siguientes recompensas: 1. La recompensa del sistema, G(z), como se define en la Ecuación 1.2. La recompensa de diferencia, Di(z), asumiendo que los agentes pueden calcular contrafactuales. 3. Estimación de la recompensa de diferencia, Dest1 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)|ci). 4. Estimación de la recompensa de diferencia, Dest2 i (z), donde los agentes estiman el contrafactual utilizando E(f(z)|zi) y E(f(z)). Estos métodos son primero probados en un dominio de tráfico aéreo con 300 aeronaves, donde 200 de las aeronaves pasan por un único punto de congestión durante una simulación de cuatro horas. Los agentes son responsables de reducir la congestión en este punto único, al mismo tiempo que intentan minimizar el retraso. Los métodos son luego probados en un problema más difícil, donde se agrega un segundo punto de congestión y los 100 aviones restantes pasan por este segundo punto de congestión. En todos los experimentos, el objetivo del sistema es maximizar el rendimiento del sistema dado por G(z) con los parámetros, a = 50, b = 0.3, τs1 igual a 200 minutos y τs1 igual a 175 minutos. Estos valores de τ se obtienen examinando el momento en que la mayoría de las aeronaves abandonan los sectores, cuando no se está realizando control de congestión. Excepto donde se indique, el intercambio entre congestión y retraso, α, se establece en 0.5. En todos los experimentos para hacer que los resultados del agente sean comparables con la estimación de Monte Carlo, se utilizan las mejores políticas elegidas por los agentes en los resultados. Todos los resultados son un promedio de treinta pruebas independientes con las diferencias en la media (σ/ √ n) mostradas como barras de error, aunque en la mayoría de los casos las barras de error son demasiado pequeñas para ver. Figura 3: Rendimiento en un problema de congestión individual, con 300 aeronaves, 20 agentes y α = .5. 4.1 Congestión Individual En el primer experimento probamos el rendimiento de los cinco métodos cuando hay un único punto de congestión, con veinte agentes. Este punto de congestión se crea al establecer una serie de planes de vuelo que provocan el número de aeronaves en 346 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) el sector de interés es significativamente mayor que el número permitido por la FAA. Los resultados mostrados en las Figuras 3 y 4 muestran el rendimiento de los cinco algoritmos en dos evaluaciones de sistemas diferentes. En ambos casos, los métodos basados en agentes superan significativamente al método de Monte Carlo. Este resultado no es sorprendente ya que los métodos basados en agentes exploran de manera inteligente su espacio, mientras que el método de Monte Carlo explora el espacio de forma aleatoria. Figura 4: Rendimiento en un problema de congestión único, con 300 aeronaves, 20 agentes y α = .75. Entre los métodos basados en agentes, los agentes que utilizan recompensas diferenciales tienen un mejor rendimiento que los agentes que utilizan la recompensa del sistema. Nuevamente, esto no es sorprendente, ya que con veinte agentes, un agente que intenta directamente maximizar la recompensa del sistema tiene dificultades para determinar el efecto de sus acciones en su propia recompensa. Aunque un agente tome una acción que reduce la congestión y la tardanza, otros agentes al mismo tiempo pueden tomar acciones que aumenten la congestión y la tardanza, haciendo que el agente crea erróneamente que su acción fue deficiente. En contraste, los agentes que utilizan la recompensa de diferencia tienen más influencia sobre el valor de su propia recompensa, por lo tanto, cuando un agente toma una buena acción, es más probable que el valor de esta acción se refleje en su recompensa. Este experimento también demuestra que estimar la diferencia de recompensa no solo es posible, sino también bastante efectivo, cuando el valor real de la diferencia de recompensa no se puede calcular. Si bien los agentes que utilizan las estimaciones no logran obtener resultados tan altos como los agentes que utilizan la recompensa de diferencia real, aún tienen un rendimiento significativamente mejor que los agentes que utilizan la recompensa del sistema. Sin embargo, ten en cuenta que el beneficio de las recompensas por la diferencia estimada solo se presenta más adelante en el aprendizaje. Al principio del aprendizaje, las estimaciones son pobres, y los agentes que utilizan las recompensas de diferencia estimadas no se desempeñan mejor que los agentes que utilizan la recompensa del sistema. 4.2 Dos Congestiones En el segundo experimento probamos el rendimiento de los cinco métodos en un problema más difícil con dos puntos de congestión. En este problema, la primera región de congestión es la misma que en el problema anterior, y la segunda región de congestión se añade en una parte diferente del país. La segunda congestión es menos severa que la primera, por lo que los agentes deben formar diferentes políticas dependiendo de qué punto de congestión estén influenciando. Figura 5: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .5. Figura 6: Rendimiento en dos problemas de congestión, con 300 aeronaves, 50 agentes y α = .5. Los resultados mostrados en la Figura 5 muestran que el rendimiento relativo de los cinco métodos es similar al caso de congestión individual. Una vez más, los métodos basados en agentes funcionan mejor que el método de Monte Carlo y los agentes que utilizan recompensas diferenciales funcionan mejor que los agentes que utilizan la recompensa del sistema. Para verificar que la mejora en el rendimiento de nuestros métodos se mantenga cuando hay un número diferente de agentes, realizamos experimentos adicionales con 50 agentes. Los resultados mostrados en la Figura 6 muestran que, de hecho, las actuaciones relativas de los métodos son comparables cuando el número de agentes se incrementa a 50. La Figura 7 muestra los resultados de escalado y demuestra que las conclusiones se mantienen en una amplia gama de número de agentes. Los agentes que utilizan Dest2 tienen un rendimiento ligeramente mejor que los agentes que utilizan Dest1 en todos los casos, excepto para 50 agentes. Esta ligera ventaja se debe a que Dest2 proporciona a los agentes una señal más limpia, ya que su estimación utiliza más puntos de datos. Penalizaciones de intercambio 4.3 La función de evaluación del sistema utilizada en los experimentos es G(z) = −((1−α)D(z)+αC(z)), que comprende penalizaciones tanto por congestión como por retraso. Esta función de evaluación The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 7: Impacto del número de agentes en el rendimiento del sistema. Dos problemas de congestión, con 300 aeronaves y α = .5, obligan a los agentes a hacer un intercambio entre estas penalizaciones relativas dependiendo del valor de α. Con un α alto, la optimización se centra en reducir la congestión, mientras que con un α bajo el sistema se enfoca en reducir la tardanza. Para verificar que los resultados obtenidos anteriormente no son específicos a un valor particular de α, repetimos el experimento con 20 agentes para α = .75. La Figura 8 muestra que cualitativamente el rendimiento relativo de los algoritmos permanece igual. A continuación, realizamos una serie de experimentos donde α varía de 0.0 a 1.0. La Figura 9 muestra los resultados que conducen a tres observaciones interesantes: • En primer lugar, hay una solución sin penalización por congestión. Esta solución hace que los agentes impongan grandes valores de MIT para bloquear todo el tráfico aéreo, lo cual parece viable cuando la evaluación del sistema no tiene en cuenta los retrasos. Todos los algoritmos encuentran esta solución, aunque tiene poco interés en la práctica debido a los grandes retrasos que causaría. • En segundo lugar, si las dos penalizaciones fueran independientes, una solución óptima sería una línea desde los dos puntos finales. Por lo tanto, a menos que D esté lejos de ser óptimo, las dos penalizaciones no son independientes. Ten en cuenta que para α = 0.5 la diferencia entre D y esta línea hipotética es tan grande como en cualquier otro lugar, lo que hace que α = 0.5 sea una elección razonable para probar los algoritmos en un entorno difícil. En tercer lugar, Monte Carlo y G son particularmente deficientes en el manejo de múltiples objetivos. Para ambos algoritmos, el rendimiento se degrada significativamente para rangos intermedios de α. 4.4 Costo Computacional Los resultados en la sección anterior muestran el rendimiento de los diferentes algoritmos después de un número específico de episodios. Esos resultados muestran que D es significativamente superior a los otros algoritmos. Una pregunta que surge, sin embargo, es cuál es el sobrecosto computacional que D impone en el sistema, y qué resultados se obtendrían si el gasto computacional adicional de D estuviera disponible para los otros algoritmos. El costo computacional de la evaluación del sistema, G (Ecuación 1), depende casi en su totalidad de la computación de la Figura 8: Rendimiento en dos problemas de congestión, con 300 aeronaves, 20 agentes y α = .75. Figura 9: Compromiso entre objetivos en dos problemas de congestión, con 300 aeronaves y 20 agentes. Ten en cuenta que Monte Carlo y G son particularmente malos para manejar múltiples objetivos. El avión cuenta para los sectores kt,s, los cuales deben ser calculados utilizando FACET. Excepto cuando se utiliza D, los valores de k se calculan una vez por episodio. Sin embargo, para calcular el término contrafactual en D, si FACET se trata como una caja negra, cada agente tendría que calcular sus propios valores de k para su contrafactual, lo que resultaría en n + 1 cálculos de k por episodio. Si bien puede ser posible optimizar el cálculo de D con cierto conocimiento de los entresijos de FACET, dada la complejidad de la simulación de FACET, no es irrazonable en este caso tratarlo como una caja negra. La Tabla 1 muestra el rendimiento de los algoritmos después de 2100 G cálculos para cada uno de los algoritmos en las simulaciones presentadas en la Figura 5, donde había 20 agentes, 2 congestiones y α = .5. Todos los algoritmos excepto el D completamente calculado alcanzan 2100 mil cálculos en el paso de tiempo 2100. Sin embargo, D calcula k una vez para el sistema, y luego una vez para cada agente, lo que resulta en 21 cálculos por paso de tiempo. Por lo tanto, alcanza 2100 cálculos en el paso de tiempo 100. También mostramos los resultados del cálculo completo de D en t=2100, que requiere 44100 cálculos de k como D44K.  Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Rendimiento del sistema para 20 agentes, 2 congestiones y α = .5, después de 2100 evaluaciones G (excepto para D44K que tiene 44100 evaluaciones G en t=2100). Aunque D44K proporciona el mejor resultado por un pequeño margen, se logra a un costo computacional considerable. De hecho, el rendimiento de los dos estimados de D es notable en este caso, ya que se obtuvieron con aproximadamente veinte veces menos cálculos de k. Además, los dos estimados de D superan significativamente el cálculo completo de D para un número dado de cálculos de k y validan las suposiciones hechas en la Sección 3.4.2. Esto demuestra que para este dominio, en la práctica es más fructífero realizar más pasos de aprendizaje y aproximar D, que pocos pasos de aprendizaje con el cálculo completo de D cuando tratamos FACET como una caja negra. 5. DISCUSIÓN La gestión eficiente, segura y confiable del flujo de tráfico aéreo es un problema complejo que requiere soluciones que integren políticas de control con horizontes temporales que van desde minutos hasta un año. La principal contribución de este artículo es presentar un algoritmo distribuido adaptativo de gestión del flujo de tráfico aéreo que puede ser implementado fácilmente y probar ese algoritmo utilizando FACET, una herramienta de simulación ampliamente utilizada por la FAA, la NASA y la industria. Nuestro método se basa en agentes que representan arreglos y que cada agente determina la separación entre aeronaves que se acercan a su arreglo. Ofrece el beneficio significativo de no requerir cambios radicales en la estructura actual de gestión del flujo de aire y, por lo tanto, es fácilmente desplegable. Los agentes utilizan el aprendizaje por refuerzo para aprender políticas de control y exploramos diferentes funciones de recompensa de los agentes y diferentes formas de estimar esas funciones. Actualmente estamos ampliando este trabajo en tres direcciones. Primero, estamos explorando nuevos métodos para estimar las recompensas de los agentes, con el fin de acelerar aún más las simulaciones. Segundo, estamos investigando estrategias de implementación y buscando modificaciones que tendrían un impacto mayor. Una de esas modificaciones es ampliar la definición de agentes de arreglos a sectores, brindando a los agentes más oportunidades para controlar el flujo de tráfico y permitirles ser más eficientes en la eliminación de la congestión. Finalmente, en colaboración con expertos del dominio, estamos investigando diferentes funciones de evaluación del sistema, más allá de la función G dependiente de la demora y la congestión presentada en este documento. Agradecimientos: Los autores agradecen a Banavar Sridhar por su valiosa ayuda en la descripción tanto del actual manejo del flujo de tráfico aéreo como de NGATS, y a Shon Grabbe por sus detallados tutoriales sobre FACET. 6. REFERENCIAS [1] A. Agogino y K. Tumer. Funciones de evaluación eficientes para sistemas de múltiples rovers. En la Conferencia de Computación Genética y Evolutiva, páginas 1-12, Seattle, WA, junio de 2004. [2] A. Agogino y K. Tumer. Análisis de recompensa multiagente para el aprendizaje en dominios ruidosos. En Actas de la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Utrecht, Países Bajos, julio de 2005. [3] A. K. Agogino y K. Tumer. Manejo de restricciones de comunicación y formación de equipos en juegos de congestión. Revista de Agentes Autónomos y Sistemas Multiagente, 13(1):97-115, 2006. [4] K. D. Bilimoria, B. Sridhar, G. B. Chatterji, K. S. Shethand, y S. R. Grabbe. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Control de Tráfico Aéreo Trimestral, 9(1), 2001. [5] Karl D. Bilimoria. Un enfoque de optimización geométrica para la resolución de conflictos de aeronaves. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [6] Martin S. Eby y Wallace E. Kelly III. Aseguramiento de separación de vuelo libre utilizando algoritmos distribuidos. En Proc de la Conferencia Aeroespacial, 1999, Aspen, CO, 1999. [7] Datos de OPSNET de la FAA de enero a diciembre de 2005. Sitio web del Departamento de Transporte de los Estados Unidos. [8] S. Grabbe y B. Sridhar. Enrutamiento de vuelos en el Pacífico central este. En la Conferencia y Exposición de Guía, Navegación y Control de AIAA, Keystone, CO, 2006. [9] Jared C. Hill, F. Ryan Johnson, James K. Archibald, Richard L. Frost y Wynn C. Stirling. Un enfoque cooperativo de múltiples agentes para el vuelo libre. En AAMAS 05: Actas de la cuarta conferencia internacional conjunta sobre agentes autónomos y sistemas multiagente, páginas 1083-1090, Nueva York, NY, EE. UU., 2005. ACM Press. [10] P. K. Menon, G. D. Sweriduk y B. Sridhar. Estrategias óptimas para la resolución de conflictos de tráfico aéreo en vuelo libre. Revista de Orientación, Control y Dinámica, 22(2):202-211, 1999. [11] Nominación al Premio al Software del Año de la NASA 2006. FACET: Herramienta de evaluación de conceptos futuros de cajeros automáticos. Caso número. ARC-14653-1, 2006. [12] M. Pechoucek, D. Sislak, D. Pavlicek y M. Uller. Agentes autónomos para la descongestión del tráfico aéreo. En Actas de la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, Hakodate, Japón, mayo de 2006. [13] B. Sridhar y S. Grabbe. Beneficios de vuelo directo en el sistema nacional de espacio aéreo. En la Conferencia de Guía, Navegación y Control de AIAA, Denver, CO, 2000. [14] B. Sridhar, T. Soni, K. Sheth y G. B. Chatterji. Modelo de flujo agregado para la gestión del tráfico aéreo. Revista de Orientación, Control y Dinámica, 29(4):992-997, 2006. [15] R. S. Sutton y A. G. Barto. Aprendizaje por refuerzo: Una introducción. MIT Press, Cambridge, MA, 1998. [16] C. Tomlin, G. Pappas, y S. Sastry. Resolución de conflictos para la gestión del tráfico aéreo. IEEE Tran on Automatic Control, 43(4):509-521, 1998. [17] K. Tumer y D. Wolpert, editores. Colectivos y el Diseño de Sistemas Complejos. Springer, Nueva York, 2004. [18] D. H. Wolpert y K. Tumer. Funciones de pago óptimas para los miembros de colectivos. Avances en Sistemas Complejos, 4(2/3):265-279, 2001. El Sexto Congreso Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 349