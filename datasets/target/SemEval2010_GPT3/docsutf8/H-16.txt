El impacto del almacenamiento en caché en los motores de búsqueda. Ricardo Baeza-Yates1 rbaeza@acm.org Aristides Gionis1 gionis@yahoo-inc.com Flavio Junqueira1 fpj@yahoo-inc.com Vanessa Murdock1 vmurdock@yahoo-inc.com Vassilis Plachouras1 vassilis@yahoo-inc.com Fabrizio Silvestri2 f.silvestri@isti.cnr.it 1 Yahoo! En este artículo estudiamos los compromisos en el diseño de sistemas de almacenamiento en caché eficientes para motores de búsqueda web. Exploramos el impacto de diferentes enfoques, como el almacenamiento en caché estático vs. dinámico, y el almacenamiento en caché de resultados de consultas vs. el almacenamiento en caché de listas de publicaciones. Utilizando un registro de consultas que abarca un año completo, exploramos las limitaciones de la memoria caché y demostramos que almacenar en caché listas de publicaciones puede lograr tasas de aciertos más altas que almacenar en caché respuestas de consultas. Proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones, que supera a los métodos anteriores. También estudiamos el problema de encontrar la manera óptima de dividir la caché estática entre respuestas y listas de publicaciones. Finalmente, medimos cómo los cambios en el registro de consultas afectan la efectividad del almacenamiento en caché estático, dado nuestro hallazgo de que la distribución de las consultas cambia lentamente con el tiempo. Nuestros resultados y observaciones son aplicables a diferentes niveles de la jerarquía de acceso a los datos, por ejemplo, para una capa de memoria/disco o una capa de servidor remoto/broker. Categorías y Descriptores de Asignaturas H.3.3 [Almacenamiento y Recuperación de Información]: Búsqueda y Recuperación de Información - Proceso de búsqueda; H.3.4 [Almacenamiento y Recuperación de Información]: Sistemas y Software - Sistemas distribuidos, Evaluación del rendimiento (eficiencia y efectividad) Términos Generales Algoritmos, Experimentación 1. INTRODUCCIÓN Diariamente se envían millones de consultas a los motores de búsqueda en la web, y los usuarios tienen altas expectativas sobre la calidad y rapidez de las respuestas. A medida que la Web indexable se vuelve más grande y grande, con más de 20 mil millones de páginas para indexar, evaluar una sola consulta requiere procesar grandes cantidades de datos. En un entorno así, para lograr un tiempo de respuesta rápido y aumentar el rendimiento de las consultas, es crucial utilizar una caché. El uso principal de una memoria caché es acelerar la computación aprovechando los datos utilizados con frecuencia o recientemente, aunque también es un objetivo importante reducir la carga de trabajo en los servidores de respaldo. El almacenamiento en caché se puede aplicar en diferentes niveles con latencias de respuesta crecientes o requisitos de procesamiento. Por ejemplo, los diferentes niveles pueden corresponder a la memoria principal, el disco o los recursos en una red local o de área amplia. La decisión de qué cachear puede ser fuera de línea (estático) u en línea (dinámico). Una caché estática se basa en información histórica y se actualiza periódicamente. Una caché dinámica reemplaza las entradas de acuerdo con la secuencia de solicitudes. Cuando llega una nueva solicitud, el sistema de caché decide si debe expulsar alguna entrada de la caché en caso de un fallo de caché. Tales decisiones en línea se basan en una política de caché, y se han estudiado varias políticas diferentes en el pasado. Para un motor de búsqueda, hay dos formas posibles de utilizar una memoria caché: almacenar respuestas. A medida que el motor devuelve respuestas a una consulta particular, puede decidir almacenar estas respuestas para resolver consultas futuras. Términos de almacenamiento en caché: A medida que el motor evalúa una consulta en particular, puede decidir almacenar en la memoria las listas de publicaciones de los términos de consulta involucrados. A menudo, el conjunto completo de listas de publicaciones no cabe en la memoria, por lo que el motor tiene que seleccionar un conjunto pequeño para mantener en la memoria y acelerar el procesamiento de consultas. Devolver una respuesta a una consulta que ya existe en la caché es más eficiente que calcular la respuesta utilizando listas de publicaciones en caché. Por otro lado, las consultas nunca antes vistas ocurren con más frecuencia que los términos nunca antes vistos, lo que implica una tasa de error más alta para las respuestas en caché. La caché de listas de publicaciones tiene desafíos adicionales. Dado que las listas de publicaciones tienen un tamaño variable, almacenarlas en caché de forma dinámica no es muy eficiente, debido a la complejidad en términos de eficiencia y espacio, y a la distribución sesgada del flujo de consultas, como se muestra más adelante. El almacenamiento en caché estático de listas de publicaciones plantea aún más desafíos: al decidir qué términos almacenar en caché, uno se enfrenta al dilema entre los términos consultados con frecuencia y los términos con listas de publicaciones pequeñas que son eficientes en espacio. Finalmente, antes de decidir adoptar una política de almacenamiento en caché estática, se debe analizar la secuencia de consultas para verificar que sus características no cambien rápidamente con el tiempo. Nivel de almacenamiento en caché estático de listas de publicación, respuestas almacenadas en caché dinámica/estática, procesador de consultas local, disco, siguiente nivel de almacenamiento en caché, acceso a red local, acceso a red remota. Figura 1: Un nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida. En este artículo exploramos los compromisos en el diseño de cada nivel de caché, mostrando que el problema es el mismo y solo cambian unos pocos parámetros. En general, asumimos que cada nivel de almacenamiento en caché en una arquitectura de búsqueda distribuida es similar al que se muestra en la Figura 1. Utilizamos un registro de consultas que abarca todo un año para explorar las limitaciones de almacenar en caché de forma dinámica respuestas de consultas o listas de publicaciones para términos de consulta. Más concretamente, nuestras principales conclusiones son que: • Almacenar en caché las respuestas de consultas resulta en ratios de aciertos más bajos en comparación con el almacenamiento en caché de listas de publicaciones para términos de consulta, pero es más rápido porque no es necesario evaluar la consulta. Proporcionamos un marco para el análisis del equilibrio entre el almacenamiento en caché estático de respuestas de consultas y listas de publicaciones; • El almacenamiento en caché estático de términos puede ser más efectivo que el almacenamiento en caché dinámico con, por ejemplo, LRU. Proporcionamos algoritmos basados en el problema de la mochila para seleccionar las listas de publicaciones que se colocarán en una caché estática, y mostramos mejoras respecto al trabajo previo, logrando una tasa de aciertos superior al 90%; los cambios en la distribución de consultas con el tiempo tienen poco impacto en el almacenamiento en caché estático. El resto de este documento está organizado de la siguiente manera. Las secciones 2 y 3 resumen el trabajo relacionado y caracterizan los conjuntos de datos que utilizamos. La sección 4 discute las limitaciones del almacenamiento en caché dinámico. Las secciones 5 y 6 presentan algoritmos para almacenar en caché listas de publicaciones, y un marco teórico para el análisis de almacenamiento en caché estático, respectivamente. La sección 7 discute el impacto de los cambios en la distribución de consultas en el almacenamiento en caché estático, y la sección 8 proporciona observaciones finales. TRABAJO RELACIONADO Existe un extenso cuerpo de trabajo dedicado a la optimización de consultas. Buckley y Lewit [3], en uno de los primeros trabajos, adoptan un enfoque de término por término para decidir cuándo las listas invertidas no necesitan ser examinadas más a fondo. Ejemplos más recientes demuestran que los k documentos principales para una consulta pueden ser devueltos sin la necesidad de evaluar el conjunto completo de listas de publicaciones [1, 4, 15]. Aunque estos enfoques buscan mejorar la eficiencia del procesamiento de consultas, difieren de nuestro trabajo actual en que no consideran el almacenamiento en caché. Podrían considerarse como separados y complementarios a un enfoque basado en caché. Raghavan y Sever [12], en uno de los primeros artículos sobre la explotación del historial de consultas de usuario, proponen utilizar una base de consultas, construida a partir de un conjunto de consultas óptimas persistentes enviadas en el pasado, para mejorar la efectividad de recuperación para consultas futuras similares. Markatos [10] muestra la existencia de la localidad temporal en las consultas, y compara el rendimiento de diferentes políticas de almacenamiento en caché. Basándose en las observaciones de Markatos, Lempel y Moran proponen una nueva política de almacenamiento en caché, llamada Almacenamiento en Caché Dirigido por Probabilidades, al intentar estimar la distribución de probabilidad de todas las consultas posibles enviadas a un motor de búsqueda [8]. Fagni et al. siguen el trabajo de Markatos al mostrar que combinar políticas de almacenamiento en caché estáticas y dinámicas junto con una política de precarga adaptativa logra una alta tasa de aciertos [7]. A diferencia de nuestro trabajo, ellos consideran el almacenamiento en caché y la precarga de páginas de resultados. Dado que los sistemas suelen ser jerárquicos, también se ha hecho un esfuerzo en arquitecturas multinivel. Saraiva et al. proponen una nueva arquitectura para motores de búsqueda web utilizando un sistema de almacenamiento en caché dinámico de dos niveles [13]. Su objetivo para tales sistemas ha sido mejorar el tiempo de respuesta de los motores jerárquicos. En su arquitectura, ambos niveles utilizan una política de expulsión LRU. Encuentran que la caché de segundo nivel puede reducir efectivamente el tráfico de disco, aumentando así el rendimiento general. Baeza-Yates y Saint-Jean proponen una organización de índice de tres niveles [2]. Long y Suel proponen un sistema de almacenamiento en caché estructurado según tres niveles diferentes [9]. El nivel intermedio contiene pares de términos que ocurren con frecuencia y almacena las intersecciones de las listas invertidas correspondientes. Estos dos últimos artículos están relacionados con el nuestro en el sentido de que explotan diferentes estrategias de almacenamiento en caché en distintos niveles de la jerarquía de memoria. Finalmente, nuestro algoritmo de almacenamiento en caché estático para publicar listas en la Sección 5 utiliza la razón frecuencia/tamaño para evaluar la bondad de un elemento para almacenar en caché. Ideas similares se han utilizado en el contexto de la caché de archivos [17], la caché web [5], e incluso la caché de listas de publicaciones [9], pero en todos los casos en un entorno dinámico. Hasta donde sabemos, somos los primeros en utilizar este enfoque para el almacenamiento en caché estático de listas de publicaciones. 3. CARACTERIZACIÓN DE LOS DATOS Nuestros datos consisten en un rastreo de documentos del dominio del Reino Unido y registros de consultas de un año de consultas enviadas a http://www.yahoo.co.uk desde noviembre de 2005 hasta noviembre de 2006. En nuestros registros, el 50% del volumen total de consultas son únicas. La longitud promedio de la consulta es de 2.5 términos, siendo la consulta más larga de 731 términos. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-08 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia (normalizada) Rango de frecuencia (normalizado) Figura 2: La distribución de consultas (curva inferior) y términos de consulta (curva intermedia) en el registro de consultas. La distribución de frecuencias de documentos de términos en el conjunto de datos del Reino Unido de 2006 (curva superior). La Figura 2 muestra las distribuciones de las consultas (curva inferior) y los términos de consulta (curva intermedia). El eje x representa la clasificación de frecuencia normalizada de la consulta o término. (La consulta más frecuente aparece más cerca del eje y). El eje y es la Tabla 1: Estadísticas de la muestra del Reino Unido-2006. Estadísticas de muestra del Reino Unido-2006: # de documentos 2,786,391 # de términos 6,491,374 # de tokens 2,109,512,558 la frecuencia normalizada para una consulta (o término) específico. Como era de esperar, la distribución de las frecuencias de consulta y las frecuencias de términos de consulta siguen distribuciones de ley de potencia, con pendientes de 1.84 y 2.26, respectivamente. En esta figura, las frecuencias de consulta se calcularon tal como aparecen en los registros sin normalización de mayúsculas o espacios en blanco. Los términos de consulta (curva del medio) han sido normalizados en cuanto a mayúsculas y minúsculas, al igual que los términos en la colección de documentos. La colección de documentos que utilizamos para nuestros experimentos es un resumen del dominio del Reino Unido rastreado en mayo de 2006. Este resumen corresponde a un máximo de 400 documentos rastreados por host, utilizando una estrategia de rastreo en anchura primero, que comprende 15GB. La distribución de las frecuencias de documentos de los términos en la colección sigue una distribución de ley de potencias con una pendiente de 2.38 (curva superior en la Figura 2). Las estadísticas de la colección se muestran en la Tabla 1. Medimos la correlación entre la frecuencia del documento de los términos en la colección y el número de consultas que contienen un término específico en el registro de consultas como 0.424. Un diagrama de dispersión para una muestra aleatoria de términos se muestra en la Figura 3. En este experimento, los términos se han convertido a minúsculas tanto en las consultas como en los documentos para que las frecuencias sean comparables. 1e-07 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 1e-06 1e-05 1e-04 0.001 0.01 0.1 1 Frecuencia de consulta Frecuencia de documento Figura 3: Gráfico de dispersión normalizado de frecuencias de términos de documento vs. frecuencias de términos de consulta. 4. El almacenamiento en caché de consultas y términos se basa en la suposición de que existe una localidad en la secuencia de solicitudes. Es decir, debe haber suficiente repetición en la secuencia de solicitudes y dentro de intervalos de tiempo que permitan que una memoria caché de tamaño razonable sea efectiva. En el registro de consultas que utilizamos, el 88% de las consultas únicas son consultas singleton, y el 44% son consultas singleton del volumen total. Por lo tanto, de todas las consultas en el flujo que componen el registro de consultas, el umbral superior de la tasa de aciertos es del 56%. Esto se debe a que solo el 56% de todas las consultas comprenden consultas que tienen múltiples ocurrencias. Es importante observar, sin embargo, que no todas las consultas en este 56% pueden ser aciertos de caché debido a los fallos obligatorios. Una falta obligatoria. La colección está disponible en la Universidad de Milán: http://law.dsi.unimi.it/. URL recuperado 05/2007. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 240 260 280 300 320 340 360 Número de elementos Número de contenedores Términos totales Diferencia de términos Consultas totales Consultas únicas Términos únicos Diferencia de consultas Figura 4: Tasa de llegada para términos y consultas. sucede cuando la caché recibe una consulta por primera vez. Esto es diferente de los fallos de capacidad, que ocurren debido a restricciones de espacio en la cantidad de memoria que utiliza la caché. Si consideramos una caché con memoria infinita, entonces la tasa de aciertos es del 50%. Ten en cuenta que para una caché infinita no hay fallos de capacidad. Como mencionamos anteriormente, otra posibilidad es almacenar en caché las listas de publicaciones de términos. Intuitivamente, esto brinda más libertad en la utilización del contenido en caché para responder a consultas, ya que los términos en caché podrían formar una nueva consulta. Por otro lado, necesitan más espacio. A diferencia de las consultas, la fracción de términos únicos en el volumen total de términos es menor. En nuestro registro de consultas, solo el 4% de los términos aparecen una vez, pero esto representa el 73% del vocabulario de términos de consulta. Mostramos en la Sección 5 que almacenar en caché una pequeña fracción de términos, teniendo en cuenta los términos que aparecen en muchos documentos, puede ser potencialmente muy efectivo. La Figura 4 muestra varios gráficos correspondientes a la tasa de llegada normalizada para diferentes casos utilizando días como contenedores. Es decir, representamos el número normalizado de elementos que aparecen en un día. Este gráfico muestra solo un período de 122 días, y normalizamos los valores por el valor máximo observado en todo el período del registro de consultas. El total de consultas y el total de términos corresponden al volumen total de consultas y términos, respectivamente. Las consultas únicas y los términos únicos corresponden a la tasa de llegada de consultas y términos únicos. Finalmente, Query diff y Terms diff corresponden a la diferencia entre las curvas totales y únicas. En la Figura 4, como era de esperar, el volumen de términos es mucho mayor que el volumen de consultas. La diferencia entre el número total de términos y el número de términos únicos es mucho mayor que la diferencia entre el número total de consultas y el número de consultas únicas. Esta observación implica que los términos se repiten significativamente más que las consultas. Si usamos contenedores más pequeños, digamos de una hora, entonces la proporción de únicos al volumen es mayor tanto para los términos como para las consultas porque deja menos espacio para la repetición. También estimamos la carga de trabajo utilizando la frecuencia de documentos de los términos como medida de cuánto trabajo impone una consulta en un motor de búsqueda. Encontramos que sigue de cerca la tasa de llegada de los términos mostrados en la Figura 4. Para demostrar el efecto de una caché dinámica en la distribución de frecuencia de consultas de la Figura 2, trazamos el mismo gráfico de frecuencia, pero ahora considerando la frecuencia de consultas de la Figura 5: Gráfico de frecuencia después de la caché LRU. En caso de fallo de caché, una caché LRU decide qué entrada desalojar utilizando la información sobre la recencia de las consultas. En este gráfico, las consultas más frecuentes no son las mismas consultas que eran más frecuentes antes de la caché. Es posible que las consultas más frecuentes después de la caché tengan características diferentes, y ajustar el motor de búsqueda a las consultas frecuentes antes de la caché pueda degradar el rendimiento para las consultas no almacenadas en caché. La frecuencia máxima después de la caché es inferior al 1% de la frecuencia máxima antes de la caché, lo que demuestra que la caché es muy efectiva en reducir la carga de consultas frecuentes. Si volvemos a clasificar las consultas según la frecuencia después de la caché, la distribución sigue siendo una ley de potencias, pero con un valor mucho menor para la frecuencia más alta. Al discutir la efectividad del almacenamiento en caché dinámico, una métrica importante es la tasa de fallos de caché. Para analizar la tasa de fallos de caché para diferentes restricciones de memoria, utilizamos el modelo de conjunto de trabajo [6, 14]. Un conjunto de trabajo, de forma informal, es el conjunto de referencias con las que una aplicación o un sistema operativo están trabajando actualmente. El modelo utiliza tales conjuntos en una estrategia que intenta capturar la localidad temporal de las referencias. La estrategia del conjunto de trabajo consiste en mantener en memoria solo los elementos que son referenciados en los θ pasos anteriores de la secuencia de entrada, donde θ es un parámetro configurable que corresponde al tamaño de la ventana. Originalmente, los conjuntos de trabajo se han utilizado para los algoritmos de reemplazo de páginas de sistemas operativos, y considerar tal estrategia en el contexto de los motores de búsqueda es interesante por tres razones. Primero, captura la cantidad de localidad de consultas y términos en una secuencia de consultas. La localidad en este caso se refiere a la frecuencia de consultas y términos en una ventana de tiempo. Si muchas consultas aparecen varias veces en una ventana, entonces la localidad es alta. Segundo, permite un análisis sin conexión de la tasa de fallos esperada dadas diferentes restricciones de memoria. Tercero, los conjuntos de trabajo capturan aspectos de algoritmos de almacenamiento en caché eficientes como LRU. LRU asume que las referencias más lejanas en el pasado son menos propensas a ser referenciadas en el presente, lo cual está implícito en el concepto de conjuntos de trabajo [14]. La Figura 6 muestra la tasa de fallos para diferentes tamaños de conjunto de trabajo, y consideramos conjuntos de trabajo tanto de consultas como de términos. Los tamaños del conjunto de trabajo se normalizan en función del número total de consultas en el registro de consultas. En el gráfico de consultas, hay una rápida disminución hasta aproximadamente 0.01, y la velocidad a la que la tasa de error disminuye disminuye a medida que aumentamos el tamaño del conjunto de trabajo por encima de 0.01. Finalmente, el valor mínimo que alcanza es una tasa de error del 50%, no mostrada en la figura ya que hemos cortado la cola de la curva con fines de presentación. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 Tasa de error Tamaño del conjunto de trabajo normalizado Consultas Términos Figura 6: Tasa de error en función del tamaño del conjunto de trabajo. 1 10 100 1000 10000 100000 1e+06 Frecuencia Distancia Figura 7: Distribución de distancias expresadas en términos de consultas distintas. En comparación con la curva de consulta, observamos que la tasa mínima de error para los términos es considerablemente menor. La tasa de error también disminuye bruscamente en valores de hasta 0.01, y disminuye mínimamente para valores más altos. El valor mínimo, sin embargo, es ligeramente superior al 10%, lo cual es mucho menor que el valor mínimo para la secuencia de consultas. Esto implica que con una política así es posible lograr una tasa de aciertos de más del 80%, si consideramos almacenar dinámicamente listas de publicaciones para términos en lugar de almacenar respuestas para consultas. Este resultado no considera el espacio requerido para cada unidad almacenada en la memoria caché, ni la cantidad de tiempo que se tarda en armar una respuesta a una consulta de usuario. Analizaremos estos temas con más detalle más adelante en este documento. Es interesante también observar el histograma de la Figura 7, que es un paso intermedio en el cálculo del gráfico de tasa de error. Informa sobre la distribución de distancias entre repeticiones de la misma consulta frecuente. La distancia en el gráfico se mide en el número de consultas distintas que separan una consulta y su repetición, y solo considera las consultas que aparecen al menos 10 veces. A partir de las Figuras 6 y 7, concluimos que incluso si configuramos el tamaño de la caché de respuestas de consulta a un número relativamente grande de entradas, la tasa de fallos es alta. Por lo tanto, almacenar en caché las listas de publicaciones de términos tiene el potencial de mejorar la proporción de aciertos. Esto es lo que exploramos a continuación. 5. La sección anterior muestra que almacenar en caché las listas de publicaciones puede obtener una tasa de aciertos más alta en comparación con almacenar en caché las respuestas de consultas. En esta sección estudiamos el problema de cómo seleccionar listas de publicaciones para colocar en una cierta cantidad de memoria disponible, asumiendo que todo el índice es más grande que la cantidad de memoria disponible. Las listas de publicaciones tienen un tamaño variable (de hecho, su distribución de tamaños sigue una ley de potencias), por lo que es beneficioso que una política de almacenamiento en caché considere los tamaños de las listas de publicaciones. Consideramos tanto el almacenamiento en caché dinámico como estático. Para el almacenamiento en caché dinámico, utilizamos dos políticas bien conocidas, LRU y LFU, así como un algoritmo modificado que tiene en cuenta el tamaño de la lista de publicaciones. Antes de discutir las estrategias de almacenamiento en caché estático, introducimos algunas notaciones. Usamos fq(t) para denotar la frecuencia del término de consulta de un término t, es decir, el número de consultas que contienen t en el registro de consultas, y fd(t) para denotar la frecuencia del documento de t, es decir, el número de documentos en la colección en los que aparece el término t. La primera estrategia que consideramos es el algoritmo propuesto por Baeza-Yates y Saint-Jean [2], que consiste en seleccionar las listas de publicaciones de los términos con las frecuencias de términos de consulta más altas fq(t). Llamamos a este algoritmo Qtf. Observamos que hay un equilibrio entre fq(t) y fd(t). Los términos con alta fq(t) son útiles para mantener en la caché porque son consultados con frecuencia. Por otro lado, los términos con un alto fd(t) no son buenos candidatos porque corresponden a listas de publicaciones largas y consumen una cantidad sustancial de espacio. De hecho, el problema de seleccionar las mejores listas de publicaciones para la caché estática corresponde al problema clásico de la mochila: dado una mochila de capacidad fija y un conjunto de n elementos, donde el i-ésimo elemento tiene un valor ci y un tamaño si, seleccionar el conjunto de elementos que quepan en la mochila y maximicen el valor total. En nuestro caso, el valor corresponde a fq(t) y el tamaño corresponde a fd(t). Por lo tanto, empleamos un algoritmo simple para el problema de la mochila, que consiste en seleccionar las listas de publicaciones de los términos con los valores más altos de la proporción fq(t) fd(t). Llamamos a este algoritmo QtfDf. Intentamos otras variaciones considerando las frecuencias de consulta en lugar de las frecuencias de términos, pero la ganancia fue mínima en comparación con la complejidad añadida. Además de los dos algoritmos estáticos anteriores, consideramos los siguientes algoritmos para el almacenamiento en caché dinámico: • LRU: Un algoritmo LRU estándar, pero muchas listas de publicaciones podrían necesitar ser expulsadas (en orden de uso menos reciente) hasta que haya suficiente espacio en la memoria para colocar la lista de publicaciones actualmente accedida; • LFU: Un algoritmo LFU estándar (expulsión del menos usado con menos frecuencia), con la misma modificación que el LRU; • Dyn-QtfDf: Una versión dinámica del algoritmo QtfDf; expulsar del caché el(los) término(s) con la proporción más baja de fq(t) fd(t). El rendimiento de todos los algoritmos anteriores para 15 semanas del registro de consultas y el conjunto de datos del Reino Unido se muestra en la Figura 8. El rendimiento se mide con la tasa de aciertos. El tamaño de la caché se mide como una fracción del espacio total requerido para almacenar las listas de publicaciones de todos los términos. Para los algoritmos dinámicos, cargamos la caché con términos en orden de fq(t) y dejamos que la caché se caliente durante 1 millón de consultas. Para los algoritmos estáticos, asumimos un conocimiento completo de las frecuencias fq(t), es decir, estimamos fq(t) a partir de todo el flujo de consultas. Como mostramos en la Sección 7, los resultados no cambian mucho si calculamos las frecuencias de términos de consulta utilizando las primeras 3 o 4 semanas del registro de consultas y medimos la tasa de aciertos en el resto. 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Caché de listas de publicaciones estático QTF/DF LRU LFU Dyn-QTF/DF QTF Figura 8: Tasa de aciertos de diferentes estrategias para la caché de listas de publicaciones. La observación más importante de nuestros experimentos es que el algoritmo estático QtfDf tiene una tasa de aciertos mejor que todos los algoritmos dinámicos. Un beneficio importante de una caché estática es que no requiere desalojo y, por lo tanto, es más eficiente al evaluar consultas. Sin embargo, si las características del tráfico de consultas cambian con frecuencia a lo largo del tiempo, entonces se requiere rellenar la caché con frecuencia o habrá un impacto significativo en la tasa de aciertos. 6. ANÁLISIS DE CACHÉ ESTÁTICO En esta sección proporcionamos un análisis detallado del problema de decidir si es preferible cachear respuestas de consultas o listas de publicaciones. Nuestro análisis tiene en cuenta el impacto del almacenamiento en caché entre dos niveles de la jerarquía de acceso a datos. Puede aplicarse en la capa de memoria/disco o en la capa de servidor/servidor remoto, como en la arquitectura que discutimos en la introducción. Utilizando un modelo de sistema particular, obtenemos estimaciones para los parámetros requeridos por nuestro análisis, que posteriormente utilizamos para decidir la compensación óptima entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. 6.1 Modelo Analítico Sea M el tamaño de la caché medido en unidades de respuestas (la caché puede almacenar M respuestas de consultas). Suponga que todas las listas de publicaciones tienen la misma longitud L, medida en unidades de respuesta. Consideramos los siguientes dos casos: (A) una caché que almacena solo respuestas precalculadas, y (B) una caché que almacena solo listas de publicaciones. En el primer caso, las respuestas Nc = M encajan en la caché, mientras que en el segundo caso las listas de publicaciones Np = M/L encajan en la caché. Por lo tanto, Np = Nc/L. Ten en cuenta que aunque publicar listas requiere más espacio, podemos combinar términos para evaluar más consultas (o consultas parciales). Para el caso (A), supongamos que una respuesta de consulta en la caché se puede evaluar en 1 unidad de tiempo. Para el caso (B), asuma que si las listas de publicación de los términos de una consulta están en la caché, entonces los resultados se pueden calcular en TR1 unidades de tiempo, mientras que si las listas de publicación no están en la caché, entonces los resultados se pueden calcular en TR2 unidades de tiempo. Por supuesto, TR2 > TR1. Ahora queremos comparar el tiempo para responder a una secuencia de Q consultas en ambos casos. Sea Vc(Nc) el volumen de las consultas más frecuentes Nc. Entonces, para el caso (A), tenemos un tiempo total TCA = Vc(Nc) + TR2(Q − Vc(Nc)). De manera similar, para el caso (B), sea Vp(Np) el número de consultas computables. Entonces tenemos el tiempo total TP L = TR1Vp(Np) + TR2(Q − Vp(Np)). Queremos verificar bajo qué condiciones tenemos TP L < TCA. Tenemos TP L − TCA = (TR2 − 1)Vc(Nc) − (TR2 − TR1)Vp(Np) > 0. La Figura 9 muestra los valores de Vp y Vc para nuestros datos. Podemos ver que la caché de respuestas se satura más rápido y para estos datos en particular no hay beneficio adicional al usar más del 10% del espacio de índice para cachear respuestas. Dado que la distribución de consultas es una ley de potencias con parámetro α > 1, la i-ésima consulta más frecuente aparece con una probabilidad proporcional a 1 iα. Por lo tanto, el volumen Vc(n), que es el número total de las n consultas más frecuentes, es Vc(n) = V0 n i=1 Q iα = γnQ (0 < γn < 1). Sabemos que Vp(n) crece más rápido que Vc(n) y asumimos, basados en resultados experimentales, que la relación es de la forma Vp(n) = k Vc(n)β. En el peor de los casos, para una caché grande, β → 1. Es decir, ambas técnicas almacenarán en caché una fracción constante del volumen total de consultas. Entonces, almacenar en caché las listas de publicaciones tiene sentido solo si L(TR2 − 1) k(TR2 − TR1) > 1. Si usamos compresión, tenemos L < L y TR1 > TR1. Según los experimentos que mostramos más adelante, la compresión siempre es mejor. Para una pequeña caché, estamos interesados en el comportamiento transitorio y luego β > 1, según se calcula a partir de nuestros datos. En este caso siempre habrá un punto donde TP L > TCA para un gran número de consultas. En realidad, en lugar de llenar la caché solo con respuestas o solo con listas de publicaciones, una estrategia mejor será dividir el espacio total de la caché en caché para respuestas y caché para listas de publicaciones. En tal caso, habrá algunas consultas que podrían ser respondidas por ambas partes de la caché. Dado que la caché de respuestas es más rápida, será la primera opción para responder esas consultas. Sean QNc y QNp el conjunto de consultas que pueden ser respondidas por las respuestas en caché y las listas de publicaciones en caché, respectivamente. Entonces, el tiempo total es T = Vc(Nc)+TR1V (QNp −QNc )+TR2(Q−V (QNp ∪QNc )), donde Np = (M − Nc)/L. Encontrar la división óptima de la caché para minimizar el tiempo total de recuperación es un problema difícil de resolver analíticamente. En la Sección 6.3 utilizamos simulaciones para derivar compensaciones óptimas de caché para ejemplos de implementación particulares. 6.2 Estimación de parámetros Ahora utilizamos una implementación particular de un sistema centralizado y el modelo de un sistema distribuido como ejemplos a partir de los cuales estimamos los parámetros del análisis de la sección anterior. Realizamos los experimentos utilizando una versión optimizada de Terrier [11] para indexar documentos y procesar consultas, en una sola máquina con un Pentium 4 a 2GHz y 1GB de RAM. Indexamos los documentos del conjunto de datos UK-2006, sin eliminar palabras vacías ni aplicar reducción de términos. Las listas de publicaciones en el archivo invertido consisten en pares de identificador de documento y frecuencia de término. Comprimimos los espacios de identificadores de documentos utilizando la codificación gamma de Elías, y la 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Volumen de consulta Espacio de respuestas precalculadas listas de publicaciones Figura 9: Saturación de caché en función del tamaño. Tabla 2: Relación entre el tiempo promedio para evaluar una consulta y el tiempo promedio para devolver respuestas en caché (caso centralizado y distribuido). Sistema centralizado TR1 TR2 TR1 TR2 Evaluación completa 233 1760 707 1140 Evaluación parcial 99 1626 493 798 Sistema LAN TRL 1 TRL 2 TR L 1 TR L 2 Evaluación completa 242 1769 716 1149 Evaluación parcial 108 1635 502 807 Sistema WAN TRW 1 TRW 2 TR W 1 TR W 2 Evaluación completa 5001 6528 5475 5908 Evaluación parcial 4867 6394 5270 5575 frecuencias de términos en documentos utilizando codificación unaria [16]. El tamaño del archivo invertido es de 1,189Mb. Una respuesta almacenada requiere 1264 bytes, y una publicación sin comprimir ocupa 8 bytes. De la Tabla 1, obtenemos que L = (8·# de publicaciones) 1264·# de términos = 0.75 y L = Tamaño del archivo invertido 1264·# de términos = 0.26. Estimamos la razón TR = T/Tc entre el tiempo promedio T que se tarda en evaluar una consulta y el tiempo promedio Tc que se tarda en devolver una respuesta almacenada para la misma consulta, de la siguiente manera. Tc se mide cargando las respuestas para 100,000 consultas en la memoria y respondiendo a las consultas desde la memoria. El tiempo promedio es Tc = 0.069ms. T se mide procesando las mismas 100,000 consultas (las primeras 10,000 consultas se utilizan para calentar el sistema). Para cada consulta, eliminamos las palabras de parada, si quedan al menos tres términos restantes. Las palabras de parada corresponden a los términos con una frecuencia mayor que el número de documentos en el índice. Utilizamos un enfoque de documento por vez para recuperar documentos que contienen todos los términos de la consulta. El único acceso al disco requerido durante el procesamiento de consultas es para leer listas de posteo comprimidas del archivo invertido. Realizamos tanto la evaluación completa como parcial de respuestas, ya que es probable que algunas consultas recuperen un gran número de documentos y solo una fracción de los documentos recuperados será vista por los usuarios. En la evaluación parcial de consultas, terminamos el procesamiento después de encontrar 10,000 documentos coincidentes. Las razones estimadas TR se presentan en la Tabla 2. La Figura 10 muestra para una muestra de consultas la carga de trabajo del sistema con evaluación parcial de consultas y listas de publicaciones comprimidas. El eje x corresponde al tiempo total que el sistema pasa procesando una consulta particular, y el eje vertical corresponde a la suma t∈q fq · fd(t). Ten en cuenta que el número total de publicaciones de los términos de la consulta no necesariamente proporciona una estimación precisa de la carga impuesta en el sistema por una consulta (lo cual es el caso para la evaluación completa y las listas sin comprimir). 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Total de publicaciones para procesar la consulta (normalizado) Tiempo total para procesar la consulta (normalizado) Procesamiento parcial de publicaciones comprimidas longitud de la consulta = 1 longitud de la consulta en [2,3] longitud de la consulta en [4,8] longitud de la consulta > 8 Figura 10: Carga de trabajo para la evaluación parcial de consultas con listas de publicaciones comprimidas. El análisis de la sección anterior también se aplica a un sistema de recuperación distribuida en uno o varios sitios. Supongamos que un sistema distribuido de particionamiento de documentos está funcionando en un grupo de máquinas interconectadas con una red de área local (LAN) en un sitio. El corredor recibe consultas y las transmite a los procesadores de consultas, los cuales responden a las consultas y devuelven los resultados al corredor. Finalmente, el corredor fusiona las respuestas recibidas y genera el conjunto final de respuestas (asumimos que el tiempo dedicado a fusionar los resultados es insignificante). La diferencia entre la arquitectura centralizada y la arquitectura de partición de documentos es la comunicación adicional entre el intermediario y los procesadores de consultas. Usando pings ICMP en una LAN de 100Mbps, hemos medido que enviar la consulta desde el broker a los procesadores de consulta que envían una respuesta de 4,000 bytes de vuelta al broker toma en promedio 0.615ms. Por lo tanto, TRL = TR + 0.615ms/0.069ms = TR + 9. En el caso en que el corredor y los procesadores de consultas se encuentren en sitios diferentes conectados con una red de área amplia (WAN), estimamos que transmitir la consulta desde el corredor a los procesadores de consultas y recibir una respuesta de 4,000 bytes lleva en promedio 329ms. Por lo tanto, TRW = TR + 329ms/0.069ms = TR + 4768. 6.3 Resultados de la simulación Ahora abordamos el problema de encontrar el equilibrio óptimo entre almacenar en caché respuestas de consultas y almacenar en caché listas de publicaciones. Para concretar el problema, asumimos un presupuesto fijo M en la memoria disponible, del cual x unidades se utilizan para almacenar en caché respuestas de consultas y M − x para almacenar en caché listas de publicaciones. Realizamos simulaciones y calculamos el tiempo de respuesta promedio en función de x. Usando una parte del registro de consultas como datos de entrenamiento, primero asignamos en la caché las respuestas a las consultas más frecuentes que quepan en el espacio x, y luego utilizamos el resto de la memoria para almacenar listas de publicaciones. Para seleccionar listas de publicación, utilizamos el algoritmo QtfDf, aplicado al registro de consultas de entrenamiento pero excluyendo las consultas que ya han sido almacenadas en caché. En la Figura 11, representamos el tiempo de respuesta simulado para un sistema centralizado en función de x. Para el índice sin comprimir usamos M = 1GB, y para el índice comprimido usamos M = 0.5GB. En el caso de la configuración que utiliza evaluación parcial de consultas con listas de publicaciones comprimidas, el tiempo de respuesta más bajo se logra cuando se asignan 0.15GB de los 0.5GB para almacenar respuestas a las consultas. Obtuvimos tendencias similares en los resultados para la configuración de LAN. La Figura 12 muestra la carga de trabajo simulada para un sistema distribuido a través de una WAN. En este caso, la cantidad total de memoria se divide entre el broker, que contiene en caché 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - máquina única completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 11: División óptima de la caché en un servidor. 3000 3500 4000 4500 5000 5500 6000 0 0.2 0.4 0.6 0.8 1 Tiempo de respuesta promedio Espacio (GB) Carga de trabajo simulada - WAN completa / sin comprimir / 1 G parcial / sin comprimir / 1 G completa / comprimida / 0.5 G parcial / comprimida / 0.5 G Figura 12: División óptima de la caché cuando el siguiente nivel requiere acceso a WAN. respuestas de consultas, y los procesadores de consultas, que contienen la caché de listas de publicaciones. Según la figura, la diferencia entre las configuraciones de los procesadores de consulta es menos importante porque el sobrecosto de la comunicación en red aumenta considerablemente el tiempo de respuesta. Cuando se utilizan listas de publicaciones sin comprimir, la asignación óptima de memoria corresponde a utilizar aproximadamente el 70% de la memoria para almacenar en caché las respuestas de las consultas. Esto se explica por el hecho de que no es necesario la comunicación de red cuando la consulta puede ser respondida por la caché en el intermediario. EFECTO DE LA DINÁMICA DE LA CONSULTA Para nuestro registro de consultas, la distribución de consultas y la distribución de términos de consulta cambian lentamente con el tiempo. Para respaldar esta afirmación, primero evaluamos cómo cambian los temas comparando la distribución de consultas de la primera semana de junio de 2006, con la distribución de consultas para el resto de 2006 que no aparecieron en la primera semana de junio. Descubrimos que un porcentaje muy pequeño de las consultas son consultas nuevas. La mayoría de las consultas que aparecen en una semana determinada se repiten en las semanas siguientes durante los próximos seis meses. Luego calculamos la tasa de aciertos de una caché estática de 128,000 respuestas entrenadas durante un período de dos semanas (Figura 13). Informamos la tasa de aciertos por hora durante 7 días, comenzando a las 5pm. Observamos que la tasa de aciertos alcanza su valor más alto durante la noche (alrededor de medianoche), mientras que alrededor de las 2-3 pm alcanza su mínimo. Después de una pequeña disminución en los valores de la tasa de aciertos, la tasa de aciertos se estabiliza entre 0.28 y 0.34 durante toda la semana, lo que sugiere que la caché estática es efectiva durante toda la semana después del período de entrenamiento. 0.26 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0 20 40 60 80 100 120 140 160 Tasa de aciertos Tiempo Aciertos en las consultas frecuentes de distancias Figura 13: Tasa de aciertos por hora para una caché estática que contiene 128,000 respuestas durante el período de una semana. La caché estática de listas de publicaciones puede ser recalculada periódicamente. Para estimar el intervalo de tiempo en el que necesitamos recalcular las listas de publicaciones en la caché estática, debemos considerar un equilibrio entre eficiencia y calidad: utilizar un intervalo de tiempo demasiado corto podría resultar prohibitivamente costoso, mientras que recalcular la caché con poca frecuencia podría llevar a tener una caché obsoleta que no corresponda a las características estadísticas del flujo de consultas actual. Medimos el efecto en el algoritmo QtfDf de los cambios en un flujo de consultas de 15 semanas (Figura 14). Calculamos las frecuencias de los términos de consulta en todo el flujo, seleccionamos qué términos almacenar en caché y luego calculamos la tasa de aciertos en todo el flujo de consultas. Esta tasa de acierto es un límite superior, y asume un conocimiento perfecto de las frecuencias de los términos de consulta. Para simular un escenario realista, utilizamos las primeras 6 (3) semanas del flujo de consultas para calcular las frecuencias de términos de consulta y las siguientes 9 (12) semanas para estimar la tasa de aciertos. Como muestra la Figura 14, la tasa de aciertos disminuye en menos del 2%. La alta correlación entre las frecuencias de términos de consulta durante diferentes períodos de tiempo explica la adaptación fluida de los algoritmos de almacenamiento en caché estáticos al flujo de consultas futuro. De hecho, la correlación por pares entre todos los posibles periodos de 3 semanas de la secuencia de consultas de 15 semanas es superior al 99.5%. CONCLUSIONES La caché es una técnica efectiva en los motores de búsqueda para mejorar el tiempo de respuesta, reducir la carga en los procesadores de consultas y mejorar la utilización del ancho de banda de la red. Presentamos resultados tanto sobre el almacenamiento en caché dinámico como estático. El almacenamiento en caché dinámico de consultas tiene una efectividad limitada debido al alto número de fallos obligatorios causados por la cantidad de consultas únicas o poco frecuentes. Nuestros resultados muestran que en nuestro registro del Reino Unido, la tasa mínima de error es del 50% utilizando una estrategia de conjunto de trabajo. El almacenamiento en caché es más efectivo en cuanto a la tasa de fallos, logrando valores tan bajos como el 12%. También proponemos un nuevo algoritmo para el almacenamiento en caché estático de listas de publicaciones que supera a los algoritmos anteriores de almacenamiento en caché estático, así como a los algoritmos dinámicos como LRU y LFU, obteniendo valores de tasa de aciertos que son más de un 10% más altos en comparación con estas estrategias. Presentamos un marco para el análisis del equilibrio entre almacenar en caché los resultados de consultas y almacenar en caché las listas de publicaciones, y simulamos diferentes tipos de arquitecturas. Nuestros resultados muestran que para entornos centralizados y de LAN, existe una asignación óptima de resultados de consultas en caché y caché de listas de publicaciones, mientras que para escenarios de WAN en los que prevalece el tiempo de red, es más importante almacenar en caché los resultados de las consultas. 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Tasa de aciertos Tamaño de caché Dinámica de la política de almacenamiento en caché estática QTF/DF conocimiento perfecto entrenamiento de 6 semanas entrenamiento de 3 semanas Figura 14: Impacto de los cambios de distribución en el almacenamiento en caché estático de listas de publicaciones. 9. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de consulta podada utilizando impactos precalculados. En ACM CIKM, 2006. [2] R. A. Baeza-Yates y F. Saint-Jean. Un índice de motor de búsqueda de tres niveles basado en la distribución del registro de consultas. En SPIRE, 2003. [3] C. Buckley y A. F. Lewit. Optimización de búsquedas de vectores invertidos. En ACM SIGIR, 1985. [4] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En ACM CIKM, 2006. [5] P. Cao y S. Irani. Algoritmos de almacenamiento en caché de proxy de WWW conscientes del costo. En USITS, 1997. [6] P. Denning. Conjuntos de trabajo pasados y presentes. IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.

Traducción: IEEE Trans. on Software Engineering, SE-6(1):64-84, 1980. [7] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Mejorando el rendimiento de los motores de búsqueda web: almacenamiento en caché y precarga de resultados de consultas mediante la explotación de datos de uso histórico. ACM Trans. I'm sorry, but the sentence "Inf." is not a complete sentence. Could you please provide more context or a full sentence for me to translate into Spanish? Syst., 24(1):51-78, 2006. [8] R. Lempel y S. Moran. Caché predictivo y precarga de resultados de consultas en motores de búsqueda. En WWW, 2003. [9] X. Largo y T. Suel. Caché de tres niveles para un procesamiento eficiente de consultas en motores de búsqueda web grandes. En WWW, 2005. [10] E. P. Markatos. Sobre el almacenamiento en caché de los resultados de consultas en motores de búsqueda. Comunicaciones de Computadoras, 24(2):137-143, 2001. [11] I. Ounis, G. Amati, V. Plachouras, B. Él, C. Macdonald y C. Lioma. Terrier: Una plataforma de recuperación de información de alto rendimiento y escalable. En el Taller SIGIR sobre Recuperación de Información de Código Abierto, 2006. [12] V. V. Raghavan y H. Sever. Sobre la reutilización de consultas óptimas pasadas. En ACM SIGIR, 1995. [13] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, R. Fonseca y B. Riberio-Neto. Caché de dos niveles que preserva el orden para motores de búsqueda escalables. En ACM SIGIR, 2001. [14] D. R. Slutz e I. L. Traiger. Una nota sobre el cálculo del tamaño promedio del conjunto de trabajo. Comunicaciones de la ACM, 17(10):563-565, 1974. [15] T. Strohman, H. Turtle y W. B. Croft. Estrategias de optimización para consultas complejas. En ACM SIGIR, 2005. [16] I. H. Witten, T. C. Bell y A. Moffat. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. 

John Wiley & Sons, Inc., NY, 1994. [17] N. E. Young. Caché de archivos en línea. Algorithmica, 33(3):371-383, 2002. 

Algoritmic, 33(3):371-383, 2002.