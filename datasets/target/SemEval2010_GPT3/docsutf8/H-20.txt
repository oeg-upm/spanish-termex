Detección de nuevos eventos basada en árbol de indexación y entidad nombrada Zhang Kuo Universidad Tsinghua Beijing, 100084, China 86-10-62771736 zkuo99@mails.tsinghua.edu.cn Li Juan Zi Universidad Tsinghua Beijing, 100084, China 86-10-62781461 ljz@keg.cs.tsinghua.edu.cn Wu Gang Universidad Tsinghua Beijing, 100084, China 86-10-62789831 wug03@keg.cs.tsinghua.edu.cn RESUMEN La Detección de Nuevos Eventos (NED) tiene como objetivo detectar de una o varias corrientes de noticias cuál se informa sobre un nuevo evento (es decir, no reportado previamente). Con el abrumador volumen de noticias disponible hoy en día, hay una creciente necesidad de un sistema NED que sea capaz de detectar eventos nuevos de manera más eficiente y precisa. En este artículo proponemos un nuevo modelo de NED para acelerar la tarea de NED mediante el uso dinámico de un árbol de indexación de noticias. Además, basándose en la observación de que los términos de diferentes tipos tienen diferentes efectos para la tarea de NED, se proponen dos enfoques de reponderación de términos para mejorar la precisión de NED. En el primer enfoque, proponemos ajustar dinámicamente los pesos de los términos basados en los clusters de historias previas y en el segundo enfoque, proponemos emplear estadísticas en los datos de entrenamiento para aprender el modelo de reajuste de entidades nombradas para cada clase de historias. Los resultados experimentales en dos conjuntos de datos del Consorcio de Datos Lingüísticos (LDC), TDT2 y TDT3, muestran que el modelo propuesto puede mejorar significativamente tanto la eficiencia como la precisión de la tarea de NED, en comparación con el sistema base y otros sistemas existentes. Categorías y Descriptores de Asignaturas H.3.3 [Sistemas de Información]: Búsqueda y Recuperación de Información; H.4.2 [Aplicaciones de Sistemas de Información]: Tipos de Sistemas de Apoyo a Decisiones. Términos generales Algoritmos, Rendimiento, Experimentación 1. El programa de Detección y Seguimiento de Temas (TDT) tiene como objetivo desarrollar técnicas que puedan organizar, buscar y estructurar de manera efectiva materiales de texto de noticias de una variedad de agencias de noticias y medios de difusión [1]. La Detección de Eventos Nuevos (DEN) es una de las cinco tareas en TDT. Es la tarea de identificación en línea del informe más temprano para cada tema tan pronto como ese informe llegue en la secuencia de documentos. Un tema se define como un evento o actividad seminal, junto con eventos y actividades directamente relacionados [2]. Un evento se define como algo (no trivial) que sucede en un lugar y en un momento determinados [3]. Por ejemplo, cuando una bomba explota en un edificio, la explosión es el evento seminal que desencadena el tema, y otras historias sobre el mismo tema serían aquellas que discuten los esfuerzos de salvamento, la búsqueda de los perpetradores, arrestos y juicios, entre otros. La información útil de noticias suele estar enterrada en una masa de datos generados a diario. Por lo tanto, los sistemas NED son muy útiles para las personas que necesitan detectar información novedosa de un flujo de noticias en tiempo real. Estas necesidades de la vida real a menudo se presentan en ámbitos como los mercados financieros, el análisis de noticias y la recopilación de inteligencia. En la mayoría de los sistemas NED de última generación, cada noticia disponible se compara con todas las noticias previamente recibidas. Si todas las similitudes entre ellos no superan un umbral, entonces la historia desencadena un nuevo evento. Por lo general, se presentan en forma de similitud de coseno o métrica de similitud de Hellinger. El problema central de NED es identificar si dos historias tratan sobre el mismo tema. Obviamente, estos sistemas no pueden aprovechar la información del tema. Además, no es aceptable en aplicaciones reales debido a la gran cantidad de cálculos requeridos en el proceso de NED. Otros sistemas organizan historias anteriores en grupos (cada grupo corresponde a un tema), y la nueva historia se compara con los grupos anteriores en lugar de con historias. Esta forma puede reducir significativamente los tiempos de comparación. Sin embargo, se ha demostrado que este método es menos preciso [4, 5]. Esto se debe a que a veces las historias dentro de un tema se alejan mucho entre sí, lo que podría resultar en una baja similitud entre una historia y su tema. Por otro lado, algunos sistemas NED propuestos intentaron mejorar la precisión al hacer un mejor uso de las entidades nombradas [10, 11, 12, 13]. Sin embargo, ninguno de los sistemas ha considerado que los términos de diferentes tipos (por ejemplo, Los sustantivos, verbos o nombres de personas tienen diferentes efectos en diferentes clases de historias para determinar si dos historias tratan sobre el mismo tema. Por ejemplo, los nombres de los candidatos electorales (Nombre de la persona) son muy importantes para las historias de la clase de elecciones; las ubicaciones (Nombre de la ubicación) donde ocurrieron los accidentes son importantes para las historias de la clase de accidentes. Por lo tanto, en NED, todavía existen los siguientes tres problemas por investigar: (1) ¿Cómo acelerar el procedimiento de detección sin disminuir la precisión de la detección? (2) ¿Cómo aprovechar mejor la información de los grupos (temas) para mejorar la precisión? (3) ¿Cómo obtener una mejor representación de historias de noticias mediante una mejor comprensión de las entidades nombradas? Impulsados por estos problemas, hemos propuesto tres enfoques en este documento. (1) Para hacer que el procedimiento de detección sea más rápido, proponemos un nuevo procedimiento de NED basado en un árbol de indexación de noticias creado dinámicamente. El índice de historias en árbol se crea ensamblando historias similares para formar grupos de noticias en diferentes jerarquías según sus valores de similitud. Las comparaciones entre la historia actual y los grupos anteriores podrían ayudar a encontrar la historia más similar en menos tiempo de comparación. El nuevo procedimiento puede reducir la cantidad de veces de comparación sin afectar la precisión. Utilizamos los grupos del primer piso en el árbol de indexación como temas de noticias, en los cuales los pesos de los términos se ajustan dinámicamente según la distribución de términos en los grupos. En este enfoque, se utiliza adecuadamente la información de los grupos (temas), por lo que se evita el problema de la descentralización del tema. Basándonos en observaciones de las estadísticas obtenidas de los datos de entrenamiento, encontramos que los términos de diferentes tipos (por ejemplo, Los sustantivos y verbos tienen diferentes efectos para diferentes clases de historias al determinar si dos historias tratan sobre el mismo tema. Y proponemos utilizar estadísticas para optimizar los pesos de los términos de diferentes tipos en una historia de acuerdo a la clase de noticias a la que pertenece la historia. En el conjunto de datos TDT3, el nuevo modelo NED solo utiliza un 14.9% en comparación con los tiempos del modelo básico, mientras que su costo normalizado mínimo es de 0.5012, lo que es 0.0797 mejor que el modelo básico, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. El resto del documento está organizado de la siguiente manera. Comenzamos este artículo resumiendo el trabajo previo en NED en la sección 2. La sección 3 presenta el modelo básico para NED que la mayoría de los sistemas actuales utilizan. La sección 4 describe nuestro nuevo procedimiento de detección basado en el índice de árbol de noticias. En la sección 5, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. La sección 6 presenta nuestros datos experimentales y métricas de evaluación. Finalmente concluimos con los resultados experimentales en la Sección 7, y las conclusiones y trabajo futuro en la Sección 8. TRABAJO RELACIONADO Papka et al. propusieron el agrupamiento de un solo paso en NED [6]. Cuando se encontraba una nueva historia, esta era procesada inmediatamente para extraer características de términos y se construía una representación de consulta del contenido de la historia. Luego se comparó con todas las consultas anteriores. Si el documento no generaba ninguna consulta al exceder un umbral, se marcaba como un nuevo evento. Lam et al construyen representaciones de consultas previas de grupos de historias, cada una de las cuales corresponde a un tema [7]. De esta manera se realizan comparaciones entre historias y grupos. En los últimos años, la mayoría de los trabajos se han centrado en proponer mejores métodos para la comparación de historias y la representación de documentos. Brants et al. [8] extendieron un modelo básico incremental de TF-IDF para incluir modelos específicos de fuente, normalización de puntajes de similitud basada en promedios específicos de documentos, normalización de puntajes de similitud basada en promedios específicos de pares de fuentes, reponderación de términos basada en frecuencias de eventos inversas y segmentación de documentos. Se mostraron buenas mejoras en los puntos de referencia de TDT. Stokes et al. [9] utilizaron una combinación de evidencia de dos representaciones distintas del contenido de un documento. Una de las representaciones era el vector de texto libre habitual, la otra hacía uso de cadenas léxicas (creadas utilizando WordNet) para construir otro vector de términos. Luego, las dos representaciones se combinan de forma lineal. Se logró un aumento marginal en la efectividad cuando se utilizó la representación combinada. Algunos esfuerzos se han realizado sobre cómo utilizar entidades nombradas para mejorar la desambiguación de entidades nombradas (NED). Yang et al. asignaron un peso cuatro veces mayor a las entidades con nombres de ubicación que a otros términos y entidades con nombres [10]. El grupo de investigación DOREMI combinó similitudes semánticas de nombres de personas, nombres de lugares y tiempo junto con similitud textual [11][12]. El grupo de investigación de UMass [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas. Y se descubrió que algunas clases de noticias podían lograr un mejor rendimiento utilizando la representación de entidades nombradas, mientras que otras clases de noticias podían lograr un mejor rendimiento utilizando la representación sin entidades nombradas. Tanto [10] como [13] utilizaron la técnica de categorización de texto para clasificar previamente las noticias. En [13] las noticias se clasifican automáticamente al principio, y luego se prueban las sensibilidades de los nombres y términos que no son nombres para NED para cada clase. En [10], se eliminan los términos frecuentes de cada clase de la representación del documento. Por ejemplo, la palabra "elección" no ayuda a identificar diferentes elecciones. En su trabajo, no se investiga la efectividad de diferentes tipos de nombres (o términos con diferentes categorías gramaticales) para NED en diferentes clases de noticias. Utilizamos análisis estadístico para revelar el hecho y utilizarlo para mejorar el rendimiento de NED. 3. MODELO BÁSICO En esta sección, presentamos el modelo básico de Detección de Nuevos Eventos que es similar a lo que la mayoría de los sistemas actuales aplican. Luego, proponemos nuestro nuevo modelo ampliando el modelo básico. Los sistemas de detección de eventos nuevos utilizan flujos de noticias como entrada, en los cuales las historias están estrictamente ordenadas por tiempo. Solo están disponibles las historias recibidas previamente al tratar con la historia actual. La salida es una decisión sobre si la historia actual es un evento nuevo o no y la confianza de la decisión. Por lo general, un modelo NED consta de tres partes: representación de la historia, cálculo de similitud y procedimiento de detección. 3.1 Representación de la historia Se necesita preprocesamiento antes de generar la representación de la historia. Para el preprocesamiento, tokenizamos palabras, reconocemos abreviaturas, normalizamos abreviaturas, agregamos etiquetas de partes del discurso, eliminamos palabras vacías incluidas en la lista de paradas utilizada en InQuery [14], reemplazamos palabras con sus raíces utilizando el algoritmo K-stem[15], y luego generamos un vector de palabras para cada noticia. Utilizamos el modelo TF-IDF incremental para el cálculo del peso de los términos [4]. En un modelo TF-IDF, la frecuencia de término en un documento de noticias se pondera por la frecuencia inversa del documento, la cual se genera a partir del corpus de entrenamiento. Cuando surge un nuevo término en el proceso de prueba, hay dos soluciones: simplemente ignorar el nuevo término o establecer df del término como una constante pequeña (por ejemplo, df = 1). El nuevo término recibe un peso demasiado bajo en la primera solución (0) y un peso demasiado alto en la segunda solución. En el modelo TF-IDF incremental, las frecuencias de documentos se actualizan dinámicamente en cada paso de tiempo t: 1( ) ( ) ( )t t D tdf w df w df w−= + (1) donde Dt representa el conjunto de noticias recibidas en el tiempo t, y dfDt(w) significa el número de documentos en los que aparece el término w, y dft(w) significa el número total de documentos en los que aparece el término w antes del tiempo t. En este trabajo, cada ventana de tiempo incluye 50 noticias. Así, cada historia d recibida en t se representa de la siguiente manera: 1 2{ ( , , ), ( , , ),..., ( , , )}nd peso d t w peso d t w peso d t w→ donde n significa el número de términos distintos en la historia d, y ( , , )peso d t w significa el peso del término w en la historia d en el tiempo t: log( ( , ) 1) log(( 1) /( ( ) 0.5)) ( , , ) log( ( , ) 1) log(( 1) /( ( ) 0.5)) t t t t w d tf d w N df w peso d t w tf d w N df w ∈ + + + = + + +∑ (2) donde Nt significa el número total de historias de noticias antes del tiempo t, y tf(d,w) significa cuántas veces aparece el término w en la historia de noticias d. 3.2 Cálculo de Similitud Utilizamos la distancia de Hellinger para el cálculo de la similitud entre dos historias, para dos historias d y d en el tiempo t, su similitud se define de la siguiente manera: , ( , , ) ( , , ) * ( , , ) w d d sim d d t peso d t w peso d t w ∈ = ∑ (3) 3.3 Procedimiento de Detección Para cada historia d recibida en el paso de tiempo t, el valor ( ) ( ) ( ) ( ( , , )) tiempo d tiempo d n d max sim d d t < = (4) es un puntaje utilizado para determinar si d es una historia sobre un nuevo tema y al mismo tiempo es una indicación de la confianza en nuestra decisión [8]. tiempo(d) significa el tiempo de publicación de la historia d. Si el puntaje supera el umbral θ new, entonces existe un documento suficientemente similar, por lo tanto, d es una historia antigua, de lo contrario, no hay un documento previo suficientemente similar, por lo tanto, d es una historia nueva. 4. Nuevo procedimiento NED Los sistemas NED tradicionales se pueden clasificar en dos tipos principales en cuanto al procedimiento de detección: (1) tipo S-S, en el que la historia en cuestión se compara con cada historia recibida previamente, y se utiliza la mayor similitud para determinar si la historia actual trata sobre un evento nuevo; (2) tipo S-C, en el que la historia en cuestión se compara con todos los clusters anteriores, cada uno de los cuales representa un tema, y se utiliza la mayor similitud para la decisión final sobre la historia actual. Si la similitud más alta supera el umbral θ nuevo, entonces es una historia antigua y se coloca en el clúster más similar; de lo contrario, es una historia nueva y se crea un nuevo clúster. Trabajos anteriores muestran que la primera forma es más precisa que la segunda [4][5]. Dado que a veces las historias dentro de un tema se alejan mucho entre sí, una historia puede tener una similitud muy baja con su tema. Por lo tanto, utilizar similitudes entre historias para determinar una nueva historia es mejor que utilizar similitudes entre historias y grupos. Sin embargo, el primer método requiere mucho más tiempo de comparación, lo que significa que es menos eficiente. Proponemos un nuevo procedimiento de detección que utiliza comparaciones con clusters anteriores para ayudar a encontrar la historia más similar en menos tiempo de comparación, y la decisión final sobre el nuevo evento se toma de acuerdo con la historia más similar. Por lo tanto, podemos obtener tanto la precisión de los métodos de tipo S-S como la eficiencia de los métodos de tipo S-C. El nuevo procedimiento crea un árbol de indexación de noticias dinámicamente, en el cual se agrupan historias similares para formar una jerarquía de clusters. Indexamos historias similares juntas por su ancestro común (un nodo de clúster). Historias diferentes están indexadas en diferentes grupos. Cuando una historia está llegando, utilizamos comparaciones entre la historia actual y los grupos jerárquicos anteriores para ayudar a encontrar la historia más similar que es útil para la toma de decisiones sobre nuevos eventos. Después de que se tome la decisión sobre el nuevo evento, la historia actual se inserta en el árbol de indexación para la detección siguiente. El índice de noticias en forma de árbol se define formalmente de la siguiente manera: S-Tree = {r, NC, NS, E}, donde r es la raíz de S-Tree, NC es el conjunto de todos los nodos de clúster, NS es el conjunto de todos los nodos de historia, y E es el conjunto de todos los bordes en S-Tree. Definimos un conjunto de restricciones para un S-Árbol: ⅰ. , es un nodo no terminal en el árbolC i i N i∀ ∈ → ⅱ. , es un nodo terminal en el árbolS i i N i∀ ∈ → ⅲ. , el grado de salida de es al menos 2C i i N i∀ ∈ → ⅳ. , se representa como el centroide de sus descendientesC i i iN∀ ∈ → Para una noticia di, el procedimiento de comparación y el procedimiento de inserción basados en el árbol de indexación se definen de la siguiente manera. Un ejemplo se muestra en la Figura 1 y la Figura 2. Figura 1. Procedimiento de comparación Figura 2. Procedimiento de comparación de inserción: Paso 1: comparar di con todos los nodos hijos directos de r y seleccionar λ nodos con las similitudes más altas, por ejemplo, C1 2 y C1 3 en la Figura 1. Paso 2: para cada nodo seleccionado en el paso anterior, por ejemplo. C1 2, compara di con todos sus nodos hijos directos y selecciona λ nodos con las similitudes más altas, por ejemplo. C2 2 y d8. Repetir el paso 2 para todos los nodos no terminales. Paso 3: registrar el nodo terminal con la mayor similitud a di, por ejemplo, s5, y el valor de similitud (0.20). Insertando di en el S-árbol con r como raíz: Encuentra el nodo n que es hijo directo de r en el camino desde r hasta el nodo terminal con la similitud más alta s, por ejemplo. C1 2. Si s es menor que θ init + (h-1)δ, entonces agregar di al árbol como un hijo directo de r. De lo contrario, si n es un nodo terminal, crear un nodo de clúster en lugar de n, y agregar tanto n como di como sus hijos directos; si n es un nodo no terminal, repetir este procedimiento e insertar di en el subárbol con n como raíz de forma recursiva. Aquí h es la longitud entre n y la raíz del árbol S. Cuanto más similares sean las historias en un grupo, mejor representará el grupo a las historias en él. Por lo tanto, no imponemos restricciones en la altura máxima de los árboles y el grado de un nodo. Por lo tanto, no podemos dar la complejidad de este procedimiento basado en árboles de indexación. Pero proporcionaremos el número de veces de comparación necesarias por el nuevo procedimiento en nuestros experimentos en la sección 7.5. En esta sección, se proponen dos métodos de reponderación de términos para mejorar la precisión de NED. En el primer método, se explora una nueva forma de utilizar de manera más efectiva la información de los grupos (temas). El segundo encuentra una mejor manera de utilizar entidades nombradas basadas en la clasificación de noticias. 5.1 Reajuste de términos basado en la distancia de distribución TF-IDF es el modelo más prevalente utilizado en sistemas de recuperación de información. La idea básica es que cuanto menos documentos aparezca un término, más importante es el término en la discriminación de documentos (relevantes o no relevantes para una consulta que contenga el término). Sin embargo, en el dominio de TDT, necesitamos discriminar documentos en función de los temas en lugar de las consultas. Intuitivamente, el uso de vectores de clúster (tema) para comparar con historias de noticias posteriores debería tener un mejor rendimiento que el uso de vectores de historias. Desafortunadamente, los resultados experimentales no respaldan esta intuición [4][5]. Basándonos en la observación de los datos, encontramos que la razón es que un tema de noticias generalmente contiene muchos eventos directa o indirectamente relacionados, y todos ellos tienen sus propios subtemas que suelen ser diferentes entre sí. Tomemos el tema descrito en la sección 1 como ejemplo, eventos como la explosión y el rescate tienen similitudes muy bajas con eventos sobre juicios criminales, por lo tanto, las historias sobre juicios tendrían una baja similitud con el vector de tema construido en base a sus eventos previos. Esta sección se centra en cómo hacer un uso efectivo de la información del tema y al mismo tiempo evitar el problema de la descentralización del contenido. Al principio, clasificamos los términos en 5 clases para ayudar en el análisis de las necesidades del modelo modificado: Clase de términos A: términos que ocurren con frecuencia en todo el corpus, por ejemplo, año y personas. Los términos de esta clase deben recibir pesos bajos porque no ayudan mucho en la discriminación de temas. Clase B de términos: términos que ocurren con frecuencia dentro de una categoría de noticias, por ejemplo, elección, tormenta. Son útiles para distinguir dos historias en diferentes categorías de noticias. Sin embargo, no pueden proporcionar información para determinar si dos historias tratan sobre el mismo tema o temas diferentes. En otras palabras, los términos elección y tormenta no son útiles para diferenciar dos campañas electorales y dos desastres de tormenta. Por lo tanto, los términos de esta clase deberían asignarse pesos más bajos. Clase de término C: términos que ocurren con frecuencia en un tema y raramente en otros temas, por ejemplo, el nombre de un avión accidentado, el nombre de un huracán específico. Las noticias que pertenecen a diferentes temas rara vez tienen términos superpuestos en esta clase. Cuanto más frecuentemente aparezca un término en un tema, más importante es el término para una historia perteneciente al tema, por lo tanto, el término debería tener un peso más alto. Clase de término D: términos que aparecen en un tema de manera exclusiva, pero no con frecuencia. Por ejemplo, el nombre de un bombero que se desempeñó muy bien en una acción de salvamento, que puede aparecer en solo dos o tres historias pero nunca ha aparecido en otros temas. Los términos de este tipo deberían recibir más peso que en el modelo TF-IDF. Sin embargo, dado que no son populares en el tema, no es apropiado asignarles pesos demasiado altos. Clase de término E: términos con baja frecuencia en documentos y que aparecen en diferentes temas. Los términos de esta clase deberían recibir pesos más bajos. Ahora analizamos si el modelo TF-IDF puede asignar pesos adecuados a las cinco clases de términos. Obviamente, los términos de la clase A tienen un peso bajo en el modelo TF-IDF, lo cual es acorde con el requisito descrito anteriormente. En el modelo TF-IDF, los términos de la clase B dependen altamente del número de historias en una clase de noticias. El modelo TF-IDF no puede proporcionar pesos bajos si la historia que contiene el término pertenece a una clase de noticias relativamente pequeña. Para un término de clase C, cuanto más frecuentemente aparezca en un tema, menos peso le da el modelo TFIDF. Esto entra en conflicto directo con el requisito de términos en la clase C. Para los términos de la clase D, el modelo TF-IDF les asigna pesos altos de manera correcta. Pero para los términos de la clase E, el modelo TF-IDF asigna pesos altos a ellos que no son conformes con el requisito de pesos bajos. En resumen, los términos de clase B, C, E no pueden ser ponderados adecuadamente en el modelo TF-IDF. Por lo tanto, proponemos un modelo modificado para resolver este problema. Cuando θ init y θ new están configurados de cerca, asumimos que la mayoría de las historias en un clúster de primer nivel (un nodo hijo directo del nodo raíz) están en el mismo tema. Por lo tanto, utilizamos un clúster de primer nivel para capturar la distribución de términos (df para todos los términos dentro del clúster) dentro del tema de forma dinámica. La divergencia de Kullback-Leibler de la distribución de términos en un clúster de primer nivel y el conjunto completo de historias se utiliza para ajustar los pesos de los términos: ( , , ) * (1 * ( || )) ( , , ) ( , , ) * (1 * ( || )) cw tw cw tw w d D peso d t w KL P P peso d t w peso d t w KL P P γ γ ∈ + = +∑ (5) donde ( ) ( ) ( ) ( ) 1,cw cw c c c c df w df w p y p y N N = = − (6) ( ) ( ) ( ) ( ) 1,t t tw tw t t df w df w p y p y N N = = − (7) donde dfc(w) es el número de documentos que contienen el término w dentro del clúster C, y Nc es el número de documentos en el clúster C, y Nt es el número total de documentos que llegan antes del paso de tiempo t. γ es un parámetro constante, actualmente configurado manualmente en 3. La divergencia de Kullback-Leibler se define de la siguiente manera [17]: La idea básica es que, para una historia en un tema, cuanto más ocurre un término dentro del tema y menos ocurre en otros temas, se le deben asignar pesos más altos. Obviamente, el modelo modificado puede cumplir con todos los requisitos de las cinco clases de términos mencionadas anteriormente. 5.2 Reajuste de Términos Basado en el Tipo de Término y la Clase de Historia. Trabajos anteriores encontraron que algunas clases de historias de noticias podrían lograr mejoras significativas al dar un peso adicional a las entidades nombradas. Pero encontramos que los términos de diferentes tipos deben recibir una cantidad diferente de peso adicional para diferentes clases de noticias. Utilizamos open-NLP1 para reconocer tipos de entidades nombradas y etiquetas de partes del discurso para términos que aparecen en noticias. Los tipos de entidades nombradas incluyen nombre de persona, nombre de organización, nombre de ubicación, fecha, hora, dinero y porcentaje, y se seleccionan cinco partes del discurso: ninguno (NN), verbo (VB), adjetivo (JJ), adverbio (RB) y número cardinal (CD). El análisis estadístico muestra tipos de términos discriminativos a nivel de tema para diferentes clases de historias. Por conveniencia, los tipos de entidad nombrada y etiquetas de parte de la oración se llaman uniformemente tipo de término en las secciones siguientes. Determinar si dos historias tratan sobre el mismo tema es un componente básico para la tarea de NED. Por lo tanto, al principio usamos la estadística χ² para calcular las correlaciones entre términos y temas. Para un término t y un tema T, se deriva una tabla de contingencia: Tabla 1. Una tabla de contingencia de 2×2 con el número de documentos pertenecientes al tema T y no pertenecientes al tema T incluye t A B no incluye t C D. La estadística χ² para un término específico t con respecto al tema T se define como [16]: χ² = ( ) ( ) * ( ) ( ) * ( ) * ( ) * ( ) w T A B C D AD CB A C B D A B C D + + + − + + + + (9). Los temas de noticias para la tarea TDT se clasifican además en 11 reglas de interpretaciones (ROIs) 2. El ROI se puede ver como una clase de historias de nivel superior. El promedio de correlación entre un tipo de término y un ROI de tema se calcula como: 2 avg 2 ( , )( ( , ) )k m m km kT R w P w TP R p w T R P χ χ ∈ ∈ ∑ ∑（ , ）= 1 1 k=1…K, m=1…M (10) donde K es el número de tipos de términos (fijado constantemente en 12 en el artículo). M es el número de nuevas clases (ROIs, establecido en el conjunto 11 del artículo). Pk representa el conjunto de todos los términos de tipo k, y Rm representa el conjunto de todos los temas de la clase m, p(t,T) significa la probabilidad de que t ocurra en el tema T. Debido a limitaciones de espacio, solo se enumeran partes de los tipos de términos (9 tipos de términos) y partes de las clases de noticias (8 clases) en la tabla 2 con los valores promedio de correlación entre ellos. Las estadísticas se derivan de datos etiquetados en el corpus TDT2. (Los resultados en la tabla 2 ya están normalizados para mayor comodidad en la comparación). Las estadísticas en la tabla 2 indican la utilidad de diferentes tipos de términos en la discriminación de temas con respecto a diferentes clases de noticias. Podemos ver que el nombre de la ubicación es el tipo de término más útil para tres clases de noticias: Desastres Naturales, Violencia o Guerra, Finanzas. Y para otras tres categorías: Elecciones, Casos Legales/Criminales, Ciencia y Descubrimiento, el nombre de la persona es el tipo de término más discriminativo. Para Escándalos/Audiencias, la fecha es la información más importante para la discriminación de temas. Además, los casos legales/criminales y los temas financieros tienen una correlación más alta con términos relacionados con el dinero, mientras que la ciencia y el descubrimiento tienen una correlación más alta con términos de porcentaje. Los términos no nominales son más estables para diferentes clases. 1. http://opennlp.sourceforge.net/ 2. http://projects.ldc.upenn.edu/TDT3/Guide/label.html A partir del análisis de la tabla 2, es razonable ajustar el peso de los términos según su tipo de término y la clase de noticias a la que pertenece la historia. Los nuevos pesos de los términos se recalculan de la siguiente manera: ( ) ( ) ( ) ( ) ( , , ) * ( , , ) ( , , ) * clase d D tipo w T clase d D tipo w w d peso d t w peso d t w peso d t w α α ∈ = ∑ (11) donde type(w) representa el tipo del término w, y class(d) representa la clase de la historia d, c kα es el parámetro de recalibración para la clase de noticias c y el tipo de término k. En el trabajo, simplemente utilizamos las estadísticas de la tabla 2 como parámetros de recalibración. Aunque usar las estadísticas directamente puede que no sea la mejor opción, no discutimos cómo obtener automáticamente los mejores parámetros. Intentaremos utilizar técnicas de aprendizaje automático para obtener los mejores parámetros en el trabajo futuro. En el trabajo, utilizamos BoosTexter [20] para clasificar todas las historias en uno de los 11 ROIs. BoosTexter es un programa de aprendizaje automático basado en boosting, que crea una serie de reglas simples para construir un clasificador de datos de texto o atributos-valor. Utilizamos el peso del término generado utilizando el modelo TF-IDF como característica para la clasificación de historias. Entrenamos el modelo con las 12000 historias en inglés evaluadas en TDT2, y clasificamos el resto de las historias en TDT2 y todas las historias en TDT3. Los resultados de clasificación se utilizan para la reponderación de términos en la fórmula (11). Dado que las etiquetas de clase de las historias de tema apagado no se proporcionan en los conjuntos de datos de TDT, no podemos proporcionar la precisión de clasificación aquí. Por lo tanto, no discutimos los efectos de la precisión de la clasificación en el rendimiento de NED en el artículo. 6. CONFIGURACIÓN EXPERIMENTAL 6.1 Conjuntos de datos Utilizamos dos conjuntos de datos de LDC [18], TDT2 y TDT3, para nuestros experimentos. TDT2 contiene noticias desde enero hasta junio de 1998. Contiene alrededor de 54,000 historias de fuentes como ABC, Associated Press, CNN, New York Times, Public Radio International, Voice of America, etc. Solo se consideraron historias en inglés en la colección. TDT3 contiene aproximadamente 31,000 historias en inglés recopiladas de octubre a diciembre de 1998. Además de las fuentes utilizadas en TDT2, también contiene historias de las transmisiones de televisión de NBC y MSNBC. Utilizamos versiones transcritas de las emisiones de televisión y radio además de noticias textuales. El conjunto de datos TDT2 está etiquetado con alrededor de 100 temas, y aproximadamente 12,000 historias en inglés pertenecen al menos a uno de estos temas. El conjunto de datos TDT3 está etiquetado con alrededor de 120 temas, y aproximadamente 8000 historias en inglés pertenecen al menos a uno de estos temas. Todos los temas están clasificados en 11 Reglas de Interpretación: (1) Elecciones, (2) Escándalos/Audiencias, (3) Casos Legales/Criminales, (4) Desastres Naturales, (5) Accidentes, (6) Violencia o Guerra en Curso, (7) Noticias de Ciencia y Descubrimientos, (8) Finanzas, (9) Nueva Ley, (10) Noticias Deportivas, (11) Varios. Noticias. 6.2 Métrica de evaluación TDT utiliza una función de costo CDet que combina las probabilidades de perder una nueva historia y una falsa alarma [19]: * * * *Det Miss Miss Target FA FA NontargetC C P P C P P= + (12) Tabla 2. Correlación promedio entre tipos de términos y clases de noticias donde CMiss significa el costo de perder una nueva historia, PMiss significa la probabilidad de perder una nueva historia, y PTarget significa la probabilidad de ver una nueva historia en los datos; CFA significa el costo de una falsa alarma, PFA significa la probabilidad de una falsa alarma, y PNontarget significa la probabilidad de ver una historia antigua. El costo CDet se normaliza de tal manera que un sistema perfecto obtiene una puntuación de 0 y un sistema trivial, que es el mejor de marcar todas las historias como nuevas o antiguas, obtiene una puntuación de 1: ( ( * , * ) ) Det Det Miss Target FA Nontarget C Norm C min C P C P = (13) El sistema de detección de eventos nuevos proporciona dos salidas para cada historia. La primera parte es sí o no, indicando si la historia desencadena un nuevo evento o no. La segunda parte es una puntuación que indica la confianza de la primera decisión. Los puntajes de confianza se pueden utilizar para trazar la curva DET, es decir, curvas que representan la probabilidad de falsa alarma frente a la probabilidad de omisión. El costo normalizado mínimo se puede determinar si se elige el umbral óptimo en la puntuación. RESULTADOS EXPERIMENTALES 7.1 Resultados principales Para probar los enfoques propuestos en el modelo, implementamos y probamos cinco sistemas: Sistema-1: este sistema se utiliza como referencia. Se implementa basándose en el modelo básico descrito en la sección 3, es decir, utilizando el modelo TF-IDF incremental para generar pesos de términos y utilizando la distancia de Hellinger para calcular la similitud entre documentos. La normalización del puntaje de similitud también se emplea [8]. Se utiliza el procedimiento de detección S-S. Sistema-2: este sistema es igual que el sistema-1 excepto que se utiliza el procedimiento de detección S-C. Sistema-3: este sistema es igual que el sistema-1 excepto que utiliza el nuevo procedimiento de detección que se basa en un árbol de indexación. Sistema-4: implementado basado en el enfoque presentado en la sección 5.1, es decir, los términos se vuelven a ponderar según la distancia entre las distribuciones de términos en un grupo y todas las historias. Se utiliza el nuevo procedimiento de detección. Sistema-5: implementado basado en el enfoque presentado en la sección 5.2, es decir, los términos de diferentes tipos son reponderados según la clase de noticias utilizando parámetros entrenados. El nuevo procedimiento de detección se utiliza. Los siguientes son algunos otros sistemas NED: Sistema-6: [21] para cada par de historias, calcula tres valores de similitud para entidad nombrada, entidad no nombrada y todos los términos respectivamente. Y emplear Máquina de Vectores de Soporte para predecir nuevo o antiguo utilizando los valores de similitud como características. Sistema-7: [8] extendió un modelo básico incremental de TF-IDF para incluir modelos específicos de origen, normalización de puntuaciones de similitud basada en promedios específicos de documentos, normalización de puntuaciones de similitud basada en promedios específicos de pares de fuentes, etc. Sistema-8: [13] dividió la representación del documento en dos partes: entidades nombradas y entidades no nombradas, y eligió una parte efectiva para cada clase de noticias. La tabla 3 y la tabla 4 muestran los costos normalizados ponderados por tema y los tiempos de comparación en los conjuntos de datos TDT2 y TDT3 respectivamente. Dado que no se contaba con un conjunto de datos de validación para ajustar el umbral θ nuevo en los experimentos en TDT2, solo informamos los costos normalizados mínimos para nuestros sistemas en la tabla 3. El sistema-5 supera a todos los demás sistemas, incluido el sistema-6, y realiza solo 2.78e+8 veces la comparación de tiempos en el procedimiento de detección, lo que representa solo el 13.4% del sistema-1. Tabla 3. Los resultados de NED en los sistemas TDT2 Min Norm(CDet) tiempos de comparación son: Sistema-1 0.5749 2.08e+9, Sistema-2 0.6673 3.77e+8, Sistema-3 0.5765 2.81e+8, Sistema-4 0.5431 2.99e+8, Sistema-5 0.5089 2.78e+8, Sistema-6 0.5300. Al evaluar en los costos normalizados en TDT3, utilizamos los umbrales óptimos obtenidos del conjunto de datos de TDT2 para todos los sistemas. El sistema-2 reduce los tiempos de comparación a 1.29e+9, que es solo el 18.3% del sistema-1, pero al mismo tiempo también obtiene un costo normalizado mínimo deteriorado que es 0.0499 más alto que el sistema-1. El Sistema-3 utiliza el nuevo procedimiento de detección basado en el índice de noticias en árbol. Requiere incluso menos tiempo de comparación que el sistema-2. Esto se debe a que las comparaciones de historias suelen arrojar mayores similitudes que las de grupos de historias, por lo que las historias tienden a combinarse en el sistema-3. Y el sistema-3 es básicamente equivalente al sistema-1 en los resultados de precisión. El System-4 ajusta los pesos de los términos basándose en la distancia de las distribuciones de términos entre todo el corpus y el conjunto de historias del clúster, lo que resulta en una mejora significativa de 0.0468 en comparación con el system-1. El mejor sistema (sistema-5) tiene un costo normalizado mínimo de 0.5012, lo cual es 0.0797 mejor que el sistema-1, y también mejor que cualquier otro resultado previamente reportado para este conjunto de datos [8, 13]. Además, el sistema-5 solo necesita 1.05e+8 veces de comparación, lo que equivale al 14.9% del sistema-1. Tabla 4. Los resultados de NED en los sistemas TDT3 Norm(CDet) Norma mínima(CDet) Tiempos de comparación Sistema-1 0.6159 0.5809 7.04e+8 Sistema-2① 0.6493 0.6308 1.29e+8 Sistema-3② 0.6197 0.5868 1.03e+8 Sistema-4② 0.5601 0.5341 1.03e+8 Sistema-5② 0.5413 0.5012 1.05e+8 Sistema-7 -- 0.5783 -Sistema-8 -- 0.5229 -① θ nuevo=0.13 ② θ inicial=0.13, λ =3, δ =0.15 La Figura 5 muestra las cinco curvas DET para nuestros sistemas en el conjunto de datos TDT3. El Sistema-5 logra el costo mínimo con una tasa de falsas alarmas de 0.0157 y una tasa de omisión de 0.4310. Podemos observar que System4 y System-5 obtienen una menor probabilidad de error de omisión en regiones de baja probabilidad de falsa alarma. La hipótesis es que se transfiere más valor de peso a los términos clave de los temas que a los términos no clave. La puntuación de similitud entre dos historias pertenecientes a diferentes temas es más baja que antes, porque los términos superpuestos suelen no ser términos clave de sus temas. 7.2 Selección de parámetros para la detección de árboles de indexación. La Figura 3 muestra los costos normalizados mínimos obtenidos por el sistema-3 en TDT3 utilizando diferentes parámetros. El parámetro de inicio θ se prueba en seis valores que van desde 0.03 hasta 0.18. Y el parámetro λ se prueba en cuatro valores: 1, 2, 3 y 4. Podemos ver que, cuando θ inicial se establece en 0.12, que es el más cercano a θ nuevo, los costos son más bajos que los demás. Esto es fácil de explicar, porque cuando se agrupan historias pertenecientes al mismo tema en un conjunto, es más razonable que el conjunto represente las historias en él. Cuando el parámetro λ se establece en 3 o 4, los costos son mejores que en otros casos, pero no hay mucha diferencia entre 3 y 4. 0 0.05 0.1 0.15 0.2 1 2 3 4 0.5 0.6 0.7 0.8 0.9 1 θ-initλ MinCost 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Figura 3. Costo mínimo en TDT3 (δ =0.15) 0 0.05 0.1 0.15 0.2 1 2 3 4 0 0.5 1 1.5 2 2.5 x 10 8 θ-inicio λ Comparando tiempos 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 x 10 8 Figura 4. Comparando los tiempos en TDT3 (δ =0.15), la Figura 4 muestra los tiempos de comparación utilizados por el sistema-3 en TDT3 con los mismos parámetros que la Figura 3. Los tiempos de comparación dependen fuertemente de θ init. Debido a que cuanto mayor sea θ init, menos historias se combinan juntas, más veces de comparación se necesitan para la decisión de un nuevo evento. Por lo tanto, usamos θ init = 0.13, λ = 3, δ = 0.15 para los sistemas 3, 4 y 5. En esta configuración de parámetros, podemos obtener tanto costos normalizados mínimos bajos como menos tiempos de comparación. CONCLUSIÓN Hemos propuesto un procedimiento de detección basado en un árbol de indexación de noticias en nuestro modelo. Reduce los tiempos de comparación a aproximadamente una séptima parte del método tradicional sin afectar la precisión de NED. También hemos presentado dos extensiones al modelo básico TF-IDF. La primera extensión se realiza ajustando los pesos de los términos basados en las distribuciones de términos entre todo el corpus y un conjunto de historias de un clúster. Y la segunda extensión al modelo básico TF-IDF es un mejor uso de los tipos de términos (tipos de entidades nombradas y partes de la velocidad) de acuerdo con las categorías de noticias. Nuestros resultados experimentales en los conjuntos de datos TDT2 y TDT3 muestran que ambas extensiones contribuyen significativamente a la mejora en la precisión. No consideramos la información de tiempo de noticias como una pista para la tarea de NED, ya que la mayoría de los temas duran mucho tiempo y los conjuntos de datos de TDT solo abarcan un período relativamente corto (no más de 6 meses). Para el trabajo futuro, queremos recopilar un conjunto de noticias que abarque un período más largo de internet e integrar información temporal en la tarea de NED. Dado que el tema es un grupo de noticias relativamente grueso, también queremos refinar la granularidad del grupo a nivel de evento e identificar diferentes eventos y sus relaciones dentro de un tema. Agradecimientos: Este trabajo ha sido apoyado por la Fundación Nacional de Ciencias Naturales de China bajo la subvención No. 90604025. Cualquier opinión, hallazgo y conclusión o recomendación expresada en este material son del autor(es) y no necesariamente reflejan las del patrocinador. REFERENCIAS [1] http://www.nist.gov/speech/tests/tdt/index.htm [2] En Detección y Seguimiento de Temas. Organización de la información basada en eventos. Kluwer Academic Publishers, 2002. .01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90 1 2 5 10 20 40 60 80 90 Probabilidad de Falsa Alarma (en %) Probabilidad de Omisión (en %) Curva Ponderada por Tema del SISTEMA1 Norma Mínima (Costo) del SISTEMA1 Curva Ponderada por Tema del SISTEMA2 Norma Mínima (Costo) del SISTEMA2 Curva Ponderada por Tema del SISTEMA3 Norma Mínima (Costo) del SISTEMA3 Curva Ponderada por Tema del SISTEMA4 Norma Mínima (Costo) del SISTEMA4 Curva Ponderada por Tema del SISTEMA5 Norma Mínima (Costo) del SISTEMA5 Rendimiento Aleatorio Figura 5. Curvas DET en TDT3 [3] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B.T. Archibald y X. Liu. Enfoques de aprendizaje para detectar y rastrear eventos de noticias. En el número especial de IEEE Intelligent Systems sobre Aplicaciones de Recuperación de Información Inteligente, volumen 14 (4), 1999, páginas 32-43. [4] Y. Yang, T. Pierce y J. Carbonell. Un estudio sobre la detección de eventos retrospectivos y en línea. En Actas de SIGIR-98, Melbourne, Australia, 1998, 28-36. [5] J. Allan, V. Lavrenko, D. Malin y R. Swan. Detecciones, límites y líneas de tiempo: Umass y tdt-3. En Actas del Taller de Detección y Seguimiento de Temas (TDT-3), Viena, VA, 2000, 167-174. [6] R. Papka y J. Allan. Detección de nuevos eventos en línea utilizando agrupamiento de un solo paso TÍTULO2:. Informe técnico UM-CS1998-021, 1998. [7] W. Lam, H. Meng, K. Wong y J. I'm sorry, but "Yen" is not a sentence. Can you please provide a sentence for me to translate into Spanish? Utilizando el Análisis Contextual para la Detección de Eventos de Noticias. Revista Internacional de Sistemas Inteligentes, 2001, 525-546. [8] B. Thorsten, C. Francine y F. Ayman. Un sistema para la detección de nuevos eventos. En Actas de la 26ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2003, 330-337. [9] S. Nicola y C. Joe. Combinando clasificadores semánticos y sintácticos de documentos para mejorar la detección de primeras noticias. En Actas de la 24ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell, and C. Jin. 

ACM Press. 2001, 424425. [10] Y. Yang, J. Zhang, J. Carbonell y C. Jin. Detección de novedades condicionada al tema. En Actas de la 8ª Conferencia Internacional de ACM SIGKDD, ACM Press. 2002, 688-693. [11] M. Juha, A.M. Helena y S. Marko. Aplicando Clases Semánticas en la Detección y Seguimiento de Eventos. En Actas de la Conferencia Internacional sobre Procesamiento del Lenguaje Natural (ICON 2002), 2002, páginas 175-183. [12] M. Juha, A.M. Helena y S. Marko. Semántica simple en la detección y seguimiento de temas. Recuperación de información, 7(3-4): 2004, 347-368. [13] K. Giridhar y J. Allan. Clasificación de texto y entidades nombradas para la detección de nuevos eventos. En Actas de la 27ª Conferencia Internacional Anual de ACM SIGIR, Nueva York, NY, EE. UU. ACM Press. 2004, 297-304. [14] J. P. Callan, W. B. Croft, y S. M. Harding. El Sistema de Recuperación INQUERY. En Actas de DEXA-92, 3ra Conferencia Internacional sobre Aplicaciones de Bases de Datos y Sistemas Expertos, 1992, 78-83. [15] R. Krovetz. Viendo la morfología como un proceso de inferencia. En Actas de ACM SIGIR93, 1993, 61-81. [16] Y. Yang y J. Pedersen. Un estudio comparativo sobre la selección de características en la categorización de textos. En J. D. H. Fisher, editor, La Decimocuarta Conferencia Internacional sobre Aprendizaje Automático (ICML97), Morgan Kaufmann, 1997, 412-420. [17] T. M. Cover y J. A. I'm sorry, but "Thomas" is not a sentence. Can you provide more context or a complete sentence for me to translate into Spanish? Elementos de la teoría de la información. Wiley. 1991. [18] El consorcio de datos lingüísticos, http://www.ldc,upenn.edu/. [19] La definición de la tarea TDT 2001 y el plan de evaluación, http://www.nist.gov/speech/tests/tdt/tdt2001/evalplan.htm. [20] R. E. Schapire y Y. Cantante. Boostexter: Un sistema basado en Boosting para la categorización de texto. En Machine Learning 39(2/3):1, Kluwer Academic Publishers, 2000, 35-168. [21] K. Giridhar y J. Allan. 2005. Utilizando nombres y temas para la detección de nuevos eventos. En Actas de la Conferencia de Tecnología Humana y la Conferencia sobre Métodos Empíricos en Lenguaje Natural, Vancouver, 2005, 121-128