Un estudio del modelo de generación de consultas de Poisson para la recuperación de información Qiaozhu Mei, Hui Fang, Chengxiang Zhai Departamento de Ciencias de la Computación Universidad de Illinois en Urbana-Champaign Urbana, IL 61801 {qmei2, hfang, czhai}@uiuc.edu RESUMEN Se han propuesto muchas variantes de modelos de lenguaje para la recuperación de información. La mayoría de los modelos existentes se basan en la distribución multinomial y puntuarían los documentos en función de la probabilidad de la consulta calculada en base a un modelo probabilístico de generación de consultas. En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en la distribución de Poisson. Mostramos que, aunque en sus formas más simples, la nueva familia de modelos y los modelos multinomiales existentes son equivalentes, se comportan de manera diferente para muchos métodos de suavizado. Mostramos que el modelo de Poisson tiene varias ventajas sobre el modelo multinomial, incluyendo la capacidad de ajuste suavizado por término de forma natural y permitiendo una modelización de fondo más precisa. Presentamos varias variantes del nuevo modelo correspondientes a diferentes métodos de suavizado, y las evaluamos en cuatro colecciones de pruebas representativas de TREC. Los resultados muestran que, si bien sus modelos básicos tienen un rendimiento comparable, el modelo de Poisson puede superar al modelo multinomial con suavizado por término. El rendimiento puede mejorarse aún más con un suavizado de dos etapas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Modelos de Recuperación Términos Generales: Algoritmos 1. INTRODUCCIÓN Como un nuevo tipo de modelos de recuperación probabilística, los modelos de lenguaje han demostrado ser efectivos para muchas tareas de recuperación [21, 28, 14, 4]. Entre muchas variantes de modelos de lenguaje propuestos, el más popular y fundamental es el modelo de lenguaje de generación de consultas [21, 13], que da lugar al método de puntuación de verosimilitud de consultas para clasificar documentos. En dicho modelo, dado una consulta q y un documento d, calculamos la probabilidad de generar la consulta q con un modelo estimado basado en el documento d, es decir, la probabilidad condicional p(q|d). Podemos entonces clasificar los documentos basados en la probabilidad de generar la consulta. Prácticamente todos los modelos de lenguaje de generación de consultas existentes se basan en una distribución multinomial [19, 6, 28] o en una distribución de Bernoulli multivariada [21, 18]. La distribución multinomial es especialmente popular y también se ha demostrado ser bastante efectiva. El uso intensivo de la distribución multinomial se debe en parte al hecho de que ha sido utilizada con éxito en el reconocimiento del habla, donde la distribución multinomial es una elección natural para modelar la ocurrencia de una palabra particular en una posición particular en el texto. En comparación con la distribución de Bernoulli multivariada, la distribución multinomial tiene la ventaja de poder modelar la frecuencia de términos en la consulta; en contraste, la distribución de Bernoulli multivariada solo modela la presencia y ausencia de términos de consulta, por lo tanto, no puede capturar diferentes frecuencias de términos de consulta. Sin embargo, la distribución Bernoulli multivariante también tiene una ventaja potencial sobre la multinomial desde el punto de vista de la recuperación: en una distribución multinomial, las probabilidades de todos los términos deben sumar 1, lo que dificulta la adaptación del suavizado por término, mientras que en una Bernoulli multivariante, las probabilidades de presencia de diferentes términos son completamente independientes entre sí, lo que facilita la adaptación del suavizado y ponderación por término. Se debe tener en cuenta que la ausencia de un término también se captura de forma indirecta en un modelo multinomial a través de la restricción de que todas las probabilidades de los términos deben sumar 1. En este artículo, proponemos y estudiamos una nueva familia de modelos de generación de consultas basados en la distribución de Poisson. En esta nueva familia de modelos, modelamos la frecuencia de cada término de forma independiente con una distribución de Poisson. Para puntuar un documento, primero estimaríamos un modelo de Poisson multivariado basado en el documento, y luego lo puntuaríamos en función de la probabilidad de la consulta dada por el modelo de Poisson estimado. En cierto sentido, el modelo de Poisson combina la ventaja de la multinomial en la modelización de la frecuencia de términos y la ventaja de la Bernoulli multivariada en la adaptación del suavizado por término. De hecho, similar a la distribución multinomial, la distribución de Poisson modela las frecuencias de términos, pero sin la restricción de que todas las probabilidades de términos deben sumar 1, y similar a la distribución de Bernoulli multivariante, modela cada término de forma independiente, por lo que puede acomodar fácilmente suavizado por término. Como en el trabajo existente sobre modelos de lenguaje multinomial, la suavización es fundamental para esta nueva familia de modelos. Derivamos varios métodos de suavizado para el modelo de Poisson en paralelo a los utilizados para distribuciones multinomiales, y comparamos los modelos de recuperación correspondientes con aquellos basados en distribuciones multinomiales. Observamos que mientras que con algunos métodos de suavizado, el nuevo modelo y el modelo multinomial conducen exactamente a la misma fórmula, con otros métodos de suavizado divergen, y el modelo de Poisson aporta más flexibilidad para el suavizado. En particular, una diferencia clave es que el modelo de Poisson puede acomodar naturalmente el suavizado por término, lo cual es difícil de lograr con un modelo multinomial sin un giro heurístico en la semántica de un modelo generativo. Explotamos esta ventaja potencial para desarrollar un nuevo algoritmo de suavizado dependiente del término para el modelo de Poisson y demostramos que este nuevo algoritmo de suavizado puede mejorar el rendimiento sobre los algoritmos de suavizado independientes del término utilizando tanto el modelo de Poisson como el multinomial. Esta ventaja se observa tanto en el suavizado de una etapa como en el de dos etapas. Otra ventaja potencial del modelo de Poisson es que su modelo de fondo correspondiente para suavizar puede mejorarse mediante el uso de un modelo de mezcla que tiene una fórmula de forma cerrada. Este nuevo modelo de fondo ha demostrado superar al modelo de fondo estándar y reducir la sensibilidad del rendimiento de recuperación al parámetro de suavizado. El resto del documento está organizado de la siguiente manera. En la Sección 2, presentamos la nueva familia de modelos de generación de consultas con distribución de Poisson, y presentamos varios métodos de suavizado que conducen a diferentes funciones de recuperación. En la Sección 3, comparamos analíticamente el modelo de lenguaje de Poisson con el modelo de lenguaje multinomial, desde la perspectiva de la recuperación. Luego diseñamos experimentos empíricos para comparar las dos familias de modelos de lenguaje en la Sección 4. Discutimos el trabajo relacionado en 5 y concluimos en 6. 2. GENERACIÓN DE CONSULTAS CON PROCESO DE POISSON En el marco de generación de consultas, se asume básicamente que una consulta se genera con un modelo estimado basado en un documento. En la mayoría de los trabajos existentes [12, 6, 28, 29], se asume que cada palabra de consulta se muestrea de forma independiente de una distribución multinomial. Alternativamente, asumimos que una consulta se genera muestreando la frecuencia de palabras de una serie de procesos de Poisson independientes [20]. 2.1 El Proceso de Generación Sea V = {w1, ..., wn} un conjunto de vocabulario. Sea w un fragmento de texto compuesto por un autor y c(w1), ..., c(wn) un vector de frecuencia que representa a w, donde c(wi, w) es el recuento de frecuencia del término wi en el texto w. En recuperación, w podría ser tanto una consulta como un documento. Consideramos los recuentos de frecuencia de los n términos únicos en w como n tipos diferentes de eventos, muestreados de n procesos de Poisson homogéneos independientes, respectivamente. Supongamos que t es el período de tiempo durante el cual el autor compuso el texto. Con un proceso de Poisson homogéneo, el recuento de frecuencia de cada evento, es decir, el número de ocurrencias de wi, sigue una distribución de Poisson con parámetro asociado λit, donde λi es un parámetro de tasa que caracteriza el número esperado de wi en una unidad de tiempo. La función de densidad de probabilidad de una Distribución de Poisson se da por P(c(wi, w) = k|λit) = e−λit (λit)k k! Sin perder generalidad, fijamos t como la longitud del texto w (las personas escriben una palabra en un tiempo unitario), es decir, t = |w|. Con n procesos de Poisson independientes, cada uno explicando la generación de un término en el vocabulario, la probabilidad de que w sea generado a partir de dichos procesos de Poisson puede escribirse como p(w|Λ) = Π i=1 p(c(wi, w)|Λ) = Π i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)! donde Λ = {λ1, ..., λn} y |w| = Σ i=1 c(wi, w). Nos referimos a estos n procesos de Poisson independientes con parámetro Λ como un Modelo de Lenguaje de Poisson. Sea D = {d1, ..., dm} un conjunto observado de muestras de documentos generadas a partir del proceso de Poisson anteriormente mencionado. La estimación de máxima verosimilitud (MLE) de λi es ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Nótese que esta MLE es diferente de la MLE para la distribución de Poisson sin considerar las longitudes de los documentos, que aparece en [22, 24]. Dado un documento d, podemos estimar un modelo de lenguaje de Poisson Λd utilizando d como muestra. La probabilidad de que una consulta q sea generada a partir del modelo de lenguaje del documento Λd se puede escribir como p(q|d) = w∈V p(c(w, q)|Λd) (1). Esta representación es claramente diferente del modelo de generación de consultas multinomial ya que (1) la probabilidad incluye todos los términos en el vocabulario V, en lugar de solo aquellos que aparecen en q, y (2) en lugar de la aparición de términos, el espacio de eventos de este modelo son las frecuencias de cada término. En la práctica, tenemos la flexibilidad de elegir el vocabulario V. En un extremo, podemos utilizar el vocabulario de toda la colección. Sin embargo, esto puede generar ruido y un costo computacional considerable. En el otro extremo, podemos centrarnos en los términos de la consulta e ignorar otros términos, pero podríamos perder información útil al ignorar los términos no relacionados con la consulta. Como compromiso, podemos fusionar todos los términos que no son consultas en un solo término pseudo. En otras palabras, podemos asumir que hay exactamente un término que no es una consulta en el vocabulario para cada consulta. En nuestros experimentos, adoptamos esta estrategia de término pseudo no consultado. Un documento puede ser puntuado con la probabilidad en la Ecuación 1. Sin embargo, si un término de consulta no se encuentra en el documento, la estimación máxima verosímil (MLE) de la distribución de Poisson asignaría probabilidad cero al término, lo que provocaría que la probabilidad de la consulta sea cero. Como en los enfoques existentes de modelado del lenguaje, el principal desafío al construir un modelo de recuperación razonable es encontrar un modelo de lenguaje suavizado para p(·|d). 2.2 Suavizado en el Modelo de Recuperación de Poisson. En general, queremos asignar tasas no nulas a los términos de consulta que no se ven en el documento d. Se han propuesto muchos métodos de suavizado para modelos de lenguaje multinomial[2, 28, 29]. En general, tenemos que descontar las probabilidades de algunas palabras vistas en el texto para dejar algo de probabilidad adicional para asignar a las palabras no vistas. En los modelos de lenguaje de Poisson, sin embargo, no tenemos la misma restricción que en un modelo multinomial (es decir, w∈V p(w|d) = 1). Por lo tanto, no tenemos que descontar la probabilidad de las palabras vistas para asignar una tasa distinta de cero a una palabra no vista. En cambio, solo necesitamos garantizar que k=0,1,2,... p(c(w, d) = k|d) = 1. En esta sección, presentamos tres estrategias diferentes para suavizar un modelo de lenguaje de Poisson, y mostramos cómo conducen a diferentes funciones de recuperación. 2.2.1 Suavizado Bayesiano usando una Prior Gamma Siguiendo el marco de minimización de riesgos en [11], asumimos que un documento es generado por la llegada de términos en un período de tiempo de |d| de acuerdo con el modelo de lenguaje del documento, que consiste esencialmente en un vector de tasas de Poisson para cada término, es decir, Λd = λd,1, ..., λd,|V |. Se asume que un documento es generado a partir de un modelo potencialmente diferente. Dado un documento particular d, queremos estimar Λd. La tasa de un término se estima de forma independiente de otros términos. Utilizamos la estimación bayesiana con la siguiente distribución previa Gamma, que tiene dos parámetros, α y β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ Para cada término w, los parámetros αw y βw se eligen como αw = µ ∗ λC,w y βw = µ, donde µ es un parámetro y λC,w es la tasa de w estimada a partir de algún modelo de lenguaje de fondo, generalmente el modelo de lenguaje de la colección. La distribución posterior de Λd está dada por p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w, que es un producto de |V| distribuciones Gamma con parámetros c(w, d) + µλC,w y |d| + µ para cada palabra w. Dado que la media de la Gamma es α β, tenemos ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ. Esta es precisamente la estimación suavizada del modelo de lenguaje multinomial con prior de Dirichlet [28]. Otra método directo es descomponer el modelo de generación de consultas como una mezcla de dos modelos de componentes. Uno es el modelo de lenguaje del documento estimado con el estimador de máxima verosimilitud, y el otro es un modelo estimado a partir del fondo de la colección, p(·|C), que asigna una tasa no nula a w. Por ejemplo, podemos usar un coeficiente de interpolación entre 0 y 1 (es decir, δ ∈ [0, 1]). Con esta simple interpolación, podemos puntuar un documento con Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Utilizando el estimador de máxima verosimilitud para p(·|d), tenemos que λd,w = c(w,d) |d| , por lo tanto, la Ecuación 2 se convierte en Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! · p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) También podemos utilizar un modelo de lenguaje de Poisson para p(·|C), o utilizar otros modelos basados en frecuencia. En la fórmula de recuperación anterior, la primera suma se puede calcular eficientemente. La segunda suma se puede tratar en realidad como un documento previo, que penaliza los documentos largos. Dado que la segunda suma es difícil de calcular eficientemente, fusionamos todos los términos no de consulta como un pseudo término no de consulta, denotado como N. Utilizando la formulación del pseudo término y un modelo de colección de Poisson, podemos reescribir la fórmula de recuperación como Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) donde λd,N = |d|− w∈q c(w,d) |d| y λC,N = |C|− w∈q c(w,C) |C| . 2.2.3 Suavizado de Dos Etapas Como se discute en [29], el suavizado cumple dos roles en la recuperación: (1) mejorar la estimación del modelo de lenguaje del documento, y (2) explicar los términos comunes en la consulta. Para distinguir el contenido y las palabras no discriminatorias en una consulta, seguimos [29] y asumimos que una consulta se genera muestreando de una mezcla de dos componentes de modelos de lenguaje de Poisson, con un componente siendo el modelo de documento Λd y el otro siendo un modelo de lenguaje de fondo de consulta p(·|U). p(·|U) modela las frecuencias típicas de términos en las consultas de los usuarios. Luego podemos puntuar cada documento con la probabilidad de consulta calculada utilizando el siguiente modelo de suavizado de dos etapas: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) donde δ es un parámetro que indica aproximadamente la cantidad de ruido en q. Esto se parece al suavizado de interpolación, excepto que ahora p(·|Λd) debería ser un modelo de lenguaje suavizado, en lugar del estimado con MLE. Sin conocimiento previo sobre p(·|U), podríamos establecerlo como p(·|C). Cualquier método de suavizado para el modelo de lenguaje del documento puede ser utilizado para estimar p(·|d), como el suavizado Gamma discutido en la Sección 2.2.1. El estudio empírico de los métodos de suavizado se presenta en la Sección 4.3. ANÁLISIS DEL MODELO DE LENGUAJE DE POISSON En la sección anterior, notamos que el modelo de lenguaje de Poisson tiene una fuerte conexión con el modelo de lenguaje multinomial. Esto se espera ya que ambos pertenecen a la familia exponencial [26]. Sin embargo, existen muchas diferencias cuando se aplican estos dos grupos de modelos con diferentes métodos de suavizado. ¿Desde la perspectiva de recuperación, estos dos modelos de lenguaje tendrán un rendimiento equivalente? ¿De lo contrario, qué modelo proporciona más beneficios para la recuperación, o brinda flexibilidad que podría conducir a beneficios potenciales? En esta sección, discutimos de manera analítica las características de recuperación de los modelos de lenguaje de Poisson, comparando su comportamiento con el de los modelos de lenguaje multinomial. 3.1 La Equivalencia de los Modelos Básicos Comencemos con la suposición de que todos los términos de la consulta aparecen en cada documento. Bajo esta suposición, no se necesita suavizado. Un documento puede ser puntuado por la verosimilitud logarítmica de la consulta con la estimación de máxima verosimilitud: Puntuación(d, q) = w∈V log e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)! (5) Utilizando la estimación de máxima verosimilitud, tenemos que λd,w = c(w,d) w∈V c(w,d) . Por lo tanto, Score(d, q) ∝ c(w,q)>0 c(w, q) log c(w, d) w∈V c(w, d) Esto es exactamente la verosimilitud logarítmica de la consulta si el modelo de lenguaje del documento es multinomial con estimación de máxima verosimilitud. De hecho, incluso con suavizado Gamma, al sustituir λd,w = c(w,d)+µλC,w |d|+µ y λC,w = c(w,C) |C| en la Ecuación 5, es fácil demostrar que Score(d, q) ∝ w∈q∩d c(w, q) log(1 + c(w, d) µ · c(w,C) |C| ) + |q| log µ |d| + µ (6), que es exactamente la fórmula de recuperación de Dirichlet en [28]. Ten en cuenta que esta equivalencia solo se cumple cuando la variación en la longitud del documento se modela con un proceso de Poisson. Esta derivación indica la equivalencia de los modelos de lenguaje básicos de Poisson y multinomial para la recuperación. Con otras estrategias de suavizado, sin embargo, los dos modelos serían diferentes. Sin embargo, con esta equivalencia en modelos básicos, podríamos esperar que el modelo de lenguaje de Poisson tenga un rendimiento comparable al modelo de lenguaje multinomial en la recuperación, si solo se explora un suavizado simple. Basándose en este análisis de equivalencia, uno podría preguntarse, ¿por qué deberíamos seguir el modelo de lenguaje de Poisson? En las siguientes secciones, demostramos que a pesar de la equivalencia en sus modelos básicos, el modelo de lenguaje de Poisson aporta una flexibilidad adicional para explorar técnicas avanzadas en diversas características de recuperación, lo cual no se podría lograr con modelos de lenguaje multinomial. 3.2 Suavizado Dependiente del Término Una flexibilidad del modelo de lenguaje de Poisson es que proporciona un marco natural para adaptar el suavizado dependiente del término (por término). El trabajo existente sobre suavizado de modelos de lenguaje ya ha demostrado que diferentes tipos de consultas deben suavizarse de manera diferente según lo discriminativos que sean los términos de la consulta. [7] también predijo que diferentes términos deberían tener diferentes pesos de suavizado. Con los modelos de generación de consultas multinomiales, las personas suelen utilizar un único coeficiente de suavizado para controlar la combinación del modelo de documento y el modelo de fondo [28, 29]. Este parámetro puede ser específico para diferentes consultas, pero siempre tiene que ser una constante para todos los términos. Esto es obligatorio ya que un modelo de lenguaje multinomial tiene la restricción de que w∈V p(w|d) = 1. Sin embargo, desde la perspectiva de recuperación, es posible que diferentes términos necesiten ser suavizados de manera diferente incluso si están en la misma consulta. Por ejemplo, se espera que un término no discriminatorio (por ejemplo, the, is) se explique más con el modelo de fondo, mientras que un término de contenido (por ejemplo, retrieval, bush) en la consulta debería explicarse con el modelo de documento. Por lo tanto, una mejor forma de suavizar sería establecer el coeficiente de interpolación (es decir, δ en la Fórmula 2 y la Fórmula 3) específicamente para cada término. Dado que el modelo de lenguaje de Poisson no tiene la restricción de suma a uno entre términos, puede acomodar fácilmente el suavizado por término sin necesidad de retorcer heurísticamente la semántica de un modelo generativo como en el caso de los modelos de lenguaje multinomial. A continuación presentamos una posible forma de explorar el suavizado dependiente del término con modelos de lenguaje de Poisson. Básicamente, queremos usar un coeficiente de suavizado específico del término δ en la combinación lineal, denominado como δw. Este coeficiente debería ser intuitivamente mayor si w es una palabra común y menor si es una palabra de contenido. El problema clave es encontrar un método para asignar valores razonables a δw. El ajuste empírico es inviable para tantos parámetros. En su lugar, podemos estimar los parámetros ∆ = {δ1, ..., δ|V |} maximizando la verosimilitud de la consulta dada el modelo de mezcla de p(q|ΛQ) y p(q|U), donde ΛQ es el modelo de consulta real para generar la consulta y p(q|U) es un modelo de fondo de consulta como se discute en la Sección 2.2.3. Con el modelo p(q|ΛQ) oculto, la probabilidad de la consulta es p(q|∆, U) = ΛQ w∈V ((1 − δw)p(c(w, q)|ΛQ) + δwp(c(w, q)|U))P(ΛQ|U)dΛQ. Si tenemos documentos relevantes para cada consulta, podemos aproximar el espacio del modelo de consulta con los modelos de lenguaje de todos los documentos relevantes. Sin documentos relevantes, optamos por aproximar el espacio del modelo de consulta con los modelos de todos los documentos en la colección. Estableciendo p(·|U) como p(·|C), la verosimilitud de la consulta se convierte en p(q|∆, U) = d∈C πd w∈V ((1−δw)p(c(w, q)|ˆΛd)+δwp(c(w, q)|C)) donde πd = p(ˆΛd|U). p(·|ˆΛd) es un modelo de lenguaje de Poisson estimado para el documento d. Si tenemos conocimiento previo sobre p(ˆΛd|U), como qué documentos son relevantes para la consulta, podemos ajustar πd en consecuencia, porque lo que queremos es encontrar ∆ que pueda maximizar la verosimilitud de la consulta dada los documentos relevantes. Sin este conocimiento previo, podemos dejar πd como parámetros libres y utilizar el algoritmo EM para estimar πd y ∆. Las funciones de actualización se dan como π (k+1) d = πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) d∈C πd w∈V ((1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) y δ (k+1) w = d∈C πd δwp(c(w, q)|C)) (1 − δw)p(c(w, q)|ˆΛd) + δwp(c(w, q)|C)) Como se discute en [29], solo necesitamos ejecutar el algoritmo EM durante varias iteraciones, por lo que el costo computacional es relativamente bajo. Nuevamente asumimos nuestro vocabulario que contiene todos los términos de consulta más un término pseudo no relacionado con la consulta. Ten en cuenta que la función no proporciona una forma explícita de estimar el coeficiente para el término no consultado no visto. En nuestros experimentos, lo configuramos al promedio sobre δw de todos los términos de consulta. Con esta flexibilidad, esperamos que los modelos de lenguaje de Poisson puedan mejorar el rendimiento de recuperación, especialmente para consultas extensas, donde los términos de la consulta tienen varios valores discriminatorios. En la Sección 4, utilizamos experimentos empíricos para probar esta hipótesis. Otro aspecto de flexibilidad es explorar diferentes modelos de fondo (colección) (es decir, p(·|U), o p(·|C)). Una suposición común hecha en la recuperación de información mediante modelado de lenguaje es que el modelo de fondo es un modelo homogéneo de los modelos de documentos [28, 29]. De manera similar, también podemos asumir que el modelo de colección es un modelo de lenguaje de Poisson, con las tasas λC,w = d∈C c(w,d) |C| . Sin embargo, esta suposición generalmente no se cumple, ya que la colección es mucho más compleja que un solo documento. De hecho, la colección suele consistir en una mezcla de documentos con diversos géneros, autores, temas, etc. Tratar el modelo de colección como una mezcla de modelos de documentos, en lugar de un único modelo de pseudo-documento, es más razonable. El trabajo existente de modelado de lenguaje multinomial ya ha demostrado que un mejor modelado del fondo mejora el rendimiento de recuperación, como los grupos [15, 10], documentos vecinos [25] y aspectos [8, 27]. Todos los enfoques pueden ser fácilmente adoptados utilizando modelos de lenguaje de Poisson. Sin embargo, un problema común de estos enfoques es que todos requieren una computación intensiva para construir el modelo de fondo. Con el modelado de lenguaje de Poisson, demostramos que es posible modelar la mezcla de fondo sin incurrir en altos costos computacionales. La mezcla de Poisson [3] ha sido propuesta para modelar una colección de documentos, lo cual puede ajustar los datos mucho mejor que un solo Poisson. La idea básica es asumir que la colección se genera a partir de una mezcla de modelos de Poisson, que tiene la forma general de p(x = k|PM) = λ p(λ)p(x = k|λ)dλ p(·|λ) es un único modelo de Poisson y p(λ) es una función de densidad de probabilidad arbitraria. Hay tres mezclas de Poisson bien conocidas [3]: 2-Poisson, Binomial Negativa y la Mezcla K de Katzs [9]. Se debe tener en cuenta que el modelo 2-Poisson ha sido explorado en modelos de recuperación probabilística, lo que condujo a la conocida fórmula BM25 [22]. Todas estas mezclas tienen formas cerradas y pueden ser estimadas eficientemente a partir de la colección de documentos. Esta es una ventaja sobre los modelos de mezcla multinomial, como PLSI [8] y LDA [1], para la recuperación. Por ejemplo, la función de densidad de probabilidad de la mezcla K de Katz se expresa como p(c(w) = k|αw, βw) = (1 − αw)ηk,0 + αw βw + 1 ( βw βw + 1 )k donde ηk,0 = 1 cuando k = 0, y 0 en otro caso. Con la observación de una colección de documentos, αw y βw pueden estimarse como βw = cf(w) − df(w) df(w) y αw = cf(w) Nβw donde cf(w) y df(w) son la frecuencia de la colección y la frecuencia del documento de w, y N es el número de documentos en la colección. Para tener en cuenta las diferentes longitudes de los documentos, asumimos que βw es una estimación razonable para generar un documento de longitud promedio, y usamos β = βw avdl |q| para generar la consulta. Este modelo de mezcla de Poisson puede ser fácilmente utilizado para reemplazar P(·|C) en las funciones de recuperación 3 y 4. 3.4 Otras posibles flexibilidades Además del suavizado dependiente del término y la mezcla eficiente de fondo, un modelo de lenguaje de Poisson también tiene algunas otras ventajas potenciales. Por ejemplo, en la Sección 2, vemos que la Fórmula 2 introduce un componente que penaliza la longitud del documento. Intuitivamente, cuando el documento tiene más palabras únicas, será penalizado más. Por otro lado, si un documento es exactamente n copias de otro documento, no sería penalizado en exceso. Esta característica es deseable y no se logra con el modelo de Dirichlet [5]. Potencialmente, este componente podría penalizar un documento según los tipos de términos que contiene. Con ajustes específicos del término δ, podríamos obtener aún más flexibilidad para la normalización de la longitud de los documentos. La pseudo-retroalimentación es otra dirección interesante donde el modelo de Poisson podría mostrar su ventaja. Con la retroalimentación basada en el modelo, podríamos relajar nuevamente los coeficientes de combinación del modelo de retroalimentación y el modelo de fondo, y permitir que diferentes términos contribuyan de manera diferente al modelo de retroalimentación. También podríamos utilizar los documentos relevantes para aprender mejor los coeficientes de suavizado por término. 4. EVALUACIÓN En la Sección 3, comparamos analíticamente los modelos de lenguaje de Poisson y los modelos de lenguaje multinomial desde la perspectiva de la generación y recuperación de consultas. En esta sección, comparamos empíricamente estas dos familias de modelos. Los resultados del experimento muestran que el modelo de Poisson con suavizado por término supera al modelo multinomial, y el rendimiento puede mejorarse aún más con un suavizado de dos etapas. El uso de una mezcla de Poisson como modelo de fondo también mejora el rendimiento de recuperación. 4.1 Conjuntos de datos Dado que el rendimiento de recuperación podría variar significativamente de una colección de pruebas a otra, y de una consulta a otra, seleccionamos cuatro colecciones de pruebas representativas de TREC: AP, Trec7, Trec8 y Wt2g (Web). Para cubrir diferentes tipos de consultas, seguimos [28, 5], y construimos consultas de palabras clave cortas (SK, título de palabra clave), consultas de texto corto (SV, descripción en una oración) y consultas de texto largo (LV, múltiples oraciones). Los documentos se han reducido con el stemmer de Porter, y no eliminamos ninguna palabra de parada. Para cada parámetro, variamos su valor para cubrir un rango razonablemente amplio. 4.2 Comparación con Multinomial Comparamos el rendimiento de los modelos de recuperación de Poisson y los modelos de recuperación multinomial utilizando suavizado de interpolación (JelinekMercer, JM) y suavizado bayesiano con priors conjugados. La Tabla 1 muestra que los dos modelos suavizados JM se comportan de manera similar en todos los conjuntos de datos. Dado que el Suavizado de Dirichlet para el modelo de lenguaje multinomial y el Suavizado de Gamma para el modelo de lenguaje de Poisson conducen a la misma fórmula de recuperación, se presentan conjuntamente el rendimiento de estos dos modelos. Vemos que los métodos de suavizado de Dirichlet/Gamma superan a ambos métodos de suavizado de Jelinek-Mercer. Las curvas de sensibilidad de parámetros para dos métodos de suavizado de Jelinek-Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parámetro: δ Precisión Promedio JM-Multinomial: LV JM-Multinomial: SV JM-Multinomial: SK JM-Poisson: SK JM-Poisson: SV JM-Poisson: LV Figura 1: Poisson y multinomial tienen un rendimiento similar con los métodos de suavizado de Jelinek-Mercer que se muestran en la Figura 1. Claramente, estos dos métodos tienen un rendimiento similar tanto en términos de optimalidad. La consulta de datos JM-Multinomial JM-Poisson Dirichlet/Gamma Por término 2-Etapa Poisson MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d MAP InitPr Pr@5d AP88-89 SK 0.203 0.585 0.356 0.203 0.585 0.358 0.224 0.629 0.393 0.226 0.630 0.396 SV 0.187 0.580 0.361 0.183 0.571 0.345 0.204 0.613 0.387 0.217* 0.603 0.390 LV 0.283 0.716 0.480 0.271 0.692 0.470 0.291 0.710 0.496 0.304* 0.695 0.510 Trec7 SK 0.167 0.635 0.400 0.168 0.635 0.404 0.186 0.687 0.428 0.185 0.646 0.436 SV 0.174 0.655 0.432 0.176 0.653 0.432 0.182 0.666 0.432 0.196* 0.660 0.440 LV 0.223 0.730 0.496 0.215 0.766 0.488 0.224 0.748 0.52 0.236* 0.738 0.512 Trec8 SK 0.239 0.621 0.440 0.239 0.621 0.436 0.257 0.718 0.496 0.256 0.704 0.468 SV 0.231 0.686 0.448 0.234 0.702 0.456 0.228 0.691 0.456 0.246* 0.692 0.476 LV 0.265 0.796 0.548 0.261 0.757 0.520 0.260 0.741 0.492 0.274* 0.766 0.508 Web SK 0.250 0.616 0.380 0.250 0.616 0.380 0.302 0.767 0.468 0.307 0.739 0.468 SV 0.214 0.611 0.392 0.217 0.609 0.384 0.273 0.693 0.508 0.292* 0.703 0.480 LV 0.266 0.790 0.464 0.259 0.776 0.452 0.283 0.756 0.496 0.311* 0.759 0.488 Tabla 1: Comparación de rendimiento entre los modelos de recuperación Poisson y Multinomial: los modelos básicos tienen un rendimiento comparable; el suavizado de dos etapas dependiente del término mejora significativamente el Poisson. Un asterisco (*) indica que la diferencia entre el rendimiento del suavizado de dos etapas dependiente del término y el del suavizado único de Dirichlet/Gamma es estadísticamente significativa según la prueba de rango con signo de Wilcoxon al nivel de 0.05. o sensibilidad. Esta similitud en el rendimiento es esperada tal como discutimos en la Sección 3.1. Aunque el modelo de Poisson y el modelo multinomial son similares en cuanto al modelo básico y/o a los métodos de suavizado simples, el modelo de Poisson tiene un gran potencial y flexibilidad para ser mejorado aún más. Como se muestra en la columna más a la derecha de la Tabla 1, el modelo de Poisson de dos etapas dependiente del término supera consistentemente a los modelos básicos de suavizado, especialmente para consultas verbosas. Este modelo se presenta en la Fórmula 4, con un suavizado Gamma para el modelo de documento p(·|d), y δw, que es dependiente del término. El parámetro µ del suavizado Gamma de la primera etapa se ajusta empíricamente. Los coeficientes de combinación (es decir, ∆) se estiman con el algoritmo EM en la Sección 3.2. Las curvas de sensibilidad de parámetros para Dirichlet/Gamma y el modelo de suavizado de dos etapas por término se representan en la Figura 2. El método de suavizado de dos etapas por término es menos sensible al parámetro µ que Dirichlet/Gamma, y produce un rendimiento óptimo mejor. 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0.1 0.12 0.14 0.16 0.18 0.2 0.22 Conjunto de datos: AP; Tipo de consulta: SV Parámetro: µ Precisión promedio Dirichlet/Gamma Suavizado por término dependiente de 2 etapas Figura 2: El suavizado por término dependiente de dos etapas de Poisson supera a Dirichlet/Gamma En las siguientes subsecciones, realizamos experimentos para demostrar cómo la flexibilidad del modelo de Poisson podría ser utilizada para lograr un mejor rendimiento, algo que no podemos lograr con modelos de lenguaje multinomial. 4.3 Suavizado Dependiente del Término Para probar la efectividad del suavizado dependiente del término, realizamos los siguientes dos experimentos. En el primer experimento, relajamos el coeficiente constante en la fórmula simple de suavizado de Jelinek-Mercer (es decir, Fórmula 3), y utilizamos el algoritmo EM propuesto en la Sección 3.2 para encontrar un δw para cada término único. Dado que estamos utilizando el algoritmo EM para estimar iterativamente los parámetros, generalmente no queremos que la probabilidad de p(·|d) sea cero. Luego utilizamos un método simple de Laplace para suavizar ligeramente el modelo del documento antes de que entre en las iteraciones de EM. Los documentos son luego evaluados con la Fórmula 3, pero utilizando δw aprendidos. Los resultados están etiquetados con JM+L en la Tabla 2. Los datos Q JM JM JM+L. 2-Stage 2-Stage (MAP) PT: No Sí Sí No Sí AP SK 0.203 0.204 0.206 0.223 0.226* SV 0.183 0.189 0.214* 0.204 0.217* Trec7 SK 0.168 0.171 0.174 0.186 0.185 SV 0.176 0.147 0.198* 0.194 0.196 Trec8 SK 0.239 0.240 0.227* 0.257 0.256 SV 0.234 0.223 0.249* 0.242 0.246* Web SK 0.250 0.236 0.220* 0.291 0.307* SV 0.217 0.232 0.261* 0.273 0.292* Tabla 2: Suavizado dependiente del término mejora el rendimiento de recuperación Un asterisco (*) en la Columna 3 indica que la diferencia entre el método JM+L. y el método JM es estadísticamente significativa; un asterisco (*) en la Columna 5 significa que la diferencia entre el método de dos etapas dependiente del término y el método de dos etapas dependiente de la consulta es estadísticamente significativa; PT significa por término. Con coeficientes dependientes del término, el rendimiento del modelo de Poisson de Jelinek-Mercer se mejora en la mayoría de los casos. Sin embargo, en algunos casos (por ejemplo, Trec7/SV), tiene un rendimiento deficiente. Esto podría ser causado por el problema de la estimación de EM con modelos de documentos no suavizados. Una vez que se asigna una probabilidad no nula a todos los términos antes de ingresar a la iteración de EM, el rendimiento en las consultas detalladas puede mejorar significativamente. Esto indica que todavía hay espacio para encontrar mejores métodos para estimar δw. Por favor, tenga en cuenta que ni el método JM perterm ni el método JM+L. tienen un parámetro para ajustar. Como se muestra en la Tabla 1, el término de suavizado de dos etapas dependiente puede mejorar significativamente el rendimiento de recuperación. Para entender si la mejora es atribuible al suavizado dependiente del término o al marco de suavizado de dos etapas, diseñamos otro experimento para comparar el suavizado de dos etapas por término con el método de suavizado de dos etapas propuesto en [29]. Su método logró encontrar coeficientes específicos para la consulta, por lo tanto, una consulta detallada usaría un δ más alto. Sin embargo, dado que su modelo se basa en modelado de lenguaje multinomial, no pudieron obtener coeficientes por término. Adoptamos su método para el suavizado de Poisson en dos etapas, y también estimamos un coeficiente por consulta para todos los términos. Comparamos el rendimiento de dicho modelo con el modelo de suavizado de dos etapas por término, y presentamos los resultados en las dos columnas de la derecha en la Tabla 2. Una vez más, vemos que el suavizado de dos etapas por término supera al suavizado de dos etapas por consulta, especialmente para consultas extensas. La mejora no es tan grande como la que logra el método de suavizado de perterm sobre Dirichlet/Gamma. Esto es esperado, ya que el suavizado por consulta ha abordado en cierta medida el problema de discriminación de consultas. Este experimento muestra que incluso si el suavizado ya es por consulta, hacerlo por término sigue siendo beneficioso. En resumen, el suavizado por término mejoró el rendimiento de recuperación tanto del método de suavizado de una etapa como de dos etapas. Modelo de Fondo de Mezcla En esta sección, realizamos experimentos para examinar los beneficios de usar un modelo de fondo de mezcla sin costo computacional adicional, lo cual no se puede lograr con modelos multinomiales. Específicamente, en la fórmula de recuperación 3, en lugar de utilizar una sola distribución de Poisson para modelar el fondo p(·|C), utilizamos el modelo de mezcla K de Katz, que es esencialmente una mezcla de distribuciones de Poisson. p(·|C) se puede calcular eficientemente con estadísticas de colección simples, como se discute en la Sección 3.3. Consulta de datos JM. Poisson JM. El modelo de fondo K-Mixture mejora el rendimiento de recuperación. Se compara el rendimiento del modelo de recuperación JM con un solo fondo de Poisson y con el modelo de fondo K-Mixture de Katz en la Tabla 3. Claramente, el uso de K-Mixture para modelar el modelo de fondo supera al modelo de fondo de Poisson único en la mayoría de los casos, especialmente para consultas detalladas donde la mejora es estadísticamente significativa. La Figura 3 muestra que el rendimiento cambia según diferentes parámetros para consultas cortas y verbosas. El modelo que utiliza un fondo de mezcla K es menos sensible que el que utiliza un fondo de Poisson único. Dado que este tipo de mezcla 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Datos: Trec8; Consulta: SV Parámetro: δ Precisión Promedio Fondo de Poisson Fondo de Mezcla K−Mixture Figura 3: El modelo de fondo de mezcla K-Mixture desvía la sensibilidad de las consultas verbosas el modelo de fondo no requiere ningún costo computacional adicional, sería interesante estudiar si el uso de otros modelos de mezcla de Poisson, como 2-Poisson y Binomial negativo, podría ayudar al rendimiento. 5. TRABAJO RELACIONADO Hasta donde sabemos, no ha habido ningún estudio de modelos de generación de consultas basados en la distribución de Poisson. Los modelos de lenguaje han demostrado ser efectivos para muchas tareas de recuperación [21, 28, 14, 4]. El más popular y fundamental es el modelo de lenguaje generador de consultas [21, 13]. Todos los modelos de lenguaje de generación de consultas existentes se basan en una distribución multinomial [19, 6, 28, 13] o en una distribución de Bernoulli multivariada [21, 17, 18]. Introducimos una nueva familia de modelos de lenguaje, basados en la distribución de Poisson. La distribución de Poisson ha sido estudiada previamente en los modelos de generación de documentos [16, 22, 3, 24], lo que ha llevado al desarrollo de una de las fórmulas de recuperación más efectivas, BM25 [23]. El estudio [24] analiza la derivación paralela de tres modelos de recuperación diferentes, lo cual está relacionado con nuestra comparación entre Poisson y multinomial. Sin embargo, el modelo de Poisson en su artículo sigue estando bajo el marco de generación de documentos, y tampoco tiene en cuenta la variación en la longitud de los documentos. [26] introduce una forma de buscar empíricamente un modelo exponencial para los documentos. Las mezclas de Poisson [3] como la 2-Poisson [22], multinomial negativa y Katzs KMixture [9] han demostrado ser efectivas para modelar y recuperar documentos. Una vez más, ninguno de estos trabajos explora la distribución de Poisson en el marco de generación de consultas. El suavizado del modelo de lenguaje [2, 28, 29] y las estructuras de fondo [15, 10, 25, 27] han sido estudiados con modelos de lenguaje multinomial. [7] muestra analíticamente que el suavizado específico de términos podría ser útil. Mostramos que el modelo de lenguaje de Poisson es natural para adaptar el suavizado por término sin giros heurísticos de la semántica de un modelo generativo, y es capaz de modelar de manera más eficiente la mezcla de fondo, tanto analítica como empíricamente. 6. CONCLUSIONES Presentamos una nueva familia de modelos de lenguaje para generación de consultas para recuperación basada en la distribución de Poisson. Derivamos varios métodos de suavizado para esta familia de modelos, incluyendo suavizado de una etapa y suavizado de dos etapas. Comparamos los nuevos modelos con los populares modelos de recuperación multinomial tanto de forma analítica como experimental. Nuestro análisis muestra que si bien nuestros nuevos modelos y modelos multinomiales son equivalentes bajo algunas suposiciones, generalmente son diferentes con algunas diferencias importantes. En particular, demostramos que Poisson tiene una ventaja sobre multinomial al adaptarse de forma natural al suavizado por término. Explotamos esta propiedad para desarrollar un nuevo algoritmo de suavizado por término para modelos de lenguaje de Poisson, que se muestra que supera al suavizado independiente de términos tanto para modelos de Poisson como multinomiales. Además, demostramos que un modelo de fondo mixto para Poisson puede ser utilizado para mejorar el rendimiento y la robustez sobre el modelo de fondo de Poisson estándar. Nuestro trabajo abre muchas direcciones interesantes para futuras exploraciones en esta nueva familia de modelos. Explorar más a fondo las flexibilidades de los modelos de lenguaje multinomial, como la normalización de longitud y la retroalimentación pseudo podrían ser buenos trabajos futuros. También resulta atractivo encontrar métodos robustos para aprender los coeficientes de suavizado por término sin costos adicionales de computación. AGRADECIMIENTOS Agradecemos a los revisores anónimos de SIGIR 07 por sus útiles comentarios. Este material se basa en parte en el trabajo apoyado por la Fundación Nacional de Ciencias bajo los números de premio IIS-0347933 y 0425852. REFERENCIAS [1] D. Blei, A. Ng y M. Jordan. Asignación latente de Dirichlet. Revista de Investigación de Aprendizaje Automático, 3:993-1022, 2003. [2] S. F. Chen y J. Goodman. Un estudio empírico de técnicas de suavizado para modelado de lenguaje. Informe técnico TR-10-98, Universidad de Harvard, 1998. [3] K. Church y W. Gale. Mezclas de Poisson. I'm sorry, but the word "Nat." is not a complete sentence. Could you please provide more context or a complete sentence for me to translate into Spanish? Lenguaje. Eng., 1(2):163-190, 1995. [4] W. B. Croft y J. Lafferty, editores. Modelado de lenguaje y recuperación de información. Kluwer Academic Publishers, 2003. [5] H. Fang, T. Tao y C. Zhai. Un estudio formal de las heurísticas de recuperación de información. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 49-56, 2004. [6] D. Hiemstra. Utilizando modelos de lenguaje para la recuperación de información. Tesis doctoral, Universidad de Twente, Enschede, Países Bajos, 2001. [7] D. Hiemstra. Suavizado específico del término para el enfoque de modelado de lenguaje en la recuperación de información: la importancia de un término de consulta. En Actas de la 25ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 35-41, 2002. [8] T. Hofmann. Indexación semántica latente probabilística. En Actas de ACM SIGIR99, páginas 50-57, 1999. [9] S. M. Katz. Distribución de palabras y frases de contenido en el modelado de texto y lenguaje. I'm sorry, but the sentence "Nat." is not a complete sentence and it's not clear what it refers to. Could you please provide more context or a complete sentence for me to translate to Spanish? Lenguaje. Eng., 2(1):15-59, 1996. [10] O. Kurland y L. Lee. Estructura de corpus, modelos de lenguaje y recuperación de información ad-hoc. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 194-201, 2004. [11] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En Actas de SIGIR01, páginas 111-119, septiembre de 2001. [12] J. Lafferty y C. Zhai. Modelos de RI probabilísticos basados en la generación de consultas y documentos. En Actas del taller de Modelado de Lenguaje e IR, páginas 1-5, 31 de mayo - 1 de junio de 2001. [13] J. Lafferty y C. Zhai. Modelos de relevancia probabilística basados en la generación de documentos y consultas. En W. B. Croft y J. Lafferty, editores, Modelado del Lenguaje y Recuperación de Información. Kluwer Academic Publishers, 2003. [14] V. Lavrenko y B. Croft. Modelos de lenguaje basados en relevancia. En Actas de SIGIR01, páginas 120-127, septiembre de 2001. [15] X. Liu y W. B. Croft. Recuperación basada en clústeres utilizando modelos de lenguaje. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 186-193, 2004. [16] E. L. Margulis. Modelando documentos con múltiples distribuciones de Poisson. I'm sorry, but the sentence "Inf." is not a complete sentence. Could you please provide more context or a full sentence for me to translate to Spanish? Proceso. Manage., 29(2):215-227, 1993. [17] A. McCallum and K. Nigam. 

Gestión., 29(2):215-227, 1993. [17] A. McCallum y K. Nigam. Una comparación de modelos de eventos para la clasificación de texto con Naive Bayes. En Actas del Taller AAAI-98 sobre Aprendizaje para la Categorización de Textos, 1998. [18] D. Metzler, V. Lavrenko y W. B. Croft. Modelos formales de múltiples Bernoulli para modelado de lenguaje. En Actas de la 27ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 540-541, 2004. [19] D. H. Miller, T. Leek y R. Schwartz. Un sistema de recuperación de información basado en un modelo oculto de Markov. En Actas de la Conferencia ACM SIGIR de 1999 sobre Investigación y Desarrollo en Recuperación de Información, páginas 214-221, 1999. [20] A. Papoulis. Probabilidad, variables aleatorias y procesos estocásticos. Nueva York: McGraw-Hill, 1984, 2da ed., 1984. [21] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En Actas de la 21ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 275-281, 1998. [22] S. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En Actas de SIGIR94, páginas 232-241, 1994. [23] S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu y M. Gatford. Okapi en TREC-3. En D. K. Harman, editor, La Tercera Conferencia de Recuperación de Texto (TREC-3), páginas 109-126, 1995. [24] T. Roelleke y J. Wang. Una derivación paralela de modelos de recuperación de información probabilística. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 107-114, 2006. [25] T. Tao, X. Wang, Q. Mei y C. Zhai. Recuperación de información del modelo de lenguaje con expansión de documentos. En Actas de HLT/NAACL 2006, páginas 407-414, 2006. [26] J. Teevan y D. R. Karger. Desarrollo empírico de un modelo probabilístico exponencial para la recuperación de texto: utilizando análisis textual para construir un mejor modelo. En Actas de la 26ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 18-25, 2003. [27] X. Wei y W. B. Croft. Modelos de documentos basados en LDA para recuperación ad-hoc. En Actas de la 29ª conferencia internacional anual de ACM SIGIR sobre Investigación y desarrollo en recuperación de información, páginas 178-185, 2006. [28] C. Zhai y J. Lafferty. Un estudio de métodos de suavizado para modelos de lenguaje aplicados a la recuperación de información ad-hoc. En Actas de ACM SIGIR01, páginas 334-342, septiembre de 2001. [29] C. Zhai y J. Lafferty. Modelos de lenguaje de dos etapas para la recuperación de información. En Actas de ACM SIGIR02, páginas 49-56, agosto de 2002.