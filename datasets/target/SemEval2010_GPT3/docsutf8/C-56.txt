Un Soporte de Ejecución de Procesos Jerárquicos para la Computación en Grid Fábio R. L. Cicerre Instituto de Computación Universidad Estatal de Campinas Campinas, Brasil fcicerre@ic.unicamp.br Edmundo R. M. Madeira Instituto de Computación Universidad Estatal de Campinas Campinas, Brasil edmundo@ic.unicamp.br Luiz E. Buzato Instituto de Computación Universidad Estatal de Campinas Campinas, Brasil buzato@ic.unicamp.br RESUMEN Grid es una infraestructura emergente utilizada para compartir recursos entre organizaciones virtuales de manera transparente y para proporcionar una potencia informática revolucionaria a bajo costo. Hoy en día existen docenas de productos académicos y comerciales que permiten la ejecución de tareas aisladas en rejillas, pero pocos productos admiten la realización de procesos de larga duración de manera distribuida. Para abordar dicho tema, este documento presenta un modelo de programación y una infraestructura que programa jerárquicamente las actividades de procesos utilizando los nodos disponibles en un entorno de red amplia. Sus ventajas son la distribución automática y estructurada de actividades, así como la fácil supervisión y control del proceso. Categorías y Descriptores de Asignaturas C.2.4 [Redes de Computadoras-Comunicación]: Sistemas Distribuidos-aplicaciones distribuidas Términos Generales Diseño, Rendimiento, Gestión, Algoritmos 1. La computación en malla es un modelo para la computación distribuida y paralela de amplia área a través de redes heterogéneas en múltiples dominios administrativos. Este campo de investigación tiene como objetivo promover el intercambio de recursos y proporcionar un gran poder de computación a través de esta amplia red de organizaciones virtuales de manera fluida [8]. Tradicionalmente, como en Globus [6], Condor-G [9] y Legion [10], existe una infraestructura mínima que proporciona el intercambio de recursos de datos, la gestión de la utilización de recursos computacionales y la ejecución distribuida. Específicamente, considerando la ejecución distribuida, la mayoría de las infraestructuras de grid existentes admiten la ejecución de tareas aisladas, pero no consideran sus interdependencias como en los procesos (flujos de trabajo) [12]. Esta deficiencia limita mejores algoritmos de programación, coordinación de ejecución distribuida y recuperación automática de la ejecución. Hay pocas infraestructuras de middleware propuestas que admitan la ejecución de procesos en la red. En general, modelan procesos interconectando sus actividades a través de dependencias de control y datos. Entre ellos, WebFlow [1] enfatiza una arquitectura para construir procesos distribuidos; Opera-G [3] proporciona recuperación y control de ejecución, GridFlow [5] se centra en algoritmos de programación mejorados que aprovechan las dependencias de actividad, y SwinDew [13] soporta una ejecución totalmente distribuida en redes peer-to-peer. Sin embargo, dichas infraestructuras contienen algoritmos de programación centralizados por proceso [1, 3, 5], o completamente distribuidos, pero difíciles de monitorear y controlar [13]. Para abordar tales limitaciones, este documento propone un modelo de programación estructurada para la descripción de procesos y una infraestructura jerárquica de ejecución de procesos. El modelo de programación emplea un flujo de control estructurado para promover la ejecución de actividades controlada y contextualizada. Complementariamente, la infraestructura de soporte, que ejecuta una especificación de proceso, aprovecha la estructura jerárquica de un proceso especificado para distribuir y programar actividades fuertemente dependientes como una unidad, permitiendo un mejor rendimiento de ejecución y tolerancia a fallos, y proporcionando comunicación localizada. El modelo de programación y la infraestructura de soporte, llamados X avantes, se encuentran en proceso de implementación con el fin de mostrar la viabilidad del modelo propuesto y demostrar sus dos principales ventajas: promover la ejecución y programación de procesos ampliamente distribuidos, pero de manera controlada, estructurada y localizada. La siguiente sección describe el modelo de programación, y la Sección 3, la infraestructura de soporte para el modelo de computación en malla propuesto. La sección 4 demuestra cómo la infraestructura de soporte ejecuta procesos y distribuye actividades. Los trabajos relacionados se presentan y se comparan con el modelo propuesto en la Sección 5. La última sección concluye este artículo abarcando las ventajas del soporte de ejecución de procesos jerárquicos propuesto para el área de la computación en malla y enumera algunos trabajos futuros. 87 Middleware 2004 Companion ProcessElement Process Activity Controller 1 * 1 * Figura 1: Marco de alto nivel del modelo de programación 2. El modelo de programación diseñado para la arquitectura de computación en malla es muy similar al especificado en el Lenguaje de Ejecución de Procesos de Negocio (BPEL) [2]. Ambos describen procesos en documentos XML [4], pero el primero especifica procesos estrictamente síncronos y estructurados, y tiene más construcciones para el control paralelo estructurado. La razón detrás de su diseño es la posibilidad de distribuir jerárquicamente el control y la coordinación del proceso basándose en construcciones estructuradas, a diferencia de BPEL, que no permite la composición jerárquica de procesos. En el modelo de programación propuesto, un proceso es un conjunto de actividades interdependientes organizadas para resolver un problema específico. En detalle, un proceso está compuesto por actividades, subprocesos y controladores (ver Figura 1). Las actividades representan tareas simples que se ejecutan en nombre de un proceso; los subprocesos son procesos ejecutados en el contexto de un proceso padre; y los controladores son elementos de control utilizados para especificar el orden de ejecución de estas actividades y subprocesos. Al igual que en los lenguajes estructurados, los controladores pueden anidarse y luego determinar el orden de ejecución de otros controladores. Los datos se intercambian entre los elementos del proceso a través de parámetros. Son pasados por valor, en caso de objetos simples, o por referencia, si son objetos remotos compartidos entre elementos del mismo controlador o proceso. Los datos externos pueden ser accedidos a través de fuentes de datos, como bases de datos relacionales u objetos distribuidos. 2.1 Controladores Los controladores son estructuras de control utilizadas para definir el flujo de control de los procesos. Hay controladores secuenciales y paralelos. Los tipos de controladores secuenciales son: bloque, interruptor, para y mientras. El controlador de bloque es una construcción secuencial simple, y los demás imitan estructuras equivalentes de lenguajes de programación estructurada. De manera similar, los tipos paralelos son: par, parswitch, parfor y parwhile. Extienden los respectivos contrapartes secuenciales para permitir la ejecución en paralelo de los elementos del proceso. Todos los tipos de controladores paralelos bifurcan la ejecución de uno o más elementos de proceso, y luego esperan a que cada ejecución termine. De hecho, contienen una bifurcación y una unión de ejecución. Con el objetivo de implementar una unión condicional, todos los tipos de controladores paralelos contienen una condición de salida, evaluada cada vez que finaliza la ejecución de un elemento, para determinar cuándo debe finalizar el controlador. El parfor y el parwhile son las versiones iterativas de los tipos de controlador paralelo. Ambos tenedores se ejecutan mientras la condición de iteración sea verdadera. Esto proporciona flexibilidad para determinar, en tiempo de ejecución, la cantidad de elementos de proceso que se ejecutarán simultáneamente. En comparación con los lenguajes de flujo de trabajo, los tipos de controladores paralelos representan versiones estructuradas de los constructores de control de flujo de trabajo, ya que pueden anidar otros controladores y también pueden expresar bifurcaciones y uniones fijas y condicionales, presentes en dichos lenguajes. Ejemplo de Proceso Esta sección presenta un ejemplo de una aplicación de búsqueda de números primos que recibe un cierto rango de enteros y devuelve un conjunto de números primos contenidos en este rango. La computación completa se realiza mediante un proceso que utiliza un controlador paralelo para iniciar y despachar varias actividades concurrentes del mismo tipo, con el fin de encontrar números primos. La porción del documento XML que describe los tipos de proceso y actividad se muestra a continuación. <PROCESS_TYPE NAME=FindPrimes> <IN_PARAMETER TYPE=int NAME=min/> <IN_PARAMETER TYPE=int NAME=max/> <IN_PARAMETER TYPE=int NAME=numPrimes/> <IN_PARAMETER TYPE=int NAME=numActs/> <BODY> <PRE_CODE> setPrimes(new RemoteHashSet()); parfor.setMin(getMin()); parfor.setMax(getMax()); parfor.setNumPrimes(getNumPrimes()); parfor.setNumActs(getNumActs()); parfor.setPrimes(getPrimes()); parfor.setCounterBegin(0); parfor.setCounterEnd(getNumActs()-1); </PRE_CODE> <PARFOR NAME=parfor> <IN_PARAMETER TYPE=int NAME=min/> <IN_PARAMETER TYPE=int NAME=max/> <IN_PARAMETER TYPE=int NAME=numPrimes/> <IN_PARAMETER TYPE=int NAME=numActs/> <IN_PARAMETER TYPE=RemoteCollection NAME=primes/> <ITERATE> <PRE_CODE> int range= (getMax()-getMin()+1)/getNumActs(); int minNum = range*getCounter()+getMin(); int maxNum = minNum+range-1; if (getCounter() == getNumActs()-1) maxNum = getMax(); findPrimes.setMin(minNum); findPrimes.setMax(maxNum); findPrimes.setNumPrimes(getNumPrimes()); findPrimes.setPrimes(getPrimes()); </PRE_CODE> <ACTIVITY TYPE=FindPrimes NAME=findPrimes/> </ITERATE> </PARFOR> </BODY> <OUT_PARAMETER TYPE=RemoteCollection NAME=primes/> </PROCESS_TYPE> Middleware para Grid Computing 88 <ACTIVITY_TYPE NAME=FindPrimes> <IN_PARAMETER TYPE=int NAME=min/> <IN_PARAMETER TYPE=int NAME=max/> <IN_PARAMETER TYPE=int NAME=numPrimes/> <IN_PARAMETER TYPE=RemoteCollection NAME=primes/> <CODE> for (int num=getMin(); num<=getMax(); num++) { // stop, required number of primes was found if (primes.size() >= getNumPrimes()) break; boolean prime = true; for (int i=2; i<num; i++) { if (num % i == 0) { prime = false; break; } } if (prime) { primes.add(new Integer(num)); } } </CODE> </ACTIVITY_TYPE> En primer lugar, se define un tipo de proceso que encuentra números primos, llamado FindPrimes. Recibe, a través de sus parámetros de entrada, un rango de enteros en el que se deben encontrar números primos, la cantidad de primos a devolver y la cantidad de actividades a ejecutar para realizar este trabajo. Al final, los números primos encontrados se devuelven como una colección a través de su parámetro de salida. Este proceso contiene un controlador PARFOR con el objetivo de ejecutar un número determinado de actividades en paralelo. Itera desde 0 hasta getNumActs() - 1, lo cual determina el número de actividades, iniciando una actividad en paralelo en cada iteración. En tal caso, el controlador divide todo el rango de números en subrangos del mismo tamaño y, en cada iteración, inicia una actividad en paralelo que encuentra números primos en un subrango específico. Estas actividades reciben un objeto compartido por referencia para almacenar los números primos recién encontrados y controlar si se ha alcanzado el número requerido de primos. Finalmente, se define el tipo de actividad, FindPrimes, utilizado para encontrar números primos en cada subrango. Recibe, a través de sus parámetros de entrada, el rango de números en el que debe encontrar números primos, el número total de números primos que se deben encontrar en todo el proceso y, pasado por referencia, un objeto de colección para almacenar los números primos encontrados. Entre sus marcadores de CODE, hay un código simple para encontrar números primos, que itera sobre el rango especificado y verifica si el entero actual es un número primo. Además, en cada iteración, el código verifica si se ha alcanzado el número requerido de números primos, insertados en la colección de primos por todas las actividades concurrentes, y sale si es verdadero. La ventaja de utilizar controladores es la posibilidad de que la infraestructura de soporte determine el punto de ejecución en el que se encuentra el proceso, permitiendo la recuperación y monitorización automáticas, así como la capacidad de instanciar y despachar elementos del proceso solo cuando haya suficientes recursos informáticos disponibles, reduciendo la sobrecarga innecesaria. Además, debido a su naturaleza estructurada, pueden ser fácilmente compuestos y la infraestructura de soporte puede aprovechar esto para distribuir jerárquicamente los controladores anidados al Grupo Servidor Grupo Máquina Virtual Java RMI JDBC Gerente de Grupo Servidor Proceso Máquina Virtual Java RMI JDBC Coordinador de Procesos Trabajador Máquina Virtual Java RMI Administrador de Actividades Repositorio Figura 2: Arquitectura de infraestructura en diferentes máquinas sobre la red, permitiendo una mayor escalabilidad y tolerancia a fallos. La infraestructura de soporte comprende herramientas para la especificación y servicios para la ejecución y monitoreo de procesos estructurados en entornos de grid altamente distribuidos, heterogéneos y autónomos. Tiene servicios para monitorear la disponibilidad de recursos en la red, interpretar procesos y programar actividades y controladores, y ejecutar actividades. 3.1 Arquitectura de Infraestructura La arquitectura de infraestructura de soporte está compuesta por grupos de máquinas y repositorios de datos, que preservan su autonomía administrativa. Generalmente, las máquinas y repositorios localizados, como en redes locales o grupos de servidores, forman un grupo. Cada máquina en un grupo debe tener una Máquina Virtual de Java (JVM) [11], y una Biblioteca de Tiempo de Ejecución de Java, además de una combinación de los siguientes servicios de soporte de cuadrícula: gestor de grupo (GM), coordinador de procesos (PC) y gestor de actividades (AM). Esta combinación determina qué tipo de nodo de grupo representa: un servidor de grupo, un servidor de procesos o simplemente un trabajador (ver Figura 2). En un grupo hay uno o más administradores de grupo, pero solo uno actúa como principal y los demás como réplicas. Ellos son responsables de mantener la información de disponibilidad de las máquinas del grupo. Además, los gerentes de grupo mantienen referencias a los recursos de datos del grupo. Utilizan repositorios de grupo para persistir y recuperar la ubicación de los nodos y su disponibilidad. Para controlar la ejecución del proceso, hay uno o más coordinadores de procesos por grupo. Son responsables de instanciar y ejecutar procesos y controladores, seleccionar recursos, y programar y despachar actividades a los trabajadores. Para persistir y recuperar la ejecución de procesos y datos, y también cargar la especificación de procesos, utilizan repositorios de grupo. Finalmente, en varios nodos de grupo hay un administrador de actividades. Es responsable de ejecutar actividades en la máquina alojada en nombre de los coordinadores de procesos de grupo, e informar sobre la disponibilidad actual de la máquina asociada a los gerentes de grupo. También tienen colas de actividad pendientes, que contienen actividades por ejecutar. 3.2 Relaciones entre grupos Para modelar una arquitectura de red real, la infraestructura debe incluir varias, potencialmente todas, las redes locales, como lo hace Internet. Con el objetivo de satisfacer esta intención, los grupos locales están conectados a otros, directa o indirectamente, a través de sus gerentes de grupo (ver Figura 3). Cada gerente de grupo se encarga de las solicitudes de su grupo (representado por elipses punteadas), con el fin de registrar las máquinas locales y mantener la disponibilidad correspondiente. Además, los gerentes de grupo se comunican con los gerentes de grupo de otros grupos. Cada gerente de grupo exporta información de disponibilidad general a los gerentes de grupos adyacentes y también recibe solicitudes de otros servicios externos para proporcionar información detallada de disponibilidad. De esta manera, si hay recursos disponibles en grupos externos, es posible enviar procesos, controladores y actividades a estos grupos para ejecutarlos en coordinadores de procesos externos y gestores de actividades, respectivamente. 4. EJECUCIÓN DEL PROCESO En la arquitectura de cuadrícula propuesta, un proceso se especifica en XML, utilizando controladores para determinar el flujo de control; haciendo referencia a otros procesos y actividades; y pasando objetos a sus parámetros para definir el flujo de datos. Después de especificado, el proceso se compila en un conjunto de clases, que representan tipos específicos de proceso, actividad y controlador. En este momento, puede ser instanciado y ejecutado por un coordinador de procesos. Modelo dinámico Para ejecutar un proceso especificado, debe ser instanciado haciendo referencia a su tipo en un servicio de coordinador de procesos de un grupo específico. Además, los parámetros iniciales deben ser pasados a él, y luego puede ser iniciado. El coordinador del proceso lleva a cabo el proceso ejecutando los elementos del proceso incluidos en su cuerpo de forma secuencial. Si el elemento es un proceso o un controlador, el coordinador de procesos puede optar por ejecutarlo en la misma máquina o pasarlo a otro coordinador de procesos en una máquina remota, si está disponible. De lo contrario, si el elemento es una actividad, se pasa a un gestor de actividades de una máquina disponible. Los coordinadores del proceso solicitan al gerente del grupo local encontrar máquinas disponibles que contengan el servicio, coordinador del proceso o gerente de actividad requeridos, para ejecutar un elemento del proceso. Entonces, puede devolver una máquina local, una máquina en otro grupo o ninguna, dependiendo de la disponibilidad de dicho recurso en la red. Devuelve un trabajador externo (máquina gestora de actividades) si no hay trabajadores disponibles en el grupo local; y devuelve un servidor de procesos externo (máquina coordinadora de procesos) si no hay servidores de procesos o trabajadores disponibles en el grupo local. Siguiendo esta regla, los gerentes de grupo intentan encontrar servidores de procesos en el mismo grupo de trabajadores disponibles. Dicho procedimiento es seguido de forma recursiva por todos los procesos coGM FindPrimes Actividad AM FindPrimes Actividad AM FindPrimes Actividad AM FindPrimes Proceso PC Figura 4: ejecución del proceso FindPrimes por coordinadores que ejecutan subprocesos o controladores de un proceso. Por lo tanto, dado que los procesos están estructurados por la anidación de elementos de proceso, la ejecución del proceso se distribuye automáticamente de forma jerárquica a través de uno o más grupos de cuadrícula según la disponibilidad y la localidad de los recursos informáticos. La ventaja de este modelo de distribución es la ejecución en áreas extensas, que aprovecha potencialmente todos los recursos de la red; y la comunicación localizada de los elementos del proceso, ya que los elementos fuertemente dependientes, que están bajo el mismo controlador, se colocan en los mismos grupos o cerca de ellos. Además, admite un monitoreo y control sencillos, gracias a sus controladores estructurados, que mantienen el estado y el control sobre sus elementos internos. Ejemplo de ejecución del proceso 4.2 Retomando el ejemplo mostrado en la Sección 2.2, se especifica un tipo de proceso para encontrar números primos en un rango determinado de números. Para resolver este problema, se crean una serie de actividades utilizando el controlador parfor. Cada actividad, entonces, encuentra números primos en una parte determinada del rango de números. La Figura 4 muestra una instancia de este tipo de proceso ejecutándose sobre la infraestructura propuesta. Se crea una instancia del proceso FindPrimes en un coordinador de procesos disponible (PC), que comienza a ejecutar el controlador parfor. En cada iteración de este controlador, el coordinador del proceso solicita al gerente del grupo (GM) un administrador de actividades disponible (AM) para ejecutar una nueva instancia de la actividad FindPrimes. Si hay algún Administrador de Actividades (AM) disponible en este grupo o en uno externo, el coordinador del proceso envía la clase de actividad y los parámetros iniciales a este administrador de actividades y solicita su ejecución. De lo contrario, si no hay un administrador de actividad disponible, entonces el controlador entra en un estado de espera hasta que un administrador de actividad esté disponible o sea creado. De manera paralela, cada vez que una actividad finaliza, su resultado se envía de vuelta al coordinador del proceso, quien lo registra en el controlador de parfor. Entonces, el controlador espera hasta que todas las actividades que se han iniciado hayan terminado, y finaliza. En este punto, el coordinador del proceso verifica que no hay otro elemento de proceso para ejecutar y finaliza el proceso. 5. TRABAJO RELACIONADO Existen varios productos académicos y comerciales que prometen apoyar la computación en malla, con el objetivo de proporcionar interfaces, protocolos y servicios para aprovechar el uso de recursos distribuidos ampliamente en redes heterogéneas y autónomas. Entre ellos, Globus [6], Condor-G [9] y Legion [10] son ampliamente conocidos. Con el objetivo de estandarizar interfaces y servicios para la red, se ha definido la Arquitectura de Servicios de Red Abierta (OGSA). Las arquitecturas de rejilla generalmente tienen servicios que gestionan los recursos informáticos y distribuyen la ejecución de tareas independientes en los disponibles. Sin embargo, las arquitecturas emergentes mantienen las dependencias de tareas y ejecutan automáticamente las tareas en el orden correcto. Aprovechan estas dependencias para proporcionar recuperación automática, y mejores algoritmos de distribución y programación. Siguiendo dicho modelo, WebFlow [1] es una herramienta de especificación de procesos y entorno de ejecución construido sobre CORBA que permite la composición gráfica de actividades y su ejecución distribuida en un entorno de grid. Opera-G [3], al igual que WebFlow, utiliza un lenguaje de especificación de procesos similar al diagrama de flujo de datos y a los lenguajes de flujo de trabajo, pero proporciona recuperación automática de la ejecución y un control limitado de la ejecución del proceso. Las arquitecturas previamente mencionadas y otras que llevan a cabo procesos sobre la red tienen una coordinación centralizada. Para superar esta limitación, sistemas como SwinDew [13] propusieron una ejecución de procesos ampliamente distribuida, en la que cada nodo sabe dónde ejecutar la siguiente actividad o unirse a actividades en un entorno de pares. En el área específica de distribución y programación de actividades, enfatizada en este trabajo, GridFlow [5] es notable. Utiliza una programación de dos niveles: global y local. A nivel local, cuenta con servicios que predicen la utilización de recursos informáticos y la duración de la actividad. Basándose en esta información, GridFlow emplea una técnica similar a PERT que intenta prever la hora de inicio y duración de la ejecución de las actividades para programarlas mejor con los recursos disponibles. La arquitectura propuesta en este documento, que abarca un modelo de programación y una infraestructura de soporte de ejecución, es ampliamente descentralizada, a diferencia de WebFlow y Opera-G, siendo más escalable y tolerante a fallos. Pero, al igual que este último, está diseñado para respaldar la recuperación de la ejecución. En comparación con SwinDew, la arquitectura propuesta contiene coordinadores de procesos ampliamente distribuidos, los cuales coordinan procesos o partes de ellos de manera diferente a SwinDew, donde cada nodo tiene una vista limitada del proceso: solo la actividad que comienza a continuación. Esto facilita la supervisión y el control de los procesos. Finalmente, la infraestructura de soporte interrumpe el proceso y sus subprocesos para la ejecución en red, permitiendo a un grupo requerir a otro grupo para la coordinación y ejecución de elementos del proceso en nombre del primero. Esto es diferente de GridFlow, que puede ejecutar un proceso en un máximo de dos niveles, teniendo el nivel global como el único responsable de programar subprocesos en otros grupos. Esto puede limitar el rendimiento general de los procesos y hacer que el sistema sea menos escalable. CONCLUSIÓN Y TRABAJO FUTURO La computación en malla es un campo de investigación emergente que tiene como objetivo promover la computación distribuida y paralela en la amplia red de área de heterogéneos y autónomos dominios administrativos de manera transparente, similar a lo que Internet hace con el intercambio de datos. Hay varios productos que admiten la ejecución de tareas independientes en una cuadrícula, pero solo unos pocos admiten la ejecución de procesos con tareas interdependientes. Para abordar dicho tema, este documento propone un modelo de programación y una infraestructura de soporte que permiten la ejecución de procesos estructurados de manera ampliamente distribuida y jerárquica. Esta infraestructura de soporte proporciona distribución automática, estructurada y recursiva de elementos de proceso sobre grupos de máquinas disponibles; mejor uso de recursos, debido a su creación bajo demanda de elementos de proceso; monitoreo y control de procesos fácil, debido a su naturaleza estructurada; y comunicación localizada entre elementos de proceso fuertemente dependientes, que se colocan bajo el mismo controlador. Estas características contribuyen a una mejor escalabilidad, tolerancia a fallos y control para la ejecución de procesos en la red. Además, abre puertas para mejores algoritmos de programación, mecanismos de recuperación y también esquemas de modificación dinámica. El próximo trabajo será la implementación de un mecanismo de recuperación que utiliza la ejecución y el estado de los datos de los procesos y controladores para recuperar la ejecución del proceso. Después de eso, es deseable avanzar en el algoritmo de programación para prever el uso de máquinas en los mismos u otros grupos y anticipar el tiempo de inicio de los elementos del proceso, con el fin de utilizar esta información para preasignar recursos y, así, obtener un mejor rendimiento en la ejecución del proceso. Finalmente, es interesante investigar esquemas de modificación dinámica de procesos en la red, con el fin de evolucionar y adaptar procesos a largo plazo al entorno de la red en constante cambio. 7. AGRADECIMIENTOS Nos gustaría agradecer a Paulo C. Oliveira, del Departamento del Tesoro del Estado de Sao Paulo, por su profunda revisión y comentarios perspicaces. 8. REFERENCIAS [1] E. Akarsu, G. C. Fox, W. Furmanski y T. Haupt. WebFlow: Entorno de programación de alto nivel y conjunto de herramientas de autoría visual para cómputo distribuido de alto rendimiento. En Actas de Supercomputación (SC98), 1998. [2] T. Andrews y F. Curbera. Especificación: Lenguaje de Ejecución de Procesos de Negocio para Servicios Web Versión 1.1. IBM DeveloperWorks, 2003. 

IBM DeveloperWorks, 2003. Disponible en http://www-106.ibm.com/developerworks/library/wsbpel. [3] W. Bausch. O PERA-G: Un microkernel para rejillas computacionales. Tesis doctoral, Instituto Federal Suizo de Tecnología, Zúrich, 2004. [4] T. Bray y J. Paoli. Lenguaje de Marcado Extensible (XML) 1.0. Grupo de Trabajo del Núcleo de XML, W3C, 2004. Disponible en http://www.w3.org/TR/2004/REC-xml-20040204. [5] J. Cao, S. A. Jarvis, S. Saini y G. R. Nudd. GridFlow: Gestión de flujo de trabajo para la computación en malla. En Actas del Simposio Internacional sobre Computación en Grupo y la Red (CCGrid 2003), 2003. [6] I. Foster y C. Kesselman. Globus: Un conjunto de herramientas de infraestructura de metacomputación. Rev. Int. Aplicaciones de Supercomputadoras, 11(2):115-128, 1997. [7] I. Foster, C. Kesselman, J. M. Nick y S. Tuecke. La Fisiología de la Red: Una Arquitectura de Servicios de Red Abierta para la Integración de Sistemas Distribuidos. 91 Middleware 2004 Companion Open Grid Service Infrastructure WG, Global Grid Forum, 2002. [8] I. Foster, C. Kesselman y S. Tuecke. La Anatomía de la Red: Permitiendo la Organización Virtual Escalable. La Revista Internacional de Aplicaciones de Computación de Alto Rendimiento, 15(3):200-222, 2001. [9] J. Frey, T. Tannenbaum, M. Livny, I. Foster y S. Tuecke. Condor-G: Un agente de gestión computacional para redes multi-institucionales. En Actas del Décimo Simposio Internacional sobre Computación Distribuida de Alto Rendimiento (HPDC-10). IEEE, 2001. [10] A. S. Grimshaw y W. A. Wulf. Legión - Una vista desde 50,000 pies. En Actas del Quinto Congreso Internacional. Simposio sobre Computación Distribuida de Alto Rendimiento. IEEE, 1996. [11] T. Lindholm y F. Yellin. La Especificación de la Máquina Virtual de Java. Sun Microsystems, edición Segunda Edición, 1999. [12] B. R. Schulze y E. R. M. Madeira. Computación en malla con servicios activos. Revista Concurrency and Computation: Practice and Experience, 5(16):535-542, 2004. [13] J. Yan, Y. Yang y G. K. Raikundalia. Implementando procesos de negocios en un entorno descentralizado con soporte de flujo de trabajo basado en P2P. En Actas de la Cuarta Conferencia Internacional sobre Gestión de Información de la Era Web (WAIM 2003), 2003. Middleware para Computación en Red 92