En este artículo presentamos el Control Basado en Dinámicas (DBC), un enfoque para la planificación y control de un agente en entornos estocásticos. A diferencia de los enfoques existentes, que buscan optimizar las recompensas esperadas (por ejemplo, en Problemas de Decisión de Markov Parcialmente Observables (POMDP)), DBC optimiza el comportamiento del sistema hacia las dinámicas del sistema especificadas. Mostramos que un enfoque de planificación y control recientemente desarrollado, Seguimiento Extendido de Markov (EMT), es una instancia de DBC. EMT emplea la selección de acciones codiciosa para proporcionar un algoritmo de control eficiente en entornos markovianos. Explotamos esta eficiencia en un conjunto de experimentos que aplicaron EMT multitarget a una clase de problemas de barrido de área (búsqueda de objetivos en movimiento). Mostramos que tales problemas pueden ser definidos de forma natural y resueltos eficientemente utilizando el marco de DBC y su instanciación de EMT. Categorías y Descriptores de Asignaturas I.2.8 [Resolución de Problemas, Métodos de Control y Búsqueda]: Teoría de Control; I.2.9 [Robótica]; I.2.11 [Inteligencia Artificial Distribuida]: Agentes Inteligentes Términos Generales Algoritmos, Teoría 1. INTRODUCCIÓN La planificación y el control constituyen un área de investigación central en sistemas multiagentes e inteligencia artificial. En los últimos años, los Procesos de Decisión de Markov Parcialmente Observables (POMDPs) [12] se han convertido en una base formal popular para la planificación en entornos estocásticos. En este marco, el problema de planificación y control suele abordarse imponiendo una función de recompensa y calculando una política (de elección de acciones) que sea óptima, en el sentido de que resultará en la mayor utilidad esperada. Si bien teóricamente atractivo, la complejidad de resolver óptimamente un POMDP es prohibitiva [8, 7]. Tomamos una visión alternativa de la planificación en entornos estocásticos. No utilizamos una función de recompensa basada en el estado, sino que optimizamos según un criterio diferente, una especificación basada en transiciones de la dinámica del sistema deseado. La idea aquí es ver la ejecución del plan como un proceso que obliga a un sistema (estocástico) a cambiar, y un plan como un proceso dinámico que moldea ese cambio de acuerdo con criterios deseados. Llamamos a este marco de planificación general Control Basado en Dinámicas (DBC). En DBC, el objetivo de un proceso de planificación (o control) es garantizar que el sistema cambiará de acuerdo con dinámicas objetivo específicas (potencialmente estocásticas). Dado que el comportamiento real del sistema puede desviarse de lo especificado por la dinámica objetivo (debido a la naturaleza estocástica del sistema), la planificación en tales entornos debe ser continua, de manera similar a los controladores en lazo cerrado clásicos. Aquí, la optimalidad se mide en términos de la probabilidad de magnitudes de desviación. En este documento, presentamos la estructura del Control Basado en Dinámicas. Mostramos que el enfoque de Seguimiento Extendido de Markov (EMT) recientemente desarrollado [13, 14, 15] está subsumido por DBC, con EMT empleando selección de acciones codiciosa, que es una parametrización específica entre las opciones posibles dentro de DBC. EMT es una implementación eficiente de DBC. Para evaluar DBC, llevamos a cabo un conjunto de experimentos aplicando EMT de múltiples objetivos al Juego de Etiquetas [11]; esta es una variante del problema de barrido de área, donde un agente intenta etiquetar a un objetivo móvil (presa) cuya posición no se conoce con certeza. Los datos experimentales demuestran que incluso con un modelo simple del entorno y un diseño simple de la dinámica del objetivo, se pueden lograr altas tasas de éxito tanto en la captura de la presa como en sorprender a la presa (como se expresa por la entropía observada de la posición de los agentes controlados). El documento está organizado de la siguiente manera. En la Sección 2 motivamos DBC utilizando problemas de barrido de área, y discutimos trabajos relacionados. La Sección 3 introduce la estructura de Control Basado en Dinámicas (DBC) y su especialización en entornos Markovianos. Esto es seguido por una revisión del enfoque de Seguimiento Markov Extendido (EMT) como un régimen de control estructurado por DBC en la Sección 4. Esa sección también discute las limitaciones del control basado en EMT en relación con el marco general de DBC. Los ajustes experimentales y los resultados se presentan a continuación en la Sección 5. La Sección 6 proporciona una breve discusión del enfoque general, y la Sección 7 ofrece algunas observaciones finales y direcciones para trabajos futuros. 790 978-81-904262-7-5 (RPS) c 2007 IFAAMAS 2. MOTIVACIÓN Y TRABAJO RELACIONADO Muchos escenarios de la vida real tienen naturalmente una especificación de dinámica de destino estocástica, especialmente en aquellos dominios donde no existe un objetivo final, sino más bien un comportamiento del sistema (con propiedades específicas) que debe ser continuamente respaldado. Por ejemplo, los guardias de seguridad realizan barridos persistentes de un área para detectar cualquier signo de intrusión. Los ladrones astutos intentarán rastrear estos barridos y sincronizar su operación con puntos clave del movimiento de los guardias. Por lo tanto, es recomendable que la dinámica de movimiento de los guardias parezca irregular y aleatoria. El trabajo reciente de Paruchuri et al. [10] ha abordado dicha aleatorización en el contexto de POMDPs de agente único y distribuidos. El objetivo de ese trabajo era generar políticas que proporcionen una medida de aleatorización en la selección de acciones, manteniendo al mismo tiempo las recompensas dentro de niveles aceptables. Nuestro enfoque difiere de este trabajo en que DBC no optimiza recompensas esperadas, de hecho, no consideramos recompensas en absoluto, sino que en su lugar mantenemos dinámicas deseadas (incluyendo, pero no limitado a, la aleatorización). El juego de la mancha es otro ejemplo de la aplicabilidad del enfoque. Fue introducido en el trabajo de Pineau et al. [11]. Hay dos agentes que pueden moverse por un área, la cual está dividida en una cuadrícula. La cuadrícula puede tener celdas bloqueadas (agujeros) en las cuales ningún agente puede moverse. Un agente (el cazador) intenta moverse a una celda ocupada por el otro (la presa), de modo que estén ubicados en el mismo lugar (esto es una etiqueta exitosa). La presa busca evitar al cazador, siempre está al tanto de la posición del cazador, pero no sabe cómo se comportará el cazador, lo que abre la posibilidad de que un cazador sorprenda a la presa. El cazador conoce la ley de movimiento probabilístico de la presa, pero no conoce su ubicación actual. Tag es una instancia de una familia de problemas de barrido de área (persecución-evasión). En [11], el cazador modeló el problema utilizando un POMDP. Se definió una función de recompensa para reflejar el deseo de marcar la presa, y se calculó una política de acción para optimizar la recompensa recolectada con el tiempo. Debido a la complejidad intratable de determinar la política óptima, la política de acción calculada en ese documento fue esencialmente una aproximación. En este artículo, en lugar de formular una función de recompensa, utilizamos EMT para resolver el problema, especificando directamente la dinámica objetivo. De hecho, cualquier problema de búsqueda con movimiento aleatorio, la llamada clase de problemas de barrido de área, puede ser descrito a través de la especificación de la dinámica de dicho sistema objetivo. El Control Basado en Dinámicas proporciona un enfoque natural para resolver estos problemas. CONTROL BASADO EN DINÁMICA La especificación del Control Basado en Dinámica (DBC) se puede dividir en tres niveles interactivos: Nivel de Diseño del Entorno, Nivel de Usuario y Nivel de Agente. • El Nivel de Diseño del Entorno se ocupa de la especificación formal y modelado del entorno. Por ejemplo, este nivel especificaría las leyes de la física dentro del sistema y establecería sus parámetros, como la constante de gravitación. El Nivel de Usuario, a su vez, se basa en el modelo de entorno producido por el Diseño de Entorno para especificar la dinámica del sistema objetivo que desea observar. El Nivel de Usuario también especifica el procedimiento de estimación o aprendizaje para la dinámica del sistema, y la medida de desviación. En el escenario del guardia de museo mencionado anteriormente, estos corresponderían a un horario de barrido estocástico y una medida de sorpresa relativa entre el barrido especificado y el real. • El Nivel de Agente, a su vez, combina el modelo del entorno del Nivel de Diseño de Entorno, el procedimiento de estimación de la dinámica, la medida de desviación y la especificación de la dinámica objetivo del Nivel de Usuario, para producir una secuencia de acciones que creen dinámicas del sistema lo más cercanas posible a la especificación deseada. Dado nuestro interés en el desarrollo continuo de un sistema estocástico, como ocurre en la teoría de control clásica [16] y la planificación continua [4], así como en nuestro ejemplo de recorridos por museos, la pregunta es cómo el Nivel de Agente debe tratar las mediciones de desviación a lo largo del tiempo. Con este fin, utilizamos un umbral de probabilidad, es decir, nos gustaría que el Nivel del Agente maximice la probabilidad de que la medida de desviación permanezca por debajo de cierto umbral. La selección de acciones específicas depende entonces de la formalización del sistema. Una posibilidad sería crear una mezcla de tendencias de sistemas disponibles, similar a lo que sucede en las arquitecturas robóticas basadas en el comportamiento [1]. La otra alternativa sería depender del procedimiento de estimación proporcionado por el Nivel de Usuario para utilizar el modelo del Nivel de Diseño del Entorno del entorno para elegir acciones, de manera que se manipule al estimador de dinámicas para que crea que se ha logrado cierta dinámica. Ten en cuenta que esta manipulación no es directa, sino a través del entorno. Por lo tanto, para algoritmos de estimación lo suficientemente fuertes, la manipulación exitosa significaría una simulación exitosa de la dinámica objetivo especificada (es decir, más allá de discernir a través de la entrada sensorial disponible). Los niveles de DBC también pueden tener un retroceso de información (ver Figura 1). Por ejemplo, el Nivel de Agente podría proporcionar datos sobre la viabilidad de la dinámica del objetivo, permitiendo al Nivel de Usuario modificar el requisito, quizás centrándose en las características alcanzables del comportamiento del sistema. Los datos también estarían disponibles sobre la respuesta del sistema a diferentes acciones realizadas; combinados con un estimador de dinámica definido por el Nivel de Usuario, esto puede proporcionar una herramienta importante para la calibración del modelo del entorno en el Nivel de Diseño del Entorno. UserEnv. El modelo de agente de diseño Estimador de Dinámicas Ideales Estimador de Dinámicas Factibilidad de Respuesta del Sistema de Datos Figura 1: Flujo de datos del marco de trabajo DBC Ampliando la idea de los algoritmos Actor-Crítico [5], el flujo de datos de DBC puede proporcionar una buena base para el diseño de un algoritmo de aprendizaje. Por ejemplo, el Nivel de Usuario puede funcionar como un dispositivo exploratorio para un algoritmo de aprendizaje, inferir un objetivo dinámico ideal a partir del modelo del entorno en cuestión que expondría y verificaría las características más críticas del comportamiento del sistema. En este caso, los datos de viabilidad y respuesta del sistema a nivel de agente proporcionarían información clave para una actualización del modelo del entorno. De hecho, la combinación de datos de viabilidad y respuesta puede proporcionar una base para la aplicación de algoritmos de aprendizaje sólidos como EM [2, 9]. 3.1 DBC para entornos markovianos Para un entorno markoviano parcialmente observable, DBC puede especificarse de una manera más rigurosa. Observa cómo DBC descarta las recompensas y las reemplaza por otro criterio de optimalidad (las diferencias estructurales se resumen en la Tabla 1): • El nivel de Diseño del Entorno es especificar una tupla < S, A, T, O, Ω, s0 >, donde: - S es el conjunto de todos los posibles estados del entorno; - s0 es el estado inicial del entorno (que también puede ser visto como una distribución de probabilidad sobre S); La Sexta Conferencia Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 791 - A es el conjunto de todas las acciones posibles aplicables en el entorno; - T es la función de transición probabilística del entorno: T: S × A → Π(S). Es decir, T(s | a, s) es la probabilidad de que el entorno se mueva del estado s al estado s bajo la acción a; - O es el conjunto de todas las observaciones posibles. Así es como se vería la entrada del sensor para un observador externo; - Ω es la función de probabilidad de observación: Ω: S × A × S → Π(O). Es decir, Ω(o|s, a, s) es la probabilidad de observar o dado que el entorno ha pasado del estado s al estado s bajo la acción a. • Nivel de Usuario, en el caso de un entorno markoviano, opera en el conjunto de dinámicas del sistema descritas por una familia de probabilidades condicionales F = {τ: S × A → Π(S)}. Por lo tanto, la especificación de la dinámica del objetivo puede expresarse como q ∈ F, y el algoritmo de aprendizaje o seguimiento puede representarse como una función L : O×(A×O)∗ → F; es decir, mapea secuencias de observaciones y acciones realizadas hasta el momento en una estimación τ ∈ F de la dinámica del sistema. Hay muchas variaciones posibles disponibles a nivel de usuario para definir la divergencia entre la dinámica del sistema; varias de ellas son: - Distancia de traza o distancia L1 entre dos distribuciones p y q definida por D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Medida de fidelidad de distancia F(p(·), q(·)) = x p(x)q(x) - Divergencia de Kullback-Leibler DKL(p(·) q(·)) = x p(x) log p(x) q(x) Nótese que las dos últimas no son en realidad métricas sobre el espacio de distribuciones posibles, pero tienen interpretaciones significativas e importantes. Por ejemplo, la divergencia de Kullback-Leibler es una herramienta importante de la teoría de la información [3] que permite medir el costo de codificar una fuente de información gobernada por q, asumiendo que está gobernada por p. El Nivel de Usuario también define el umbral de probabilidad de desviación de la dinámica θ. • El Nivel de Agente se enfrenta entonces a un problema de seleccionar una función de señal de control a∗ para satisfacer un problema de minimización de la siguiente manera: a∗ = arg min a Pr(d(τa, q) > θ) donde d(τa, q) es una variable aleatoria que describe la desviación de la estimación de la dinámica τa, creada por L bajo la señal de control a, de la dinámica ideal q. Implícito en este problema de minimización es que L es manipulado a través del entorno, basado en el modelo de entorno producido por el Nivel de Diseño del Entorno. 3.2 Vista DBC del Espacio de Estados Es importante tener en cuenta la visión complementaria que DBC y POMDPs tienen sobre el espacio de estados del entorno. Los POMDP consideran el estado como una instantánea estacionaria del entorno; cualquier atributo de la secuencia de estados que se busque se alcanza a través de las propiedades del proceso de control, en este caso la acumulación de recompensas. Esto se puede ver como si la secuencia de estados y los atributos de esa secuencia solo fueran introducidos por y para el mecanismo de control, la política POMDP. DBC se enfoca en el principio subyacente de la secuenciación de estados, la dinámica del sistema. La especificación de la dinámica de los DBCs puede utilizar el espacio de estado del entorno como un medio para describir, discernir y preservar los cambios que ocurren dentro del sistema. Como resultado, DBC tiene una mayor capacidad para expresar propiedades de secuenciación de estados, las cuales están fundamentadas en el modelo del entorno y su definición del espacio de estados. Por ejemplo, considera la tarea de moverse a través de terreno accidentado hacia un objetivo y alcanzarlo lo más rápido posible. Los POMDPs codificarían el terreno como puntos en el espacio de estados, mientras que la velocidad se garantizaría mediante una recompensa negativa por cada paso dado sin alcanzar la meta. Acumular una recompensa más alta solo se puede lograr mediante un movimiento más rápido. Alternativamente, el espacio de estados podría incluir directamente la noción de velocidad. Para los POMDPs, esto significaría que el mismo concepto está codificado dos veces, en cierto sentido: directamente en el espacio de estados e indirectamente en la acumulación de recompensas. Ahora, incluso si la función de recompensa codificara más y detalles más finos de las propiedades del movimiento, la solución POMDP tendría que buscar en un espacio mucho más grande de políticas, aunque aún esté guiada por el concepto implícito del procedimiento de acumulación de recompensas. Por otro lado, la expresión del objetivo táctico de variaciones y correlaciones entre la posición y la velocidad del movimiento ahora se basa en la representación del espacio de estados. En esta situación, cualquier restricción adicional, como la suavidad del movimiento, los límites de velocidad en diferentes ubicaciones o las reducciones de velocidad durante giros bruscos, son expresadas explícita y uniformemente por el objetivo táctico, y pueden resultar en una selección de acciones más rápida y efectiva por un algoritmo DBC. 4. El Control Basado en EMT como un DBC Recientemente, se introdujo un algoritmo de control llamado Control Basado en EMT [13], que instancia el marco de trabajo DBC. Aunque proporciona una solución ávida aproximada en el sentido de DBC, los experimentos iniciales utilizando el control basado en EMT han sido alentadores [14, 15]. El control basado en EMT se basa en la definición del entorno markoviano, como en el caso de los POMDP, pero sus Niveles de Usuario y Agente son del tipo de optimalidad DBC markoviana. • El Nivel de Usuario del control basado en EMT define una dinámica del sistema objetivo en un caso limitado independiente de la acción: qEMT: S → Π(S). Luego utiliza la medida de divergencia de Kullback-Leibler para componer un estimador de dinámica de sistemas momentáneos: el algoritmo de Seguimiento Extendido de Markov (EMT). El algoritmo mantiene una estimación de la dinámica del sistema τt EMT que es capaz de explicar el cambio reciente en un estimador de estado auxiliar bayesiano de pt−1 a pt, y lo actualiza de manera conservadora utilizando la divergencia de Kullback-Leibler. Dado que τt EMT y pt−1,t son respectivamente las probabilidades condicionales y marginales sobre el espacio de estados del sistema, la explicación simplemente significa que pt(s) = s τt EMT (s | s)pt−1(s), y la actualización de la estimación de la dinámica se realiza resolviendo un 792 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Tabla 1: Estructura de POMDP vs. Control basado en Dinámicas en Entorno Markoviano Nivel Enfoque MDP Markoviano DBC Entorno < S, A, T, O, Ω >, donde S - conjunto de estados A - conjunto de acciones Diseño T : S × A → Π(S) - transición O - conjunto de observaciones Ω : S × A × S → Π(O) Usuario r : S × A × S → R q : S × A → Π(S) F(π∗ ) → r L(o1, ..., ot) → τ r - función de recompensa q - dinámicas ideales F - remodelación de recompensa L - estimador de dinámicas θ - umbral Agente π∗ = arg max π E[ γt rt] π∗ = arg min π Prob(d(τ q) > θ) problema de minimización: τt EMT = H[pt, pt−1, τt−1 EMT ] = arg min τ DKL(τ × pt−1 τt−1 EMT × pt−1) s.t. pt(s ) = s (τ × pt−1)(s , s) pt−1(s) = s (τ × pt−1)(s , s) • El nivel de Agente en el control basado en EMT es subóptimo con respecto a DBC (aunque permanece dentro del marco de DBC), realizando selección de acciones codiciosas basadas en la predicción de la reacción de EMT. La predicción se basa en el modelo del entorno proporcionado por el nivel de Diseño del Entorno, de modo que si denotamos por Ta la función de transición de entornos limitada a la acción a, y pt−1 es el estimador de estado auxiliar del sistema bayesiano, entonces la elección de control basada en EMT se describe por a∗ = arg min a∈A DKL(H[Ta × pt, pt, τt EMT ] qEMT × pt−1). Nótese que esto sigue precisamente el marco DBC Markoviano: la optimalidad recompensante de los POMDPs se descarta, y en su lugar se manipula un estimador de dinámicas (EMT en este caso) a través de los efectos de la acción en el entorno para producir una estimación cercana a la dinámica del sistema objetivo especificado. Sin embargo, como mencionamos, el control EMT ingenuo es subóptimo en el sentido de DBC y tiene varias limitaciones adicionales que no existen en el marco general de DBC (discutido en la Sección 4.2). 4.1 EMT de múltiples objetivos. En ocasiones, puede haber varias preferencias de comportamiento. Por ejemplo, en el caso de los guardias de museo, algunos objetos de arte están más fuertemente custodiados, lo que requiere que los guardias permanezcan más a menudo en su cercanía. Por otro lado, no se debe dejar ningún rincón del museo sin revisar, lo que requiere un movimiento constante. El éxito de la seguridad en un museo requeriría que los guardias se adhieran y equilibren ambos comportamientos. Para el control basado en EMT, esto significaría enfrentar varios objetivos tácticos {qk}K k=1, y la pregunta sería cómo fusionar y equilibrarlos. Un mecanismo de equilibrio se puede aplicar para resolver este problema. Ten en cuenta que el control basado en EMT, al seleccionar una acción, crea un vector de preferencia sobre el conjunto de acciones basado en su rendimiento predicho con respecto a un objetivo dado. Si estos vectores de preferencia están normalizados, pueden combinarse en una única preferencia unificada. Esto requiere reemplazar la selección de acciones basada en EMT estándar por el algoritmo a continuación [15]: • Dado: - un conjunto de dinámicas objetivo {qk}K k=1, - vector de pesos w(k) • Seleccionar la acción de la siguiente manera - Para cada acción a ∈ A predecir la distribución de estado futuro ¯pa t+1 = Ta ∗ pt; - Para cada acción, calcular Da = H(¯pa t+1, pt, PDt) - Para cada a ∈ A y objetivo táctico qk, denotar V (a, k) = DKL (Da qk) pt. Sea Vk(a) = 1 Zk V (a, k), donde Zk = a∈A V (a, k) es un factor de normalización. - Selecciona a∗ = arg min a k k=1 w(k)Vk(a) El vector de pesos w = (w1, ..., wK ) permite ajustar la importancia entre las dinámicas objetivo sin necesidad de rediseñar los objetivos mismos. Este método de equilibrio también está integrado de forma transparente en el flujo de operación basado en EMT. Limitaciones del Control Basado en EMT El control basado en EMT es una representación subóptima (en el sentido de DBC) de la estructura DBC. Limita al Usuario al obligar a EMT a ser su algoritmo de seguimiento dinámico, y reemplaza la optimización del Agente por la selección de acciones codiciosa. Este tipo de combinación, sin embargo, es común en algoritmos en línea. Aunque es necesario un mayor desarrollo de los controladores basados en EMT, la evidencia hasta ahora sugiere que incluso la forma más simple del algoritmo posee una gran cantidad de potencia y muestra tendencias que son óptimas en el sentido de DBC. Hay dos limitaciones adicionales específicas de los técnicos en emergencias médicas (EMT, por sus siglas en inglés) al control basado en EMT que son evidentes en este momento. Ambos ya tienen soluciones parciales y son objeto de investigación continua. La primera limitación es el problema de la preferencia negativa. En el marco de POMDP, por ejemplo, esto se captura de manera sencilla, a través de The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 793 la aparición de valores con signos diferentes dentro de la estructura de recompensa. Para el control basado en EMT, sin embargo, la preferencia negativa significa que uno quisiera evitar cierta distribución sobre las secuencias de desarrollo del sistema; sin embargo, el control basado en EMT se concentra en acercarse lo más posible a una distribución. La evitación es, por lo tanto, antinatural en el control basado en EMT nativo. La segunda limitación proviene del hecho de que el modelado del entorno estándar puede crear acciones sensoriales puras, acciones que no cambian el estado del mundo y solo difieren en la forma en que se reciben las observaciones y la calidad de las observaciones recibidas. Dado que el estado del mundo no cambia, el control basado en EMT no sería capaz de diferenciar entre diferentes acciones sensoriales. Ten en cuenta que ambas limitaciones del control basado en EMT están ausentes en el marco general de DBC, ya que puede tener un algoritmo de seguimiento capaz de considerar acciones sensoriales puras y, a diferencia de la divergencia de Kullback-Leibler, una medida de desviación de distribución capaz de manejar preferencias negativas. 5. El juego de la "carrera de relevos" fue introducido por primera vez en [11]. Es un problema de agente único de capturar una presa, y pertenece a la clase de problemas de barrido de área. Un ejemplo de dominio se muestra en la Figura 2. 0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11Q A 20 Figura 2: Dominio de etiquetas; un agente (A) intenta buscar y capturar una presa (Q). El juego de etiquetas limita extremadamente la percepción de los agentes, de modo que el agente solo puede detectar la presa si están ubicados en la misma celda del mundo de cuadrícula. En la versión clásica del juego, la co-locación conduce a una observación especial, y la acción de Etiqueta se puede realizar. Modificamos ligeramente esta configuración: el momento en que ambos agentes ocupan la misma celda, el juego termina. Como resultado, tanto el agente como su presa tienen la misma capacidad de movimiento, lo que les permite moverse en cuatro direcciones, Norte, Sur, Este y Oeste. Estos forman un espacio formal de acciones dentro de un entorno markoviano. El espacio de estados del entorno formal markoviano está descrito por el producto cruz de las posiciones del agente y la presa. Para la Figura 2, sería S = {s0, ..., s23} × {s0, ..., s23}. Los efectos de una acción tomada por el agente son deterministas, pero el entorno en general tiene una respuesta estocástica debido al movimiento de la presa. Con probabilidad q0 1 se queda en su lugar, y con probabilidad 1 − q0 se mueve a una celda adyacente más lejos del agente 1. En nuestros experimentos, esto se tomó como q0 = 0.2. Por lo tanto, para la instancia mostrada en la Figura 2 y q0 = 0.1: P(Q = s9|Q = s9, A = s11) = 0.1 P(Q = s2|Q = s9, A = s11) = 0.3 P(Q = s8|Q = s9, A = s11) = 0.3 P(Q = s14|Q = s9, A = s11) = 0.3 Aunque el comportamiento evasivo de la presa es conocido por el agente, la posición de la presa no lo es. La única información sensorial disponible para el agente es su propia ubicación. Utilizamos EMT y especificamos directamente la dinámica del objetivo. Para el juego de la mancha, podemos formular fácilmente tres tendencias principales: atrapar al perseguido, mantenerse en movimiento y acechar al perseguido. Esto resulta en las siguientes tres dinámicas de objetivo: Tcatch(At+1 = si|Qt = sj, At = sa) ∝ 1 si = sj 0 en otro caso Tmobile(At+1 = si|Qt = so, At = sj) ∝ 0 si = sj 1 en otro caso Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1 dist(si,so) Nótese que ninguno de los objetivos anteriores es directamente alcanzable; por ejemplo, si Qt = s9 y At = s11, no hay ninguna acción que pueda llevar al agente a At+1 = s9 como lo requiere la dinámica de objetivo Tcatch. Realizamos varios experimentos para evaluar el rendimiento de EMT en el Juego de Etiquetas. Se utilizaron tres configuraciones del dominio mostrado en la Figura 3, cada una planteando un desafío diferente para el agente debido a la observabilidad parcial. En cada escenario, se realizó un conjunto de 1000 ejecuciones con un límite de tiempo de 100 pasos. En cada ejecución, la posición inicial tanto del agente como de su presa fue seleccionada al azar; esto significa que, en lo que respecta al agente, la posición inicial de la presa estaba distribuida de forma uniforme en todo el espacio de celdas del dominio. También utilizamos dos variaciones de la función de observabilidad del entorno. En la primera versión, la función de observabilidad mapeaba todas las posiciones articulares del cazador y la presa en la posición del cazador como observación. En el segundo, solo aquellas posiciones conjuntas en las que el cazador y la presa ocupaban ubicaciones diferentes fueron mapeadas en la ubicación del cazador. La segunda versión de hecho utilizó y expresó el hecho de que una vez que el cazador y la presa ocupan la misma celda, el juego termina. Los resultados de estos experimentos se muestran en la Tabla 2. Equilibrando la dinámica de captura, movimiento y seguimiento descrita en la sección anterior mediante el vector de peso [0.8, 0.1, 0.1], EMT logró un rendimiento estable en los tres dominios. Aunque las comparaciones directas son difíciles de hacer, el rendimiento de los EMT mostró una notable eficiencia en comparación con el enfoque POMDP. A pesar de una implementación simple e ineficiente del algoritmo EMT en Matlab, el tiempo de decisión para cualquier paso dado promedió significativamente por debajo de 1 segundo en todos los experimentos. Para el dominio de la arena abierta irregular, que resultó ser el más difícil, se completaron 1000 ejecuciones de experimentos limitadas a 100 pasos cada una, un total de 42411 pasos, en poco menos de 6 horas. Es decir, más de 4 × 104 pasos en línea tomaron una orden de magnitud menos tiempo que la computación sin conexión de la política POMDP en [11]. La importancia de esta diferencia se destaca aún más por el hecho de que, en caso de que cambien los parámetros del modelo del entorno, la naturaleza en línea de EMT le permitiría mantener su rendimiento, mientras que la política POMDP necesitaría ser recalculada, lo que requeriría una vez más un gran costo computacional adicional. También probamos la entropía de la frecuencia de las células de comportamiento, medidas empíricas de los datos de prueba. Como muestran la Figura 4 y la Figura 5, empir794 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 3: Se utilizaron estas configuraciones del espacio de juego de Tag: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. Tabla 2: Rendimiento de la solución basada en EMT en tres dominios de juego de Tag y dos modelos de observabilidad: I) cantera de omniposición, II) la cantera no está en la posición de los cazadores. Modelo Dominio Captura% E(Pasos) Tiempo/Paso I Callejones sin salida 100 14.8 72(mSeg) Arena 80.2 42.4 500(mSeg) Círculo 91.4 34.6 187(mSeg) II Callejones sin salida 100 13.2 91(mSeg) Arena 96.8 28.67 396(mSeg) Círculo 94.4 31.63 204(mSeg) La entropía lógica crece con la duración de la interacción. Para las carreras en las que la presa no fue capturada de inmediato, la entropía alcanza valores entre 0.85 y 0.952 para diferentes carreras y escenarios. A medida que el agente busca activamente la presa, la entropía nunca alcanza su máximo. Una característica del gráfico de entropía para el escenario de arena abierta llamó especialmente nuestra atención en el caso del modelo de observación de cantera omniposicional. Cerca del límite máximo de longitud de prueba (100 pasos), la entropía cayó repentinamente. Un análisis adicional de los datos mostró que bajo ciertas circunstancias, se produce un comportamiento fluctuante en el que el agente se enfrenta a versiones igualmente viables del comportamiento de seguir a la presa. Dado que el algoritmo EMT tiene una selección de acciones codiciosa, y el espacio de estados no codifica ninguna forma de compromiso (ni siquiera velocidad o aceleración), el agente queda atrapado dentro de una pequeña porción de celdas. Básicamente, se intenta seguir simultáneamente varios cursos de acción, todos los cuales son consistentes con la dinámica objetivo. Este comportamiento no ocurrió en nuestro segundo modelo de observación, ya que redujo significativamente el conjunto de cursos de acción elegibles, contribuyendo esencialmente a romper el empate entre ellos. 6. DISCUSIÓN El diseño de la solución EMT para el Juego de Etiquetas expone la diferencia fundamental en el enfoque de planificación y control entre EMT o DBC, por un lado, y el enfoque más familiar de POMDP, por otro lado. POMDP define una estructura de recompensa para optimizar e influencia la dinámica del sistema de forma indirecta a través de esa optimización. EMT descarta cualquier esquema de recompensa y, en cambio, mide e influye directamente en la dinámica del sistema. Se calculó la entropía 2 utilizando el logaritmo con base igual al número de ubicaciones posibles dentro del dominio; esto escala adecuadamente la expresión de entropía en el rango [0, 1] para todos los dominios. Por lo tanto, para el Juego de Etiquetas, no buscamos una función de recompensa que codificara y expresara nuestra preferencia sobre el comportamiento de los agentes, sino que establecimos directamente tres preferencias de comportamiento (heurísticas) como base para mantener la dinámica del objetivo. Los datos experimentales muestran que estos objetivos no necesitan ser directamente alcanzables a través de las acciones de los agentes. Sin embargo, aún queda por explorar la relación entre el rendimiento de la EMT y la alcanzabilidad de la dinámica objetivo. Los datos del experimento del juego de etiquetas también revelaron la diferente importancia que DBC y POMDPs otorgan a la formulación del espacio de estados del entorno. Los POMDPs dependen enteramente del mecanismo de maximización de la acumulación de recompensas, es decir, la formación del procedimiento de selección de acciones para lograr la secuenciación de estados necesaria. DBC, por otro lado, tiene dos fuentes de especificación de secuenciación: a través de las propiedades de un procedimiento de selección de acciones y a través de una especificación directa dentro de la dinámica objetivo. La importancia de la segunda fuente fue subrayada por los datos del experimento del Juego de Etiquetas, en el que el algoritmo EMT codicioso, aplicado a una especificación de espacio de estados tipo POMDP, falló, ya que la descripción del objetivo sobre dicho espacio de estados no era capaz de codificar las tendencias de comportamiento necesarias, por ejemplo, la resolución de empates y el compromiso con el movimiento dirigido. Las diferencias estructurales entre DBC (y EMT en particular) y POMDPs impiden la comparación directa de rendimiento y los sitúan en pistas complementarias, cada uno dentro de un nicho adecuado. Por ejemplo, los POMDP podrían ser vistos como una formulación mucho más natural de problemas económicos de toma de decisiones secuenciales, mientras que EMT se ajusta mejor a la demanda continua de cambios estocásticos, como ocurre en muchos problemas de robótica o de agentes incorporados. Las propiedades complementarias de POMDPs y EMT pueden ser aún más explotadas. Existe un interés reciente en utilizar POMDPs en soluciones híbridas [17], en las cuales los POMDPs pueden ser utilizados junto con otros enfoques de control para proporcionar resultados que no son fácilmente alcanzables con ninguno de los enfoques por sí solos. DBC puede ser un socio efectivo en una solución híbrida como esta. Por ejemplo, los POMDP tienen requisitos de tiempo fuera de línea prohibitivamente grandes para el cálculo de políticas, pero pueden ser fácilmente utilizados en entornos más simples para exponer tendencias de comportamiento beneficiosas; esto puede servir como una forma de dinámica objetivo que se proporciona a EMT en un dominio más grande para su operación en línea. 7. CONCLUSIONES Y TRABAJO FUTURO En este artículo, hemos presentado una nueva perspectiva sobre el proceso de planificación y control en entornos estocásticos, en forma del marco de Control Basado en Dinámicas (DBC). DBC formula la tarea de planificar como el apoyo a la dinámica de un sistema objetivo especificado, que describe las propiedades necesarias del cambio dentro del entorno. La optimalidad de los planes de acción de DBC se mide en el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 795 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Calles sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 4: Modelo de Observación I: Cantera omniposicional. Desarrollo de entropía con longitud de juego de etiqueta para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular. 0 10 20 30 40 50 60 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Callejones sin salida 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Arena 0 20 40 60 80 100 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Pasos Entropía Círculo Figura 5: Modelo de observación II: cantera no observada en la posición de los cazadores. Desarrollo de la entropía con la longitud del juego de etiquetas para los tres escenarios experimentales: a) múltiples callejones sin salida, b) arena abierta irregular, c) pasillo circular, con respecto a la desviación de la dinámica del sistema real de la dinámica objetivo. Mostramos que una técnica recientemente desarrollada de Seguimiento Markov Extendido (EMT) [13] es una instancia de DBC. De hecho, EMT puede ser visto como un caso específico de la parametrización DBC, que emplea un procedimiento de selección de acciones codicioso. Dado que EMT exhibe las características clave del marco general DBC, así como una complejidad temporal polinómica, utilizamos la versión multitarget de EMT [15] para demostrar que la clase de problemas de barrido de área se presta naturalmente a descripciones basadas en dinámicas, tal como se ejemplifica en nuestros experimentos en el dominio del Juego de Etiquetas. Como se enumera en la Sección 4.2, EMT tiene varias limitaciones, como la dificultad para manejar la preferencia dinámica negativa. Esto evita la aplicación directa de la EMT a problemas como los Juegos de Rendezvous-Evasión (por ejemplo, [6]). Sin embargo, DBC en general no tiene tales limitaciones y permite fácilmente la formulación de juegos de evasión. En trabajos futuros, tenemos la intención de continuar con el desarrollo de controladores basados en dinámica para estos problemas. 8. AGRADECIMIENTO El trabajo de los dos primeros autores fue parcialmente apoyado por la subvención #898/05 de la Fundación para la Ciencia de Israel, y el tercer autor fue parcialmente apoyado por una subvención del Ministerio de Ciencia y Tecnología de Israel. REFERENCIAS [1] R. C. Arkin. Robótica basada en el comportamiento. MIT Press, 1998. [2] J. 

MIT Press, 1998. [2] J. A. Bilmes. Un tutorial detallado del algoritmo EM y su aplicación en la estimación de parámetros para mezclas gaussianas y modelos ocultos de Markov. Informe técnico TR-97-021, Departamento de Ingeniería Eléctrica e Informática, Universidad de California en Berkeley, 1998. [3] T. M. Cover y J. A. Thomas. Elementos de teoría de la información. Wiley, 1991. [4] M. E. desJardins, E. H. Durfee, C. L. Ortiz y M. J. Wolverton. Una encuesta de investigación en planificación distribuida y continua. Revista de Inteligencia Artificial, 4:13-22, 1999. [5] V. R. Konda y J. N. Tsitsiklis. Algoritmos Actor-Crítico. Revista SIAM de Control y Optimización, 42(4):1143-1166, 2003. [6] W. S. Lim. Un juego de encuentro-evasión en ubicaciones discretas con aleatorización conjunta. Avances en Probabilidad Aplicada, 29(4):1004-1017, diciembre de 1997. [7] M. L. Littman, T. L. Dean y L. P. Kaelbling. Sobre la complejidad de resolver problemas de decisión de Markov. En Actas de la 11ª Conferencia Anual sobre Incertidumbre en Inteligencia Artificial (UAI-95), páginas 394-402, 1995. [8] O. Madani, S. Hanks y A. Condon. Sobre la indecidibilidad de la planificación probabilística y problemas relacionados de optimización estocástica. Revista de Inteligencia Artificial, 147(1-2):5-34, julio de 2003. [9] R. M. Neal y G. E. Hinton. Una vista del algoritmo EM 796 El Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) que justifica enfoques incrementales, dispersos y otras variantes. En M. I. Jordan, editor, Aprendizaje en Modelos Gráficos, páginas 355-368. Kluwer Academic Publishers, 1998. [10] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En Actas de AAMAS 2006, 2006. [11] J. Pineau, G. Gordon y S. Thrun. Iteración de valor basada en puntos: Un algoritmo en cualquier momento para POMDPs. En la Conferencia Conjunta Internacional de Inteligencia Artificial (IJCAI), páginas 1025-1032, agosto de 2003. [12] M. L. Puterman. Procesos de Decisión de Markov. Serie Wiley en Probabilidad y Estadística Matemática: Sección de Probabilidad y Estadística Aplicada. Publicación de Wiley-Interscience, Nueva York, 1994. [13] Z. Rabinovich y J. S. Rosenschein. Seguimiento Markov extendido con una aplicación al control. En el taller sobre seguimiento de agentes: modelando otros agentes a partir de observaciones, en la Tercera Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 95-100, Nueva York, julio de 2004. [14] Z. Rabinovich y J. S. Rosenschein. Coordinación multiagente mediante Seguimiento Markov Extendido. En la Cuarta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 431-438, Utrecht, Países Bajos, julio de 2005. [15] Z. Rabinovich y J. S. Rosenschein. Sobre la respuesta del control basado en EMT a objetivos y modelos interactivos. En la Quinta Conferencia Internacional Conjunta sobre Agentes Autónomos y Sistemas Multiagente, páginas 465-470, Hakodate, Japón, mayo de 2006. [16] R. F. Stengel. Control óptimo y estimación. Dover Publications, 1994. [17] M. Tambe, E. Bowring, H. Jung, G. Kaminka, R. Maheswaran, J. Marecki, J. Modi, R. Nair, J. Pearce, P. Paruchuri, D. Pynadath, P. Scerri, N. Schurr, y P. Varakantham. Conflictos en el trabajo en equipo: Híbridos para el Sexto Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 797