Aprender de los registros de búsqueda en la web para organizar los resultados de búsqueda. Xuanhui Wang, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, xwang20@cs.uiuc.edu. ChengXiang Zhai, Departamento de Ciencias de la Computación, Universidad de Illinois en Urbana-Champaign, Urbana, IL 61801, czhai@cs.uiuc.edu. RESUMEN La organización efectiva de los resultados de búsqueda es fundamental para mejorar la utilidad de cualquier motor de búsqueda. Agrupar los resultados de búsqueda es una forma efectiva de organizarlos, lo que permite a un usuario navegar rápidamente hacia documentos relevantes. Sin embargo, dos deficiencias de este enfoque hacen que no siempre funcione bien: (1) los grupos descubiertos no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios; y (2) las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir a un usuario identificar el grupo correcto. En este artículo, proponemos abordar estas dos deficiencias mediante (1) el aprendizaje de aspectos interesantes de un tema a partir de registros de búsqueda en la web y organizando los resultados de búsqueda en consecuencia; y (2) generando etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Evaluamos nuestro método propuesto en datos de registro de un motor de búsqueda comercial. En comparación con los métodos tradicionales de agrupación de resultados de búsqueda, nuestro método puede proporcionar una mejor organización de resultados y etiquetas más significativas. Categorías y Descriptores de Asignaturas: H.3.3 [Búsqueda y Recuperación de Información]: Agrupamiento, Proceso de búsqueda Términos Generales: Algoritmo, Experimentación 1. La utilidad de un motor de búsqueda se ve afectada por múltiples factores. Si bien el factor principal es la solidez del modelo de recuperación subyacente y la función de clasificación, la forma de organizar y presentar los resultados de búsqueda también es un factor muy importante que puede afectar significativamente la utilidad de un motor de búsqueda. Sin embargo, en comparación con la gran cantidad de literatura sobre modelos de recuperación, hay relativamente poca investigación sobre cómo mejorar la efectividad de la organización de los resultados de búsqueda. La estrategia más común para presentar los resultados de búsqueda es una simple lista clasificada. De manera intuitiva, tal estrategia de presentación es razonable para resultados de búsqueda no ambiguos y homogéneos; en general, funcionaría bien cuando los resultados de búsqueda son buenos y un usuario puede encontrar fácilmente muchos documentos relevantes en los resultados mejor clasificados. Sin embargo, cuando los resultados de la búsqueda son diversos (por ejemplo, debido a la ambigüedad o a múltiples aspectos de un tema), como suele ser el caso en las búsquedas en la web, la presentación de la lista clasificada no sería efectiva; en ese caso, sería mejor agrupar los resultados de la búsqueda en clusters para que un usuario pueda navegar fácilmente hacia un grupo particular de interés. Por ejemplo, los resultados en la primera página devueltos por Google para la consulta ambigua "jaguar" (a partir del 2 de diciembre de 2006) contienen al menos cuatro sentidos diferentes de "jaguar" (es decir, automóvil, animal, software y un equipo deportivo); incluso para una consulta más refinada como "imagen del equipo jaguar", los resultados siguen siendo bastante ambiguos, incluyendo al menos cuatro equipos de jaguares diferentes: un equipo de lucha, un equipo de automóviles jaguar, el equipo de softbol de Southwestern College Jaguar y el equipo de fútbol americano de Jacksonville Jaguar. Además, si un usuario quiere encontrar un lugar para descargar un software de jaguar, una consulta como descargar jaguar tampoco es muy efectiva, ya que los resultados dominantes son sobre descargar el folleto de jaguar, fondos de pantalla de jaguar y DVD de jaguar. En estos ejemplos, una vista de agrupación de los resultados de búsqueda sería mucho más útil para un usuario que una simple lista clasificada. El agrupamiento también es útil cuando los resultados de búsqueda son pobres, en cuyo caso, de lo contrario, un usuario tendría que recorrer una larga lista secuencialmente para llegar al primer documento relevante. Como estrategia alternativa principal para presentar resultados de búsqueda, la agrupación de resultados de búsqueda ha sido estudiada de manera relativamente extensa [9, 15, 26, 27, 28]. La idea general en prácticamente todo el trabajo existente es realizar agrupamientos en un conjunto de resultados de búsqueda de alto rango para dividir los resultados en clústeres naturales, que a menudo corresponden a diferentes subtemas del tema general de la consulta. Se generará una etiqueta para indicar de qué se trata cada grupo. Un usuario puede luego ver las etiquetas para decidir en qué grupo investigar. Tal estrategia ha demostrado ser más útil que la simple presentación de una lista clasificada en varios estudios [8, 9, 26]. Sin embargo, esta estrategia de agrupación tiene dos deficiencias que hacen que no siempre funcione bien: en primer lugar, los grupos descubiertos de esta manera no necesariamente corresponden a los aspectos interesantes de un tema desde la perspectiva de los usuarios. Por ejemplo, los usuarios suelen estar interesados en encontrar códigos telefónicos o códigos postales al ingresar la consulta códigos de área. Pero los grupos descubiertos por los métodos actuales pueden dividir los resultados en códigos locales y códigos internacionales. Tales agrupaciones no serían muy útiles para los usuarios; incluso el mejor grupo seguiría teniendo una baja precisión. Segundo, las etiquetas de los grupos generadas no son lo suficientemente informativas como para permitir que un usuario identifique el grupo correcto. Hay dos razones para este problema: (1) Los grupos no corresponden a los intereses de los usuarios, por lo que sus etiquetas no serían muy significativas o útiles. (2) Incluso si un grupo realmente corresponde a un aspecto interesante del tema, la etiqueta puede no ser informativa porque generalmente se genera en función de los contenidos en un grupo, y es posible que el usuario no esté muy familiarizado con algunos de los términos. Por ejemplo, la consulta ambigua "jaguar" puede referirse a un animal o a un coche. Un grupo puede ser etiquetado como panthera onca. Aunque esta es una etiqueta precisa para un grupo con el sentido animal de jaguar, si un usuario no está familiarizado con la frase, la etiqueta no sería útil. En este artículo, proponemos una estrategia diferente para la partición de los resultados de búsqueda, la cual aborda estas dos deficiencias mediante la imposición de una partición orientada al usuario de los resultados de búsqueda. Es decir, intentamos descubrir qué aspectos de un tema de búsqueda son probablemente interesantes para un usuario y organizar los resultados en consecuencia. Específicamente, proponemos hacer lo siguiente: Primero, aprenderemos aspectos interesantes de temas similares de los registros de búsqueda y organizaremos los resultados de búsqueda basados en estos aspectos interesantes. Por ejemplo, si la consulta actual ha ocurrido muchas veces en los registros de búsqueda, podemos observar qué tipos de páginas ven los usuarios en los resultados y qué tipo de palabras se utilizan junto con dicha consulta. En caso de que la consulta sea ambigua, como por ejemplo "jaguar", podemos esperar ver algunos grupos claros correspondientes a diferentes sentidos de "jaguar". Más importante aún, incluso si una palabra no es ambigua (por ejemplo, coche), aún podemos descubrir aspectos interesantes como el alquiler de coches y la fijación de precios de los coches (que resultaron ser los dos aspectos principales descubiertos en nuestros datos de registro de búsqueda). Tales aspectos pueden ser muy útiles para organizar futuros resultados de búsqueda sobre automóviles. Ten en cuenta que en el caso del automóvil, los grupos generados utilizando agrupamiento regular no necesariamente reflejan aspectos interesantes sobre el automóvil desde la perspectiva de los usuarios, aunque los grupos generados sean coherentes y significativos de otras maneras. Segundo, generaremos etiquetas de clúster más significativas utilizando palabras de consulta anteriores ingresadas por los usuarios. Suponiendo que los registros de búsqueda pasados pueden ayudarnos a aprender qué aspectos específicos son interesantes para los usuarios dada la temática de la consulta actual, también podríamos esperar que las palabras de consulta ingresadas por los usuarios en el pasado que están asociadas con la consulta actual puedan proporcionar descripciones significativas de los aspectos distintos. Por lo tanto, pueden ser etiquetas mejores que las extraídas de los contenidos ordinarios de los resultados de búsqueda. Para implementar las ideas presentadas anteriormente, confiamos en los registros del motor de búsqueda y construimos una colección de historial que contiene las consultas pasadas y los clics asociados. Dada una nueva consulta, encontramos sus consultas pasadas relacionadas en la colección de historial y aprendemos aspectos aplicando el algoritmo de agrupamiento estelar [2] a estas consultas pasadas y clics. Luego podemos organizar los resultados de la búsqueda en estos aspectos utilizando técnicas de categorización y etiquetar cada aspecto con la consulta pasada más representativa en el grupo de consultas. Evaluamos nuestro método de organización de resultados utilizando registros de un motor de búsqueda comercial. Comparamos nuestro método con la clasificación predeterminada del motor de búsqueda y el agrupamiento tradicional de los resultados de búsqueda. Los resultados muestran que nuestro método es efectivo para mejorar la utilidad de búsqueda y las etiquetas generadas utilizando palabras de consultas anteriores son más legibles que aquellas generadas utilizando enfoques de agrupamiento tradicionales. El resto del documento está organizado de la siguiente manera. Primero revisamos el trabajo relacionado en la Sección 2. En la Sección 3, describimos los datos de registro del motor de búsqueda y nuestro procedimiento para construir una colección de historial. En la Sección 4, presentamos nuestro enfoque en detalle. Describimos el conjunto de datos en la Sección 5 y los resultados experimentales se discuten en la Sección 6. Finalmente, concluimos nuestro artículo y discutimos el trabajo futuro en la Sección 7.2. TRABAJO RELACIONADO Nuestro trabajo está estrechamente relacionado con el estudio de la agrupación de resultados de búsqueda. En [9, 15], los autores utilizaron el algoritmo Scatter/Gather para agrupar los documentos principales devueltos por un sistema tradicional de recuperación de información. Sus resultados validan la hipótesis de agrupamiento [20] de que los documentos relevantes tienden a formar grupos. El sistema Grouper fue descrito en [26, 27]. En estos documentos, los autores propusieron agrupar los resultados de un motor de búsqueda real basándose en los fragmentos o el contenido de los documentos devueltos. Varios algoritmos de agrupamiento son comparados y se demostró que el algoritmo de Agrupamiento de Árbol de Sufijos (STC) es el más efectivo. También demostraron que el uso de fragmentos es tan efectivo como el uso de documentos completos. Sin embargo, un desafío importante del agrupamiento de documentos es generar etiquetas significativas para los grupos. Para superar esta dificultad, en [28], se estudiaron algoritmos de aprendizaje supervisado para extraer frases significativas de los fragmentos de resultados de búsqueda y luego se utilizaron estas frases para agrupar los resultados de búsqueda. En [13], los autores propusieron utilizar un algoritmo de agrupamiento monotético, en el cual un documento se asigna a un clúster basado en una única característica, para organizar los resultados de búsqueda, y la única característica se utiliza para etiquetar el clúster correspondiente. La agrupación de resultados de búsqueda también ha atraído mucha atención en la industria y en servicios web comerciales como Vivisimo [22]. Sin embargo, en todos estos trabajos, los grupos se generan únicamente en función de los resultados de la búsqueda. Por lo tanto, los grupos obtenidos no necesariamente reflejan las preferencias de los usuarios y es posible que las etiquetas generadas no sean informativas desde el punto de vista de los usuarios. Los métodos de organización de los resultados de búsqueda basados en la categorización de texto se estudian en [6, 8]. En este trabajo, se entrena un clasificador de texto utilizando un directorio web y luego se clasifican los resultados de búsqueda en las categorías predefinidas. Los autores diseñaron y estudiaron diferentes interfaces de categorías y descubrieron que las interfaces de categorías son más efectivas que las interfaces de listas. Sin embargo, las categorías predefinidas suelen ser demasiado generales para reflejar los aspectos de granularidad más fina de una consulta. Los registros de búsqueda han sido explotados con varios propósitos diferentes en el pasado. Por ejemplo, el agrupamiento de consultas de búsqueda para encontrar las Preguntas Frecuentes (FAQ) se estudia en [24, 4]. Recientemente, los registros de búsqueda se han utilizado para sugerir sustitutos de consultas [12], búsqueda personalizada [19], diseño de sitios web [3], Análisis Semántico Latente [23], y aprendizaje de funciones de clasificación de recuperación [16, 10, 1]. En nuestro trabajo, exploramos el historial de consultas pasadas para organizar mejor los resultados de búsqueda para consultas futuras. Utilizamos el algoritmo de agrupamiento estelar [2], que es un enfoque basado en partición de grafos, para aprender aspectos interesantes de los registros de búsqueda dados una nueva consulta. Por lo tanto, las consultas pasadas se agrupan de una manera específica para la consulta, lo cual es otra diferencia con respecto a trabajos anteriores como [24, 4], en los que todas las consultas en los registros se agrupan de manera batch sin conexión. 3. Los registros del motor de búsqueda registran las actividades de los usuarios web, que reflejan las necesidades o intereses reales de los usuarios al realizar una consulta de ID URL de tiempo 1 win zip http://www.winzip.com xxxx 1 win zip http://www.swinzip.com/winzip xxxx 2 zonas horarias http://www.timeanddate.com xxxx ... ... ... ... Tabla 1: Ejemplos de entradas de registros del motor de búsqueda. Diferentes identificaciones significan diferentes sesiones. Búsqueda web. Generalmente tienen la siguiente información: consultas de texto que los usuarios enviaron, las URL a las que hicieron clic después de enviar las consultas y la hora en que hicieron clic. Los registros del motor de búsqueda están separados por sesiones. Una sesión incluye una sola consulta y todas las URL a las que un usuario hizo clic después de realizar la consulta [24]. Se muestra una pequeña muestra de datos de registro de búsqueda en la Tabla 1. Nuestra idea de utilizar los registros de motores de búsqueda es tratar estos registros como historia pasada, aprender los intereses de los usuarios utilizando estos datos históricos de forma automática y representar sus intereses mediante consultas representativas. Por ejemplo, en los registros de búsqueda, muchas consultas están relacionadas con automóviles y esto refleja que un gran número de usuarios están interesados en información sobre automóviles. Los usuarios probablemente están interesados en diferentes aspectos del automóvil. Algunos buscan alquilar un coche, por lo tanto, pueden enviar una consulta como alquiler de coches; otros están más interesados en comprar un coche usado, y pueden enviar una consulta como coche usado; y otros pueden estar más interesados en comprar un accesorio para coche, por lo que pueden usar una consulta como audio para coche. Al minar todas las consultas relacionadas con el concepto de automóvil, podemos aprender los aspectos que probablemente resulten interesantes desde la perspectiva de los usuarios. Como ejemplo, a continuación se presentan algunos aspectos sobre automóviles aprendidos a partir de nuestros datos de registro de búsqueda (ver Sección 5). 1. alquiler de coches, alquiler de coches Hertz, alquiler de coches Enterprise, ... 2. precios de coches, coches usados, valores de coches, ... 3. accidentes de coches, choques de coches, accidentes de coches, ... 4. audio para coches, estéreo para coches, altavoces para coches, ... Para aprender aspectos de los registros de motores de búsqueda, preprocesamos los registros en bruto para construir una colección de datos históricos. Como se muestra arriba, los registros de motores de búsqueda consisten en sesiones. Cada sesión contiene la información de la consulta de texto y las URL de las páginas web clicadas, junto con la hora en que el usuario realizó los clics. Sin embargo, esta información es limitada ya que las URL por sí solas no son lo suficientemente informativas para determinar con precisión el significado previsto de una consulta enviada. Para recopilar información detallada, enriquecemos cada URL con contenido de texto adicional. Específicamente, dado el query en una sesión, obtenemos sus resultados mejor clasificados utilizando el motor de búsqueda del cual obtuvimos nuestros datos de registro, y extraemos los fragmentos de las URL que son clickeadas de acuerdo a la información de registro en la sesión correspondiente. Todos los títulos, fragmentos y URL de las páginas web clicadas de esa consulta se utilizan para representar la sesión. Las diferentes sesiones pueden contener las mismas consultas. Por lo tanto, el número de sesiones podría ser bastante grande y la información en las sesiones con las mismas consultas podría ser redundante. Para mejorar la escalabilidad y reducir la dispersión de datos, agregamos todas las sesiones que contienen exactamente las mismas consultas juntas. Es decir, para cada consulta única, construimos un pseudo-documento que consiste en todas las descripciones de sus clics en todas las sesiones agregadas. Las palabras clave contenidas en las consultas mismas pueden considerarse como breves resúmenes de los pseudo-documentos. Todos estos pseudo-documentos forman nuestra colección de datos históricos, que se utiliza para aprender aspectos interesantes en la siguiente sección. 4. NUESTRO ENFOQUE Nuestro enfoque consiste en organizar los resultados de búsqueda por aspectos aprendidos de los registros del motor de búsqueda. Dado una consulta de entrada, el procedimiento general de nuestro enfoque es: 1. Obtén su información relacionada de los registros del motor de búsqueda. Toda la información forma un conjunto de trabajo. 2. Aprende aspectos de la información en el conjunto de trabajo. Estos aspectos corresponden a los intereses de los usuarios dados por la consulta de entrada. Cada aspecto está etiquetado con una consulta representativa. 3. Categoriza y organiza los resultados de búsqueda de la consulta de entrada de acuerdo a los aspectos aprendidos anteriormente. Ahora presentamos detalladamente cada paso. 4.1 Encontrar Consultas Pasadas Relacionadas Dada una consulta q, un motor de búsqueda devolverá una lista clasificada de páginas web. Para saber en qué están realmente interesados los usuarios dada esta consulta, primero recuperamos sus consultas similares pasadas en nuestra colección de datos históricos preprocesados. Formalmente, asumimos que tenemos N pseudo-documentos en nuestro conjunto de datos históricos: H = {Q1, Q2, ..., QN }. Cada Qi corresponde a una consulta única y está enriquecido con información de clics como se discute en la Sección 3. Para encontrar consultas relacionadas con qs en H, una forma natural es utilizar un algoritmo de recuperación de texto. Aquí utilizamos el método OKAPI [17], uno de los métodos de recuperación más avanzados. Específicamente, utilizamos la siguiente fórmula para calcular la similitud entre la consulta q y el pseudo-documento Qi:  w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) donde k1 y b son parámetros OKAPI establecidos empíricamente, c(w, Qi) y c(w, q) son el conteo de la palabra w en Qi y q respectivamente, IDF(w) es la frecuencia inversa del documento de la palabra w, y avdl es la longitud promedio del documento en nuestra colección de historial. Basándonos en las puntuaciones de similitud, clasificamos todos los documentos en H. Los documentos mejor clasificados nos proporcionan un conjunto de trabajo para aprender los aspectos en los que los usuarios suelen estar interesados. Cada documento en H corresponde a una consulta pasada, por lo que los documentos mejor clasificados corresponden a consultas pasadas relacionadas. 4.2 Aprendizaje de aspectos mediante agrupamiento Dada una consulta q, usamos Hq = {d1, ..., dn} para representar los pseudo-documentos mejor clasificados de la colección de historial H. Estos pseudo-documentos contienen los aspectos en los que los usuarios están interesados. En esta subsección, proponemos utilizar un método de agrupamiento para descubrir estos aspectos. Cualquier algoritmo de agrupamiento podría aplicarse aquí. En este artículo, utilizamos un algoritmo basado en partición de grafos: el algoritmo de agrupamiento estelar [2]. Una buena propiedad de la agrupación de estrellas en nuestro entorno es que puede sugerir de forma natural una buena etiqueta para cada grupo. Describimos el algoritmo de agrupación de estrellas a continuación. 4.2.1 Agrupación de estrellas Dado Hq, la agrupación de estrellas comienza con la construcción de un grafo de similitud par a par en esta colección basado en el modelo de espacio vectorial en la recuperación de información [18]. Entonces, los grupos se forman por subgrafos densos que tienen forma de estrella. Estos grupos forman una cobertura del grafo de similitud. Formalmente, para cada uno de los n pseudo-documentos {d1, ..., dn} en la colección Hq, calculamos un vector TF-IDF. Entonces, para cada par de documentos di y dj (i = j), su similitud se calcula como la puntuación del coseno de sus vectores correspondientes vi y vj, es decir, sim(di, dj) = cos(vi, vj) = vi · vj |vi| · |vj|. Un grafo de similitud Gσ puede ser construido de la siguiente manera utilizando un parámetro de umbral de similitud σ. Cada documento di es un vértice de Gσ. Si sim(di, dj) > σ, habría un borde conectando los dos vértices correspondientes. Después de construir el grafo de similitud Gσ, el algoritmo de agrupamiento estelar agrupa los documentos utilizando un algoritmo voraz de la siguiente manera: 1. Asocie cada vértice en Gσ con una bandera, inicializada como no marcada. 2. De entre esos vértices no marcados, encuentra aquel que tenga el grado más alto y llámalo u. Marca la bandera de u como centro. 4. Formar un grupo C que contenga a u y a todos sus vecinos que no estén marcados como centro. Marque a todos los vecinos seleccionados como satélites. 5. Repetir desde el paso 2 hasta que todos los vértices en Gσ estén marcados. Cada grupo es en forma de estrella, el cual consiste en un centro único y varios satélites. Solo hay un parámetro σ en el algoritmo de agrupamiento estelar. Un σ grande garantiza que los documentos conectados tengan similitudes altas, por lo que los grupos tienden a ser pequeños. Por otro lado, un σ pequeño hará que los grupos sean grandes y menos coherentes. Estudiaremos el impacto de este parámetro en nuestros experimentos. Una buena característica del algoritmo de agrupamiento de estrellas es que produce un centro para cada grupo. En la colección de consultas pasadas Hq, cada documento corresponde a una consulta. Esta consulta central puede considerarse como la más representativa para todo el grupo, y por lo tanto proporciona naturalmente una etiqueta para el grupo. Todos los grupos obtenidos están relacionados con la consulta de entrada q desde diferentes perspectivas, y representan los posibles aspectos de interés sobre la consulta q de los usuarios. 4.3 Categorización de los Resultados de Búsqueda Para organizar los resultados de búsqueda según los intereses de los usuarios, utilizamos los aspectos aprendidos de las consultas pasadas relacionadas para categorizar los resultados de búsqueda. Dado las primeras m páginas web devueltas por un motor de búsqueda para q: {s1, ..., sm}, las agrupamos en diferentes aspectos utilizando un algoritmo de categorización. En principio, cualquier algoritmo de categorización puede ser utilizado aquí. Aquí utilizamos un método simple basado en el centroide para la categorización. Naturalmente, se espera que métodos más sofisticados como SVM [21] logren un rendimiento aún mejor. Basándonos en los pseudo-documentos de cada aspecto descubierto Ci, construimos un prototipo de centroide pi tomando el promedio de todos los vectores de los documentos en Ci: pi = 1 |Ci|   l∈Ci vl. Todas estas etiquetas se utilizan para categorizar los resultados de la búsqueda. Específicamente, para cualquier resultado de búsqueda sj, construimos un vector TF-IDF. El método basado en el centroide calcula la similitud del coseno entre la representación vectorial de sj y cada prototipo de centroide pi. Luego asignamos sj al aspecto con el que tiene la puntuación de similitud de coseno más alta. Todos los aspectos finalmente se clasifican según el número de resultados de búsqueda que tienen. Dentro de cada aspecto, los resultados de búsqueda se clasifican según su clasificación original en el motor de búsqueda. 5. RECOLECCIÓN DE DATOS Construimos nuestro conjunto de datos basado en el conjunto de datos de registros de búsqueda de MSN publicado por Microsoft Live Labs en 2006 [14]. En total, estos datos de registro abarcan 31 días, desde el 05/01/2006 hasta el 05/31/2006. Hay 8,144,000 consultas, 3,441,000 consultas distintas y 4,649,000 URL distintas en los datos sin procesar. Para probar nuestro algoritmo, dividimos el conjunto de datos completo en dos partes según el tiempo: los primeros 2/3 de los datos se utilizan para simular los datos históricos que acumuló un motor de búsqueda, y usamos el último 1/3 para simular consultas futuras. En la colección de historial, limpiamos los datos manteniendo solo aquellas consultas en inglés frecuentes, bien formateadas (consultas que solo contienen caracteres a, b, ..., z y espacio, y aparecen más de 5 veces). Después de limpiar, obtenemos un total de 169,057 consultas únicas en nuestra colección de datos históricos. En promedio, cada consulta tiene 3.5 clics distintos. Construimos los pseudo-documentos para todas estas consultas como se describe en la Sección 3. La longitud promedio de estos pseudo-documentos es de 68 palabras y el tamaño total de datos de nuestra colección de historias es de 129MB. Construimos nuestros datos de prueba a partir del último tercio de los datos. Según el tiempo, dividimos estos datos en dos conjuntos de prueba de manera equitativa para validación cruzada y ajuste de parámetros. Para cada conjunto de pruebas, utilizamos cada sesión como un caso de prueba. Cada sesión contiene una sola consulta y varios clics. (Tenga en cuenta que no agregamos sesiones para casos de prueba). Los casos de prueba diferentes pueden tener las mismas consultas pero posiblemente diferentes clics. Dado que es inviable pedir al usuario original que envió una consulta que juzgue los resultados de la consulta, seguimos el trabajo [11] y optamos por utilizar los clics asociados con la consulta en una sesión para aproximar los documentos relevantes. Usando clics como juicios, podemos comparar diferentes algoritmos para organizar los resultados de búsqueda y ver qué tan bien estos algoritmos pueden ayudar a los usuarios a llegar a las URL clicadas. Organizar los resultados de búsqueda en diferentes aspectos se espera que ayude a las consultas informativas. Por lo tanto, tiene sentido centrarse en las consultas informativas en nuestra evaluación. Para cada caso de prueba, es decir, cada sesión, contamos el número de clics diferentes y filtramos aquellos casos de prueba con menos de 4 clics bajo la suposición de que una consulta con más clics es más probable que sea una consulta informativa. Dado que queremos probar si nuestro algoritmo puede aprender de las consultas anteriores, también filtramos aquellos casos de prueba cuyas consultas no puedan recuperar al menos 100 pseudo-documentos de nuestra colección histórica. Finalmente, obtenemos 172 y 177 casos de prueba en el primer y segundo conjunto de pruebas respectivamente. En promedio, tenemos 6.23 y 5.89 clics para cada caso de prueba en los dos conjuntos de pruebas respectivamente. EXPERIMENTOS En esta sección, describimos nuestros experimentos sobre la organización de resultados de búsqueda basada en registros pasados del motor de búsqueda. 6.1 Diseño Experimental Utilizamos dos métodos de referencia para evaluar el método propuesto de organización de resultados de búsqueda. Para cada caso de prueba, el primer método es la lista clasificada predeterminada de un motor de búsqueda (referencia). El segundo método es organizar los resultados de la búsqueda agrupándolos por clústeres (basado en clústeres). Para una comparación justa, utilizamos el mismo algoritmo de agrupamiento que en nuestro método basado en registros (es decir, agrupamiento estelar). Es decir, tratamos cada resultado de búsqueda como un documento, construimos el grafo de similitud y encontramos los clusters en forma de estrella. Comparamos nuestro método (basado en logaritmos) con los dos métodos de referencia en los siguientes experimentos. Tanto para los métodos basados en clústeres como para los basados en registros, los resultados de búsqueda dentro de cada clúster se clasifican según su clasificación original dada por el motor de búsqueda. Para comparar diferentes métodos de organización de resultados, adoptamos un método similar al del artículo [9]. Es decir, comparamos la calidad (por ejemplo, precisión) del mejor grupo, que se define como aquel con el mayor número de documentos relevantes. Organizar los resultados de búsqueda en grupos ayuda a los usuarios a navegar rápidamente hacia los documentos relevantes. La métrica anterior es para simular un escenario en el que los usuarios siempre eligen el clúster correcto y lo investigan. Específicamente, descargamos y organizamos los 100 resultados de búsqueda principales en aspectos para cada caso de prueba. Utilizamos la Precisión en 5 documentos (P@5) en el mejor clúster como la medida principal para comparar diferentes métodos. P@5 es una medida muy significativa, ya que nos indica la precisión percibida cuando el usuario abre un grupo y mira los primeros 5 documentos. También utilizamos el Mean Reciprocal Rank (MRR) como otra métrica. MRR se calcula como MRR = 1 |T|   q∈T 1 rq donde T es un conjunto de consultas de prueba, rq es la posición del primer documento relevante para q. Para realizar una comparación justa entre diferentes algoritmos de organización, obligamos tanto a los métodos basados en clústeres como a los basados en registros a producir el mismo número de aspectos y a que cada resultado de búsqueda esté en un único aspecto. El número de aspectos está fijo en 10 en todos los experimentos siguientes. El algoritmo de agrupamiento estelar puede producir un número diferente de grupos para diferentes entradas. Para limitar el número de grupos a 10, ordenamos todos los grupos por su tamaño, y seleccionamos los 10 primeros como candidatos a aspectos. Luego reasignamos cada resultado de búsqueda a uno de estos 10 aspectos seleccionados que tenga la puntuación de similitud más alta con el centroide del aspecto correspondiente. En nuestros experimentos, observamos que los tamaños de los mejores grupos son todos mayores a 5, lo que asegura que P@5 es una métrica significativa. Nuestra hipótesis principal es que organizar los resultados de búsqueda basados en los intereses de los usuarios aprendidos de un conjunto de datos de registro de búsqueda es más beneficioso que organizar los resultados utilizando una simple lista o agrupación de resultados de búsqueda. En lo siguiente, probamos nuestra hipótesis desde dos perspectivas: organización y etiquetado. Tabla 2: Comparación de diferentes métodos por MMR y P@5. También mostramos el porcentaje de mejora relativa en la parte inferior. Conjunto de pruebas de comparación 1 Conjunto de pruebas 2 Aum./Decr. Aumento/Disminución. Cluster/Baseline 53/55 50/64 Log/Baseline 55/44 60/45 Log/Cluster 68/47 69/44 Tabla 3: Comparación de pares con respecto al número de casos de prueba cuyos P@5 mejoran versus disminuyen con respecto al valor base. 6.2.1 Rendimiento general Comparamos tres métodos, clasificación básica de motores de búsqueda (base), método basado en agrupamiento tradicional (basado en cluster) y nuestro método basado en registros (basado en log), en la Tabla 2 utilizando MRR y P@5. Optimizamos el parámetro σs para cada colección de forma individual basándonos en los valores de P@5. Esto muestra el mejor rendimiento que cada método puede lograr. En esta tabla, podemos ver que en ambas colecciones de pruebas, nuestro método es mejor que tanto el método base como los métodos basados en clústeres. Por ejemplo, en la primera colección de pruebas, el método base de MMR es 0.734, el método basado en clusters es 0.773 y nuestro método es 0.783. Obtenemos una precisión más alta que tanto el método basado en clústeres (mejora del 1.27%) como el método de referencia (mejora del 6.62%). Los valores de P@5 son 0.332 para el método base, 0.316 para el método basado en clusters, pero 0.353 para nuestro método. Nuestro método mejora sobre el valor base en un 6.31%, mientras que el método basado en clusters incluso disminuye la precisión. Esto se debe a que el método basado en clusters organiza los resultados de búsqueda únicamente en función de los contenidos. Por lo tanto, podría organizar los resultados de manera diferente a las preferencias de los usuarios. Esto confirma nuestra hipótesis sobre el sesgo del método basado en clusters. Al comparar nuestro método con el método basado en clusters, logramos una mejora significativa en ambas colecciones de pruebas. Los valores p de las pruebas de significancia basadas en P@5 en ambas colecciones son 0.01 y 0.02 respectivamente. Esto demuestra que nuestro método basado en registros es efectivo para aprender las preferencias de los usuarios a partir del historial de consultas pasadas, y por lo tanto puede organizar los resultados de búsqueda de una manera más útil para los usuarios. Mostramos los resultados óptimos arriba. Para probar la sensibilidad del parámetro σ de nuestro método basado en logaritmos, utilizamos uno de los conjuntos de pruebas para ajustar el parámetro de manera óptima y luego usamos el parámetro ajustado en el otro conjunto. Comparamos este resultado (ajustado por registro externo) con los resultados óptimos de ambos métodos basados en clústeres (optimizado por clúster) y basados en registros (optimizado por registro) en la Figura 1. Podemos ver que, como era de esperar, el rendimiento utilizando el parámetro ajustado en un conjunto separado es peor que el rendimiento óptimo. Sin embargo, nuestro método sigue funcionando mucho mejor que los resultados óptimos del método basado en clúster en ambas colecciones de pruebas. 0.27 0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 Conjunto de pruebas 1 Conjunto de pruebas 2 P@5 clúster optimizado registro optimizado registro ajustado externamente Figura 1: Resultados utilizando parámetros ajustados de la otra colección de pruebas. Comparamos con el rendimiento óptimo de los métodos basados en clústeres y en registros. 0 10 20 30 40 50 60 1 2 3 4 Número de bin #Consultas Mejorado Disminuido Figura 2: La correlación entre el cambio de rendimiento y la diversidad de resultados. En la Tabla 3, mostramos las comparaciones de a pares de los tres métodos en términos de la cantidad de casos de prueba para los cuales P@5 aumenta versus disminuye. Podemos ver que nuestro método mejora más casos de prueba en comparación con los otros dos métodos. En la siguiente sección, mostramos un análisis más detallado para ver qué tipos de casos de prueba pueden ser mejorados por nuestro método. 6.2.2 Análisis Detallado Para comprender mejor los casos en los que nuestro método basado en registros puede mejorar la precisión, probamos dos propiedades: diversidad de resultados y dificultad de la consulta. Todo el análisis a continuación está basado en el conjunto de pruebas 1. Análisis de la diversidad: Intuitivamente, organizar los resultados de búsqueda en diferentes aspectos es más beneficioso para aquellas consultas cuyos resultados son más diversos, ya que para dichas consultas, los resultados tienden a formar dos o más grandes grupos. Para probar la hipótesis de que el método basado en registros ayuda más a aquellas consultas con resultados diversos, calculamos las proporciones de tamaño de los dos clusters más grandes en nuestros resultados basados en registros y utilizamos esta proporción como indicador de diversidad. Si la proporción es pequeña, significa que los dos primeros grupos tienen una pequeña diferencia, por lo tanto, los resultados son más diversos. En este caso, esperaríamos que nuestro método ayude más. Los resultados se muestran en la Figura 2. En esta figura, dividimos las proporciones en 4 contenedores. Los 4 contenedores corresponden a los rangos de proporción [1, 2), [2, 3), [3, 4), y [4, +∞) respectivamente. ([i, j) significa que i ≤ proporción < j.) En cada contenedor, contamos el número de casos de prueba cuyos P@5 han mejorado en comparación con la línea base de clasificación, y los representamos en esta figura. Podemos observar que cuando la proporción es menor, el método basado en logaritmos puede mejorar más casos de prueba. Pero cuando el número de contenedores es grande, el método basado en logaritmos no puede mejorar sobre el valor base. Por ejemplo, en el contenedor 1, se mejoran 48 casos de prueba y 34 disminuyen. Pero en el contenedor 4, todos los 4 casos de prueba han disminuido. Esto confirma nuestra hipótesis de que nuestro método puede ser más útil si la consulta arroja resultados más diversos. Esto también sugiere que deberíamos desactivar la opción de reorganizar los resultados de búsqueda si los resultados no son muy diversos (por ejemplo, como lo indica la proporción del tamaño del grupo). Análisis de la dificultad: Las consultas difíciles han sido estudiadas en los últimos años [7, 25, 5]. Aquí analizamos la efectividad de nuestro método en ayudar con consultas difíciles. Medimos la dificultad de la consulta mediante la Precisión Promedio Media (MAP) de la clasificación original del motor de búsqueda para cada caso de prueba. Luego ordenamos los 172 casos de prueba en el conjunto de pruebas 1 en orden creciente de valores de MAP. Dividimos los casos de prueba en 4 contenedores, cada uno con un número aproximadamente igual de casos de prueba. Un MAP pequeño significa que la utilidad del ranking original es baja. El contenedor 1 contiene los casos de prueba con los MAP más bajos y el contenedor 4 contiene los casos de prueba con los MAP más altos. Para cada contenedor, calculamos la cantidad de casos de prueba cuyos P@5 han mejorado en comparación con los que han disminuido. La Figura 3 muestra los resultados. Claramente, en el contenedor 1, la mayoría de los casos de prueba han mejorado (24 vs 3), mientras que en el contenedor 4, el método basado en registros puede disminuir el rendimiento (3 vs 20). Esto demuestra que nuestro método es más beneficioso para consultas difíciles, lo cual es lo esperado ya que el agrupamiento de resultados de búsqueda está destinado a ayudar con consultas difíciles. Esto también muestra que nuestro método realmente no ayuda con consultas sencillas, por lo tanto, deberíamos desactivar nuestra opción de organización para consultas sencillas. 6.2.3 Ajuste de parámetros Examinamos la sensibilidad de los parámetros en esta sección. Para el algoritmo de agrupamiento de estrellas, estudiamos el parámetro de umbral de similitud σ. Para la función de recuperación OKAPI, estudiamos los parámetros k1 y b. También estudiamos el impacto del número de consultas pasadas recuperadas en nuestro método basado en registros. La Figura 4 muestra el impacto del parámetro σ tanto para los métodos basados en clústeres como en registros en ambos conjuntos de pruebas. Variamos σ de 0.05 a 0.3 con un paso de 0.05. La Figura 4 muestra que el rendimiento no es muy sensible al parámetro σ. Siempre podemos obtener el mejor resultado en el rango de 0.1 ≤ σ ≤ 0.25. En la Tabla 4, mostramos el impacto de los parámetros de OKAPI. Variamos k1 de 1.0 a 2.0 con un paso de 0.2 y b de 0 a 1 con un paso de 0.2. De esta tabla, se desprende que P@5 tampoco es muy sensible a la configuración de parámetros. La mayoría de los valores son mayores a 0.35. Los valores predeterminados k1 = 1.2 y b = 0.8 dan resultados óptimos aproximados. Estudiamos además el impacto de la cantidad de historial 0.2 0.25 0.3 0.35 0.4 0.05 0.1 0.15 0.2 0.25 0.3 P@5 umbral de similitud: sigma basado en clúster 1 basado en registro 1 basado en clúster 2 basado en registro 2 Figura 4: El impacto del umbral de similitud σ en los métodos basados en clúster y en registro. Mostramos el resultado en ambas colecciones de pruebas. b 0.0 0.2 0.4 0.6 0.8 1.0 1.0 0.3476 0.3406 0.3453 0.3616 0.3500 0.3453 1.2 0.3418 0.3383 0.3453 0.3593 0.3534 0.3546 k1 1.4 0.3337 0.3430 0.3476 0.3604 0.3546 0.3465 1.6 0.3476 0.3418 0.3523 0.3534 0.3581 0.3476 1.8 0.3465 0.3418 0.3546 0.3558 0.3616 0.3476 2.0 0.3453 0.3500 0.3534 0.3558 0.3569 0.3546 Tabla 4: Impacto de los parámetros k1 y b de OKAPI. información para aprender variando el número de consultas pasadas a recuperar para aprender aspectos. Los resultados en ambas colecciones de pruebas se muestran en la Figura 5. Podemos ver que el rendimiento aumenta gradualmente a medida que aumentamos el número de consultas pasadas recuperadas. Por lo tanto, nuestro método podría potencialmente aprender más a medida que acumulamos más historia. Más importante aún, a medida que pasa el tiempo, más y más consultas tendrán suficiente historial, por lo que podremos mejorar cada vez más consultas. 6.2.4 Un Ejemplo Ilustrativo Utilizamos el área de consultas de códigos para mostrar la diferencia en los resultados del método basado en registros y el método basado en clústeres. Esta consulta puede referirse a códigos telefónicos o códigos postales. La Tabla 5 muestra las palabras clave representativas extraídas de los tres mayores grupos de ambos métodos. En el método basado en clústeres, los resultados se dividen en función de las ubicaciones: locales o internacionales. En el método basado en registros, los resultados se desambiguan en dos sentidos: códigos telefónicos o códigos postales. Si bien ambas son particiones razonables, nuestra evaluación indica que la mayoría de los usuarios que utilizan una consulta de este tipo suelen estar interesados en códigos telefónicos o códigos postales, ya que los valores de P@5 de los métodos basados en clústeres y en registros son de 0.2 y 0.6, respectivamente. Por lo tanto, nuestro método basado en registros es más efectivo para ayudar a los usuarios a navegar hacia los resultados deseados. Método basado en clústeres Método basado en registros ciudad, estado teléfono, ciudad, internacional local, área teléfono, marcación internacional código postal, postal Tabla 5: Un ejemplo que muestra la diferencia entre el método basado en clústeres y nuestro método basado en registros 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 1501201008050403020 P@5 #consultas recuperadas Conjunto de pruebas 1 Conjunto de pruebas 2 Figura 5: El impacto del número de consultas pasadas recuperadas. 6.2.5 Comparación de etiquetas Ahora comparamos las etiquetas entre el método basado en clústeres y el método basado en registros. El método basado en clusters tiene que depender de las palabras clave extraídas de los fragmentos para construir la etiqueta de cada cluster. Nuestro método basado en registros puede evitar esta dificultad aprovechando las consultas. Específicamente, para el método basado en clusters, contamos la frecuencia de una palabra clave que aparece en un cluster y usamos las palabras clave más frecuentes como la etiqueta del cluster. Para el método basado en registros, utilizamos el centro de cada cúmulo estelar como la etiqueta para el cúmulo correspondiente. En general, no es fácil cuantificar la legibilidad de una etiqueta de grupo de forma automática. Utilizamos ejemplos para mostrar la diferencia entre los métodos basados en clústeres y los métodos basados en registros. En la Tabla 6, enumeramos las etiquetas de los 5 grupos principales para dos ejemplos: jaguar y manzana. Para el método basado en clusters, separamos las palabras clave por comas ya que no forman una frase. Desde esta tabla, podemos ver que nuestro método basado en registros proporciona etiquetas más legibles porque genera etiquetas basadas en las consultas de los usuarios. Esta es otra ventaja de nuestra forma de organizar los resultados de búsqueda sobre el enfoque de agrupamiento. Comparación de etiquetas para la consulta jaguar Método basado en registros Método basado en clústeres 1. jaguar animal 1. jaguar, auto, accesorios 2. jaguar auto accesorios 2. jaguar, tipo, precios 3. jaguar gatos 3. jaguar, panthera, gatos 4. jaguar reparación 4. jaguar, servicios, boston 5. imágenes de animales de jaguar 5. jaguar, colección, ropa Comparación de etiquetas para la consulta manzana Método basado en registros Método basado en clústeres 1. computadora de manzana 1. manzana, soporte, producto 2. ipod de manzana 2. manzana, sitio, computadora 3. receta de manzana crujiente 3. manzana, mundo, visita 4. pastel de manzana fresca 4. manzana, ipod, amazon 5. computadora portátil de manzana 5. manzana, productos, noticias Tabla 6: Comparación de etiquetas de clústeres. CONCLUSIONES Y TRABAJO FUTURO En este artículo, estudiamos el problema de organizar los resultados de búsqueda de una manera orientada al usuario. Para lograr este objetivo, confiamos en los registros de los motores de búsqueda para aprender aspectos interesantes desde la perspectiva de los usuarios. Dada una consulta, recuperamos sus consultas relacionadas de la historia de consultas pasadas, aprendemos los aspectos mediante la agrupación de las consultas pasadas y la información de clics asociada, y categorizamos los resultados de búsqueda en los aspectos aprendidos. Comparamos nuestro método basado en registros con el método tradicional basado en clusters y la línea base del ranking de motores de búsqueda. Los experimentos muestran que nuestro método basado en registros puede superar consistentemente al método basado en clústeres y mejorar sobre la línea base de clasificación, especialmente cuando las consultas son difíciles o los resultados de búsqueda son diversos. Además, nuestro método basado en registros puede generar etiquetas de aspectos más significativas que las etiquetas de clúster generadas en función de los resultados de búsqueda al agrupar los resultados de búsqueda. Hay varias direcciones interesantes para extender aún más nuestro trabajo: En primer lugar, aunque los resultados de nuestro experimento han mostrado claramente la promesa de la idea de aprender de los registros de búsqueda para organizar los resultados de búsqueda, los métodos con los que hemos experimentado son relativamente simples. Sería interesante explorar otros métodos potencialmente más efectivos. En particular, esperamos desarrollar modelos probabilísticos para aprender aspectos y organizar resultados simultáneamente. Segundo, con la forma propuesta de organizar los resultados de búsqueda, podemos esperar obtener información de retroalimentación informativa de un usuario (por ejemplo, el aspecto elegido por un usuario para ver). Sería interesante estudiar cómo mejorar aún más la organización de los resultados basándose en esa información de retroalimentación. Finalmente, podemos combinar un registro de búsqueda general con cualquier registro de búsqueda personal para personalizar y optimizar la organización de los resultados de búsqueda para cada usuario individual. 8. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios. Este trabajo cuenta con el apoyo parcial de una beca de investigación de Microsoft Live Labs, una beca de investigación de Google y una beca NSF CAREER IIS-0347933. REFERENCIAS [1] E. Agichtein, E. Brill y S. T. Dumais. Mejorando la clasificación de búsqueda web al incorporar información sobre el comportamiento del usuario. En SIGIR, páginas 19-26, 2006. [2] J. A. Aslam, E. Pelekov y D. Rus. El algoritmo de agrupamiento estelar para la organización de información estática y dinámica. Revista de Algoritmos y Aplicaciones de Grafos, 8(1):95-129, 2004. [3] R. A. Baeza-Yates. Aplicaciones de la minería de consultas web. En ECIR, páginas 7-22, 2005. [4] D. Beeferman y A. L. Berger. Aglomeración de clústeres de un registro de consultas de un motor de búsqueda. En KDD, páginas 407-416, 2000. [5] D. Carmel, E. Yom-Tov, A. Darlow y D. Pelleg. ¿Qué hace que una consulta sea difícil? En SIGIR, páginas 390-397, 2006. [6] H. Chen y S. T. Dumais. Dando orden a la web: categorizando automáticamente los resultados de búsqueda. En CHI, páginas 145-152, 2000. [7] S. Cronen-Townsend, Y. Zhou y W. B. Croft. Predicción del rendimiento de la consulta. En Actas de ACM SIGIR 2002, páginas 299-306, 2002. [8] S. T. Dumais, E. Cutrell y H. Chen. Optimizando la búsqueda mostrando resultados en contexto. En CHI, páginas 277-284, 2001. [9] M. A. Hearst y J. O. Pedersen. Reexaminando la hipótesis del clúster: Dispersión/recolección en los resultados de recuperación. En SIGIR, páginas 76-84, 1996. [10] T. Joachims. Optimización de motores de búsqueda utilizando datos de clics. En KDD, páginas 133-142, 2002. [11] T. Joachims. Evaluación del rendimiento de recuperación utilizando datos de clics., páginas 79-96. Physica/Springer Verlag, 2003. en J. Franke y G. Nakhaeizadeh e I. Renz, Text Mining. [12] R. Jones, B. Rey, O. Madani y W. Greiner. Generando sustituciones de consulta. En WWW, páginas 387-396, 2006. [13] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal y R. Krishnapuram. Un algoritmo jerárquico monotético de agrupamiento de documentos para resumen y navegación de resultados de búsqueda. En WWW, páginas 658-665, 2004. [14] Microsoft Live Labs. Acelerando la búsqueda en la investigación académica, 2006. http://research.microsoft.com/ur/us/fundingopps/RFPs/ Search 2006 RFP.aspx. [15] P. Pirolli, P. K. Schank, M. A. Hearst y C. Diehl. La navegación de dispersión/recolección comunica la estructura temática de una colección de texto muy grande. En CHI, páginas 213-220, 1996. [16] F. Radlinski y T. Joachims. Cadenas de consulta: aprendizaje para clasificar a partir de retroalimentación implícita. En KDD, páginas 239-248, 2005. [17] S. E. Robertson y S. Walker. Algunas aproximaciones simples y efectivas al modelo 2-poisson para la recuperación ponderada probabilística. En SIGIR, páginas 232-241, 1994. [18] G. Salton, A. Wong y C. S. Yang. Un modelo de espacio vectorial para indexación automática. Comunicación. ACM, 18(11):613-620, 1975. [19] X. Shen, B. Tan y C. Zhai. Recuperación de información sensible al contexto utilizando retroalimentación implícita. En SIGIR, páginas 43-50, 2005. [20] C. J. van Rijsbergen. Recuperación de información, segunda edición. Butterworths, Londres, 1979. [21] V. N. Vapnik. La naturaleza de la teoría del aprendizaje estadístico. Springer-Verlag, Berlín, 1995. [22] Vivisimo. http://vivisimo.com/. [23] X. Wang, J.-T. Sun, Z. Chen y C. Zhai. Análisis semántico latente para objetos de datos interrelacionados de múltiples tipos. En SIGIR, páginas 236-243, 2006. [24] J.-R. Wen, J.-Y. Nie, y H. Zhang. Agrupación de consultas de usuarios de un motor de búsqueda. En WWW, páginas 162-168, 2001. [25] E. Yom-Tov, S. Fine, D. Carmel y A. Darlow. Aprendiendo a estimar la dificultad de la consulta: incluyendo aplicaciones para la detección de contenido faltante y la recuperación de información distribuida. En SIGIR, páginas 512-519, 2005. [26] O. Zamir y O. Etzioni. Agrupamiento de documentos web: Una demostración de viabilidad. En SIGIR, páginas 46-54, 1998. [27] O. Zamir y O. Etzioni. Grouper: Una interfaz de agrupación dinámica para los resultados de búsqueda en la web. Redes de Computadoras, 31(11-16):1361-1374, 1999. [28] H.-J. Zeng, Q.-C. Él, Z. Chen, W.-Y. Ma, y J. Ma. Aprendiendo a agrupar los resultados de búsqueda web. En SIGIR, páginas 210-217, 2004.