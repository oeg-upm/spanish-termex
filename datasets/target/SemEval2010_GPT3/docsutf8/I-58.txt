Un Enfoque Heurístico Eficiente para la Seguridad Contra Múltiples Adversarios Praveen Paruchuri, Jonathan P. Pearce, Milind Tambe, Fernando Ordonez Universidad del Sur de California Los Ángeles, CA 90089 {paruchur, jppearce, tambe, fordon}@usc.edu Sarit Kraus Universidad Bar-Ilan Ramat-Gan 52900, Israel sarit@cs.biu.ac.il RESUMEN En dominios multiagentes adversariales, la seguridad, comúnmente definida como la capacidad de lidiar con amenazas intencionales de otros agentes, es un tema crítico. Este documento se centra en los dominios de donde provienen estas amenazas de adversarios desconocidos. Estos dominios pueden ser modelados como juegos bayesianos; se ha realizado mucho trabajo en encontrar equilibrios para tales juegos. Sin embargo, es frecuente en dominios de seguridad multiagente que un agente pueda comprometerse con una estrategia mixta que sus adversarios observan antes de elegir sus propias estrategias. En este caso, el agente puede maximizar la recompensa al encontrar una estrategia óptima, sin necesidad de equilibrio. Trabajos anteriores han demostrado que este problema de selección de estrategia óptima es NP-duro. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar el problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Categorías y Descriptores de Asignaturas I.2.11 [Metodologías de Computación]: Inteligencia Artificial: Inteligencia Artificial Distribuida - Agentes Inteligentes Términos Generales Seguridad, Diseño, Teoría 1. INTRODUCCIÓN En muchos dominios multiagentes, los agentes deben actuar para proporcionar seguridad contra ataques de adversarios. Un problema común al que se enfrentan los agentes en estos dominios de seguridad es la incertidumbre sobre los adversarios a los que podrían enfrentarse. Por ejemplo, un robot de seguridad puede necesitar tomar decisiones sobre qué áreas patrullar y con qué frecuencia [16]. Sin embargo, no sabrá de antemano exactamente dónde elegirá un ladrón atacar. Un equipo de vehículos aéreos no tripulados (UAVs) monitoreando una región que atraviesa una crisis humanitaria también puede necesitar elegir una política de patrullaje. Deben tomar esta decisión sin saber de antemano si terroristas u otros adversarios pueden estar esperando para interrumpir la misión en un lugar determinado. De hecho, puede ser posible modelar las motivaciones de los tipos de adversarios que es probable que el agente o equipo de agentes enfrenten para poder dirigirse más de cerca a estos adversarios. Sin embargo, en ambos casos, el robot de seguridad o el equipo de UAV no sabrán exactamente qué tipos de adversarios pueden estar activos en un día determinado. Un enfoque común para elegir una política para agentes en tales escenarios es modelar los escenarios como juegos bayesianos. Un juego bayesiano es un juego en el que los agentes pueden pertenecer a uno o más tipos; el tipo de un agente determina sus posibles acciones y recompensas. La distribución de los tipos de adversarios a los que se enfrentará un agente puede ser conocida o inferida a partir de datos históricos. Por lo general, estos juegos se analizan según el concepto de solución de equilibrio de Bayes-Nash, una extensión del equilibrio de Nash para juegos bayesianos. Sin embargo, en muchos contextos, un equilibrio de Nash o de Bayes-Nash no es un concepto de solución apropiado, ya que asume que las estrategias de los agentes son elegidas simultáneamente [5]. En algunos escenarios, un jugador puede (o debe) comprometerse con una estrategia antes de que los otros jugadores elijan sus estrategias. Estos escenarios son conocidos como juegos de Stackelberg [6]. En un juego de Stackelberg, un líder se compromete primero a una estrategia, y luego un seguidor (o grupo de seguidores) optimiza egoístamente sus propias recompensas, considerando la acción elegida por el líder. Por ejemplo, el agente de seguridad (líder) primero debe comprometerse con una estrategia para patrullar diversas áreas. Esta estrategia podría ser una estrategia mixta para ser impredecible para los ladrones (seguidores). Los ladrones, después de observar el patrón de patrullas con el tiempo, pueden entonces elegir su estrategia (qué ubicación robar). A menudo, el líder en un juego de Stackelberg puede obtener una recompensa mayor que si las estrategias fueran elegidas simultáneamente. Para ver la ventaja de ser el líder en un juego de Stackelberg, considera un juego simple con la tabla de pagos como se muestra en la Tabla 1. El líder es el jugador de la fila y el seguidor es el jugador de la columna. Aquí se lista primero el pago de los líderes. 1 2 3 1 5,5 0,0 3,10 2 0,0 2,2 5,0 Tabla 1: Tabla de pagos para el juego de forma normal de ejemplo. El único equilibrio de Nash para este juego es cuando el líder juega 2 y el seguidor juega 2, lo que le da al líder un pago de 2. Sin embargo, si el líder se compromete a una estrategia mixta uniforme de jugar 1 y 2 con igual probabilidad (0.5), la mejor respuesta del seguidor es jugar 3 para obtener un pago esperado de 5 (10 y 0 con igual probabilidad). El pago de los líderes sería entonces de 4 (3 y 5 con igual probabilidad). En este caso, el líder ahora tiene un incentivo para desviarse y elegir una estrategia pura de 2 (para obtener un pago de 5). Sin embargo, esto haría que el seguidor también se desviara hacia la estrategia 2, lo que resultaría en el equilibrio de Nash. Por lo tanto, al comprometerse con una estrategia que es observada por el seguidor, y al evitar la tentación de desviarse, el líder logra obtener una recompensa mayor que la del mejor equilibrio de Nash. El problema de elegir una estrategia óptima para que el líder se comprometa en un juego de Stackelberg se analiza en [5] y se encuentra que es NP-duro en el caso de un juego bayesiano con múltiples tipos de seguidores. Por lo tanto, las técnicas heurísticas eficientes para elegir estrategias de alto rendimiento en estos juegos son un tema importante pendiente. Los métodos para encontrar estrategias óptimas de líder para juegos no bayesianos [5] se pueden aplicar a este problema convirtiendo el juego bayesiano en un juego en forma normal mediante la transformación de Harsanyi [8]. Si, por otro lado, deseamos calcular el equilibrio de Nash de mayor recompensa, se pueden utilizar nuevos métodos que empleen programas lineales enteros mixtos (MILPs) [17], ya que el equilibrio de Bayes-Nash de mayor recompensa es equivalente al equilibrio de Nash correspondiente en el juego transformado. Sin embargo, al transformar el juego, se pierde la estructura compacta del juego bayesiano. Además, dado que el equilibrio de Nash asume una elección simultánea de estrategias, no se consideran las ventajas de ser el líder. Este documento presenta un método heurístico eficiente para aproximar la estrategia óptima del líder en dominios de seguridad, conocido como ASAP (Seguridad del Agente a través de Políticas Aproximadas). Este método tiene tres ventajas clave. Primero, busca directamente una estrategia óptima, en lugar de un equilibrio de Nash (o Bayes-Nash), lo que le permite encontrar estrategias no equilibradas de alto rendimiento como la del ejemplo anterior. Segundo, genera políticas con un soporte que puede expresarse como una distribución uniforme sobre un multiconjunto de tamaño fijo como se propone en [12]. Esto permite políticas que son simples de entender y representar [12], así como un parámetro ajustable (el tamaño del multiconjunto) que controla la simplicidad de la política. Tercero, el método permite expresar de manera compacta un juego de Bayes-Nash sin necesidad de convertirlo a un juego en forma normal, lo que permite acelerar significativamente en comparación con los métodos de Nash existentes como [17] y [11]. El resto del documento está organizado de la siguiente manera. En la Sección 2 describimos completamente el dominio de patrullaje y sus propiedades. La sección 3 introduce el juego bayesiano, la transformación de Harsanyi y los métodos existentes para encontrar una estrategia óptima del líder en un juego de Stackelberg. Luego, en la Sección 4 se presenta el algoritmo ASAP para juegos en forma normal, y en la Sección 5 mostramos cómo puede adaptarse a la estructura de juegos bayesianos con adversarios inciertos. Los resultados experimentales que muestran una recompensa más alta y una computación de políticas más rápida que los métodos Nash existentes se presentan en la Sección 6, y concluimos con una discusión sobre trabajos relacionados en la Sección 7. EN EL DOMINIO DE PATRULLAJE En la mayoría de los dominios de patrullaje de seguridad, los agentes de seguridad (como UAVs [1] o robots de seguridad [16]) no pueden patrullar todas las áreas todo el tiempo de manera factible. En cambio, deben elegir una política en la que patrullen diversas rutas en diferentes momentos, teniendo en cuenta factores como la probabilidad de crimen en diferentes áreas, posibles objetivos de crimen y los recursos propios de los agentes de seguridad (número de agentes de seguridad, cantidad de tiempo disponible, combustible, etc.). Por lo general, es beneficioso que esta política sea no determinista para que los ladrones no puedan robar ciertas ubicaciones con seguridad, sabiendo que estarán a salvo de los agentes de seguridad [14]. Para demostrar la utilidad de nuestro algoritmo, utilizamos una versión simplificada de dicho dominio, expresada como un juego. La versión más básica de nuestro juego consiste en dos jugadores: el agente de seguridad (el líder) y el ladrón (el seguidor) en un mundo compuesto por m casas, 1 . . . m. El conjunto de estrategias puras del agente de seguridad consiste en las posibles rutas de d casas para patrullar (en un orden). El agente de seguridad puede elegir una estrategia mixta para que el ladrón no esté seguro de exactamente dónde puede patrullar el agente de seguridad, pero el ladrón conocerá la estrategia mixta que el agente de seguridad ha elegido. Por ejemplo, el ladrón puede observar con el tiempo con qué frecuencia el agente de seguridad patrulla cada área. Con este conocimiento, el ladrón debe elegir una sola casa para robar. Suponemos que el ladrón generalmente tarda mucho tiempo en robar una casa. Si la casa elegida por el ladrón no está en la ruta de los agentes de seguridad, entonces el ladrón la roba con éxito. De lo contrario, si está en la ruta de los agentes de seguridad, cuanto antes esté la casa en la ruta, más fácil será para el agente de seguridad atrapar al ladrón antes de que termine de robarla. Modelamos los pagos para este juego con las siguientes variables: • vl,x: valor de los bienes en la casa l para el agente de seguridad. • vl,q: valor de los bienes en la casa l para el ladrón. • cx: recompensa para el agente de seguridad por atrapar al ladrón. • cq: costo para el ladrón de ser atrapado. • pl: probabilidad de que el agente de seguridad pueda atrapar al ladrón en la l-ésima casa en la patrulla (pl < pl ⇐⇒ l < l). El conjunto de posibles estrategias puras (rutas de patrulla) de los agentes de seguridad se denota por X e incluye todos los d-tuplas i =< w1, w2, ..., wd > con w1 . . . wd = 1 . . . m donde ningún par de elementos es igual (al agente no se le permite regresar a la misma casa). El conjunto de estrategias puras posibles de los ladrones (casas para robar) se denota por Q e incluye todos los enteros j = 1 . . . m. Las ganancias (agente de seguridad, ladrón) para las estrategias puras i, j son: • −vl,x, vl,q, para j = l /∈ i. • plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), para j = l ∈ i. Con esta estructura es posible modelar muchos tipos diferentes de ladrones que tienen motivaciones diferentes; por ejemplo, un ladrón puede tener un menor costo de ser atrapado que otro, o puede valorar de manera diferente los bienes en las distintas casas. Si se conoce o se infiere la distribución de diferentes tipos de ladrones a partir de datos históricos, entonces el juego se puede modelar como un juego bayesiano [6]. 3. JUEGOS BAYESIANOS Un juego bayesiano contiene un conjunto de N agentes, y cada agente n debe ser uno de un conjunto dado de tipos θn. Para nuestro dominio de patrullaje, tenemos dos agentes, el agente de seguridad y el ladrón. θ1 es el conjunto de tipos de agentes de seguridad y θ2 es el conjunto de tipos de ladrones. Dado que solo hay un tipo de agente de seguridad, θ1 contiene solo un elemento. Durante el juego, el ladrón conoce su tipo pero el agente de seguridad no conoce el tipo de los ladrones. Para cada agente (el agente de seguridad o el ladrón) n, hay un conjunto de estrategias σn y una función de utilidad un: θ1 × θ2 × σ1 × σ2 → . Un juego bayesiano puede transformarse en un juego en forma normal utilizando la transformación de Harsanyi [8]. Una vez que esto se haya completado, se pueden utilizar nuevos métodos basados en programación lineal (LP) para encontrar estrategias de alto rendimiento para juegos en forma normal [5] y encontrar una estrategia en el juego transformado; esta estrategia luego se puede utilizar para el juego bayesiano. Si bien existen métodos para encontrar equilibrios de Bayes-Nash directamente, sin la transformación de Harsanyi [10], solo encuentran un 312 The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) un equilibrio único en el caso general, que puede no ser de alta recompensa. El trabajo reciente [17] ha llevado al desarrollo de técnicas eficientes de programación lineal entera mixta para encontrar el mejor equilibrio de Nash para un agente dado. Sin embargo, estas técnicas requieren un juego en forma normal, por lo que para comparar las políticas proporcionadas por ASAP con la política óptima, así como con el equilibrio de Nash de mayor recompensa, debemos aplicar estas técnicas a la matriz transformada por Harsanyi. Las dos siguientes subsecciones detallan cómo se hace esto. 3.1 Transformación de Harsanyi El primer paso para resolver juegos bayesianos es aplicar la transformación de Harsanyi [8] que convierte el juego bayesiano en un juego en forma normal. Dado que la transformación de Harsanyi es un concepto estándar en la teoría de juegos, la explicamos brevemente a través de un ejemplo simple en nuestro dominio de patrullaje sin introducir las formulaciones matemáticas. Supongamos que hay dos tipos de ladrones, a y b, en el juego bayesiano. El ladrón a estará activo con probabilidad α, y el ladrón b estará activo con probabilidad 1 − α. Las reglas descritas en la Sección 2 nos permiten construir tablas de pagos simples. Suponga que hay dos casas en el mundo (1 y 2) y, por lo tanto, hay dos rutas de patrulla (estrategias puras) para el agente: {1,2} y {2,1}. El ladrón puede robar la casa 1 o la casa 2 y, por lo tanto, tiene dos estrategias (denominadas como 1l, 2l para el tipo de ladrón l). Dado que se asumen dos tipos (denotados como a y b), construimos dos tablas de pagos (mostradas en la Tabla 2) correspondientes al agente de seguridad jugando un juego separado con cada uno de los dos tipos de ladrones con probabilidades α y 1 − α. Primero, considera el tipo de ladrón a. Tomando prestada la notación de la sección de dominio, asignamos los siguientes valores a las variables: v1,x = v1,q = 3/4, v2,x = v2,q = 1/4, cx = 1/2, cq = 1, p1 = 1, p2 = 1/2. Usando estos valores, construimos una tabla base de pagos como el pago por el juego contra el tipo de ladrón a. Por ejemplo, si el agente de seguridad elige la ruta {1,2} cuando el ladrón a está activo, y el ladrón a elige la casa 1, el ladrón recibe una recompensa de -1 (por ser atrapado) y el agente recibe una recompensa de 0.5 por atrapar al ladrón. Los pagos para el juego contra el tipo de ladrón b se construyen utilizando diferentes valores. El agente de seguridad: {1,2} {2,1} Ladrón a 1a -1, .5 -.375, .125 .125 2a -.125, -.125 -1, .5 Ladrón b 1b -.9, .6 -.275, .225 .025 2b -.025, -.025 -.9, .6 Tabla 2: Tablas de pago: Agente de seguridad vs Ladrones a y b El uso de la técnica de Harsanyi implica introducir un nodo de probabilidad, que determina el tipo de ladrón, transformando así la información incompleta del agente de seguridad sobre el ladrón en información imperfecta [3]. El equilibrio bayesiano del juego es entonces precisamente el equilibrio de Nash del juego de información imperfecta. El juego en forma normal transformado se muestra en la Tabla 3. En el juego transformado, el agente de seguridad es el jugador de la columna, y el conjunto de todos los tipos de ladrones juntos es el jugador de la fila. Supongamos que el ladrón tipo a roba la casa 1 y el ladrón tipo b roba la casa 2, mientras que el agente de seguridad elige patrullar {1,2}. Entonces, el agente de seguridad y el ladrón reciben una ganancia esperada correspondiente a sus ganancias cuando el agente se encuentra con el ladrón a en la casa 1 con probabilidad α y con el ladrón b en la casa 2 con probabilidad 1 − α. 3.2 Encontrar una Estrategia Óptima Aunque un equilibrio de Nash es el concepto estándar de solución para juegos en los que los agentes eligen estrategias simultáneamente, en nuestro dominio de seguridad, el agente de seguridad (el líder) puede obtener una ventaja comprometiéndose con una estrategia mixta de antemano. Dado que los seguidores (los ladrones) conocerán la estrategia de los líderes, la respuesta óptima para los seguidores será una estrategia pura. Dada la suposición común, tomada en [5], en el caso en que los seguidores sean indiferentes, elegirán la estrategia que beneficie al líder, debe existir una estrategia óptima garantizada para el líder [5]. A partir del juego bayesiano en la Tabla 2, construimos el bimatrix transformado de Harsanyi en la Tabla 3. Las estrategias para cada jugador (agente de seguridad o ladrón) en el juego transformado corresponden a todas las combinaciones de estrategias posibles tomadas por cada uno de los tipos de jugadores. Por lo tanto, denotamos X = σθ1 1 = σ1 y Q = σθ2 2 como los conjuntos de índices de las estrategias puras del agente de seguridad y los ladrones respectivamente, con R y C como las matrices de pagos correspondientes. Rij es la recompensa del agente de seguridad y Cij es la recompensa de los ladrones cuando el agente de seguridad elige la estrategia pura i y los ladrones eligen la estrategia pura j. Una estrategia mixta para el agente de seguridad es una distribución de probabilidad sobre su conjunto de estrategias puras y será representada por un vector x = (px1, px2, . . . , px|X|), donde pxi ≥ 0 y Σ pxi = 1. Aquí, pxi es la probabilidad de que el agente de seguridad elija su i-ésima estrategia pura. La estrategia mixta óptima para el agente de seguridad se puede encontrar en tiempo polinómico en el número de filas en el juego de forma normal utilizando la siguiente formulación de programa lineal del [5]. Para cada estrategia pura posible j por el seguidor (el conjunto de todos los tipos de ladrones), maximizar P i∈X pxiRij sujeto a ∀j ∈ Q, P i∈σ1 pxiCij ≥ P i∈σ1 pxiCij P i∈X pxi = 1 ∀i∈X , pxi >= 0 (1). Luego, para todas las estrategias factibles del seguidor j, elegir aquella que maximice P i∈X pxiRij, la recompensa para el agente de seguridad (líder). Las variables pxi dan la estrategia óptima para el agente de seguridad. Ten en cuenta que aunque este método es polinómico en el número de filas en el juego transformado en forma normal, el número de filas aumenta exponencialmente con el número de tipos de ladrones. Usar este método para un juego bayesiano requiere ejecutar |σ2||θ2| programas lineales separados. Esto no es sorprendente, ya que encontrar la estrategia óptima de los líderes en un juego de Stackelberg bayesiano es NP-difícil [5]. 4. Enfoques heurísticos Dado que encontrar la estrategia óptima para el líder es NP-duro, proporcionamos un enfoque heurístico. En esta heurística limitamos las posibles estrategias mixtas del líder para seleccionar acciones con probabilidades que son múltiplos enteros de 1/k para un entero k predeterminado. Trabajos anteriores [14] han demostrado que las estrategias con alta entropía son beneficiosas para aplicaciones de seguridad cuando las utilidades de los oponentes son completamente desconocidas. En nuestro dominio, si no se consideran los servicios públicos, este método resultará en estrategias de distribución uniforme. Una ventaja de tales estrategias es que son compactas para representar (como fracciones) y simples de entender; por lo tanto, pueden ser implementadas eficientemente por organizaciones reales. Buscamos mantener la ventaja proporcionada por estrategias simples para nuestro problema de aplicación de seguridad, incorporando el efecto de las recompensas de los ladrones en las recompensas de los agentes de seguridad. Por lo tanto, la heurística ASAP producirá estrategias que son k-uniformes. Una estrategia mixta se denota k-uniforme si es una distribución uniforme en un multiconjunto S de estrategias puras con |S| = k. Un multiconjunto es un conjunto cuyos elementos pueden repetirse varias veces; así, por ejemplo, la estrategia mixta correspondiente al multiconjunto {1, 1, 2} tomaría la estrategia 1. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 313 {1,2} {2,1} {1a, 1b} −1α − .9(1 − α), .5α + .6(1 − α) −.375α − .275(1 − α), .125α + .225(1 − α) {1a, 2b} −1α − .025(1 − α), .5α − .025(1 − α) −.375α − .9(1 − α), .125α + .6(1 − α) {2a, 1b} −.125α − .9(1 − α), −.125α + .6(1 − α) −1α − .275(1 − α), .5α + .225(1 − α) {2a, 2b} −.125α − .025(1 − α), −.125α − .025(1 − α) −1α − .9(1 − α), .5α + .6(1 − α) Tabla 3: Tabla de Pagos Transformada por Harsanyi con probabilidad 2/3 y estrategia 2 con probabilidad 1/3. ASAP permite elegir el tamaño del multiconjunto para equilibrar la complejidad de la estrategia alcanzada con el objetivo de que la estrategia identificada genere una alta recompensa. Otra ventaja de la heurística ASAP es que opera directamente sobre la representación bayesiana compacta, sin necesidad de la transformación de Harsanyi. Esto se debe a que los diferentes tipos de seguidores (ladrones) son independientes entre sí. Por lo tanto, evaluar la estrategia del líder contra una matriz de juego transformada por Harsanyi es equivalente a evaluarla contra cada una de las matrices de juego para los tipos individuales de seguidores. Esta propiedad de independencia es explotada en ASAP para producir un esquema de descomposición. Se debe tener en cuenta que el método de LP introducido por [5] para calcular políticas óptimas de Stackelberg es poco probable que sea descomponible en un número reducido de juegos, ya que se demostró que es NP-duro para problemas de Bayes-Nash. Finalmente, cabe destacar que ASAP requiere la solución de un solo problema de optimización, en lugar de resolver una serie de problemas como en el método LP de [5]. Para un solo tipo de seguidor, el algoritmo funciona de la siguiente manera. Dado un k particular, para cada estrategia mixta x posible para el líder que corresponda a un multiconjunto de tamaño k, evaluar la recompensa del líder a partir de x cuando el seguidor juega una estrategia pura que maximiza la recompensa. Luego elegimos la estrategia mixta con el mayor beneficio. Solo necesitamos considerar las estrategias puras que maximizan la recompensa de los seguidores (ladrones), ya que para una estrategia fija x del agente de seguridad, cada tipo de ladrón enfrenta un problema con recompensas lineales fijas. Si una estrategia mixta es óptima para el ladrón, entonces también lo son todas las estrategias puras en el soporte de esa estrategia mixta. También hay que tener en cuenta que al limitar las estrategias de los líderes a valores discretos, la suposición de la Sección 3.2 de que los seguidores desempatarán a favor de los líderes no es significativa, ya que es poco probable que se produzcan empates. Esto se debe a que, en dominios donde las recompensas se extraen de cualquier distribución aleatoria, la probabilidad de que un seguidor tenga más de una respuesta óptima pura a una estrategia dada del líder tiende a cero, y el líder solo tendrá un número finito de posibles estrategias mixtas. Nuestro enfoque para caracterizar la estrategia óptima para el agente de seguridad hace uso de propiedades de programación lineal. Breve resumimos estos resultados aquí para completitud, para una discusión detallada y pruebas consulte una de las muchas referencias sobre el tema, como [2]. Cada problema de programación lineal, como: max cT x | Ax = b, x ≥ 0, tiene un programa lineal dual asociado, en este caso: min bT y | AT y ≥ c. Estos pares primal/dual de problemas satisfacen la dualidad débil: Para cualquier x e y soluciones factibles primal y dual respectivamente, cT x ≤ bT y. Por lo tanto, un par de soluciones factibles es óptimo si cT x = bT y, y se dice que los problemas satisfacen dualidad fuerte. De hecho, si un programa lineal es factible y tiene una solución óptima acotada, entonces el dual también es factible y existe un par x∗ , y∗ que satisface cT x∗ = bT y∗. Estas soluciones óptimas se caracterizan con las siguientes condiciones de optimalidad (según se define en [2]): • factibilidad primal: Ax = b, x ≥ 0 • factibilidad dual: AT y ≥ c • holgura complementaria: xi(AT y − c)i = 0 para todo i. Cabe destacar que esta última condición implica que cT x = xT AT y = bT y, lo cual demuestra la optimalidad para las soluciones factibles primal-dual x e y. En las siguientes subsecciones, primero definimos el problema en su forma más intuitiva como un programa cuadrático de enteros mixtos (MIQP), y luego mostramos cómo este problema puede convertirse en un programa lineal de enteros mixtos (MILP). 4.1 Programa Cuadrático de Enteros Mixtos Comenzamos con el caso de un solo tipo de seguidor. Que el líder sea el jugador de la fila y el seguidor el jugador de la columna. Denotamos por x al vector de estrategias del líder y q al vector de estrategias del seguidor. También denotamos con X y Q los conjuntos de índices de las estrategias puras del líder y los seguidores, respectivamente. Las matrices de pago R y C corresponden a: Rij es la recompensa del líder y Cij es la recompensa del seguidor cuando el líder toma la estrategia pura i y el seguidor toma la estrategia pura j. Sea k el tamaño del multiconjunto. Primero fijamos la política del líder a alguna política x k-uniforme. El valor xi es el número de veces que la estrategia pura i se utiliza en la política k-uniforme, la cual se selecciona con probabilidad xi/k. Formulamos el problema de optimización que el seguidor resuelve para encontrar su respuesta óptima a x como el siguiente programa lineal: max X j∈Q X i∈X 1 k Cijxi qj s.t. La función objetivo maximiza la recompensa esperada de los seguidores dada x, mientras que las restricciones hacen factible cualquier estrategia mixta q para el seguidor. El dual de este problema de programación lineal es el siguiente: min a tal que a ≥ X i∈X 1 k Cijxi j ∈ Q. (3) A partir de la dualidad fuerte y la holgura complementaria, obtenemos que el valor máximo de recompensa de los seguidores, a, es el valor de cada estrategia pura con qj > 0, es decir, en el soporte de la estrategia mixta óptima. Por lo tanto, cada una de estas estrategias puras es óptima. Las soluciones óptimas al problema de los seguidores se caracterizan por las condiciones de optimalidad de programación lineal: restricciones de factibilidad primal en (2), restricciones de factibilidad dual en (3) y holguras complementarias qj a − X i∈X 1 k Cijxi ! = 0 j ∈ Q. Estas condiciones deben ser incluidas en el problema resuelto por el líder para considerar solo las mejores respuestas por parte del seguidor a la política k-uniforme x. 314 El Sexto Internacional. La Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) El líder busca la solución k-uniforme x que maximiza su propia ganancia, dado que el seguidor utiliza una respuesta óptima q(x). Por lo tanto, el líder resuelve el siguiente problema entero: max X i∈X X j∈Q 1 k Rijq(x)j xi sujeto a. El problema (4) maximiza la recompensa de los líderes con la mejor respuesta de los seguidores (qj para una política fija de los líderes x y por lo tanto denotada q(x)j) seleccionando una política uniforme de un multiconjunto de tamaño constante k. Completamos este problema incluyendo la caracterización de q(x) a través de las condiciones de optimalidad de programación lineal. Para simplificar la escritura de las condiciones de holgura complementaria, restringiremos q(x) para que solo sean estrategias puras óptimas al considerar soluciones enteras de q(x). El problema de los líderes se convierte en: maxx,q X i∈X X j∈Q 1 k Rijxiqj s.t. P i xi = kP j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cijxi) ≤ (1 − qj)M xi ∈ {0, 1, ...., k} qj ∈ {0, 1}. (5) Aquí, la constante M es algún número grande. Las primeras y cuartas restricciones imponen una política k-uniforme para el líder, y las segundas y quintas restricciones imponen una estrategia pura factible para el seguidor. La tercera restricción garantiza la factibilidad dual del problema de los seguidores (desigualdad más a la izquierda) y la restricción de holgura complementaria para una estrategia pura óptima q para el seguidor (desigualdad más a la derecha). De hecho, dado que solo una estrategia pura puede ser seleccionada por el seguidor, digamos qh = 1, esta última restricción impone que a = P i∈X 1 k Cihxi sin imponer ninguna restricción adicional para todas las demás estrategias puras que tienen qj = 0. Concluimos esta subsección señalando que el Problema (5) es un programa entero con un objetivo cuadrático no convexo en general, ya que la matriz R no necesita ser semidefinida positiva. Los métodos de solución eficientes para problemas enteros no lineales y no convexos siguen siendo una cuestión de investigación desafiante. En la siguiente sección mostramos una reformulación de este problema como un problema de programación lineal entera, para el cual existen varios solucionadores comerciales eficientes. 4.2 Programa Lineal Entero Mixto Podemos linearizar el programa cuadrático del Problema 5 a través del cambio de variables zij = xiqj, obteniendo el siguiente problema maxq,z P i∈X P j∈Q 1 k Rijzij s.t. P i∈X P j∈Q zij = k P j∈Q zij ≤ k kqj ≤ P i∈X zij ≤ k P j∈Q qj = 1 0 ≤ (a − P i∈X 1 k Cij( P h∈Q zih)) ≤ (1 − qj)M zij ∈ {0, 1, ...., k} qj ∈ {0, 1} (6) PROPOSICIÓN 1. Los problemas (5) y (6) son equivalentes. Prueba: Considera x, q una solución factible de (5). Mostraremos que q, zij = xiqj es una solución factible de (6) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 6 y 7 de (6) se satisfacen por construcción. El hecho de que P j∈Q zij = xi y P j∈Q qj = 1 explica las restricciones 1, 2 y 5 de (6). La restricción 3 de (6) se cumple porque P i∈X zij = kqj. Consideremos ahora q, z factibles para (6). Mostraremos que q y xi = P j∈Q zij son factibles para (5) con el mismo valor objetivo. De hecho, todas las restricciones de (5) se satisfacen fácilmente por construcción. Para ver que los objetivos coinciden, observe que si qh = 1, entonces la tercera restricción en (6) implica que P i∈X zih = k, lo que significa que zij = 0 para todo i ∈ X y todo j = h. Por lo tanto, xiqj = X l∈Q zilqj = zihqj = zij. Esta última igualdad se debe a que ambos son 0 cuando j = h. Esto demuestra que la transformación conserva el valor de la función objetivo, completando la prueba. Dada esta transformación a un programa lineal entero mixto (MILP), ahora mostramos cómo podemos aplicar nuestra técnica de descomposición en el MILP para obtener mejoras significativas en la velocidad para juegos bayesianos con múltiples tipos de seguidores. 5. DECOMPOSICIÓN PARA MÚLTIPLES ADVERSARIOS El MILP desarrollado en la sección anterior solo maneja a un seguidor. Dado que nuestro escenario de seguridad contiene múltiples tipos de seguidores (ladrones), cambiamos la función de respuesta para el seguidor de una estrategia pura a una combinación ponderada de varias estrategias puras de seguidores donde los pesos son las probabilidades de ocurrencia de cada uno de los tipos de seguidores. 5.1 MIQP descompuesto Para admitir múltiples adversarios en nuestro marco, modificamos la notación definida en la sección anterior para razonar sobre múltiples tipos de seguidores. Denotamos por x al vector de estrategias del líder y ql al vector de estrategias del seguidor l, con L denotando el conjunto de índices de tipos de seguidores. También denotamos por X y Q los conjuntos de índices de las estrategias puras del líder y seguidor, respectivamente. También indexamos las matrices de pago en cada seguidor l, considerando las matrices Rl y Cl. Utilizando esta notación modificada, caracterizamos la solución óptima del problema del seguidor ls dado la política k-uniforme de los líderes x, con las siguientes condiciones de optimalidad: X j∈Q ql j = 1 al − X i∈X 1 k Cl ijxi ≥ 0 ql j(al − X i∈X 1 k Cl ijxi) = 0 ql j ≥ 0 Nuevamente, considerando solo estrategias puras óptimas para el problema del seguidor ls, podemos linearizar la restricción de complementariedad anterior. Incorporamos estas restricciones en el problema de los líderes que selecciona la política óptima k-uniforme. Por lo tanto, dadas las probabilidades a priori pl, con l ∈ L de enfrentar a cada seguidor, el líder resuelve el siguiente problema: La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 315 maxx,q X i∈X X l∈L X j∈Q pl k Rl ijxiql j s.t. El problema (7) para un juego bayesiano con múltiples tipos de seguidores es de hecho equivalente al Problema (5) en la matriz de pagos obtenida a partir de la transformación de Harsanyi del juego. De hecho, cada estrategia pura j en el Problema (5) corresponde a una secuencia de estrategias puras jl, una para cada seguidor l ∈ L. Esto significa que qj = 1 si y solo si ql jl = 1 para todos los l ∈ L. Además, dadas las probabilidades a priori pl de enfrentarse al jugador l, la recompensa en la tabla de pagos de la transformación de Harsanyi es Rij = P l∈L pl Rl ijl. La misma relación se mantiene entre C y Cl. Estas relaciones entre una estrategia pura en el juego de forma normal equivalente y las estrategias puras en los juegos individuales con cada seguidor son clave para demostrar que estos problemas son equivalentes. 5.2 MILP descompuesto Podemos linearizar el problema de programación cuadrática 7 a través del cambio de variables zl ij = xiql j, obteniendo el siguiente problema maxq,z P i∈X P l∈L P j∈Q pl k Rl ijzl ij s.t. P i∈X P j∈Q zl ij = k P j∈Q zl ij ≤ k kql j ≤ P i∈X zl ij ≤ k P j∈Q ql j = 1 0 ≤ (al − P i∈X 1 k Cl ij( P h∈Q zl ih)) ≤ (1 − ql j)M P j∈Q zl ij = P j∈Q z1 ij zl ij ∈ {0, 1, ...., k} ql j ∈ {0, 1} (8) PROPOSICIÓN 2. Los problemas (7) y (8) son equivalentes. Prueba: Considera x, ql, al con l ∈ L una solución factible de (7). Mostraremos que ql , al , zl ij = xiql j es una solución factible de (8) con el mismo valor de la función objetivo. La equivalencia de las funciones objetivas y las restricciones 4, 7 y 8 de (8) se satisfacen por construcción. El hecho de que P j∈Q zl ij = xi como P j∈Q ql j = 1 explica las restricciones 1, 2, 5 y 6 de (8). La restricción 3 de (8) se cumple porque P i∈X zl ij = kql j. Consideremos ahora ql, zl, al factibles para (8). Mostraremos que ql, al y xi = P j∈Q z1 ij son factibles para (7) con el mismo valor objetivo. De hecho, todas las restricciones de (7) se satisfacen fácilmente por construcción. Para verificar que los objetivos coincidan, observe que para cada l, ql j debe ser igual a 1 y el resto igual a 0. Digamos que ql jl = 1, entonces la tercera restricción en (8) implica que P i∈X zl ijl = k, lo que significa que zl ij = 0 para todo i ∈ X y todo j = jl. En particular, esto implica que xi = X j∈Q z1 ij = z1 ij1 = zl ijl, la última igualdad proviene de la restricción 6 de (8). Por lo tanto xiql j = zl ijl ql j = zl ij. Esta última igualdad se debe a que ambos son 0 cuando j = jl. Efectivamente, la restricción 6 asegura que todos los adversarios estén calculando sus mejores respuestas contra una política fija particular del agente. Esto muestra que la transformación preserva el valor de la función objetivo, completando la prueba. Por lo tanto, podemos resolver este programa lineal entero equivalente con paquetes eficientes de programación entera que pueden manejar problemas con miles de variables enteras. Implementamos el MILP descompuesto y los resultados se muestran en la siguiente sección. 6. RESULTADOS EXPERIMENTALES El dominio de patrullaje y las recompensas para el juego asociado se detallan en las Secciones 2 y 3. Realizamos experimentos para este juego en mundos de tres y cuatro casas con patrullas compuestas por dos casas. La descripción proporcionada en la Sección 2 se utiliza para generar un caso base tanto para las funciones de pago del agente de seguridad como para las del ladrón. Las tablas de pagos para tipos adicionales de ladrones se construyen y se añaden al juego mediante la adición de una distribución aleatoria de tamaño variable a los pagos en el caso base. Todos los juegos están normalizados de manera que, para cada tipo de ladrón, las ganancias mínimas y máximas para el agente de seguridad y el ladrón son 0 y 1, respectivamente. Utilizando los datos generados, realizamos los experimentos utilizando cuatro métodos para generar la estrategia de los agentes de seguridad: • aleatorización uniforme • ASAP • el método de múltiples programas lineales de [5] (para encontrar la estrategia óptima real) • el equilibrio de Bayes-Nash de mayor recompensa, encontrado utilizando el algoritmo MIP-Nash [17]. Los tres últimos métodos fueron aplicados utilizando CPLEX 8.1. Dado que los dos últimos métodos están diseñados para juegos en forma normal en lugar de juegos bayesianos, los juegos fueron primero convertidos utilizando la transformación de Harsanyi [8]. El método de aleatorización uniforme consiste en elegir simplemente una política aleatoria uniforme entre todas las posibles rutas de patrulla. Utilizamos este método como una línea base simple para medir el rendimiento de nuestras heurísticas. Anticipamos que la política uniforme funcionaría razonablemente bien, ya que se ha demostrado que las políticas de máxima entropía son efectivas en dominios de seguridad multiagente [14]. Se utilizaron los equilibrios de Bayes-Nash de mayor recompensa para demostrar la mayor recompensa obtenida al buscar una política óptima en lugar de un equilibrio en juegos de Stackelberg como nuestro dominio de seguridad. Basándonos en nuestros experimentos, presentamos tres conjuntos de gráficos para demostrar (1) el tiempo de ejecución de ASAP en comparación con otros métodos comunes para encontrar una estrategia, (2) la recompensa garantizada por ASAP en comparación con otros métodos, y (3) el efecto de variar el parámetro k, el tamaño del multiconjunto, en el rendimiento de ASAP. En los dos primeros conjuntos de gráficos, se ejecuta ASAP utilizando un multiconjunto de 80 elementos; en el tercer conjunto este número varía. El primer conjunto de gráficos, mostrado en la Figura 1, muestra los gráficos de tiempo de ejecución para los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Cada una de las tres filas de gráficos corresponde a un escenario generado aleatoriamente diferente. El eje x muestra la cantidad de tipos de ladrones a los que se enfrenta el agente de seguridad y el eje y del gráfico muestra el tiempo de ejecución en segundos. Todos los experimentos que no se concluyeron en 30 minutos (1800 segundos) fueron interrumpidos. El tiempo de ejecución para la política uniforme siempre es despreciable independientemente del número de adversarios y, por lo tanto, no se muestra. El algoritmo ASAP claramente supera al método óptimo de múltiples LP, así como al algoritmo MIP-Nash para encontrar el equilibrio de Bayes-Nash de mayor recompensa con respecto al tiempo de ejecución. Para un 316 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) Figura 1: Tiempos de ejecución para varios algoritmos en problemas de 3 y 4 casas. En el dominio de tres casas, el método óptimo no puede alcanzar una solución para más de siete tipos de ladrones, y para cuatro casas no puede resolver más de seis tipos dentro del tiempo límite en ninguno de los tres escenarios. MIP-Nash resuelve incluso para menos tipos de ladrones dentro del tiempo límite. Por otro lado, ASAP se ejecuta mucho más rápido y es capaz de resolver al menos 20 adversarios para los escenarios de tres casas y al menos 12 adversarios en los escenarios de cuatro casas dentro del tiempo límite. El tiempo de ejecución de ASAP no aumenta estrictamente con el número de tipos de ladrones para cada escenario, pero en general, la adición de más tipos incrementa el tiempo de ejecución requerido. El segundo conjunto de gráficos, Figura 2, muestra la recompensa al agente de patrulla otorgada por cada método para tres escenarios en los dominios de tres casas (columna izquierda) y cuatro casas (columna derecha). Esta recompensa es la utilidad recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima, ya que no fue posible obtener la recompensa óptima a medida que aumentaba el número de tipos de ladrones. La política uniforme proporciona consistentemente la recompensa más baja en ambos dominios; mientras que el método óptimo, por supuesto, produce la recompensa óptima. El método ASAP se mantiene consistentemente cerca del óptimo, incluso a medida que aumenta el número de tipos de ladrones. Los equilibrios de Bayes-Nash de mayor recompensa, proporcionados por el método MIPNash, produjeron recompensas más altas que el método uniforme, pero más bajas que ASAP. Esta diferencia ilustra claramente las ganancias en el dominio del patrullaje al comprometerse con una estrategia como líder en un juego de Stackelberg, en lugar de jugar una estrategia estándar de Bayes-Nash. El tercer conjunto de gráficos, mostrado en la Figura 3, muestra el efecto del tamaño del multiconjunto en el tiempo de ejecución en segundos (columna izquierda) y la recompensa (columna derecha), nuevamente expresada como la recompensa recibida por el agente de seguridad en el juego de patrullaje, y no como un porcentaje de la recompensa óptima de la Figura 2: Recompensa para varios algoritmos en problemas de 3 y 4 casas. Los resultados aquí son para el dominio de las tres casas. La tendencia es que a medida que el tamaño del multiconjunto aumenta, tanto el tiempo de ejecución como el nivel de recompensa aumentan. No sorprende que la recompensa aumente monótonamente a medida que aumenta el tamaño del multiconjunto, pero lo interesante es que hay relativamente poco beneficio en utilizar un multiconjunto grande en este dominio. En todos los casos, la recompensa otorgada por un multiconjunto de 10 elementos estaba dentro de al menos el 96% de la recompensa otorgada por un multiconjunto de 80 elementos. El tiempo de ejecución no siempre aumenta estrictamente con el tamaño del multiconjunto; de hecho, en un ejemplo (escenario 2 con 20 tipos de ladrones), el uso de un multiconjunto de 10 elementos tomó 1228 segundos, mientras que el uso de 80 elementos solo tomó 617 segundos. En general, el tiempo de ejecución debería aumentar ya que un multiconjunto más grande implica un dominio más amplio para las variables en el MILP, y por lo tanto, un espacio de búsqueda más grande. Sin embargo, un aumento en el número de variables a veces puede permitir la construcción de una política de manera más rápida debido a una mayor flexibilidad en el problema. RESUMEN Y TRABAJO RELACIONADO Este artículo se centra en la seguridad para agentes que patrullan en entornos hostiles. En estos entornos, las amenazas intencionales son causadas por adversarios sobre los cuales los agentes de patrullaje de seguridad tienen información incompleta. Específicamente, nos ocupamos de situaciones en las que las acciones y ganancias de los adversarios son conocidas, pero el tipo exacto de adversario es desconocido para el agente de seguridad. Los agentes que actúan en el mundo real con bastante frecuencia tienen información tan incompleta sobre otros agentes. Los juegos bayesianos han sido una elección popular para modelar juegos con información incompleta [3]. El kit de herramientas Gala es un método para definir tales juegos [9] sin necesidad de representar el juego en forma normal a través de la transformación de Harsanyi [8]; las garantías de Gala se centran en juegos completamente competitivos. Se ha realizado mucho trabajo en encontrar equilibrios de Bayes-Nash óptimos para The Sixth Intl. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07) 317 Figura 3: Recompensa para ASAP utilizando multiconjuntos de 10, 30 y 80 elementos subclases de juegos bayesianos, encontrando equilibrios de Bayes-Nash individuales para juegos bayesianos generales [10] o equilibrios de Bayes-Nash aproximados [18]. Se ha prestado menos atención a encontrar la estrategia óptima a comprometerse en un juego bayesiano (el escenario de Stackelberg [15]). Sin embargo, se demostró que la complejidad de este problema es NP-duro en el caso general [5], lo cual también proporciona algoritmos para este problema en el caso no bayesiano. Por lo tanto, presentamos una heurística llamada ASAP, con tres ventajas clave para abordar este problema. Primero, ASAP busca la estrategia de mayor recompensa, en lugar de un equilibrio de Bayes-Nash, lo que le permite encontrar estrategias factibles que exploten la ventaja natural del jugador que actúa primero en el juego. Segundo, proporciona estrategias que son simples de entender, representar e implementar. Tercero, opera directamente sobre la representación compacta del juego bayesiano, sin necesidad de convertirla a forma normal. Proporcionamos una implementación eficiente de Programación Lineal Entera Mixta (MILP) para ASAP, junto con resultados experimentales que ilustran mejoras significativas en la velocidad y recompensas más altas en comparación con otros enfoques. Nuestras estrategias k-uniformes son similares a las estrategias k-uniformes de [12]. Si bien ese trabajo proporciona límites de error epsilon basados en las estrategias k-uniformes, su concepto de solución sigue siendo el de un equilibrio de Nash, y no proporcionan algoritmos eficientes para obtener dichas estrategias k-uniformes. Esto contrasta con ASAP, donde nuestro énfasis está en un enfoque heurístico altamente eficiente que no se centra en soluciones de equilibrio. Finalmente, el problema de patrullaje que motivó nuestro trabajo ha recibido recientemente una creciente atención por parte de la comunidad de agentes múltiples debido a su amplio rango de aplicaciones [4, 13]. Sin embargo, la mayor parte de este trabajo se centra en limitar el consumo de energía involucrado en la patrulla [7] o en optimizar criterios como la longitud del camino recorrido [4, 13], sin razonar sobre ningún modelo explícito de un adversario[14]. Agradecimientos: Esta investigación está respaldada por el Departamento de Seguridad Nacional de los Estados Unidos a través del Centro para el Análisis de Riesgos y Economía de Eventos Terroristas (CREATE). También cuenta con el respaldo de la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA), a través del Departamento del Interior, NBC, División de Servicios de Adquisición, bajo el Contrato No. NBCHD030010. Sarit Kraus también está afiliada a UMIACS. 8. REFERENCIAS [1] R. W. Beard y T. McLain. Búsqueda cooperativa de múltiples UAV con restricciones de evasión de colisiones y comunicación de rango limitado. En IEEE CDC, 2003. [2] D. Bertsimas y J. Tsitsiklis. Introducción a la Optimización Lineal. Athena Scientific, 1997. [3] J. Brynielsson y S. Arnborg. Juegos bayesianos para la predicción de amenazas y análisis de situaciones. En FUSION, 2004. [4] Y. Chevaleyre. Análisis teórico del problema de patrullaje multiagente. En AAMAS, 2004. [5] V. Conitzer y T. Sandholm. Elegir la mejor estrategia a comprometerse. En la Conferencia ACM sobre Comercio Electrónico, 2006. [6] D. Fudenberg y J. Tirole. Teoría de juegos. MIT Press, 1991. [7] C. Gui y P. Mohapatra. Patrulla virtual: Un nuevo diseño de conservación de energía para vigilancia utilizando redes de sensores. En IPSN, 2005. [8] J. C. Harsanyi y R. Selten. Una solución Nash generalizada para juegos de negociación de dos personas con información incompleta. Ciencia de la Gestión, 18(5):80-106, 1972. [9] D. Koller y A. Pfeffer. Generando y resolviendo juegos de información imperfecta. En IJCAI, páginas 1185-1193, 1995. [10] D. Koller y A. Pfeffer. Representaciones y soluciones para problemas de teoría de juegos. Inteligencia Artificial, 94(1):167-215, 1997. [11] C. Lemke y J. Howson. Puntos de equilibrio de juegos bimatrix. Revista de la Sociedad de Matemáticas Industriales y Aplicadas, 12:413-423, 1964. [12] R. J. Lipton, E. Markakis y A. Mehta. Jugando juegos grandes utilizando estrategias simples. En la Conferencia ACM sobre Comercio Electrónico, 2003. [13] A. Machado, G. Ramalho, J. D. Zucker y A. Drougoul. Patrullaje multiagente: un análisis empírico sobre arquitecturas alternativas. En MABS, 2002. [14] P. Paruchuri, M. Tambe, F. Ordonez y S. Kraus. Seguridad en sistemas multiagentes mediante la aleatorización de políticas. En AAMAS, 2006. [15] T. Roughgarden. Estrategias de programación de Stackelberg. En ACM Symposium on TOC, 2001. [16] S. Ruan, C. Meirina, F. Yu, K. R. Pattipati y R. L. Popp. Patrullando en un entorno estocástico. En la 10ª Conferencia Internacional. Simposio de Investigación en Comando y Control, 2005. [17] T. Sandholm, A. Gilpin y V. Conitzer. Métodos de programación entera mixta para encontrar equilibrios de Nash. En AAAI, 2005. [18] S. Singh, V. Soni y M. Wellman. Calculando equilibrios de Bayes-Nash aproximados con juegos de árbol de información incompleta. En la Conferencia ACM sobre Comercio Electrónico, 2004. 318 La Sexta Internacional. Conferencia Conjunta sobre Agentes Autónomos y Sistemas Multiagente (AAMAS 07)