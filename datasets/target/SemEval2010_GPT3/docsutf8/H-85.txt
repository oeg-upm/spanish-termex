Aprender modelos de interacción de usuario para predecir las preferencias de resultados de búsqueda web Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Robert Ragno Microsoft Research rragno@microsoft.com RESUMEN Evaluar las preferencias de los usuarios de los resultados de búsqueda web es crucial para el desarrollo, despliegue y mantenimiento de motores de búsqueda. Presentamos un estudio del mundo real sobre la modelización del comportamiento de los usuarios de búsqueda web para predecir las preferencias de resultados de búsqueda web. La modelización precisa e interpretación del comportamiento del usuario tiene aplicaciones importantes en la clasificación, detección de spam de clics, personalización de la búsqueda web y otras tareas. Nuestra clave para mejorar la robustez en la interpretación de la retroalimentación implícita es modelar las desviaciones dependientes de la consulta del comportamiento ruidoso esperado del usuario. Mostramos que nuestro modelo de interpretación de clics mejora la precisión de predicción sobre los métodos de clics más avanzados. Generalizamos nuestro enfoque para modelar el comportamiento del usuario más allá del clic, lo que resulta en una mayor precisión en la predicción de preferencias que los modelos basados únicamente en la información de clics. Informamos los resultados de una evaluación experimental a gran escala que muestra mejoras sustanciales sobre los métodos de interpretación de retroalimentación implícita publicados. Categorías y Descriptores de Asignaturas H.3.3 [Búsqueda y Recuperación de Información]: Proceso de búsqueda, retroalimentación de relevancia. Términos generales Algoritmos, Medición, Rendimiento, Experimentación. 1. La medición de la relevancia es crucial para la búsqueda en la web y para la recuperación de información en general. Tradicionalmente, la relevancia de la búsqueda se mide utilizando evaluadores humanos para juzgar la relevancia de los pares de consulta-documento. Sin embargo, las calificaciones humanas explícitas son costosas y difíciles de obtener. Al mismo tiempo, millones de personas interactúan diariamente con los motores de búsqueda web, proporcionando valiosos comentarios implícitos a través de sus interacciones con los resultados de búsqueda. Si pudiéramos convertir estas interacciones en juicios de relevancia, podríamos obtener grandes cantidades de datos para evaluar, mantener y mejorar los sistemas de recuperación de información. Recientemente, la retroalimentación automática o implícita de relevancia se ha convertido en un área activa de investigación en la comunidad de recuperación de información, al menos en parte debido a un aumento en los recursos disponibles y a la creciente popularidad de la búsqueda en la web. Sin embargo, la mayoría del trabajo tradicional de IR se realizaba sobre colecciones de pruebas controladas y conjuntos de consultas y tareas cuidadosamente seleccionados. Por lo tanto, no está claro si estas técnicas funcionarán para la búsqueda web general del mundo real. Una distinción significativa es que la búsqueda en la web no está controlada. Los usuarios individuales pueden comportarse de manera irracional o maliciosa, o incluso no ser usuarios reales; todo esto afecta los datos que se pueden recopilar. Pero la cantidad de datos de interacción del usuario es de órdenes de magnitud mayor que cualquier cosa disponible en un entorno que no sea de búsqueda web. Al utilizar el comportamiento agregado de grandes cantidades de usuarios (y no tratar a cada usuario como un experto individual) podemos corregir el ruido inherente en las interacciones individuales, y generar juicios de relevancia que son más precisos que las técnicas no específicamente diseñadas para el entorno de búsqueda en la web. Además, las observaciones y percepciones obtenidas en entornos de laboratorio no necesariamente se traducen al uso en el mundo real. Por lo tanto, es preferible inducir automáticamente estrategias de interpretación de retroalimentación a partir de grandes cantidades de interacciones de usuario. Aprender automáticamente a interpretar el comportamiento del usuario permitiría a los sistemas adaptarse a condiciones cambiantes, patrones de comportamiento del usuario cambiantes y diferentes configuraciones de búsqueda. Presentamos técnicas para interpretar automáticamente el comportamiento colectivo de los usuarios que interactúan con un motor de búsqueda web para predecir las preferencias de los usuarios para los resultados de búsqueda. Nuestras contribuciones incluyen: • Un modelo de distribución del comportamiento del usuario, robusto al ruido dentro de las sesiones individuales de usuario, que puede recuperar las preferencias de relevancia a partir de las interacciones del usuario (Sección 3). • Extensiones de estrategias existentes de clics para incluir características de navegación e interacción más ricas (Sección 4). • Una evaluación exhaustiva de nuestros modelos de comportamiento del usuario, así como de técnicas de vanguardia previamente publicadas, sobre un gran conjunto de sesiones de búsqueda web (Secciones 5 y 6). Discutimos nuestros resultados y esbozamos futuras direcciones y diversas aplicaciones de este trabajo en la Sección 7, que concluye el artículo. ANTECEDENTES Y TRABAJO RELACIONADO Clasificar los resultados de búsqueda es un problema fundamental en la recuperación de información. Los enfoques más comunes en el contexto de la web utilizan tanto la similitud de la consulta con el contenido de la página, como la calidad general de una página [3, 20]. Un motor de búsqueda de última generación puede utilizar cientos de características para describir una página candidata, empleando algoritmos sofisticados para clasificar las páginas en función de estas características. Los motores de búsqueda actuales suelen estar ajustados según las evaluaciones de relevancia humana. Los anotadores humanos califican un conjunto de páginas para una consulta según la relevancia percibida, creando el estándar de oro contra el cual se pueden evaluar diferentes algoritmos de clasificación. Reducir la dependencia de los juicios humanos explícitos mediante el uso de retroalimentación implícita de relevancia ha sido un tema activo de investigación. Varios grupos de investigación han evaluado la relación entre medidas implícitas y el interés del usuario. En estos estudios, se recopilan tanto el tiempo de lectura como las calificaciones explícitas de interés. Morita y Shinoda [14] estudiaron la cantidad de tiempo que los usuarios pasaban leyendo artículos de noticias de Usenet y descubrieron que el tiempo de lectura podía predecir los niveles de interés de los usuarios. Konstan et al. [13] demostraron que el tiempo de lectura era un fuerte predictor del interés del usuario en su sistema GroupLens. Oard y Kim [15] estudiaron si la retroalimentación implícita podría sustituir a las calificaciones explícitas en los sistemas de recomendación. Más recientemente, Oard y Kim [16] presentaron un marco para caracterizar los comportamientos observables de los usuarios utilizando dos dimensiones: el propósito subyacente del comportamiento observado y el alcance del elemento sobre el que se actúa. Goecks y Shavlik [8] aproximaron las etiquetas humanas recolectando un conjunto de medidas de actividad de la página mientras los usuarios navegaban por la World Wide Web. Los autores plantearon correlaciones entre un alto grado de actividad en la página y el interés de los usuarios. Si bien los resultados fueron prometedores, el tamaño de la muestra era pequeño y las medidas implícitas no fueron probadas frente a juicios explícitos del interés del usuario. Claypool et al. [6] estudiaron cómo varias medidas implícitas se relacionaban con los intereses del usuario. Desarrollaron un navegador personalizado llamado Navegador Curioso para recopilar datos, en un laboratorio de computación, sobre indicadores de interés implícito y para investigar juicios explícitos de las páginas web visitadas. Claypool et al. encontraron que el tiempo pasado en una página, la cantidad de desplazamiento en una página y la combinación de tiempo y desplazamiento tienen una relación positiva fuerte con el interés explícito, mientras que los métodos individuales de desplazamiento y clics de ratón no estaban correlacionados con el interés explícito. Fox et al. [7] exploraron la relación entre medidas implícitas y explícitas en la búsqueda en la web. Construyeron un navegador instrumentado para recopilar datos y luego desarrollaron modelos bayesianos para relacionar medidas implícitas y juicios explícitos de relevancia tanto para consultas individuales como para sesiones de búsqueda. Encontraron que el clic fue la variable individual más importante, pero que la precisión predictiva podría mejorarse al utilizar variables adicionales, especialmente el tiempo de permanencia en una página. Joachims [9] desarrolló valiosas ideas sobre la recopilación de medidas implícitas, introduciendo una técnica basada completamente en datos de clics para aprender funciones de clasificación. Más recientemente, Joachims et al. [10] presentaron una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar las predicciones de sus estrategias con las calificaciones explícitas, los autores demostraron que es posible interpretar con precisión los eventos de clics en un entorno controlado de laboratorio. Una visión más completa de los estudios de medidas implícitas se describe en Kelly y Teevan [12]. Desafortunadamente, no está claro en qué medida la investigación existente se aplica a la búsqueda web del mundo real. En este documento, nos basamos en investigaciones previas para desarrollar modelos robustos de interpretación del comportamiento del usuario para el entorno real de búsqueda en la web. 3. MODELOS DE COMPORTAMIENTO DEL USUARIO EN APRENDIZAJE Como mencionamos anteriormente, el comportamiento real del usuario en la búsqueda web puede ser ruidoso en el sentido de que los comportamientos de los usuarios solo están relacionados probabilísticamente con juicios de relevancia explícitos y preferencias. Por lo tanto, en lugar de tratar a cada usuario como un experto confiable, agregamos información de muchas trazas de sesiones de búsqueda de usuarios no confiables. Nuestro enfoque principal es modelar el comportamiento de búsqueda web del usuario como si fuera generado por dos componentes: un componente de relevancia - comportamiento específico de la consulta influenciado por la relevancia aparente de los resultados, y un componente de fondo - usuarios haciendo clic indiscriminadamente. Nuestra idea general es modelar las desviaciones del comportamiento esperado del usuario. Por lo tanto, además de las características básicas, que describiremos detalladamente en la Sección 3.2, calculamos características derivadas que miden la desviación del valor de la característica observada para un resultado de búsqueda dado de los valores esperados para un resultado, sin información dependiente de la consulta. Motivamos nuestras intuiciones con una característica de comportamiento particularmente importante, el clic en los resultados, que se analiza a continuación, y luego presentamos nuestro modelo general de comportamiento del usuario que incorpora otras acciones del usuario (Sección 3.2). 3.1 Un estudio de caso en las distribuciones de clics Como discutimos, agregamos estadísticas a lo largo de muchas sesiones de usuario. Un clic en un resultado puede significar que algún usuario encontró prometedor el resumen del resultado; también podría ser causado por personas haciendo clic indiscriminadamente. En general, el comportamiento individual de los usuarios, ya sea al hacer clic o de otra manera, es ruidoso y no se puede confiar en él para obtener juicios precisos de relevancia. El conjunto de datos se describe con más detalle en la Sección 5.2. Por el momento basta con señalar que nos enfocamos en una muestra aleatoria de 3,500 consultas que fueron seleccionadas al azar de los registros de consultas. Para estas consultas agregamos datos de clics de más de 120,000 búsquedas realizadas durante un período de tres semanas. También tenemos juicios de relevancia explícitos para los 10 mejores resultados de cada consulta. La Figura 3.1 muestra la frecuencia relativa de clics en función de la posición del resultado. La frecuencia de clics agregada en la posición de resultado p se calcula primero computando la frecuencia de un clic en p para cada consulta (es decir, aproximando la probabilidad de que un clic elegido al azar para esa consulta caiga en la posición p). Estas frecuencias se promedian entre las consultas y se normalizan de manera que la frecuencia relativa de un clic en la posición superior sea 1. La distribución resultante concuerda con observaciones previas que indican que los usuarios hacen clic con más frecuencia en los resultados mejor clasificados. Esto refleja el hecho de que los motores de búsqueda hacen un trabajo razonable al clasificar los resultados, así como los sesgos para hacer clic en los resultados principales y el ruido. Intentamos separar estos componentes en el análisis que sigue. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 posición del resultado Frecuencia de clics relativa Figura 3.1: Frecuencia de clics relativa para las 30 primeras posiciones de resultados en más de 3,500 consultas y 120,000 búsquedas. Primero consideramos la distribución de clics para los documentos relevantes de estas consultas. La Figura 3.2 informa la distribución de clics agregada para consultas con diferentes Posiciones del Documento Relevante Superior (PTR). Si bien hay muchos clics por encima del primer documento relevante para cada distribución, claramente hay picos en la frecuencia de clics para el primer resultado relevante. Por ejemplo, para las consultas con el resultado relevante superior en la posición 2, la frecuencia relativa de clics en esa posición (segunda barra) es mayor que la frecuencia de clics en otras posiciones para estas consultas. Sin embargo, muchos usuarios todavía hacen clic en los resultados no relevantes en la posición 1 para tales consultas. Esto muestra una propiedad más fuerte del sesgo en la distribución de clics hacia los resultados principales: los usuarios hacen clic con más frecuencia en los resultados que están mejor clasificados, incluso cuando no son relevantes. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 posición del resultado frecuenciadeclicsrelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.2: Frecuencia de clics relativa para consultas con PTR (Posición del Documento Relevante Principal) variable. -0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 posición del resultado frecuenciadeclicscorregidarelativa PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figura 3.3: Frecuencia de clics corregida relativa para documentos relevantes con PTR (Posición del Documento Relevante Principal) variable. Si restamos la distribución de fondo de la Figura 3.1 de la distribución mixta de la Figura 3.2, obtenemos la distribución en la Figura 3.3, donde la distribución de frecuencia de clics restante se puede interpretar como el componente de relevancia de los resultados. Ten en cuenta que la distribución corregida de clics se correlaciona estrechamente con la relevancia real de los resultados tal como fue calificada explícitamente por los jueces humanos. 3.2 Modelo Robusto de Comportamiento del Usuario Los clics en los resultados de búsqueda constituyen solo una pequeña fracción de las actividades posteriores a la búsqueda que suelen realizar los usuarios. Ahora presentamos nuestras técnicas para ir más allá de las estadísticas de clics y modelar explícitamente el comportamiento del usuario después de la búsqueda. Aunque las distribuciones de clics están fuertemente sesgadas hacia los resultados principales, acabamos de demostrar cómo la distribución de clics impulsada por la relevancia puede ser recuperada corrigiendo la distribución previa de fondo. Conjeturamos que otros aspectos del comportamiento del usuario (por ejemplo, el tiempo de permanencia en la página) también están distorsionados de manera similar. Nuestro modelo general incluye dos tipos de características para describir el comportamiento del usuario: directas y desviacionales, donde las primeras son los valores medidos directamente, y las segundas son la desviación de los valores esperados estimados a partir de las distribuciones generales (independientes de la consulta) para las características observadas directamente correspondientes. De manera más formal, postulamos que el valor observado o de una característica f para una consulta q y un resultado r puede expresarse como una mezcla de dos componentes: ),,()(),,( frqrelfCfrqo += (1) donde )( fC es la distribución de fondo previa para los valores de f agregados en todas las consultas, y rel(q,r,f) es el componente del comportamiento influenciado por la relevancia del resultado r. Como se ilustra arriba con la característica de clics, si restamos la distribución de fondo (es decir, el clic esperado para un resultado en una posición dada) de la frecuencia observada de clics en una posición dada, podemos aproximar el componente de relevancia del valor de clic1. Para reducir el efecto de las variaciones individuales en el comportamiento de los usuarios, promediamos los valores de las características observadas en todos los usuarios y sesiones de búsqueda para cada par de consulta-URL. Esta agregación proporciona una mayor robustez al no depender de las interacciones individuales ruidosas de los usuarios. En resumen, el comportamiento del usuario para un par de consulta-URL está representado por un vector de características que incluye tanto las características observadas directamente como los valores de características derivados y corregidos. Ahora describimos las características reales que utilizamos para representar el comportamiento del usuario. 3.3 Características para Representar el Comportamiento del Usuario Nuestro objetivo es idear un conjunto lo suficientemente rico de características que nos permitan caracterizar cuándo un usuario estará satisfecho con un resultado de búsqueda web. Una vez que el usuario ha enviado una consulta, realizan muchas acciones diferentes (leyendo fragmentos, haciendo clic en resultados, navegando, refinando su consulta) que capturamos y resumimos. Esta información se obtuvo a través de la instrumentación del lado del cliente con consentimiento de los usuarios de un importante motor de búsqueda web. Esta rica representación del comportamiento del usuario es similar en muchos aspectos al trabajo reciente de Fox et al. [7]. Una diferencia importante es que muchas de nuestras características son (por diseño) específicas de la consulta, mientras que la suya era (por diseño) un modelo general, independiente de la consulta, del comportamiento del usuario. Además, incluimos características derivadas y distribucionales calculadas como se describe anteriormente. Las características que utilizamos para representar las interacciones de búsqueda de usuarios se resumen en la Tabla 3.1. Para mayor claridad, organizamos las características en los grupos Texto de consulta, Clics y Navegación. Características de consulta de texto: Los usuarios deciden qué resultados examinar en más detalle al observar el título del resultado, la URL y el resumen; en algunos casos, ni siquiera es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, definimos características para caracterizar la naturaleza de la consulta y su relación con el texto del fragmento. Estos incluyen características como la superposición entre las palabras en el título y en la consulta (TitleOverlap), la fracción de palabras compartidas por la consulta y el resumen del resultado (SummaryOverlap), etc. Características de navegación: Los aspectos simples de las interacciones de la página web del usuario pueden ser capturados y cuantificados. Estas características se utilizan para caracterizar las interacciones con las páginas más allá de la página de resultados. Por ejemplo, calculamos cuánto tiempo los usuarios permanecen en una página (Tiempo en la página) o dominio (Tiempo en el dominio), y la desviación del tiempo de permanencia respecto al tiempo de permanencia esperado en una página para una consulta. Estas características nos permiten modelar la diversidad intraconsulta del comportamiento de navegación de páginas (por ejemplo, las consultas de navegación, en promedio, probablemente tengan un tiempo de permanencia en la página más corto que las consultas transaccionales o informativas). Incluimos tanto las características directas como las características derivadas descritas anteriormente. Características de clics: Los clics son un caso especial de interacción del usuario con el motor de búsqueda. Incluimos todas las características necesarias para aprender las estrategias basadas en el clic descritas en las Secciones 4.1 y 4.4. Por ejemplo, para un par de consulta-URL proporcionamos el número de clics para el resultado (FrecuenciaDeClics), como 1. Por supuesto, esto es solo una estimación aproximada, ya que la distribución de fondo observada también incluye el componente de relevancia, así como si hubo un clic en el resultado debajo o arriba de la URL actual (¿SeHizoClicAbajo, ¿SeHizoClicArriba). Los valores de las características derivadas, como ClickRelativeFrequency y ClickDeviation, se calculan según se describe en la Ecuación 1. Características de consulta Superposición de títulos Fracción de palabras compartidas entre la consulta y el título Superposición de resúmenes Fracción de palabras compartidas entre la consulta y el resumen Superposición de URL de consulta Fracción de palabras compartidas entre la consulta y la URL Superposición de dominio de consulta Fracción de palabras compartidas entre la consulta y el dominio Longitud de consulta Número de tokens en la consulta Superposición de consulta siguiente Fracción promedio de palabras compartidas con la consulta siguiente Características de navegación Tiempo en la página Tiempo de permanencia en la página Tiempo acumulado en la página Tiempo acumulado para todas las páginas subsecuentes después de la búsqueda Tiempo en el dominio Tiempo de permanencia acumulado en este dominio Tiempo en URL corta Tiempo acumulado en el prefijo de URL, excluyendo parámetros EsLinkSeguido 1 si se siguió el enlace al resultado, 0 en caso contrario EsCoincidenciaExactaURL 0 si se utilizó normalización agresiva, 1 en caso contrario EsRedirigido 1 si la URL inicial es igual a la URL final, 0 en caso contrario EsCaminoDesdeBúsqueda 1 si solo se siguieron enlaces después de la consulta, 0 en caso contrario ClicsDesdeBúsqueda Número de saltos para llegar a la página desde la consulta TiempoPromedioPermanencia Tiempo promedio en la página para esta consulta DesviaciónTiempoPermanencia Desviación del tiempo promedio de permanencia en la página DesviaciónAcumulada Desviación del tiempo promedio acumulado en la página DesviaciónDominio Desviación del tiempo promedio en el dominio DesviaciónURLCorta Desviación del tiempo promedio en la URL corta Características de clics Posición Posición de la URL en el ranking actual FrecuenciaClics Número de clics para esta consulta y URL FrecuenciaRelativaClics Frecuencia relativa de un clic para esta consulta y URL DesviaciónClics Desviación de la frecuencia de clics esperada EsClicSiguiente 1 si hay un clic en la siguiente posición, 0 en caso contrario EsClicAnterior 1 si hay un clic en la posición anterior, 0 en caso contrario EsClicArriba 1 si hay un clic arriba, 0 en caso contrario EsClicAbajo 1 si hay un clic abajo, 0 en caso contrario Tabla 3.1: Características utilizadas para representar interacciones post-búsqueda para una consulta dada y URL de resultado de búsqueda 3.4 Aprendizaje de un modelo de comportamiento predictivo Habiendo descrito nuestras características, ahora pasamos al método real de mapear las características a las preferencias del usuario. Intentamos aprender una estrategia general de interpretación de retroalimentación implícita de forma automática en lugar de depender de heurísticas o percepciones. Consideramos que este enfoque es preferible a las estrategias heurísticas, porque siempre podemos extraer más datos en lugar de depender únicamente de nuestra intuición y de la evidencia limitada del laboratorio. Nuestro enfoque general es entrenar un clasificador para inducir pesos a las características del comportamiento del usuario, y consecuentemente derivar un modelo predictivo de las preferencias del usuario. El entrenamiento se realiza comparando una amplia gama de medidas de comportamiento implícitas con juicios explícitos de usuarios para un conjunto de consultas. Para esto, utilizamos una gran muestra aleatoria de consultas en el registro de consultas de búsqueda de un motor de búsqueda web popular, los conjuntos de resultados (identificados por URL) devueltos para cada una de las consultas, y cualquier juicio de relevancia explícito disponible para cada par consulta/resultado. Podemos analizar el comportamiento del usuario para todas las instancias en las que estas consultas fueron enviadas al motor de búsqueda. Para aprender el mapeo de características a preferencias de relevancia, utilizamos una implementación escalable de redes neuronales, RankNet [4], capaz de aprender a clasificar un conjunto de elementos dados. Más específicamente, para cada consulta evaluada verificamos si se ha evaluado un enlace de resultado. Si es así, la etiqueta se asigna al par consulta/URL y al vector de características correspondiente para ese resultado de búsqueda. Estos vectores de valores de características correspondientes a URL juzgadas como relevantes o no relevantes por los anotadores humanos se convierten en nuestro conjunto de entrenamiento. RankNet ha demostrado un excelente rendimiento en el aprendizaje para clasificar objetos en un entorno supervisado, por lo tanto, utilizamos RankNet para nuestros experimentos. PREDICIENDO PREFERENCIAS DE USUARIO En nuestros experimentos, exploramos varios modelos para predecir las preferencias de usuario. Estos modelos van desde no utilizar retroalimentación implícita del usuario hasta utilizar toda la retroalimentación implícita disponible. La clasificación de los resultados de búsqueda para predecir las preferencias del usuario es un problema fundamental en la recuperación de información. La mayoría de los enfoques tradicionales de IR y búsqueda web utilizan una combinación de características de página y enlaces para clasificar los resultados de búsqueda, y un sistema de clasificación de vanguardia representativo se utilizará como nuestro clasificador base (Sección 4.1). Al mismo tiempo, las interacciones de los usuarios con un motor de búsqueda proporcionan una gran cantidad de información. Un tipo de interacción comúnmente considerado es cuando el usuario hace clic en los resultados de búsqueda. Trabajos anteriores [9], como se describió anteriormente, también examinaron qué resultados se omitieron (por ejemplo, omitir arriba y omitir siguiente) y otras estrategias relacionadas para inducir juicios de preferencia de los usuarios que omiten resultados y no hacen clic en los resultados siguientes. También hemos añadido refinamientos de estas estrategias para tener en cuenta la variabilidad observada en escenarios web realistas. Describimos estas estrategias en la Sección 4.2. Dado que los clics son solo un aspecto de la interacción del usuario, ampliamos la estimación de relevancia introduciendo un modelo de aprendizaje automático que incorpora los clics, así como otros aspectos del comportamiento del usuario, como las consultas de seguimiento y el tiempo de permanencia en la página (Sección 4.3). Concluimos esta sección describiendo brevemente nuestra línea base: un algoritmo de clasificación de última generación utilizado por un motor de búsqueda web operativo. Modelo de línea base 4.1 Una pregunta clave es si el comportamiento de navegación puede proporcionar información ausente de los juicios explícitos existentes utilizados para entrenar un clasificador de rango existente. Para nuestro sistema base utilizamos un sistema de clasificación de páginas de última generación actualmente utilizado por un importante motor de búsqueda web. Por lo tanto, llamaremos a este sistema Corriente para la discusión posterior. Si bien los algoritmos específicos utilizados por el motor de búsqueda están más allá del alcance de este documento, el algoritmo clasifica los resultados en función de cientos de características como la similitud entre la consulta y el documento, la similitud entre la consulta y el texto del enlace, y la calidad intrínseca de la página. Las clasificaciones actuales de los motores de búsqueda web proporcionan un sistema sólido para la comparación y experimentación de las siguientes dos secciones. Modelo de clics 4.2 Si asumimos que cada clic de usuario fue motivado por un proceso racional que seleccionó el resumen de resultado más prometedor, entonces podemos interpretar cada clic como se describe en Joachims et al.[10]. Al estudiar el seguimiento ocular y comparar los clics con juicios explícitos, identificaron algunas estrategias básicas. Discutimos las dos estrategias que tuvieron mejor desempeño en sus experimentos, Skip Above y Skip Next. Estrategia SA (Omitir Arriba): Para un conjunto de resultados de una consulta y un resultado clicado en la posición p, se predice que todos los resultados no clicados clasificados por encima de p son menos relevantes que el resultado en p. Además de la información sobre los resultados por encima del resultado clicado, también tenemos información sobre el resultado inmediatamente posterior al clicado. El estudio de seguimiento ocular realizado por Joachims et al. [10] mostró que los usuarios suelen considerar el resultado inmediatamente posterior al resultado clicado en la clasificación actual. Su estrategia de Salto Siguiente utiliza esta observación para predecir que un resultado que sigue al resultado clicado en p es menos relevante que el resultado clicado, con una precisión comparable a la estrategia SA mencionada anteriormente. Para una mejor cobertura, combinamos la estrategia SA con esta extensión para derivar la estrategia Salto Arriba + Salto Siguiente: Estrategia SA+N (Salto Arriba + Salto Siguiente): Esta estrategia predice que todos los resultados no clicados inmediatamente después de un resultado clicado son menos relevantes que el resultado clicado, y combina estas predicciones con las de la estrategia SA mencionada anteriormente. Experimentamos con variaciones de estas estrategias, y encontramos que SA+N superó tanto a SA como a la estrategia original de Saltar Siguiente, por lo que consideraremos las estrategias SA y SA+N en el resto del artículo. Estas estrategias están motivadas y probadas empíricamente en usuarios individuales en un entorno de laboratorio. Como mostraremos, estas estrategias no funcionan tan bien en un entorno real de búsqueda en la web debido a la inconsistencia inherente y al ruido del comportamiento de los usuarios individuales. El enfoque general para utilizar nuestros modelos de clics directamente es filtrar los clics que reflejan una frecuencia de clics mayor que la esperada por azar. Luego utilizamos las mismas estrategias de SA y SA+N, pero solo para los clics que tienen una frecuencia mayor de lo esperado según nuestro modelo. Para esto, estimamos el componente de relevancia rel(q,r,f) de la característica de clic observada f como la desviación de la distribución esperada (de fondo) de clics )( fC. Estrategia CD (desviación d): Para una consulta dada, calcular la distribución de frecuencia de clics observada o(r, p) para todos los resultados r en las posiciones p. La desviación de clics para un resultado r en la posición p, dev(r, p) se calcula como: )(),(),( pCproprdev −= donde C(p) es el clic esperado en la posición p. Si dev(r,p)>d, conservar el clic como entrada para la estrategia SA+N anterior y aplicar la estrategia SA+N sobre el conjunto filtrado de eventos de clics. La elección de d determina el equilibrio entre la sensibilidad y la precisión. Si bien la estrategia anterior extiende SA y SA+N, aún asume que un resultado (filtrado) clicado es preferido sobre todos los resultados no clicados presentados al usuario por encima de una posición clicada. Sin embargo, para consultas informativas, se pueden hacer clic en múltiples resultados, con frecuencias variables. Por lo tanto, es preferible comparar individualmente los resultados de una consulta considerando la diferencia entre los componentes de relevancia estimados de la distribución de clics de los resultados de la consulta correspondiente. Ahora definimos una generalización de la estrategia de interpretación de clics anterior: Estrategia CDiff (margen m): Calcular la desviación dev(r,p) para cada resultado r1...rn en la posición p. Para cada par de resultados ri y rj, predecir la preferencia de ri sobre rj si dev(ri,pi)-dev(ri,pj)>m. Como en CD, la elección de m selecciona el equilibrio entre la recuperación y la precisión. Los pares pueden ser preferidos en el orden original o en su reverso. Dado el margen, dos resultados podrían ser efectivamente indistinguibles, pero solo uno posiblemente puede ser preferido sobre el otro. De manera intuitiva, CDiff generaliza la idea de salto anterior para incluir casos en los que el usuario saltó (es decir, hizo clic menos de lo esperado) en uj y prefirió (es decir, hizo clic más de lo esperado) en ui. Además, esta estrategia permite la diferenciación dentro del conjunto de resultados clicados, lo que la hace más adecuada para el comportamiento ruidoso del usuario. CDiff y CD son complementarios. CDiff es una generalización del modelo de frecuencia de clics de CD, pero ignora la información posicional utilizada en CD. Por lo tanto, combinar las dos estrategias para mejorar la cobertura es un enfoque natural: Estrategia CD+CDiff (desviación d, margen m): Unión de las predicciones de CD y CDiff. Otras variaciones de las estrategias anteriores fueron consideradas, pero estos cinco métodos cubren el rango de rendimiento observado. 4.3 Modelo de Comportamiento General del Usuario. Las estrategias descritas en la sección anterior generan ordenamientos basados únicamente en las frecuencias de clics observadas. Como discutimos, el clic es solo un aspecto, aunque importante, de las interacciones de los usuarios con los resultados de búsqueda en los motores de búsqueda web. Ahora presentamos nuestra estrategia general que se basa en los modelos predictivos del comportamiento del usuario derivados automáticamente (Sección 3). La estrategia UserBehavior: Para una consulta dada, cada resultado se representa con las características en la Tabla 3.1. Las preferencias relativas de los usuarios se estiman luego utilizando el modelo de comportamiento del usuario aprendido descrito en la Sección 3.4. Recuerde que para aprender un modelo de comportamiento predictivo utilizamos las características de la Tabla 3.1 junto con juicios de relevancia explícitos como entrada a RankNet, que aprende un peso óptimo de las características para predecir preferencias. Esta estrategia modela la interacción del usuario con el motor de búsqueda, permitiéndole beneficiarse de la sabiduría de las multitudes que interactúan con los resultados y las páginas más allá. Como demuestran nuestros experimentos en las secciones siguientes, modelar un conjunto más amplio de interacciones de usuario más allá de los clics resulta en predicciones más precisas de las preferencias del usuario. CONFIGURACIÓN EXPERIMENTAL Ahora describimos nuestra configuración experimental. Primero describimos la metodología utilizada, incluyendo nuestras métricas de evaluación (Sección 5.1). Luego describimos los conjuntos de datos (Sección 5.2) y los métodos que comparamos en este estudio (Sección 5.3). Metodología de Evaluación y Métricas Nuestra evaluación se centra en el acuerdo par a par entre las preferencias de los resultados. Esto nos permite comparar con trabajos anteriores [9,10]. Además, para muchas aplicaciones como ajustar funciones de clasificación, las preferencias por pares se pueden utilizar directamente para el entrenamiento [1,4,9]. La evaluación se basa en comparar las preferencias predichas por varios modelos con las preferencias correctas derivadas de los juicios explícitos de relevancia del usuario. Discutimos otras aplicaciones de nuestros modelos más allá del ranking de búsqueda en la Sección 7. Para crear nuestro conjunto de pares de prueba, tomamos cada consulta y calculamos el producto cruzado entre todos los resultados de búsqueda, devolviendo preferencias para los pares según el orden de las etiquetas de relevancia asociadas. Para evitar ambigüedades en la evaluación, descartamos todos los empates (es decir, pares con etiquetas iguales). Para calcular la precisión de nuestras predicciones de preferencias con respecto a las preferencias correctas, adaptamos las medidas estándar de Recuperación y Precisión [20]. Si bien nuestra tarea de calcular el acuerdo por pares es diferente de la tarea de clasificación de relevancia absoluta, las métricas se utilizan de manera similar. Específicamente, informamos la recuperación promedio de consultas y la precisión. Para nuestra tarea, la Precisión de la Consulta y la Recuperación de la Consulta para una consulta q se definen como: • Precisión de la Consulta: Fracción de preferencias predichas para los resultados de q que coinciden con las preferencias obtenidas a partir de juicios humanos explícitos. • Recuperación de la Consulta: Fracción de preferencias obtenidas a partir de juicios humanos explícitos para q que fueron predichas correctamente. El Recuerdo y la Precisión totales se calculan como el promedio del Recuerdo de Consulta y la Precisión de Consulta, respectivamente. Una desventaja de esta medida de evaluación es que algunas preferencias pueden ser más valiosas que otras, lo cual el acuerdo por pares no captura. Discutimos este tema más a fondo cuando consideramos extensiones al trabajo actual en la Sección 7. 5.2 Conjuntos de datos Para la evaluación, utilizamos 3,500 consultas que fueron muestreadas al azar de los registros de consultas (para un motor de búsqueda web importante). Para cada consulta, los 10 resultados de búsqueda devueltos fueron evaluados manualmente en una escala de 6 puntos por jueces capacitados como parte del esfuerzo continuo de mejora de relevancia. Además de estas consultas, también contábamos con datos de interacción de usuario para más de 120,000 instancias de estas consultas. Las interacciones de los usuarios fueron recopiladas a partir de rastros de navegación anónimos que siguieron inmediatamente a una consulta enviada al motor de búsqueda web. Esta recopilación de datos fue parte de comentarios voluntarios enviados por los usuarios desde el 11 de octubre hasta el 31 de octubre. Estos tres semanas (21 días) de datos de interacción de usuarios fueron filtrados para incluir solo a los usuarios en el mercado de inglés de Estados Unidos. Para entender mejor el efecto de la cantidad de datos de interacción de usuario disponibles para una consulta en la precisión, creamos subconjuntos de nuestros datos (Q1, Q10 y Q20) que contienen diferentes cantidades de datos de interacción: • Q1: Consultas calificadas por humanos con al menos 1 clic en los resultados registrados (3500 consultas, 28,093 pares de consulta-URL) • Q10: Consultas en Q1 con al menos 10 clics (1300 consultas, 18,728 pares de consulta-URL). • Q20: Consultas en Q1 con al menos 20 clics (1000 consultas en total, 12,922 pares de consulta-URL). Estos conjuntos de datos fueron recopilados como parte de la experiencia normal del usuario y, por lo tanto, tienen características diferentes a los conjuntos de datos previamente reportados recopilados en entornos de laboratorio. Además, el tamaño de los datos es de un orden de magnitud mayor que cualquier estudio reportado en la literatura. 5.3 Métodos Comparados Consideramos varios métodos para la comparación. Comparamos nuestro modelo UserBehavior (Sección 4.3) con técnicas de interpretación de retroalimentación implícita previamente publicadas y algunas variantes de estos enfoques (Sección 4.2), y con la clasificación actual del motor de búsqueda basada únicamente en las características de la consulta y la página (Sección 4.1). Específicamente, comparamos las siguientes estrategias: • SA: La estrategia de clics omitidos (Skip Above) (Sección 4.2) • SA+N: Una extensión más completa de SA que aprovecha mejor el ranking actual del motor de búsqueda. • CD: Nuestro refinamiento de SA+N que aprovecha nuestro modelo mixto de distribución de clics para seleccionar clics confiables para su interpretación (Sección 4.2). • CDiff: Nuestra generalización de la estrategia CD que utiliza explícitamente el componente de relevancia de las probabilidades de clics para inducir preferencias entre los resultados de búsqueda (Sección 4.2). • CD+CDiff: La estrategia que combina CD y CDiff como la unión de las preferencias predichas de ambos (Sección 4.2). • Comportamiento del Usuario: Ordenamos las predicciones basadas en la disminución de la puntuación más alta de cualquier página. En nuestros experimentos preliminares observamos que puntajes más altos indican mayor confianza en las predicciones. Esta heurística nos permite realizar un equilibrio elegante entre la recuperación y precisión utilizando la puntuación del resultado de mayor rango para ajustar el umbral de las consultas (Sección 4.3) • Actual: Clasificación actual del motor de búsqueda (sección 4.1). Ten en cuenta que la implementación actual del clasificador fue entrenada sobre un superset de los pares de consultas/URL calificados en nuestros conjuntos de datos, pero utilizando las mismas etiquetas de verdad que usamos para nuestra evaluación. División de Entrenamiento/Prueba: La única estrategia para la cual era necesario dividir los conjuntos de datos en entrenamiento y prueba fue el método UserBehavior. Para evaluar el UserBehavior, entrenamos y validamos el 75% de las consultas etiquetadas, y probamos en el 25% restante. El muestreo se realizó por consulta (es decir, todos los resultados para una consulta elegida se incluyeron en el conjunto de datos respectivo, y no hubo superposición de consultas entre los conjuntos de entrenamiento y prueba). Vale la pena señalar que tanto el SA ad-hoc como el SA+N, así como las estrategias basadas en distribución (CD, CDiff y CD+CDiff), no requieren un conjunto de entrenamiento y prueba separado, ya que se basan en heurísticas para detectar frecuencias de clics anómalas en los resultados. Por lo tanto, todas las estrategias excepto UserBehavior fueron probadas en el conjunto completo de consultas y preferencias de relevancia asociadas, mientras que UserBehavior fue probado en un subconjunto de consultas seleccionado al azar como se describe arriba. Para asegurarnos de que no estamos favoreciendo a UserBehavior, también probamos todas las demás estrategias en los mismos conjuntos de prueba de retención, lo que resultó en los mismos resultados de precisión que al probar sobre los conjuntos de datos completos. 6. RESULTADOS Ahora pasamos a la evaluación experimental de la predicción de la preferencia de relevancia de los resultados de búsqueda web. La Figura 6.1 muestra los resultados de recuperación-precisión sobre el conjunto de consultas Q1 (Sección 5.2). Los resultados indican que las estrategias de interpretación de clics anteriores, SA y SA+N, tienen un rendimiento subóptimo en este entorno, mostrando una precisión de 0.627 y 0.638 respectivamente. Además, no hay un mecanismo para hacer un equilibrio entre la recuperación y la precisión con SA y SA+N, ya que no proporcionan confianza en la predicción. En contraste, nuestras técnicas basadas en la distribución de clics CD y CD+CDiff muestran una precisión algo mayor que SA y SA+N (0.648 y 0.717 en Recall de 0.08, máximo logrado por SA o SA+N). SA+N SA 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 Recall Precision SA SA+N CD CDiff CD+CDiff UserBehavior Current Figura 6.1: Precisión vs. Recall de los métodos de predicción de relevancia SA, SA+N, CD, CDiff, CD+CDiff, UserBehavior y Current sobre el conjunto de datos Q1. Curiosamente, CDiff por sí solo muestra una precisión igual a SA (0.627) con el mismo nivel de recuperación de 0.08. Por el contrario, al combinar las estrategias de CD y CDiff (método CD+CDiff) logramos el mejor rendimiento de todas las estrategias basadas en clics, mostrando una precisión superior a 0.66 para valores de recuperación de hasta 0.14, y mayor en niveles de recuperación más bajos. Claramente, la agregación e interpretación inteligente de los clics resulta en una ganancia significativa para la búsqueda web realista, en comparación con las estrategias previamente descritas. Sin embargo, incluso la estrategia de interpretación de clics CD+CDiff puede mejorarse al aprender automáticamente a interpretar la evidencia de clics agregada. Pero primero, consideramos la estrategia de mejor rendimiento, UserBehavior. Incorporar el historial de navegación posterior a la búsqueda además de los clics (funciones de navegación) da como resultado el mayor nivel de recuperación y precisión entre todos los métodos comparados. La búsqueda muestra una precisión superior a 0.7 con un recuerdo de 0.16, superando significativamente nuestras estrategias de línea base y solo de clics. Además, Browse puede lograr un alto recuerdo (tan alto como 0.43) manteniendo una precisión (0.67) significativamente mayor que la clasificación base. Para analizar más a fondo el valor de las diferentes dimensiones de la retroalimentación implícita modeladas por la estrategia UserBehavior, consideramos cada grupo de características de forma individual. La Figura 6.2 muestra la Precisión vs. la Recuperación para cada grupo de características. Curiosamente, solo el texto de consulta tiene una precisión baja (solo ligeramente mejor que al azar). Además, las características de navegación por sí solas tienen una mayor precisión (con un menor máximo de recuperación logrado) que considerar todas las características en nuestro modelo de Comportamiento del Usuario. Aplicar diferentes métodos de aprendizaje automático para combinar las predicciones de los clasificadores puede aumentar el rendimiento al utilizar todas las características para todos los valores de recuperación. 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 Recuperación Precisión Todas las características Clic Consulta-texto Navegación Figura 6.2: Precisión vs. recuperación para predecir relevancia con cada grupo de características individualmente. 0.65 0.67 0.69 0.71 0.73 0.75 0.77 0.79 0.81 0.83 0.85 0.01 0.05 0.09 0.13 0.17 0.21 0.25 0.29 0.33 0.37 0.41 0.45 0.49 Recuperación Precisión CD+CDiff:Q1 ComportamientoDelUsuario:Q1 CD+CDiff:Q10 ComportamientoDelUsuario:Q10 CD+CDiff:Q20 ComportamientoDelUsuario:Q20 Figura 6.3: Recuperación vs. Precisión de CD+CDiff y Comportamiento del Usuario para los conjuntos de consultas Q1, Q10 y Q20 (consultas con al menos 1, al menos 10 y al menos 20 clics respectivamente). Curiosamente, el clasificador entrenado sobre características solo de clics logra un recuerdo y precisión sustancialmente más altos que las estrategias de interpretación de clics diseñadas por humanos descritas anteriormente. Por ejemplo, el clasificador entrenado con clickthrough logra una precisión de 0.67 con un Recall de 0.42 frente al máximo Recall de 0.14 logrado por la estrategia CD+CDiff. Nuestras estrategias de interpretación del comportamiento de clics y usuarios se basan en datos extensos de interacción de usuarios. Consideramos los efectos de tener suficientes datos de interacción disponibles para una consulta antes de proponer una reorganización de los resultados para esa consulta. La Figura 6.3 muestra las curvas de recuperación-precisión para los métodos CD+CDiff y UserBehavior en diferentes conjuntos de consultas de prueba con al menos 1 clic (Q1), 10 clics (Q10) y 20 clics (Q20) disponibles por consulta. No sorprende que CD+CDiff mejore con más clics. Esto indica que la precisión mejorará a medida que estén disponibles más historiales de interacción de usuarios, y más consultas del conjunto Q1 tendrán historiales de interacción completos. De manera similar, la estrategia UserBehavior funciona mejor para consultas con 10 y 20 clics, aunque la mejora es menos dramática que para CD+CDiff. Para consultas con suficientes clics, CD+CDiff muestra una precisión comparable a Browse con un menor recuerdo. 0 0.05 0.1 0.15 0.2 7 12 17 21 Días de datos de interacción del usuario recolectados Recuerdo CD+CDiff ComportamientoDelUsuario Figura 6.4: Recuerdo de las estrategias CD+CDiff y ComportamientoDelUsuario con una precisión mínima fija de 0.7 para diferentes cantidades de datos de actividad del usuario (7, 12, 17, 21 días). Nuestras técnicas a menudo no hacen predicciones de relevancia para los resultados de búsqueda (es decir, si no hay datos de interacción disponibles para los resultados de menor rango), manteniendo así una mayor precisión a expensas de la recuperación. Por el contrario, el motor de búsqueda actual siempre hace una predicción para cada resultado de una consulta dada. Como consecuencia, el recuerdo de Current es alto (0.627) a expensas de una menor precisión. Como otra dimensión para adquirir datos de entrenamiento, consideramos la curva de aprendizaje con respecto a la cantidad (días) de datos de entrenamiento disponibles. La Figura 6.4 informa sobre la Recuperación de las estrategias CD+CDiff y UserBehavior para diferentes cantidades de datos de entrenamiento recopilados a lo largo del tiempo. Establecimos una precisión mínima para ambas estrategias en 0.7, un punto sustancialmente más alto que el valor base (0.625). Como era de esperar, el recuerdo de ambas estrategias mejora rápidamente a medida que se examinan más días de datos de interacción. Ahora resumimos brevemente nuestros resultados experimentales. Mostramos que al agregar de manera inteligente los clics de los usuarios en varias consultas y usuarios, podemos lograr una mayor precisión al predecir las preferencias de los usuarios. Debido a la distribución sesgada de los clics de los usuarios, nuestras estrategias basadas únicamente en clics tienen una alta precisión, pero baja recuperación (es decir, no intentan predecir la relevancia de muchos resultados de búsqueda). Sin embargo, nuestra estrategia de clics CD+CDiff supera con creces la mayoría de los resultados más recientes de vanguardia (0.72 de precisión para CD+CDiff frente a 0.64 para SA+N) en el nivel de recuperación más alto de SA+N. Además, al considerar las completas características de UserBehavior que modelan las interacciones del usuario después de la búsqueda y más allá del clic inicial, podemos lograr una precisión y recuperación sustancialmente más altas que al considerar solo el clic. Nuestra estrategia de UserBehavior logra un recuerdo de más de 0.43 con una precisión de más de 0.67 (con una precisión mucho mayor en niveles de recuerdo más bajos), superando sustancialmente la clasificación de preferencia del motor de búsqueda actual y todos los demás métodos de interpretación de retroalimentación implícita. 7. CONCLUSIONES Y TRABAJO FUTURO Nuestro artículo es, hasta donde sabemos, el primero en interpretar el comportamiento del usuario después de la búsqueda para estimar las preferencias del usuario en un entorno real de búsqueda web. Demostramos que nuestros modelos robustos resultan en una mayor precisión de predicción que las técnicas previamente publicadas. Introdujimos nuevas técnicas robustas y probabilísticas para interpretar la evidencia de clics al agregar información de usuarios y consultas. Nuestros métodos resultan en una interpretación de clics sustancialmente más precisa que los resultados previamente publicados que no fueron específicamente diseñados para escenarios de búsqueda en la web. Nuestros métodos de predicción de preferencias de relevancia son sustancialmente más precisos que la clasificación actual de resultados de búsqueda de vanguardia que no considera las interacciones del usuario. También presentamos un modelo general para interpretar el comportamiento del usuario después de la búsqueda que incorpora características de clics, navegación y consultas. Al considerar la experiencia de búsqueda completa después de la consulta inicial y el clic, demostramos una precisión de predicción que supera con creces la de interpretar solo la información limitada de los clics. Además, demostramos que aprender automáticamente a interpretar el comportamiento del usuario resulta en un rendimiento sustancialmente mejor que las estrategias de interpretación de clics ad-hoc diseñadas por humanos. Otro beneficio de aprender automáticamente a interpretar el comportamiento del usuario es que dichos métodos pueden adaptarse a condiciones cambiantes y perfiles de usuario cambiantes. Por ejemplo, el modelo de comportamiento del usuario en la búsqueda de intranet puede ser diferente del comportamiento de búsqueda en la web. Nuestro método general de UserBehavior sería capaz de adaptarse a estos cambios aprendiendo automáticamente a mapear nuevos patrones de comportamiento a calificaciones de relevancia explícitas. Una aplicación natural de nuestros modelos de predicción de preferencias es mejorar la clasificación de búsqueda en la web [1]. Además, nuestro trabajo tiene muchas aplicaciones potenciales, incluyendo la detección de clics fraudulentos, la detección de abuso en búsquedas, la personalización y la clasificación específica de dominios. Por ejemplo, nuestros modelos de comportamiento derivados automáticamente podrían ser entrenados con ejemplos de abuso de búsqueda o comportamiento de spam de clics en lugar de etiquetas de relevancia. Alternativamente, nuestros modelos podrían ser utilizados directamente para detectar anomalías en el comportamiento de los usuarios, ya sea debido a abusos o a problemas operativos con el motor de búsqueda. Si bien nuestras técnicas funcionan bien en promedio, nuestras suposiciones sobre las distribuciones de clics (y el aprendizaje de los modelos de comportamiento del usuario) pueden no ser igualmente válidas para todas las consultas. Por ejemplo, las consultas con patrones de acceso divergentes (por ejemplo, consultas ambiguas con múltiples significados) pueden dar lugar a un comportamiento inconsistente con el modelo aprendido para todas las consultas. Por lo tanto, agrupar consultas y aprender diferentes modelos predictivos para cada tipo de consulta es una dirección de investigación prometedora. Las distribuciones de las consultas también cambian con el tiempo, y sería productivo investigar cómo eso afecta la capacidad predictiva de estos modelos. Además, algunas preferencias predichas pueden ser más valiosas que otras, y planeamos investigar diferentes métricas para capturar la utilidad de las preferencias predichas. Como mostramos en este artículo, el uso de la sabiduría de las multitudes puede brindarnos una interpretación precisa de las interacciones de los usuarios incluso en el entorno inherentemente ruidoso de la búsqueda en la web. Nuestras técnicas nos permiten predecir automáticamente las preferencias de relevancia para los resultados de búsqueda en la web con una precisión mayor que los métodos previamente publicados. Las preferencias de relevancia predichas pueden ser utilizadas para la evaluación automática de relevancia y ajuste, para implementar búsquedas en nuevos entornos y, en última instancia, para mejorar la experiencia general de búsqueda en la web. REFERENCIAS [1] E. Agichtein, E. Brill y S. Dumais, Mejorando la clasificación de búsqueda en la web mediante la incorporación del comportamiento del usuario, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006 [2] J. Allan. Resumen de la pista HARD en TREC 2003: Recuperación de alta precisión de documentos. En Actas de TREC 2003, 24-37, 2004. [3] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala. En Actas de WWW7, 107-117, 1998. [4] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender, Aprendizaje para clasificación utilizando descenso de gradiente, en Actas de la Conferencia Internacional sobre Aprendizaje Automático (ICML), 2005 [5] D.M. Chickering, The WinMine Toolkit, Informe Técnico de Microsoft MSR-TR-2002-103, 2002 [6] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario, en IEEE Internet Computing. 2001 [7] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluando medidas implícitas para mejorar la experiencia de búsqueda. En ACM Transactions on Information Systems, 2005 [8] J. Goecks y J. Shavlick. Aprendiendo los intereses de los usuarios observando de manera discreta su comportamiento normal. En Actas del Taller de IJCAI sobre Aprendizaje Automático para Filtrado de Información. 1999. [9] T. Joachims, Optimización de Motores de Búsqueda Utilizando Datos de Clics, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (SIGKDD), 2002 [10] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, Interpretación Precisa de Datos de Clics como Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2005 [11] T. Joachims, Haciendo que el Aprendizaje SVM a Gran Escala Sea Práctico. Avances en Métodos de Núcleo, en Aprendizaje de Vectores de Soporte, MIT Press, 1999 [12] D. Kelly y J. Teevan, Retroalimentación implícita para inferir preferencias de usuario: una bibliografía. En el Foro SIGIR, 2003 [13] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. GroupLens: Aplicando filtrado colaborativo a las noticias de Usenet. En Comunicaciones de ACM, 1997. [14] M. Morita y Y. Shinoda, Filtrado de información basado en análisis del comportamiento del usuario y recuperación de texto de mejor coincidencia. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 1994 [15] D. Oard y J. Kim. Retroalimentación implícita para sistemas de recomendación. en Actas del Taller de Sistemas de Recomendación de AAAI. 1998 [16] D. Oard y J. Kim. Modelando el contenido de la información utilizando el comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencia de la Información y Tecnología. 2001 [17] P. Pirolli, El Uso del Rastro de Información Próxima para Buscar Contenido Distal en la World Wide Web. Trabajando con la tecnología en mente: Brunswikiano. Recursos para la Ciencia Cognitiva e Ingeniería, Oxford University Press, 2004 [18] F. Radlinski y T. Joachims, Cadenas de Consultas: Aprendizaje para Clasificar a partir de Retroalimentación Implícita, en Actas de la Conferencia de la ACM sobre Descubrimiento de Conocimiento y Minería de Datos (KDD), ACM, 2005 [19] F. Radlinski y T. Joachims, Evaluando la Robustez del Aprendizaje a partir de Retroalimentación Implícita, en el Taller de ICML sobre Aprendizaje en la Búsqueda Web, 2005 [20] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [21] E.M. Voorhees, D. Harman, Resumen de TREC, 2001