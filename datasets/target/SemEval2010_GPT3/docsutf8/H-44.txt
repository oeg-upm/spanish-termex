Una máquina del tiempo para la búsqueda de texto Klaus Berberich Srikanta Bedathur Thomas Neumann Gerhard Weikum Instituto Max-Planck de Informática Saarbrücken, Alemania {kberberi, bedathur, neumann, weikum}@mpi-inf.mpg.de RESUMEN La búsqueda de texto en colecciones de documentos versionados temporalmente, como los archivos web, ha recibido poca atención como problema de investigación. Como consecuencia, no hay una solución escalable y basada en principios para buscar una colección en un tiempo especificado t. En este trabajo, abordamos esta deficiencia y proponemos una solución eficiente para la búsqueda de texto de viaje en el tiempo mediante la extensión del índice de archivo invertido para que esté listo para la búsqueda temporal. Introducimos la coalescencia temporal aproximada como un método ajustable para reducir el tamaño del índice sin afectar significativamente la calidad de los resultados. Para mejorar aún más el rendimiento de las consultas de viaje en el tiempo, introducimos dos técnicas fundamentales para intercambiar el tamaño del índice por su rendimiento. Estas técnicas pueden formularse como problemas de optimización que pueden resolverse casi óptimamente. Finalmente, nuestro enfoque es evaluado en una serie exhaustiva de experimentos en dos conjuntos de datos del mundo real a gran escala. Los resultados muestran de manera inequívoca que nuestros métodos hacen posible construir una máquina del tiempo eficiente escalable a grandes colecciones de textos versionados. Categorías y Descriptores de Asignaturas H.3.1 [Análisis de Contenido e Indexación]: Métodos de indexación; H.3.3 [Búsqueda de Información y Recuperación]: Modelos de recuperación, Proceso de búsqueda Términos Generales Algoritmos, Experimentación, Rendimiento 1. En este trabajo abordamos la búsqueda de texto de viaje en el tiempo en colecciones de documentos versionados temporalmente. Dado un término de búsqueda q y un tiempo t, nuestro objetivo es identificar y clasificar documentos relevantes como si la colección estuviera en su estado en el tiempo t. Un número creciente de colecciones de documentos versionados está disponible hoy en día, incluidos archivos web, entornos de autoría colaborativa como Wikis o flujos de información con marcas de tiempo. La búsqueda de texto en estas colecciones, sin embargo, es en su mayoría ajena al tiempo: mientras que la colección buscada cambia con el tiempo, a menudo solo se indexa la versión más reciente de un documento, o las versiones se indexan de forma independiente y se tratan como documentos separados. Aún peor, para algunas colecciones, en particular archivos web como el Internet Archive [18], a menudo falta por completo una funcionalidad de búsqueda de texto integral. La búsqueda de texto de viaje en el tiempo, tal como la desarrollamos en este documento, es una herramienta crucial para explorar estas colecciones y desplegar todo su potencial, como lo demuestra el siguiente ejemplo. Para un documental sobre un escándalo político pasado, un periodista necesita investigar las opiniones y declaraciones iniciales hechas por los políticos involucrados. Enviando una consulta adecuada a un motor de búsqueda web importante, la mayoría de los resultados devueltos contienen solo cobertura reciente, ya que muchas de las primeras páginas web han desaparecido y solo se conservan en archivos web. Si la consulta pudiera enriquecerse con un punto temporal, digamos el 20 de agosto de 2003 como el día después de que se revelara el escándalo, y se emitiera contra un archivo web, solo se podrían recuperar las páginas que existían específicamente en ese momento, satisfaciendo así mejor la necesidad de información de los periodistas. Colecciones de documentos como la Web o Wikipedia [32], tal como las consideramos aquí, ya son grandes si solo se toma en cuenta una única instantánea. Al observar su historia evolutiva, nos enfrentamos a volúmenes de datos aún más grandes. Como consecuencia, los enfoques ingenuos para la búsqueda de texto en viajes en el tiempo fallan, y los enfoques viables deben escalar bien para tales volúmenes de datos grandes. Este documento presenta una solución eficiente para la búsqueda de texto de viaje en el tiempo al hacer las siguientes contribuciones clave: 1. El popular y bien estudiado índice de archivo invertido [35] se extiende de manera transparente para permitir la búsqueda de texto de viaje en el tiempo. 2. La coalescencia temporal se introduce para evitar una explosión del tamaño del índice manteniendo resultados altamente precisos. Desarrollamos dos técnicas de materialización de sublistas para mejorar el rendimiento del índice que permiten intercambiar espacio por rendimiento. 4. En una evaluación experimental exhaustiva, nuestro enfoque se evalúa en la Wikipedia en inglés y partes del Archivo de Internet como dos conjuntos de datos del mundo real a gran escala con documentos versionados. El resto de este documento está organizado de la siguiente manera. El trabajo presentado se sitúa en contexto con trabajos relacionados en la Sección 2. Delimitamos nuestro modelo de una colección de documentos versionados temporalmente en la Sección 3. Presentamos nuestro índice invertido de viaje en el tiempo en la Sección 4. Basándose en ello, la coalescencia temporal se describe en la Sección 5. En la Sección 6 describimos técnicas fundamentadas para mejorar el rendimiento del índice, antes de presentar los resultados de nuestra evaluación experimental en la Sección 7. TRABAJO RELACIONADO Podemos clasificar el trabajo relacionado principalmente en las siguientes dos categorías: (i) métodos que tratan explícitamente con colecciones de documentos versionados o bases de datos temporales, y (ii) métodos para reducir el tamaño del índice aprovechando la superposición de contenido de documentos o podando porciones del índice. Breve revisamos el trabajo bajo estas categorías aquí. Hasta donde sabemos, hay muy pocos trabajos previos que traten sobre la búsqueda histórica en documentos versionados temporalmente. Anick y Flynn [3], mientras pioneros en esta investigación, describen un sistema de ayuda que respalda consultas históricas. Los costos de acceso están optimizados para acceder a las versiones más recientes y aumentan a medida que se retrocede en el tiempo. Burrows y Hisgen [10], en una descripción de patente, delinean un método para indexar valores basados en rangos y mencionan su posible uso para la búsqueda basada en fechas asociadas con documentos. El trabajo reciente de Nørv˚ag y Nybø [25] y sus propuestas anteriores se centran en el problema relativamente más simple de apoyar solo consultas de contención de texto y descuidan la puntuación de relevancia de los resultados. Stack [29] informa sobre experiencias prácticas realizadas al adaptar el motor de búsqueda de código abierto Nutch para buscar en archivos web. Sin embargo, esta adaptación no proporciona la funcionalidad de búsqueda de texto de viaje en el tiempo prevista. Por el contrario, la investigación en bases de datos temporales ha producido varias estructuras de índices diseñadas para bases de datos en constante evolución temporal; una visión general exhaustiva del estado del arte está disponible en [28]. A diferencia del índice de archivo invertido, su aplicabilidad a la búsqueda de texto no está bien comprendida. Pasando a la segunda categoría de trabajos relacionados, Broder et al. [8] describen una técnica que explota grandes superposiciones de contenido entre documentos para lograr una reducción en el tamaño del índice. Su técnica hace suposiciones fuertes sobre la estructura de las superposiciones de documentos, lo que la hace inaplicable a nuestro contexto. Enfoques más recientes de Hersovici et al. [17] y Zhang y Suel [34] explotan superposiciones de contenido arbitrarias entre documentos para reducir el tamaño del índice. Ninguno de los enfoques, sin embargo, considera el tiempo de forma explícita o proporciona la funcionalidad deseada de búsqueda de texto de viaje en el tiempo. Las técnicas de poda de índices estáticos [11, 12] tienen como objetivo reducir el tamaño efectivo del índice, eliminando porciones del índice que se espera que tengan poco impacto en el resultado de la consulta. Tampoco consideran los aspectos temporales de los documentos, por lo que son técnicamente bastante diferentes de nuestra propuesta a pesar de tener un objetivo compartido de reducción del tamaño del índice. Cabe destacar que las técnicas de poda de índices pueden adaptarse para funcionar junto con el índice de texto temporal que proponemos aquí. 3. En el presente trabajo, nos ocupamos de una colección de documentos D versionada temporalmente que se modela como se describe a continuación. Cada documento d ∈ D es una secuencia de sus versiones d = dt1 , dt2 , . . . . Cada versión dti tiene un sello de tiempo asociado ti que refleja cuándo se creó la versión. Cada versión es un vector de términos o características buscables. Cualquier modificación a una versión de un documento resulta en la inserción de una nueva versión con la marca de tiempo correspondiente. Empleamos una definición discreta del tiempo, de modo que las marcas de tiempo son enteros no negativos. La eliminación de un documento en el tiempo ti, es decir, su desaparición del estado actual de la colección, se modela como la inserción de una versión especial de lápida ⊥. El intervalo de tiempo de validez val(dti) de una versión dti es [ti, ti+1), si existe una versión más reciente con una marca de tiempo asociada ti+1, y [ti, ahora) en caso contrario, donde ahora apunta al mayor valor posible de una marca de tiempo (es decir, ∀t: t < ahora). Reuniendo todo esto, definimos el estado Dt de la colección en el tiempo t (es decir, el conjunto de versiones válidas en t que no son eliminaciones) como Dt = [ d∈D {dti ∈ d | t ∈ val(dti ) ∧ dti = ⊥} . Como se mencionó anteriormente, queremos enriquecer una consulta de palabras clave q con una marca de tiempo t, de modo que q se evalúe sobre Dt, es decir, el estado de la colección en el tiempo t. La consulta enriquecida de viaje en el tiempo se escribe como q t para mayor brevedad. Como modelo de recuperación en este trabajo adoptamos Okapi BM25 [27], pero cabe destacar que las técnicas propuestas no dependen de esta elección y son aplicables a otros modelos de recuperación como tf-idf [4] o modelos de lenguaje [26] también. Para nuestro entorno considerado, adaptamos ligeramente Okapi BM25 como w(q t , dti ) = X v∈q wtf (v, dti ) · widf (v, t) . En la fórmula anterior, se define la relevancia w(q t , dti ) de una versión del documento dti para la consulta de viaje en el tiempo q t. Reiteramos que q t se evalúa sobre Dt para que solo se considere la versión dti válida en el tiempo t. El primer factor wtf (v, dti) en la suma, posteriormente referido como el tfscore, se define como wtf (v, dti) = (k1 + 1) · tf(v, dti) / k1 · ((1 − b) + b · dl(d ti) / avdl(ti)) + tf(v, dti). Considera la frecuencia simple del término tf(v, dti) del término v en la versión dti normalizándola, teniendo en cuenta tanto la longitud dl(dti) de la versión como la longitud promedio del documento avdl(ti) en la colección en el tiempo ti. El parámetro de normalización de longitud b y el parámetro de saturación de tf k1 se heredan del Okapi BM25 original y comúnmente se establecen en los valores 1.2 y 0.75 respectivamente. El segundo factor widf (v, t), al que nos referimos como la puntuación idf en el resto, transmite la frecuencia inversa del documento del término v en la colección en el tiempo t y se define como widf (v, t) = log N(t) − df(v, t) + 0.5 df(v, t) + 0.5 donde N(t) = |Dt | es el tamaño de la colección en el tiempo t y df(v, t) da el número de documentos en la colección que contienen el término v en el tiempo t. Mientras que la puntuación idf depende de todo el corpus en el momento de la consulta t, la puntuación tf es específica para cada versión. El índice de archivo invertido es una técnica estándar para la indexación de texto, utilizada en muchos sistemas. En esta sección, revisamos brevemente esta técnica y presentamos nuestras extensiones al índice de archivo invertido que lo preparan para la búsqueda de texto en viajes en el tiempo. 4.1 Índice de Archivo Invertido Un índice de archivo invertido consiste en un vocabulario, comúnmente organizado como un árbol B+, que mapea cada término a su puntaje idf y lista invertida. La lista de índice Lv perteneciente al término v contiene entradas de la forma (d, p) donde d es un identificador de documento y p es la carga útil llamada así. La carga útil p contiene información sobre la frecuencia del término v en d, pero también puede incluir información posicional sobre dónde aparece el término en el documento. El orden de clasificación de las listas de índices depende de qué consultas se deben admitir de manera eficiente. Para consultas booleanas es favorable ordenar las listas de índices en orden de documentos. Las listas de índices ordenadas por frecuencia y por impacto son beneficiosas para consultas clasificadas y permiten un procesamiento de consultas optimizado que se detiene temprano después de haber identificado los k documentos más relevantes [1, 2, 9, 15, 31]. Se han propuesto una variedad de técnicas de compresión, como codificar de forma más compacta los identificadores de documentos, para reducir el tamaño de las listas de índices. Para obtener una excelente encuesta reciente sobre índices de archivos invertidos, nos referimos a [35]. 4.2 Índice de Archivos Invertidos de Viaje en el Tiempo. Para preparar un índice de archivos invertidos para viajes en el tiempo, extendemos tanto las listas invertidas como la estructura de vocabulario incorporando explícitamente información temporal. La idea principal de las listas invertidas es que incluimos un intervalo de tiempo de validez [tb, te) en las entradas para indicar cuándo la información de carga útil era válida. Las publicaciones en nuestro índice de archivo invertido de viaje en el tiempo tienen la forma (d, p, [tb, te)) donde d y p se definen como en el índice de archivo invertido estándar anterior y [tb, te) es el intervalo de tiempo de validez. Como ejemplo concreto, en nuestra implementación, para una versión dti que tiene el puntaje tf Okapi BM25 wtf (v, dti) para el término v, la lista de índices Lv contiene la publicación (d, wtf (v, dti), [ti, ti+1)). De manera similar, la estructura de vocabulario extendido mantiene para cada término una serie temporal de puntuaciones idf organizadas como un árbol B+. A diferencia del tf-score, el idf-score de cada término podría variar con cada cambio en el corpus. Por lo tanto, adoptamos un enfoque simplificado para el mantenimiento de la puntuación idf, calculando las puntuaciones idf para todos los términos en el corpus en momentos específicos (posiblemente periódicos). Durante el procesamiento de una consulta de viaje en el tiempo q t , para cada término de la consulta se recupera la puntuación idf correspondiente válida en el tiempo t del vocabulario extendido. Entonces, las listas de índices se leen secuencialmente desde el disco, acumulando así la información contenida en las entradas. Extendemos de manera transparente la lectura secuencial, que es, según nuestro conocimiento, común a todas las técnicas de procesamiento de consultas en índices de archivos invertidos, haciéndolos adecuados para el procesamiento de consultas de viaje en el tiempo. Con este fin, la lectura secuencial se extiende al omitir todas las publicaciones cuyo intervalo de tiempo de validez no contiene t (es decir, t ∈ [tb, te)). Si se puede omitir una publicación solo se puede decidir después de que la publicación se haya transferido del disco a la memoria y, por lo tanto, todavía incurre en un costo significativo de E/S. Como remedio, proponemos técnicas de organización de índices en la Sección 6 que tienen como objetivo reducir significativamente la sobrecarga de E/S. Observamos que nuestra propuesta de extensión del índice de archivo invertido no hace suposiciones sobre el orden de clasificación de las listas de índices. Como consecuencia, las técnicas existentes de procesamiento de consultas y la mayoría de las optimizaciones (por ejemplo, técnicas de compresión) siguen siendo igualmente aplicables. 5. Si empleamos el índice invertido de viaje en el tiempo, como se describe en la sección anterior, en una colección de documentos versionados, obtenemos una entrada por término por versión del documento. Para términos frecuentes y colecciones grandes altamente dinámicas, este puntaje de tiempo no coalescido coalescido Figura 1: La coalescencia temporal aproximada conduce a listas de índices extremadamente largas con un rendimiento de procesamiento de consultas muy pobre. La técnica de coalescencia temporal aproximada que proponemos en esta sección contrarresta este aumento en el tamaño de la lista de índices. Se basa en la observación de que la mayoría de los cambios en una colección de documentos versionados son menores, dejando grandes partes del documento intactas. Como consecuencia, la carga útil de muchas publicaciones pertenecientes a versiones temporalmente adyacentes diferirá solo ligeramente o no diferirá en absoluto. La coalescencia temporal aproximada reduce el número de publicaciones en una lista de índices al fusionar una secuencia de publicaciones que tienen cargas casi iguales, manteniendo el error máximo acotado. Esta idea se ilustra en la Figura 1, que representa las puntuaciones no coalescidas y coalescidas de las publicaciones pertenecientes a un solo documento. La coalescencia temporal aproximada es muy efectiva dadas las cargas fluctuantes y reduce el número de publicaciones de 9 a 3 en el ejemplo. La noción de coalescencia temporal fue introducida originalmente en la investigación de bases de datos temporales por Böhlen et al. [6], donde se consideró el problema más simple de coalescer solo información igual. A continuación, formulamos formalmente el problema tratado en la coalescencia temporal aproximada y discutimos el cálculo de soluciones óptimas y aproximadas. Ten en cuenta que la técnica se aplica a cada lista de índices por separado, por lo que las siguientes explicaciones asumen un término fijo v y una lista de índices Lv. Como entrada, se nos da una secuencia de publicaciones temporalmente adyacentes I = ( d, pi, [ti, ti+1) ), . . . , ( d, pn−1, [tn−1, tn)) ) . Cada secuencia representa un período de tiempo contiguo durante el cual el término estuvo presente en un único documento d. Si un término desaparece de d pero reaparece más tarde, obtenemos múltiples secuencias de entrada que se tratan por separado. Buscamos generar la secuencia de publicaciones de longitud mínima O = (d, pj, [tj, tj+1), ..., (d, pm−1, [tm−1, tm))) que cumpla con las siguientes restricciones: Primero, O e I deben cubrir el mismo rango de tiempo, es decir, ti = tj y tn = tm. Segundo, al fusionar una subsecuencia de publicaciones de la entrada en una sola publicación de la salida, queremos que el error de aproximación esté por debajo de un umbral. En otras palabras, si (d, pi, [ti, ti+1)) y (d, pj, [tj, tj+1)) son publicaciones de I y O respectivamente, entonces lo siguiente debe cumplirse para una función de error elegida y un umbral: tj ≤ ti ∧ ti+1 ≤ tj+1 ⇒ error(pi, pj) ≤ . En este documento, como función de error empleamos el error relativo entre las cargas útiles (es decir, puntuaciones tf) de un documento en I y O, definido como: errrel(pi, pj) = |pi − pj| / |pi|. Encontrar una secuencia óptima de publicaciones de salida puede ser planteado como encontrar una representación constante a trozos para los puntos (ti, pi) que utilice un número mínimo de segmentos mientras se conserva la garantía de aproximación mencionada anteriormente. Problemas similares ocurren en la segmentación de series temporales [21, 30] y en la construcción de histogramas [19, 20]. Normalmente la programación dinámica se aplica para obtener una solución óptima en un tiempo de O(n2 m∗) [20, 30], donde m∗ es el número de segmentos en una secuencia óptima. En nuestro entorno, como diferencia clave, solo se conserva una garantía sobre el error local, en contraste con una garantía sobre el error global en los entornos mencionados anteriormente. Explotando este hecho, una solución óptima es computable mediante inducción [24] en tiempo O(n2). Los detalles del algoritmo óptimo se omiten aquí pero se pueden encontrar en el informe técnico adjunto [5]. La complejidad cuadrática del algoritmo óptimo lo hace inapropiado para los grandes conjuntos de datos encontrados en este trabajo. Como alternativa, presentamos un algoritmo aproximado de tiempo lineal que se basa en el algoritmo de ventana deslizante presentado en [21]. Este algoritmo produce secuencias de salida casi óptimas que mantienen el límite en el error relativo, pero posiblemente requieren unos pocos segmentos adicionales más que una solución óptima. Algoritmo 1 Coalescencia Temporal (Aproximada) 1: I = ( d, pi, [ti, ti+1) ), . . . El algoritmo 1 realiza un pase sobre la secuencia de entrada I. Mientras lo hace, coalesce secuencias de publicaciones de longitud máxima. El representante óptimo para una secuencia de publicaciones depende solo de su carga mínima y máxima (pmin y pmax) y se puede buscar utilizando optrep en O(1) (ver [16] para más detalles). Al leer la siguiente publicación, el algoritmo intenta agregarla a la secuencia actual de publicaciones. Calcula el nuevo representante hipotético p y verifica si mantendría la garantía de aproximación. Si esta prueba falla, se agrega una publicación coalescida que lleva el representante antiguo a la secuencia de salida O y, después de eso, se reinicia la contabilidad. La complejidad temporal del algoritmo es de O(n). Ten en cuenta que, dado que no hacemos suposiciones sobre el orden de clasificación de las listas de índices, los algoritmos de coalescencia temporal tienen un costo de preprocesamiento adicional en O(|Lv| log |Lv|) para ordenar la lista de índices y dividirla en subsecuencias para cada documento. 6. La eficiencia de procesamiento de una consulta q t en nuestro índice invertido de viaje en el tiempo se ve afectada negativamente por la E/S desperdiciada debido a la lectura de publicaciones omitidas. La coalescencia temporal aborda implícitamente este problema al reducir el tamaño total de la lista de índices, pero aún queda un sobrecoste significativo. En esta sección, abordamos este problema proponiendo la idea de materializar sublistas, cada una de las cuales corresponde a un subintervalo contiguo de tiempo abarcado por el índice completo. Cada una de estas sub-listas contiene todas las publicaciones fusionadas que se superponen con el intervalo de tiempo correspondiente de la sub-lista. Ten en cuenta que todas aquellas publicaciones cuyo intervalo de tiempo de validez abarca los límites temporales de varias sub-listas se replican en cada una de las sub-listas abarcadas. Por lo tanto, para procesar la consulta q en el tiempo t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 d1 d2 d3 del documento 1 2 3 4 5 6 7 8 9 10, Figura 2: Materialización de sublistas, es suficiente escanear cualquier sublista materializada cuyo intervalo de tiempo contenga t. Ilustramos la idea de la materialización de sublistas utilizando un ejemplo mostrado en la Figura 2. La lista de índices Lv visualizada en la figura contiene un total de 10 entradas de tres documentos d1, d2 y d3. Para facilitar la descripción, hemos numerado los límites de los intervalos de tiempo de validez, en orden creciente de tiempo, como t1, . . . , t10 y numerado las publicaciones mismas como 1, . . . , 10. Ahora, considera el procesamiento de una consulta q t con t ∈ [t1, t2) utilizando esta lista invertida. Aunque solo tres publicaciones (publicaciones 1, 5 y 8) son válidas en el tiempo t, la lista invertida completa debe ser leída en el peor de los casos. Supongamos que dividimos el eje del tiempo de la lista en el tiempo t2, formando dos sub-listas con los elementos {1, 5, 8} y {2, 3, 4, 5, 6, 7, 8, 9, 10} respectivamente. Entonces, podemos procesar la consulta anterior con un costo óptimo leyendo solo aquellas publicaciones que existían en este momento t. A primera vista, puede parecer contraintuitivo reducir el tamaño del índice en el primer paso (usando la coalescencia temporal) y luego aumentarlo nuevamente utilizando las técnicas de materialización de sublistas presentadas en esta sección. Sin embargo, reiteramos que nuestro objetivo principal es mejorar la eficiencia del procesamiento de consultas, no solo reducir el tamaño del índice. El uso de la coalescencia temporal mejora el rendimiento al reducir el tamaño del índice, mientras que la materialización de sublistas mejora el rendimiento al replicar entradas de manera juiciosa. Además, las dos técnicas pueden aplicarse por separado y son independientes. Si se aplican en conjunto, sin embargo, hay un efecto sinérgico: las sublistas que se materializan a partir de un índice temporalmente fusionado suelen ser más pequeñas. Empleamos la notación Lv : [ti, tj) para referirnos a la sublista materializada para el intervalo de tiempo [ti, tj), que está formalmente definida como, Lv : [ti, tj) = {( d, p, [tb, te) ) ∈ Lv | tb < tj ∧ te > ti}. Para ayudar en la presentación en el resto del documento, primero proporcionamos algunas definiciones. Sea T = t1 . . . tn la secuencia ordenada de todos los límites de intervalo de tiempo únicos de una lista invertida Lv. Entonces definimos E = { [ti, ti+1) | 1 ≤ i < n} como el conjunto de intervalos de tiempo elementales. Nos referimos al conjunto de intervalos de tiempo para los cuales se materializan las sublistas como M ⊆ { [ti, tj) | 1 ≤ i < j ≤ n }, y exigimos que ∀ t ∈ [t1, tn) ∃ m ∈ M : t ∈ m, es decir, los intervalos de tiempo en M deben cubrir completamente el intervalo de tiempo [t1, tn), para que las consultas de viaje en el tiempo q t para todos los t ∈ [t1, tn) puedan ser procesadas. También asumimos que los intervalos en M son disjuntos. Podemos hacer esta suposición sin descartar ninguna solución óptima en cuanto al espacio o rendimiento definidos a continuación. El espacio requerido para la materialización de sublistas en un conjunto M se define como S( M ) = X m∈M |Lv : m|, es decir, la longitud total de todas las listas en M. Dado un conjunto M, permitimos que π( [ti, ti+1) ) = [tj, tk) ∈ M : [ti, ti+1) ⊆ [tj, tk) denote el intervalo de tiempo que se utiliza para procesar consultas q t con t ∈ [ti, ti+1). El rendimiento del procesamiento de consultas q t para t ∈ [ti, ti+1) depende inversamente de su costo de procesamiento PC( [ti, ti+1) ) = |Lv : π( [ti, ti+1) )| , que se asume proporcional a la longitud de la lista Lv : π( [ti, ti+1) ). Por lo tanto, para optimizar el rendimiento del procesamiento de consultas, minimizamos sus costos de procesamiento. Enfoques de rendimiento/espacio óptimos. Una estrategia para eliminar el problema de las entradas omitidas es materializar ansiosamente sub-listas para todos los intervalos de tiempo elementales, es decir, elegir M = E. Al hacerlo, para cada consulta q t solo se leen las entradas válidas en el tiempo t y, por lo tanto, se logra el mejor rendimiento posible. Por lo tanto, nos referiremos a este enfoque como Popt en lo sucesivo. El enfoque inicial descrito anteriormente que mantiene solo la lista completa Lv y por lo tanto elige M = { [t1, tn) } se denomina Sopt en el resto. Este enfoque requiere un espacio mínimo, ya que mantiene cada publicación exactamente una vez. Popt y Sopt son extremos: el primero ofrece el mejor rendimiento posible pero no es eficiente en espacio, el segundo requiere un espacio mínimo pero no proporciona un buen rendimiento. Los dos enfoques presentados en el resto de esta sección permiten intercambiar de manera mutua espacio y rendimiento, y por lo tanto pueden considerarse como medios para explorar el espectro de configuración entre el enfoque Popt y el enfoque Sopt. Enfoque de Garantía de Rendimiento El enfoque Popt claramente desperdicia mucho espacio materializando muchas sublistas casi idénticas. En el ejemplo ilustrado en la Figura 2, las sublistas materializadas para [t1, t2) y [t2, t3) difieren solo por una publicación. Si en lugar de eso se materializara la sublista para [t1, t3), se podría ahorrar un espacio significativo incurriendo solo en un sobrecosto de un posting omitido para todos los t ∈ [t1, t3). La técnica presentada a continuación se basa en la idea de que es posible lograr ahorros significativos de espacio en comparación con Popt, si se puede tolerar una pérdida limitada en el rendimiento, o dicho de otra manera, si se desea mantener una garantía de rendimiento en relación con el óptimo. En detalle, la técnica, a la que nos referimos como PG (Garantía de Rendimiento) en el resto del documento, encuentra un conjunto M que tiene el espacio mínimo requerido, pero garantiza que para cualquier intervalo de tiempo elemental [ti, ti+1) (y por lo tanto para cualquier consulta q t con t ∈ [ti, ti+1)) el rendimiento sea como máximo un factor de γ ≥ 1 peor que el óptimo. Formalmente, este problema se puede expresar como argmin M S( M ) sujeto a ∀ [ti, ti+1) ∈ E : PC( [ti, ti+1) ) ≤ γ · |Lv : [ti, ti+1)| . Una solución óptima al problema puede ser calculada mediante inducción utilizando la recurrencia C( [t1, tk+1) ) = min {C( [t1, tj) ) + |Lv : [tj, tk+1)| | 1 ≤ j ≤ k ∧ condición} , donde C( [t1, tj) ) es el costo óptimo (es decir, el espacio requerido) para el subproblema de prefijo { [ti, ti+1) ∈ E | [ti, ti+1) ⊆ [t1, tj) } y la condición significa ∀ [ti, ti+1) ∈ E : [ti, ti+1) ⊆ [tj, tk+1) ⇒ |Lv : [tj, tk+1)| ≤ γ · |Lv : [ti, ti+1)| . De manera intuitiva, la recurrencia establece que una solución óptima para [t1, tk+1) se puede combinar a partir de una solución óptima para un subproblema de prefijo C( [t1, tj) ) y un intervalo de tiempo [tj, tk+1) que se puede materializar sin violar la garantía de rendimiento. El pseudocódigo del algoritmo se omite por razones de espacio, pero se puede encontrar en el informe técnico adjunto [5]. La complejidad temporal del algoritmo es O(n^2) - para cada subproblema de prefijo, la recurrencia anterior debe ser evaluada, lo cual es posible en tiempo lineal si los tamaños de la lista |L: [ti, tj)| están precalculados. La complejidad espacial es de O(n2) - el costo de mantener las longitudes de sublistas precalculadas y memorizar soluciones óptimas a subproblemas de prefijo. Enfoque de límite de espacio. Hasta ahora hemos considerado el problema de materializar sublistas que garantizan un rendimiento mientras requieren un espacio mínimo. En muchas situaciones, sin embargo, el espacio de almacenamiento es limitado y el objetivo sería materializar un conjunto de sublistas que optimice el rendimiento esperado sin exceder un límite de espacio dado. La técnica presentada a continuación, llamada SB, aborda este mismo problema. La restricción de espacio está modelada mediante un parámetro κ especificado por el usuario, con κ ≥ 1, que limita el aumento máximo permitido en el tamaño del índice respecto a la solución óptima en espacio proporcionada por Sopt. La técnica SB busca encontrar un conjunto M que se adhiera a este límite de espacio pero minimice el costo de procesamiento esperado (y así optimice el rendimiento esperado). En la definición del costo esperado de procesamiento, P( [ti, ti+1) ) denota la probabilidad de que un punto de tiempo de consulta esté en [ti, ti+1). Formalmente, este problema de sublista-materialización en el espacio puede ser expresado como argmin M X [ti, ti+1) ∈ E P( [ti, ti+1) ) · PC( [ti, ti+1) ) sujeto a. X m∈M |Lv : m| ≤ κ |Lv| .

X m∈M |Lv : m| ≤ κ |Lv| .

X m∈M |Lv : m| ≤ κ |Lv| . El problema se puede resolver utilizando programación dinámica sobre un número creciente de intervalos de tiempo: En cada intervalo de tiempo en E, el algoritmo decide si comenzar un nuevo intervalo de tiempo de materialización, utilizando la mejor decisión de materialización conocida de los intervalos de tiempo anteriores, y llevando un registro del consumo de espacio requerido para la materialización. Una descripción detallada del algoritmo se omite aquí, pero se puede encontrar en el informe técnico adjunto [5]. Desafortunadamente, el algoritmo tiene una complejidad temporal de O(n3 |Lv|) y su complejidad espacial es de O(n2 |Lv|), lo cual no es práctico para conjuntos de datos grandes. Obtenemos una solución aproximada al problema utilizando el recocido simulado [22, 23]. El recocido simulado toma un número fijo R de rondas para explorar el espacio de soluciones. En cada ronda se examina un sucesor aleatorio de la solución actual. Si el sucesor no cumple con el límite de espacio, siempre es rechazado (es decir, se mantiene la solución actual). Un sucesor que cumpla con el límite de espacio siempre es aceptado si logra un costo de procesamiento esperado más bajo que la solución actual. Si logra un costo de procesamiento esperado más alto, se acepta aleatoriamente con una probabilidad de e−∆/r donde ∆ es el aumento en el costo de procesamiento esperado y R ≥ r ≥ 1 denota el número de rondas restantes. Además, a lo largo de todas las rondas, el método lleva un registro de la mejor solución vista hasta el momento. El espacio de soluciones para el problema en cuestión puede ser explorado de manera eficiente. Como argumentamos anteriormente, solo tenemos que observar conjuntos M que cubran completamente el intervalo de tiempo [t1, tn) y no contengan intervalos de tiempo superpuestos. Representamos un conjunto M como un arreglo de n variables booleanas b1 . . . bn que transmiten los límites de los intervalos de tiempo en el conjunto. Ten en cuenta que b1 y bn siempre se establecen como verdaderos. Inicialmente, todas las n − 2 variables intermedias asumen el valor falso, lo cual corresponde al conjunto M = { [t1, tn) }. Un sucesor aleatorio puede ser generado fácilmente ahora al cambiar el valor de una de las n − 2 variables intermedias. La complejidad temporal del método es de O(n2) - el costo de procesamiento esperado debe ser calculado en cada ronda. Su complejidad espacial es de O(n) - para mantener las n variables booleanas. Como observación adicional, cabe destacar que para κ = 1.0, el método SB no necesariamente produce la solución que se obtiene de Sopt, pero puede producir una solución que requiere la misma cantidad de espacio mientras logra un mejor rendimiento esperado. 7. EVALUACIÓN EXPERIMENTAL Realizamos una serie completa de experimentos en dos conjuntos de datos del mundo real para evaluar las técnicas propuestas en este artículo. 7.1 Configuración y Conjuntos de Datos Las técnicas descritas en este artículo fueron implementadas en un sistema prototipo utilizando Java JDK 1.5. Todos los experimentos descritos a continuación se ejecutaron en una sola máquina SUN V40z que cuenta con cuatro CPUs AMD Opteron, 16GB de RAM, un gran conjunto de discos RAID-5 conectado en red y que ejecuta Microsoft Windows Server 2003. Todos los datos e índices se mantienen en una base de datos Oracle 10g que se ejecuta en la misma máquina. Para nuestros experimentos utilizamos dos conjuntos de datos diferentes. El historial de revisiones de la Wikipedia en inglés (referido como WIKI en el resto) está disponible para descarga gratuita como un único archivo XML. Este gran conjunto de datos, que totaliza 0.7 TBytes, contiene el historial completo de edición de la Wikipedia en inglés desde enero de 2001 hasta diciembre de 2005 (momento de nuestra descarga). Indexamos todos los artículos de la enciclopedia, excluyendo las versiones que fueron marcadas como resultado de una edición menor (por ejemplo, la corrección de errores ortográficos, etc.). Esto produjo un total de 892,255 documentos con 13,976,915 versiones, con una media (µ) de 15.67 versiones por documento y una desviación estándar (σ) de 59.18. Construimos una carga de trabajo de consultas de viaje en el tiempo utilizando el registro de consultas temporalmente disponible recientemente por AOL Research de la siguiente manera: primero extraímos las 300 consultas de palabras clave más frecuentes que arrojaron un clic en un artículo de Wikipedia (por ejemplo, revolución francesa, temporada de huracanes 2005, código da vinci, etc.). Las consultas extraídas contenían un total de 422 términos distintos. Para cada consulta extraída, elegimos aleatoriamente un punto de tiempo para cada mes cubierto por el conjunto de datos. Esto resultó en un total de 18,000 (= 300 × 60) consultas de viajes en el tiempo. El segundo conjunto de datos utilizado en nuestros experimentos se basó en un subconjunto del Archivo Europeo [13], que contenía rastreos semanales de 11 sitios web .gov.uk a lo largo de los años 2004 y 2005, lo que equivale a cerca de 2 TBytes de datos en bruto. Filtramos los documentos que no pertenecen a los tipos MIME text/plain y text/html, para obtener un conjunto de datos que suma 0.4 TBytes y al que nos referimos como UKGOV en el resto del documento. Esto incluyó un total de 502,617 documentos con 8,687,108 versiones (µ = 17.28 y σ = 13.79). Creamos una carga de trabajo de consultas correspondiente como se mencionó anteriormente, esta vez eligiendo consultas de palabras clave que llevaron a un sitio en el dominio .gov.uk (por ejemplo, salario mínimo, impuesto de herencia, fechas de ceremonias de ciudadanía, etc.), y muestreando aleatoriamente un punto de tiempo para cada mes dentro del período de dos años abarcado por el conjunto de datos. Así, obtuvimos un total de 7,200 (= 300 × 24) consultas de viaje en el tiempo para el conjunto de datos del UKGOV. En total aparecen 522 términos en las consultas extraídas. Las estadísticas de colección (es decir, N y avdl) y las estadísticas de términos (es decir, DF) se calcularon a nivel mensual para ambos conjuntos de datos. 7.2 Impacto de la Coalescencia Temporal Nuestro primer conjunto de experimentos tiene como objetivo evaluar la técnica de coalescencia temporal aproximada, descrita en la Sección 5, en términos de reducción del tamaño del índice y su efecto en la calidad de los resultados. Para ambos conjuntos de datos WIKI y UKGOV, comparamos índices temporalmente fusionados para diferentes valores del umbral de error calculados utilizando el Algoritmo 1 con el índice no fusionado como referencia. WIKI UKGOV # Ratio de Publicaciones # Ratio de Publicaciones - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Tabla 1: Tamaños de índice para índices no fusionados (-) e índices fusionados para diferentes valores de La Tabla 1 resume los tamaños de índice medidos como el número total de publicaciones. Como demuestran estos resultados, la coalescencia temporal aproximada es altamente efectiva en la reducción del tamaño del índice. Incluso un valor de umbral pequeño, por ejemplo, = 0.01, tiene un efecto considerable al reducir el tamaño del índice casi en un orden de magnitud. Ten en cuenta que en el conjunto de datos del UKGOV, incluso con una coalescencia precisa ( = 0), se logra reducir el tamaño del índice a menos del 38% del tamaño original. El tamaño del índice sigue disminuyendo en ambos conjuntos de datos, a medida que aumentamos el valor de . ¿Cómo afecta la reducción del tamaño del índice a los resultados de la consulta? Para evaluar este aspecto, comparamos los resultados principales k calculados utilizando un índice fusionado con el resultado de verdad terreno obtenido del índice original, para diferentes niveles de corte k. Sean Gk y Ck los documentos principales k del resultado de verdad terreno y del índice fusionado respectivamente. Utilizamos las siguientes dos medidas para la comparación: (i) Recuperación Relativa en el nivel de corte k (RR@k), que mide la superposición entre Gk y Ck, que varía en [0, 1] y se define como RR@k = |Gk ∩ Ck|/k. (ii) Tau de Kendall (ver [7, 14] para una definición detallada) en el nivel de corte k (KT@k), que mide la concordancia entre dos resultados en el orden relativo de los elementos en Gk ∩ Ck, con un valor de 1 (o -1) que indica total concordancia (o discordancia). La Figura 3 traza, para los niveles de corte 10 y 100, la media de RR@k y KT@k junto con los percentiles 5% y 95%, para diferentes valores del umbral comenzando desde 0.01. Se debe tener en cuenta que para = 0, los resultados coinciden con los obtenidos por el índice original, por lo que se omiten del gráfico. Es reconfortante ver en estos resultados que la coalescencia temporal aproximada induce una interrupción mínima en los resultados de la consulta, ya que RR@k y KT@k se encuentran dentro de límites razonables. Para = 0.01, el valor más pequeño de en nuestros experimentos, RR@100 para WIKI es 0.98 indicando que los resultados son -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 10 (WIKI) τ de Kendall @ 10 (WIKI) Recuperación Relativa @ 10 (UKGOV) τ de Kendall @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Recuperación Relativa @ 100 (WIKI) τ de Kendall @ 100 (WIKI) Recuperación Relativa @ 100 (UKGOV) τ de Kendall @ 100 (UKGOV) (b) @100 Figura 3: Recuperación relativa y τ de Kendall observados en índices coalescentes para diferentes valores casi indistinguibles de los obtenidos a través del índice original. Incluso el orden relativo de estos resultados comunes es bastante alto, ya que la media de KT@100 está cerca de 0.95. Para el valor extremo de = 0.5, que resulta en un tamaño de índice de solo el 2.35% del original, el RR@100 y KT@100 son aproximadamente 0.8 y 0.6 respectivamente. En el conjunto de datos UKGOV relativamente menos dinámico (como se puede ver en los valores de σ anteriores), los resultados fueron aún mejores, con valores altos de RR y KT observados en todo el espectro de valores para ambos valores de corte. 7.3 Materialización de sublistas Ahora dirigimos nuestra atención hacia la evaluación de las técnicas de materialización de sublistas introducidas en la Sección 6. Para ambos conjuntos de datos, comenzamos con el índice fusionado producido por una configuración de umbral moderado de = 0.10. Para reducir el esfuerzo computacional, los límites de los intervalos de tiempo elementales se redondearon a la granularidad del día antes de calcular las materializaciones de la sublista. Sin embargo, ten en cuenta que las publicaciones en las sublistas materializadas aún conservan sus marcas de tiempo originales. Para una evaluación comparativa de los cuatro enfoques - Popt, Sopt, PG y SB - medimos el espacio y el rendimiento de la siguiente manera. El espacio requerido S(M), como se definió anteriormente, es igual al número total de publicaciones en las sublistas materializadas. Para evaluar el rendimiento, calculamos el costo esperado de procesamiento (CEP) para todos los términos en la carga de trabajo de consulta respectiva, asumiendo una distribución de probabilidad uniforme entre los puntos de tiempo de consulta. Informamos la media de EPC, así como el percentil 5% y 95%. En otras palabras, la EPC media refleja la longitud esperada de la lista de índices (en términos de publicaciones de índices) que debe ser escaneada para un punto de tiempo aleatorio y un término aleatorio de la carga de trabajo de consulta. Los enfoques Sopt y Popt son, por definición, libres de parámetros. Para el enfoque PG, variamos su parámetro γ, que limita la degradación máxima del rendimiento, entre 1.0 y 3.0. Análogamente, para el enfoque SB el parámetro κ, como límite superior en la expansión del espacio permitida, se varió entre 1.0 y 3.0. Las soluciones para el enfoque SB se obtuvieron ejecutando recocido simulado durante R = 50,000 rondas. La Tabla 2 enumera las cifras de espacio y rendimiento obtenidas. Ten en cuenta que los valores de EPC son más bajos en WIKI que en UKGOV, ya que los términos en la carga de trabajo de consulta utilizados para WIKI son relativamente más raros en el corpus. Basándonos en los resultados mostrados, hacemos las siguientes observaciones clave. i) Como se esperaba, Popt logra un rendimiento óptimo a costa de un consumo de espacio enorme. Por el contrario, el sopt, aunque consume una cantidad óptima de espacio, ofrece solo un bajo costo de procesamiento esperado. Los métodos PG y SB, para diferentes valores de sus respectivos parámetros, producen soluciones cuyo espacio y rendimiento se encuentran entre los extremos que representan Popt y Sopt. ii) Para el método PG vemos que, para una degradación de rendimiento aceptable de solo el 10% (es decir, γ = 1.10), el espacio requerido disminuye en más de un orden de magnitud en comparación con Popt en ambos conjuntos de datos. iii) El enfoque SB logra un rendimiento cercano al óptimo en ambos conjuntos de datos, si se le permite consumir como máximo tres veces la cantidad óptima de espacio (es decir, κ = 3.0), lo que en nuestros conjuntos de datos sigue correspondiendo a una reducción de espacio respecto a Popt de más de un orden de magnitud. También medimos los tiempos de reloj de pared en una muestra de las consultas, con resultados que indican mejoras en el tiempo de ejecución de hasta un factor de 12.8. CONCLUSIONES En este trabajo hemos desarrollado una solución eficiente para la búsqueda de texto de viaje en el tiempo sobre colecciones de documentos versionados temporalmente. Experimentos en dos conjuntos de datos del mundo real mostraron que una combinación de las técnicas propuestas puede reducir el tamaño del índice hasta en un orden de magnitud, al mismo tiempo que logra un rendimiento casi óptimo y resultados altamente precisos. El presente trabajo plantea muchas preguntas interesantes para investigaciones futuras, por ejemplo: ¿Cómo podemos mejorar aún más el rendimiento aplicando (y posiblemente ampliando) técnicas de codificación, compresión y salto [35]? ¿Cómo podemos extender el enfoque para consultas q [tb, te] especificando un intervalo de tiempo en lugar de un punto de tiempo? ¿Cómo puede la funcionalidad de búsqueda de texto de viaje en el tiempo descrita permitir o acelerar la minería de texto a lo largo del eje temporal (por ejemplo, rastreando cambios de sentimiento en las opiniones de los clientes)? 9. AGRADECIMIENTOS Agradecemos a los revisores anónimos por sus valiosos comentarios, en particular al revisor que señaló la oportunidad de mejoras algorítmicas en la Sección 5 y la Sección 6.2. 10. REFERENCIAS [1] V. N. Anh y A. Moffat. Evaluación de Consultas Podadas Utilizando Impactos Precomputados. En SIGIR, 2006. [2] V. N. Anh y A. Moffat. Estrategias de poda para consultas de modo mixto. En CIKM, 2006. WIKI UKGOV S(M) EPC S(M) EPC 5% Media 95% 5% Media 95% Popt 54,821,634,137 11.22 3,132.29 15,658.42 21,372,607,052 39.93 15,593.60 66,938.86 Sopt 379,962,802 114.05 30,186.52 149,820.1 187,387,342 63.15 22,852.67 102,923.85 PG γ = 1.10 3,814,444,654 11.30 3,306.71 16,512.88 1,155,833,516 40.66 16,105.61 71,134.99 PG γ = 1.25 1,827,163,576 12.37 3,629.05 18,120.86 649,884,260 43.62 17,059.47 75,749.00 PG γ = 1.50 1,121,661,751 13.96 4,128.03 20,558.60 436,578,665 46.68 18,379.69 78,115.89 PG γ = 1.75 878,959,582 15.48 4,560.99 22,476.77 345,422,898 51.26 19,150.06 82,028.48 PG γ = 2.00 744,381,287 16.79 4,992.53 24,637.62 306,944,062 51.48 19,499.78 87,136.31 PG γ = 2.50 614,258,576 18.28 5,801.66 28,849.02 269,178,107 53.36 20,279.62 87,897.95 PG γ = 3.00 552,796,130 21.04 6,485.44 32,361.93 247,666,812 55.95 20,800.35 89,591.94 SB κ = 1.10 412,383,387 38.97 12,723.68 60,350.60 194,287,671 63.09 22,574.54 102,208.58 SB κ = 1.25 467,537,173 26.87 9,011.81 45,119.08 204,454,800 57.42 22,036.39 95,337.33 SB κ = 1.50 557,341,140 19.84 6,699.36 32,810.85 246,323,383 53.24 20,566.68 91,691.38 SB κ = 1.75 647,187,522 16.59 5,769.40 28,272.89 296,345,976 49.56 19,065.99 84,377.44 SB κ = 2.00 737,819,354 15.86 5,358.99 27,112.01 336,445,773 47.58 18,569.08 81,386.02 SB κ = 2.50 916,308,766 13.99 4,639.77 23,037.59 427,122,038 44.89 17,153.94 74,449.28 SB κ = 3.00 1,094,973,140 13.01 4,343.72 22,708.37 Tabla 2: Espacio requerido y costo de procesamiento esperado (en # publicaciones) observado en índices fusionados ( = 0.10) [3] P. G. Anick y R. A. Flynn. Creación de versiones de un sistema de recuperación de información de texto completo. En SIGIR, 1992. [4] R. A. Baeza-Yates y B. Ribeiro-Neto. Recuperación de información moderna. Addison-Wesley, 1999. [5] K. Berberich, S. Bedathur, T. Neumann y G. Weikum. Una máquina del tiempo para búsqueda de texto. Informe técnico MPI-I-2007-5-002, Instituto Max Planck de Informática, 2007. [6] M. H. Böhlen, R. T. Snodgrass y M. D. Soo. Fusionando en bases de datos temporales. En VLDB, 1996. [7] P. Boldi, M. Santini y S. Vigna. Haz tu peor esfuerzo para lograr lo mejor: Efectos paradójicos en los cálculos incrementales de PageRank. En WAW, 2004. [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, R. Lempel, J. McPherson, R. Qi y E. J. Shekita. Indexación de contenido compartido en sistemas de recuperación de información. En EDBT, 2006. [9] C. Buckley y A. F. Lewit. Optimización de Búsquedas de Vectores Invertidos. En SIGIR, 1985. [10] M. Burrows y A. L. Hisgen. Método y aparato para generar y buscar un índice basado en rangos de ubicaciones de palabras. Patente de EE. UU. 5,915,251, 1999. [11] S. B¨uttcher y C. L. A. Clarke. Un enfoque centrado en el documento para la poda de índices estáticos en sistemas de recuperación de texto. En CIKM, 2006. [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek y A. Soffer. Poda de Índice Estático para Sistemas de Recuperación de Información. En SIGIR, 2001. [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar y D. Sivakumar. Comparando listas Top k. SIAM J. Discrete Math., 17(1):134-160, 2003. [15] R. Fagin, A. Lotem, y M. Naor. Algoritmos de Agregación Óptimos para Middleware. J. Comput. This is not a complete sentence. Please provide more context or a complete sentence to be translated. Cienc., 66(4):614-656, 2003. [16] S. Guha, K. Shim y J. ¡Guau! REHIST: Algoritmos de Construcción de Histogramas de Error Relativo. En VLDB, 2004. [17] M. Hersovici, R. Lempel y S. Yogev. Indexación eficiente de secuencias de documentos versionados. En ECIR, 2007. [18] http://www.archive.org. [19] Y. E. Ioannidis y V. Poosala. Equilibrando la optimalidad y la practicidad del histograma para la estimación del tamaño de los resultados de la consulta. En SIGMOD, 1995. [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, V. Poosala, K. C. Sevcik y T. Suel. Histogramas óptimos con garantías de calidad. En VLDB, 1998. [21] E. J. Keogh, S. Chu, D. Hart y M. J. Pazzani. Un algoritmo en línea para segmentar series temporales. En ICDM, 2001. [22] S. Kirkpatrick, D. G. Jr., y M. P. Vecchi. Optimización por Recocido Simulado. Ciencia, 220(4598):671-680, 1983. [23] J. Kleinberg y E. Tardos. Diseño de algoritmos. Addison-Wesley, 2005. [24] U. Manber. 

Addison-Wesley, 2005. [24] U. Manber. Introducción a los Algoritmos: Un Enfoque Creativo. Addison-Wesley, 1989. [25] K. Nørv˚ag y A. O. N. Nybø. DyST: Indexación de texto temporal dinámica y escalable. En TIME, 2006. [26] J. M. Ponte y W. B. Croft. Un enfoque de modelado del lenguaje para la recuperación de información. En SIGIR, 1998. [27] S. E. Robertson y S. Walker. Okapi/Keenbow en TREC-8. En TREC, 1999. [28] B. Salzberg y V. J. Tsotras. Comparación de métodos de acceso para datos en evolución temporal. ACM Comput. Rev., 31(2):158-221, 1999. [29] M. Stack. Búsqueda de texto completo en colecciones de archivos web. En IWAW, 2006. [30] E. Terzi y P. Tsaparas. Algoritmos eficientes para la segmentación de secuencias. En SIAM-DM, 2006. [31] M. Theobald, G. Weikum y R. Schenkel. Evaluación de consultas Top-k con garantías probabilísticas. En VLDB, 2004. [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat y T. C. Bell. Gestionando Gigabytes: Comprimiendo e Indexando Documentos e Imágenes. Morgan Kaufmann publishers Inc., 1999. [34] J. Zhang y T. Suel. Búsqueda eficiente en colecciones textuales grandes con redundancia. En WWW, 2007. [35] J. Zobel y A. Moffat. Archivos invertidos para motores de búsqueda de texto. ACM Comput. Rev., 38(2):6, 2006.