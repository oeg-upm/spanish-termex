{
    "original_text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.",
    "original_translation": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos.",
    "error_count": 0,
    "errors": [],
    "keys": {
        "a set of orthogonal vectors": {
            "translated_key": "un conjunto de vectores ortogonales",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es \"un conjunto de vectores ortogonales\" ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "data mining": {
            "translated_key": "minería de datos",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la \"minería de datos\", han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "dimensionality reduction": {
            "translated_key": "reducción de dimensionalidad",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la \"reducción de dimensionalidad\".Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "finds the mutually-uncorrelated vectors": {
            "translated_key": "encuentra los vectores mutuamente organizados",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que \"encuentra los vectores mutuamente organizados\" en el que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "linear PCA": {
            "translated_key": "PCA lineal",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (\"PCA lineal\") es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "N-dimensional vector space basis": {
            "translated_key": "base del espacio del vector n-dimensional",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la \"base del espacio del vector n-dimensional\", mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "N^ synthetic KPIs": {
            "translated_key": "KPI sintéticos",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los \"KPI sintéticos\" representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "PCA": {
            "translated_key": "PCA",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método de análisis de componentes principales (\"PCA\") (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de \"PCA\" (\"PCA\" lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "Principal Component Analysis method": {
            "translated_key": "Método de análisis de componentes principales",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el \"Método de análisis de componentes principales\" (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "the first N^": {
            "translated_key": "el primer n^",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, \"el primer n^\", es suficiente para dar cuenta de la mayor parte de la varianza de los datos."
        },
        "the projection of the samples generates the highest variances": {
            "translated_key": "la proyección de las muestras genera las variaciones más altas",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que \"la proyección de las muestras genera las variaciones más altas\".El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "the simplest version of PCA": {
            "translated_key": "la versión más simple de PCA",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, \"la versión más simple de PCA\" (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "the variance of the projection of the samples is maximum": {
            "translated_key": "la varianza de la proyección de las muestras es máxima",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello sobre el cual \"la varianza de la proyección de las muestras es máxima\".En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se pueden calcular hasta N KPI ortogonales sintéticos.Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        },
        "up to N synthetic orthogonal KPIs": {
            "translated_key": "hasta n KPI ortogonales sintéticos",
            "translated_annotated_text": "En los últimos años y principalmente motivados por el impulso de la minería de datos, han surgido muchos métodos para la reducción de dimensionalidad.Dentro de estos, vale la pena resaltar el método principal de análisis de componentes (PCA) (Jolliffe, 2002).En un espacio vectorial n-dimensional, la versión más simple de PCA (PCA lineal) es una técnica que encuentra los vectores mutuamente organizados en los que la proyección de las muestras genera las variaciones más altas.El resultado es un conjunto de vectores ortogonales ordenados en orden descendente de varianza alcanzada.El primero de estos vectores es aquello en el que la varianza de la proyección de las muestras es máxima.En este sentido, los KPI originales constituyen la base del espacio del vector n-dimensional, mientras que los KPI sintéticos N^ representan los vectores ortogonales con la mayor varianza.Para ser riguroso, se puede calcular \"hasta n KPI ortogonales sintéticos\".Sin embargo, solo un pequeño conjunto de ellos, el primer n^, es suficiente para explicar la mayor parte de la varianza de los datos."
        }
    }
}