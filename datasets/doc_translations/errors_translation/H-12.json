{
    "id": "H-12",
    "original_text": "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA. hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users. In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets. We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library. These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process. Using simulation, we examine snippet generation performance for different size RAM caches. Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality. This scheme effectively doubles the number of documents that can fit in a fixed size cache. Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1. INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type. In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents. Snippets are short fragments of text extracted from the document content (or its metadata). They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20]. A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query. The addition of informative snippets to search results may substantially increase their value to searchers. Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored. In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization. Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets). The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required. The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load. Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems. Special-purpose filesystems have been built to address these problems [6]. Note that the utility of snippets is by no means restricted to whole-of-Web search applications. Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages). Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present. We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets. We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected. As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required. However, this is only true if there is no caching of documents in RAM. Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation. In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24]. As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document. This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2. RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query. This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries. Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document. Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms. To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work. Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries. Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature. Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm. White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3. SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1. Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation. Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine. Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources. Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram. For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted. Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine. In addition to data and programs operating on that data, each sub-system also has its own memory management scheme. The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system. There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries. When one of the popular queries is issued, the result page is fetched straight from the query cache. If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1. The Lexicon Engine maps query terms to integers. 2. The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3. The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4. The Meta Data Engine fetches other information about each document to construct the results page. IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key. OUT Remove the number of sentences required from the heap to form the summary. Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4. THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document. Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12]. Accordingly, we also assume a web snippet extraction process will extract sentences from documents. In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet. The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important. Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document. The final score of a sentence, assigned in Step 7, can be derived in many different ways. In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets. We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens. We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols. Both are limited to a maximum of 50 characters. Adjacent, repeating characters are removed from the punctuation. Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ?!.. Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences. In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10]. Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded. A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content. For example, The most wonderful company on earth. Products. Service. About us. Contact us. Try before you buy. Similar, but often not identical, navigational material is typically presented on every page within a site. This material tends to lower the quality of summaries and slow down summary generation. In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing. In wt100g, the average web page size is more than half the current Web average [11]. Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them. Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2. Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1. Each document is stored in a single file. While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6]. For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed. The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible. The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24]. Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits. Words and non-words strictly alternate in the compressed file, which always begins with a word. In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency. We use the vbyte coding scheme to code the word tokens [22]. The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code. The remaining 2 bits of each punctuation symbol are used to store capitalization information. The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high. If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23]. Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model. Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence. During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words. Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue. In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one. If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol. The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model. Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model. We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database. We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important. While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file. That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded. The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file. Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings. The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping. This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10. The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No. Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g. RAM. All source code was compiled using gcc4.1.1 with -O9 optimisation. Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files. In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments. Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable. Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy. We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period. Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g. The first two rows of Table 1 give the number of documents and the size in Mb of these collections. The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes. As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size. Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM. It is 1024 Mb in these experiments. The Zettair search engine [25] was used to produce a list of documents to summarize for each query. For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings. For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query. This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine. In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component. Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log. Only the data for wt10g is shown, but the other collections showed similar patterns. The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100. Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed. All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests. This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system. Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching. But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure). This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times. This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris). With disk caching turned off, the average time to generate snippets varies little. The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2. Once the system has stabilized, CTS is over 50% faster than the Baseline system. This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system. Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed. As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents. Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet. As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5. DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection. For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM. In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available. In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies. We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted. Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4. The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine. All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically. This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity. Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g. The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents. From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation. With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided. The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles). In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out. The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection. Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others. This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17]. Documents with high static scores are much more likely to be retrieved than others. In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here). This is more than compensated for by the reduced size of each document, allowing more documents into the document cache. For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection). In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times. This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols. Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1. With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size. Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols). Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models. Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases. Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache. Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk. This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6. SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document. Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored. Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk. Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine. We now introduce four sentence reordering approaches. 1. Natural order The first few sentences of a well authored document usually best describe the document content [12]. Thus simply processing a document in order should yield a quality snippet. Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit. More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2. Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20]. Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms. The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence. The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence. We then sort the sentences by this score. 3. Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9]. In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted. In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log. To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4. Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once. By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k). Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing. Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method. It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components. The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered. The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets. Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method. Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7. DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines. Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem. Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation. Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided. Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12]. Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets. Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created. We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains. In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web. We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine. We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice. Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]). Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents. Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents. The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes. Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine. Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services. These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache. In this work we have deliberately avoided committing to one particular scoring method for sentences in documents. Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20]. The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words. The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used. A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.) As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper. We will explore alternate compression schemes in future work. Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT). Thanks to Nick Lester and Justin Zobel for valuable discussions. 8. REFERENCES [1] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search engine. In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J. A. Tomlin, and D. P. Williamson. Searching the workplace web. In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data. ACM Trans. Inf. Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler. Zlib Compression Library. www.zlib.net. Accessed January 2007. [5] S. Garcia, H.E. Williams, and A. Cannane. Access-ordered indexes. In V. Estivill-Castro, editor, Proc. Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung. The google file system. In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003. ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell. Summarizing text documents: sentence selection and evaluation metrics. In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite. Overview of TREC-7 Very Large Collection Track. In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen. A temporal comparison of altavista web searching. J. Am. Soc. Inf. Sci. Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen. A trainable document summarizer. In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L. Giles. Accessibility of information on the web. Nature, 400:107-109, July 1999. [12] H.P. Luhn. The automatic creation of literature abstracts. IBM Journal, pages 159-165, April 1958. [13] I. Mani. Automatic Summarization, volume 3 of Natural Language Processing. John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman. Text compression for dynamic document databases. Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen. Compressed full text indexes. ACM Computing Surveys, 2007. To appear. [16] D. R. Radev, E. Hovy, and K. McKeown. Introduction to the special issue on summarization. Comput. Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill. Beyond pagerank: machine learning for static ranking. In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones. Generic summaries for indexing in information retrieval. In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Comput. Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose. Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes. In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel. Compressing integers for fast file access. Comp. J., 42(3):193-201, 1999. [23] H.E. Williams and J. Zobel. Searchable words on the Web. International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair. Accessed January 2007.",
    "original_translation": "Generación rápida de fragmentos de resultados en la búsqueda web Andrew Turpin & Yohannes Tsegay Rmit University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking Csiro ICT Center Canberra, Australia David.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA. hughw@microsoft.com Resumen La presentación de fragmentos de documentos sesgados de consulta como parte de las páginas de resultados presentadas por los motores de búsqueda se ha convertido en una expectativa de los usuarios de los motores de búsqueda. En este artículo exploramos los algoritmos y las estructuras de datos requeridas como parte de un motor de búsqueda para permitir una generación eficiente de fragmentos sesgados de consulta. Comenzamos proponiendo y analizando un método de compresión de documentos que reduce el tiempo de generación de fragmentos en un 58% sobre una línea de base utilizando la biblioteca de compresión ZLIB. Estos experimentos revelan que encontrar documentos sobre el almacenamiento secundario domina el costo total de generar fragmentos y, por lo tanto, los documentos de almacenamiento en caché en RAM son esenciales para un proceso de generación de fragmentos rápidos. Usando la simulación, examinamos el rendimiento de la generación de fragmentos para cachés RAM de diferentes tamaños. Finalmente, proponemos y analizamos la reordenamiento de documentos y la compactación, revelando un esquema que aumenta el número de éxitos de caché de documentos con solo un efecto marginal en la calidad del fragmento. Este esquema duplica efectivamente el número de documentos que pueden caber en un caché de tamaño fijo. Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información;H.3.4 [Almacenamiento y recuperación de información]: Sistemas y evaluación de rendimiento de software (eficiencia y efectividad);Algoritmos de términos generales, experimentación, medición, rendimiento 1. Introducción Cada resultado en la lista de resultados de búsqueda entregada por los motores de búsqueda WWW actuales como Search.yahoo.com, Google.com y Search.msn.com generalmente contiene el título y la URL del documento real, enlaces a versiones en vivo y almacenadas en caché del documentoy a veces una indicación de tamaño y tipo de archivo. Además, generalmente se presentan uno o más fragmentos, lo que le da al buscador una vista previa del contenido del documento. Los fragmentos son fragmentos cortos de texto extraídos del contenido del documento (o sus metadatos). Pueden ser estáticos (por ejemplo, siempre muestren las primeras 50 palabras del documento, o el contenido de sus metadatos de descripción, o una descripción tomada de un sitio de directorio como dmoz.org) o sesgo de consulta [20]. Un fragmento con sesgo de consulta se extrae selectivamente sobre la base de su relación con la consulta de los buscadores. La adición de fragmentos informativos a los resultados de búsqueda puede aumentar sustancialmente su valor para los buscadores. Los fragmentos precisos permiten al buscador tomar buenas decisiones sobre qué resultados valen la pena acceder y cuáles pueden ignorarse. En el mejor de los casos, los fragmentos pueden obviar la necesidad de abrir cualquier documento proporcionando directamente la respuesta a los buscadores que necesitan información real, como los datos de contacto de una persona o una organización. Generación de fragmentos con sesgo de consulta mediante la indexación de motores de búsqueda web de la orden de diez mil millones de páginas web y manejar cientos de millones de consultas de búsqueda por día impone una carga computacional muy significativa (recordando que cada búsqueda generalmente genera diez fips). El enfoque de forma simple de mantener una copia de cada documento en un archivo y generar fragmentos abriendo y escaneando archivos, funciona cuando las tasas de consulta son bajas y las colecciones son pequeñas, pero no escala en el grado requerido. La sobrecarga de abrir y leer diez archivos por consulta además de acceder a la estructura de índice para ubicarlos, sería manifiestamente excesivo bajo una carga de consulta pesada. Incluso almacenar diez mil millones de archivos y los cientos de terabytes de datos correspondientes están fuera del alcance de los sistemas de archivos tradicionales. Se han creado sistemas de archivos de propósito especial para abordar estos problemas [6]. Tenga en cuenta que la utilidad de los fragmentos de ninguna manera está restringida a las aplicaciones de búsqueda de todo WEB. La generación eficiente de fragmentos también es importante a escala de servicios de búsqueda de todo el gobierno como www.firstgov.gov (c. 25 millones de páginas) y GovSearch.australia.gov.au (c. 5 millones de páginas) y dentro de grandesempresas como IBM [2] (c. 50 millones de páginas). Los fragmentos pueden ser aún más útiles en las aplicaciones de búsqueda de bases de datos o sistemas de archivos en las que no hay información útil de URL o título. Presentamos un nuevo algoritmo y una estructura compacta de un solo archivo diseñada para una generación rápida de fragmentos de alta calidad y comparamos su rendimiento de espacio/tiempo con una línea de base obvia basada en el compresor ZLIB en varios conjuntos de datos. Reportamos la proporción del tiempo dedicado a las búsqueda de disco, lecturas de disco y procesamiento de CPU;Demostrar que el tiempo para localizar cada documento (buscar tiempo) domina, como se esperaba. Como el tiempo para procesar un documento en RAM es pequeño en comparación con la ubicación y la lectura del documento en la memoria, puede parecer que no se requiere compresión. Sin embargo, esto solo es cierto si no hay caché de documentos en la RAM. El control de la RAM de los sistemas físicos para la experimentación es difícil, por lo tanto, usamos la simulación para mostrar que los documentos de almacenamiento en caché mejoran drásticamente el rendimiento de la generación de fragmentos. A su vez, cuanto más se puedan comprimir los documentos, más se puede ajustar en el caché y, por lo tanto, se puede evitar cuanto más se puedan evitar el disco: la compresión de datos clásica que se explota en estructuras de archivos invertidos y las listas de documentos clasificadas en la computación [24]. Como la presentación de la memoria caché del documento es importante, examinamos la compactación del documento, a diferencia de la compresión, los esquemas imponiendo un pedido a priori de las oraciones dentro de un documento, y luego solo permitiendo oraciones líderes en caché para cada documento. Esto lleva a más ahorros de tiempo, con solo un impacto marginal en la calidad de los fragmentos que se devuelven.2. La generación del fragmento de trabajo relacionado es un tipo especial de resumen de documentos extractivos, en el que las oraciones o fragmentos de oraciones se seleccionan para su inclusión en el resumen sobre el grado en que coinciden con la consulta de búsqueda. Este proceso recibió el nombre de la resumen sesgada por la consulta por Tombros y Sanderson [20] El lector se remite a Mani [13] y a Radev et al.[16] Para descripción general de las muchas aplicaciones diferentes de resumen y para los métodos igualmente diversos para producir resúmenes. Los primeros motores de búsqueda web presentaron fragmentos independientes de la consulta que consisten en los primeros k bytes del documento de resultados. Generar esto es claramente mucho más simple y mucho menos costoso que el procesamiento de documentos para extraer resúmenes sesgados de consulta, ya que no es necesario buscar en el documento los fragmentos de texto que contienen términos de consulta. Hasta donde sabemos, Google fue el primer motor de búsqueda de todo tuweb en proporcionar resúmenes sesgados de consulta, pero Brin y la página [1] solo se encuentran bajo el encabezado del trabajo futuro. La mayor parte del trabajo experimental utilizando resumen sesgado de consultas se ha centrado en comparar su valor con los buscadores en relación con otros tipos de resumen [20, 21], en lugar de una generación eficiente de resúmenes. A pesar de la importancia de la generación de resumen eficiente en la búsqueda web, pocos algoritmos aparecen en la literatura. Silber y McKoy [19] describen un algoritmo de encadenamiento léxico de tiempo lineal para su uso en resúmenes genéricos, pero no ofrecen datos empíricos para el rendimiento de su algoritmo. White et al [21] informan algunos tiempos experimentales de su sistema webDocsum, pero los algoritmos de generación de fragmentos en sí mismos no están aislados, por lo que es difícil inferir el tiempo de generación de fragmentos comparables a los tiempos que informamos en este documento.3. Arquitecturas de motores de búsqueda Un motor de búsqueda debe realizar una variedad de actividades, y se compone de muchos subsistemas, como lo representan las cajas de la Figura 1. Tenga en cuenta que puede haber varios otros subsistemas, como el motor de publicidad o el motor de análisis que se podría agregar fácilmente al diagrama, pero nos hemos concentrado en los subsistemas que son relevantes para la generación de fragmentos. Dependiendo de la cantidad de documentos que los indexan el motor de búsqueda, los datos y los procesos para cada motor de clasificación que se arrastra el motor del motor del motor del motor Léxico Léxico Meta Data Engine Motor Snippet Term & Doc#S SpippetPerdoc Web Consuly Consulta Resultados Página Término#S DOC#S Invertidos Docs PerdocTítulo, URL, ETC Doc#S Documento Meta Data Términos Consulta Término#S Figura 1: Una abstracción de algunos de los subsistemas en un motor de búsqueda. Dependiendo del número de documentos indexados, cada subsistema podría residir en una sola máquina, distribuirse en miles de máquinas, o una combinación de ambas.El subsistema podría distribuirse a través de muchas máquinas, o todas ocupan un solo servidor y sistema de archivos, compitiendo entre sí por los recursos. Del mismo modo, puede ser más eficiente combinar algunos subsistemas en una implementación del diagrama. Por ejemplo, los metadatos, como el título del documento y la URL, requieren un cálculo mínimo además de resaltar las palabras de consulta, pero observamos que es probable que la búsqueda de disco se minimice si el título, la URL y la información de resumen fija se almacenan contiguamente con el texto desde el cual la consulta.Se extraen resúmenes sesgados. Aquí ignoramos el texto fijo y consideramos solo la generación de resúmenes sesgados de consulta: nos concentramos en el motor del fragmento. Además de los datos y programas que operan en esos datos, cada subsistema también tiene su propio esquema de gestión de memoria. El sistema de gestión de memoria puede ser simplemente la jerarquía de memoria proporcionada por el sistema operativo utilizado en máquinas en el subsistema, o puede codificarse explícitamente para optimizar los procesos en el subsistema. Hay muchos documentos sobre el almacenamiento en caché en los motores de búsqueda (ver [3] y las referencias en ellas para un resumen actual), pero parece razonable que haya un caché de consulta en el motor de consulta que almacena páginas de resultados finales precomputados para consultas muy populares. Cuando se emite una de las consultas populares, la página de resultados se obtiene directamente del caché de consulta. Si la consulta emitida no está en la memoria caché de la consulta, entonces el motor de consulta usa los cuatro subsistemas a su vez para ensamblar una página de resultados.1. El motor Léxico mapea los términos de consulta a enteros.2. El motor de clasificación recupera listas invertidas para cada término, usándolas para obtener una lista clasificada de documentos.3. El motor del fragmento utiliza esos números de documentos y números de términos de consulta para generar fragmentos.4. El motor de meta data obtiene otra información sobre cada documento para construir la página de resultados. En un documento dividido en una oración por línea, y una secuencia de términos de consulta.1 para cada línea del texto, l = [w1, w2 ,..., wm] 2 Sea h 1 si l es un encabezado, 0 de lo contrario.3 Sea 2 si L es la primera línea de un documento, 1 si es la segunda línea, 0 de lo contrario.4 Sea C el número de WI que son términos de consulta, contando repeticiones.5 Sea D el número de términos de consulta distintos que coinciden con algunos WI.6 Identifique la ejecución contigua más larga de términos de consulta en L, digamos WJ...WJ+K.7 Use una combinación ponderada de C, D, K, H y para obtener una puntuación s.8 Inserte L en un máximo de montón usando S como clave. Elimine el número de oraciones requeridas del montón para formar el resumen. Figura 2: Ranker de oración simple que opera en texto sin procesar con una oración por línea.4. El motor de fragmento para cada identificador de documento pasado al motor del fragmento, el motor debe generar texto, preferiblemente que contenga términos de consulta, que intentan resumir ese documento. El trabajo previo sobre el resumen identifica la oración como la unidad mínima para la extracción y presentación al usuario [12]. En consecuencia, también asumimos que un proceso de extracción de fragmentos web extraerá oraciones de los documentos. Para construir un fragmento, todas las oraciones en un documento deben clasificarse contra la consulta, y los dos o tres principales regresaron como el fragmento. La puntuación de las oraciones contra las consultas se ha explorado en varios documentos [7, 12, 18, 20, 21], con diferentes características de las oraciones consideradas importantes. Según estas observaciones, la Figura 2 muestra el algoritmo general para calificar oraciones en documentos relevantes, con las oraciones más altas de puntuación que hacen el fragmento para cada documento. El puntaje final de una oración, asignada en el Paso 7, puede derivarse de muchas maneras diferentes. Para evitar el sesgo con cualquier mecanismo de puntuación en particular, comparamos la calidad de las oraciones más adelante en el documento utilizando los componentes individuales de la puntuación, en lugar de una combinación arbitraria de los componentes.4.1 Analización de documentos web A diferencia de las colecciones de texto bien editadas que a menudo son el objetivo de los sistemas de resumen, los datos web a menudo están mal estructurados, mal puntuados y contienen muchos datos que no forman parte de oraciones válidas que serían candidatos para partes de fragmentos. Suponemos que los documentos pasados al motor del fragmento por el motor de indexación tienen todas las etiquetas HTML y JavaScript, y que cada documento se reduce a una serie de tokens de palabras separados por tokens no palabras. Definimos un token de palabras como una secuencia de caracteres alfanuméricos, mientras que una no palabra es una secuencia de caracteres no alfanuméricos como Whitpace y los otros símbolos de puntuación. Ambos se limitan a un máximo de 50 caracteres. Los caracteres adyacentes y repetidos se eliminan de la puntuación. ¿Se incluye en el conjunto de puntuación un marcador de finalización especial de la oración que reemplaza los tres terminadores habituales de oraciones?! ... A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como <br> y <p> terminan las oraciones. Además, una oración debe contener al menos cinco palabras y no más de veinte palabras, con oraciones más largas o más cortas que se rompen y se unen según lo requerido para cumplir con estos criterios [10]. Etiquetas HTML no terminadas: es decir, etiquetas con un aparato ortopédico abierto, pero no hay una núcleo cercana de todo el texto desde la abrazadera abierta a la siguiente abrazadera abierta para descartarse. Un problema importante al resumir las páginas web es la presencia de grandes cantidades de material promocional y de navegación (barras de navegación) visualmente arriba y a la izquierda del contenido de la página real. Por ejemplo, la compañía más maravillosa del mundo. El material de navegación similar, pero a menudo no idéntico, generalmente se presenta en cada página dentro de un sitio. Este material tiende a reducir la calidad de los resúmenes y ralentizar la generación de resumen. En nuestros experimentos, no utilizamos ninguna heurística particular para eliminar la información de navegación, ya que la recopilación de pruebas en uso (WT100G) está previamente con la realización generalizada del estilo actual de publicación web. En WT100G, el tamaño promedio de la página web es más de la mitad del promedio web actual [11]. Anecdóticamente, el aumento se debe a la inclusión de elementos sofisticados de navegación e interfaz y las funciones de JavaScript para apoyarlos. Habiendo definido el formato de documentos que se presentan al motor del fragmento, ahora definimos nuestro esquema de almacenamiento de documentos del sistema de token comprimido (CTS), y el sistema de referencia utilizado para la comparación.4.2 Motor de fragmento de línea de base Un esquema obvio de representación del documento es simplemente comprimir cada documento con un compresor adaptativo bien conocido, y luego descomprimir el documento según sea necesario [1], utilizando un algoritmo de coincidencia de cadenas para efectuar el algoritmo en la Figura 2. En consecuencia, implementamos dicho sistema, utilizando ZLIB [4] con parámetros predeterminados para comprimir cada documento después de haber sido analizado como en la Sección 4.1. Cada documento se almacena en un solo archivo. Si bien es manejable para nuestras pequeñas colecciones de pruebas o pequeñas empresas con millones de documentos, un motor de búsqueda web completo puede requerir múltiples documentos para habitar archivos únicos o un sistema de archivos de propósito especial [6]. Para la generación del fragmento, los documentos requeridos se descomprimen uno a la vez, y se emplea una búsqueda lineal de los términos de consulta proporcionados. La búsqueda está optimizada para nuestra tarea específica, que está restringida a las palabras completas coincidentes y la oración que termina el token, en lugar de la coincidencia de patrones generales.4.3 El motor del fragmento CTS varias optimizaciones sobre la línea de base son posibles. El primero es emplear un método de compresión semiestática en toda la recopilación de documentos, que permitirá una descompresión más rápida con una pérdida de compresión mínima [24]. El uso de un enfoque semistático implica mapear palabras y no palabras producidas por el analizador a tokens enteros individuales, con símbolos frecuentes que reciben enteros pequeños y luego eligen un esquema de codificación que asigna a pequeños números un pequeño número de bits. Las palabras y las no palabras se alternan estrictamente en el archivo comprimido, que siempre comienza con una palabra. En este caso, simplemente asignamos a cada símbolo su número ordinal en una lista de símbolos ordenados por frecuencia. Utilizamos el esquema de codificación VBYTE para codificar la palabra tokens [22]. El conjunto de no palabras se limita a las 64 secuencias de puntuación más comunes en la colección en sí, y están codificadas con un código binario plano de 6 bits. Los 2 bits restantes de cada símbolo de puntuación se utilizan para almacenar información de capitalización. El proceso de calcular el modelo semiestático se complica por el hecho de que el número de palabras y no palabras que aparecen en grandes colecciones web es alto. Si almacenamos todas las palabras y no palabras que aparecen en la colección, y su frecuencia asociada, se requerirían muchos gigabytes de RAM o un árbol B o una estructura similar en el disco [23]. Moffat et al.[14] han examinado esquemas para los modelos de poda durante la compresión utilizando alfabetos grandes, y concluyen que los términos que rara vez ocurren no necesitan residir en el modelo. Más bien, se explican términos raros en el archivo comprimido final, utilizando un token de palabra especial (símbolo de escape), para señalar su ocurrencia. Durante el primer pase de codificación, se guardan dos colas de movimiento hacia adelante;uno para palabras y otro para no palabras. Cada vez que se consume la memoria disponible y se descubre un nuevo símbolo en la colección, se descarta un símbolo existente desde el final de la cola. En nuestra implementación, aplicamos la condición más estricta en el desalojo de que, cuando sea posible, el símbolo desalojado debe tener una frecuencia de uno. Si no hay símbolo con frecuencia uno en la última mitad de la cola, entonces desalojamos los símbolos de frecuencia dos, y así sucesivamente hasta que haya suficiente espacio disponible en el modelo para el nuevo símbolo. El segundo pase de codificación reemplaza cada palabra con su número codificado Vbyte, o el símbolo de escape y una representación ASCII de la palabra si no está en el modelo. Del mismo modo, cada secuencia que no es de palabras se reemplaza con su codeword, o el codeword para un solo carácter espacial si no está en el modelo. Observamos que esta compresión sin pérdidas de no palabras es aceptable cuando los documentos se usan para la generación de fragmentos, pero puede no ser aceptable para una base de datos de documentos. Suponemos que un subsistema separado contendría documentos en caché para otros fines donde la puntuación exacta es importante. Si bien este esquema semiestático debería permitir una descompresión más rápida que la línea de base, también permite la coincidencia directa de los términos de consulta como enteros comprimidos en el archivo comprimido. Es decir, las oraciones se pueden calificar sin tener que descomprimir un documento, y solo las oraciones devueltas como parte de un fragmento deben decodificarse. El sistema CTS almacena todos los documentos contiguamente en un archivo, y una tabla auxiliar de enteros de 64 bits que indica el desplazamiento de inicio de cada documento en el archivo. Además, debe tener acceso a la asignación inversa de números de términos, lo que permite que esas palabras no se explicen en el documento se recuperen y devuelvan al motor de consulta como cadenas. La primera de estas estructuras de datos se puede dividir y distribuir fácilmente si el motor del fragmento ocupa múltiples máquinas;Sin embargo, el segundo no se divide tan fácilmente, ya que cualquier documento en una máquina remota puede requerir acceso a todo el mapeo entero a la cuerda. Esta es la segunda razón para emplear el paso de poda del modelo durante la construcción del código semiestático: limita el tamaño de la tabla de mapeo inverso que debe estar presente en cada máquina que implementa el motor de fragmento.4.4 Evaluación experimental de CTS Todos los experimentos informados en este documento se ejecutaron en un servidor Sun Fire V210 que ejecuta Solaris 10. La máquina consta de procesadores ultrasparc IIII duales de 1.34 GHz y 4 GB de WT10g WT50G WT100G No. Docs.(× 106) 1.7 10.1 18.5 Texto crudo 10, 522 56, 684 102, 833 línea de base (Zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Tabla 1: Espacio de almacenamiento total (MB) para documentos para las tres colecciones de prueba comprimidas y sin comprimir.0 20 40 60 60 0.00.20.40.60.8 Consultas agrupadas en 100s de tiempo (segundos) 0 20 40 60 0.00.20.40.60.8 Consultas agrupadas en 100s tiempo (segundos) 0 20 40 60 0.00.20.40.60.8 Queridas agrupadas en 100s en el tiempo (segundos (segundos (segundos) CTS de línea de base con CTS de almacenamiento en caché sin almacenamiento en caché Figura 3: Tiempo para generar fragmentos para 10 documentos por consulta, promediado sobre cubos de 100 consultas, para las primeras 7000 consultas excitadas en WT10G. Todo el código fuente se compiló usando GCC4.1.1 con -O9 Optimización. Los tiempos se ejecutaron en una máquina desocupada y se promediaron en 10 corridas, con memoria enjuagada entre ejecuciones para eliminar cualquier almacenamiento en caché de archivos de datos. En ausencia de evidencia de lo contrario, suponemos que es importante modelar secuencias de llegada de consultas realistas y la distribución de repeticiones de consulta para nuestros experimentos. En consecuencia, las colecciones de prueba que carecen de registros de consultas reales, como TREC ad-hoc y .gov2 no se consideraron adecuadas. La obtención de registros de consulta extensos y los DOC de resultados asociados para una colección grande correspondiente no es fácil. Hemos utilizado dos colecciones (WT10G y WT100G) de la pista web de TREC [8] junto con consultas de los registros de Excite del mismo período (c. 1997). Además, también utilizamos una colección de tamaño mediano WT50G, obtenida muestran aleatoriamente la mitad de los documentos de WT100G. Las dos primeras filas de la Tabla 1 dan el número de documentos y el tamaño en MB de estas colecciones. Las dos filas finales de la Tabla 1 muestran el tamaño de los conjuntos de documentos resultantes después de la compresión con los esquemas de línea de base y CTS. Como se esperaba, CTS admite una pequeña pérdida de compresión sobre ZLIB, pero ambos reducen sustancialmente el tamaño del texto a aproximadamente el 20% del tamaño original y sin comprimir. Tenga en cuenta que las cifras para CTS no incluyen la asignación inversa de token entero a cadena que se requiere para producir los fragmentos finales, ya que ocupa RAM. Es 1024 MB en estos experimentos. El motor de búsqueda Zettair [25] se utilizó para producir una lista de documentos para resumir para cada consulta. Para la mayoría de los experimentos, se utilizó el esquema de puntuación OKAPI BM25 para determinar las clasificaciones de documentos. Para los experimentos de almacenamiento de almacenamiento estático reportados en la Sección 5, la puntuación de cada documento WT10g WT50G WT100G BASED 75 157 183 CTS 38 70 77 Reducción en el tiempo 49% 56% 58% Tabla 2: Tiempo promedio (MSEC) para las preguntas finales de 7000 en las cuentas finales en elExcite registros utilizando los sistemas de línea de base y CTS en las 3 colecciones de prueba.es un promedio ponderado de 50:50 del puntaje BM25 (normalizado por el documento de puntuación superior para cada consulta) y un puntaje para cada documento independiente de cualquier consulta. Esto es para simular los efectos de los algoritmos de clasificación como PageRank [1] en la distribución de solicitudes de documentos al motor del fragmento. En nuestro caso, utilizamos el recuento de acceso normalizado [5] calculado desde los 20 documentos principales que regresaron a los primeros 1 millón de consultas del registro de excite para determinar el componente de puntaje independiente de la consulta. Los puntos en la Figura 3 indican el tiempo medio de ejecución para generar 10 fragmentos para cada consulta, promediado en grupos de 100 consultas, para las primeras 7000 consultas en el registro de consultas de excita. Solo se muestran los datos para WT10G, pero las otras colecciones mostraron patrones similares. El eje x indica el grupo de 100 consultas;Por ejemplo, 20 indica las consultas 2001 a 2100. Claramente, hay un efecto de almacenamiento en caché, con tiempos que caen sustancialmente después de que se procesan las primeras 1000 consultas más o menos. Todo esto se debe a los bloques de disco de almacenamiento en caché del sistema operativo y quizás prefabando datos antes de solicitudes de lectura específicas. Esto es evidente porque el sistema de referencia no tiene grandes estructuras de datos internos para aprovechar el almacenamiento en caché no basado en el disco, simplemente abre y procesa archivos, y la velocidad es evidente para el sistema de referencia. Parte de esta ganancia se debe a la localidad espacial de las referencias de disco generadas por la transmisión de consulta: las consultas repetidas ya tendrán sus archivos de documentos almacenados en caché en la memoria;y similares consultas diferentes que devuelven los mismos documentos se beneficiarán del almacenamiento en caché de documentos. Pero cuando el registro se procesa después de eliminar todas, excepto la primera solicitud, para cada documento, la aceleración pronunciada sigue siendo evidente a medida que se procesan más consultas (no se muestran en la figura). Esto sugiere que el sistema operativo (o el disco en sí) está leyendo y almacenando una mayor cantidad de datos que la cantidad solicitada y que esto brinda beneficio con suficiente frecuencia para marcar una diferencia apreciable en los tiempos de generación de fragmentos. Esto se confirma por la curva etiquetada con CTS sin almacenamiento en caché, que se generó después de montar el sistema de archivos con una opción de caché (directio en Solaris). Con el almacenamiento en caché del disco apagado, el tiempo promedio para generar fragmentos varía poco. El tiempo para generar diez fragmentos para una consulta, promediado sobre las finales consultas de 7000 en el registro de excite a medida que se han disipado los efectos de almacenamiento en caché, se muestran en la Tabla 2. Una vez que el sistema se ha estabilizado, CTS es más del 50% más rápido que el sistema de referencia. Esto se debe principalmente a los CTS que coinciden con enteros individuales para la mayoría de las palabras de consulta, en lugar de comparar cadenas en el sistema de referencia. La Tabla 3 muestra un desglose del tiempo promedio para generar diez fragmentos en las finales consultas de 7000 del registro de excite en la colección WT50G cuando se procesan los documentos completos, y cuando solo se procesa la primera mitad de cada documento. Como se puede ver, la mayoría del tiempo dedicado a generar un fragmento es localizar el documento en el disco (buscar): 64% para documentos completos y 75% para la mitad de los documentos. Incluso si el monto de procesamiento de un documento debe ser% de DOC procesado para buscar puntaje de lectura y decodificación 100% 45 4 21 50% 45 4 11 Tabla 3: Tiempo para generar 10 fragmentos para una sola consulta (MSEC) para la colección WT50G promediada sobre elConsultas finales de Excite 7000 cuando se procesa todo cada documento (100%) o solo la primera mitad de cada documento (50%).Someterse a la mitad, como en la segunda fila de la tabla, solo hay una reducción del 14% en el tiempo total requerido para generar un fragmento. Como la ubicación de documentos en el almacenamiento secundario ocupa una proporción tan grande del tiempo de generación de fragmentos, parece lógico tratar de reducir su impacto a través del almacenamiento en caché.5. El almacenamiento en caché de documentos en la Sección 3 Observamos que el motor del fragmento tendría su propia RAM en proporción al tamaño de la recopilación de documentos. Por ejemplo, en un motor de búsqueda completo de Web, el motor de fragmento se distribuiría a través de muchas estaciones de trabajo, cada una con al menos 4 GB de RAM. En una pequeña empresa, el motor del fragmento puede estar compartiendo RAM con todos los demás subsistemas en una sola estación de trabajo, por lo tanto, solo tiene 100 MB disponibles. En esta sección usamos simulación para medir el número de golpes de caché en el motor de fragmento a medida que varía el tamaño de la memoria. Comparamos dos políticas de almacenamiento en caché: un caché estático, donde el caché se carga con tantos documentos como puede contener antes de que el sistema comience a responder consultas, y luego nunca cambia;y un caché de uso menos reciente, que comienza como para el caché estático, pero cada vez que se accede a un documento se mueve al frente de una cola, y si se obtiene un documento desde el disco, el último elemento en la cola se desalija. Tenga en cuenta que los documentos se cargan primero en los cachés en orden de disminución de la puntuación independiente de consulta, que se calcula como se describe en la Sección 4.4. Las simulaciones también suponen que existe un caché de consulta para las consultas más frecuentes de Q, y que el motor de fragmento nunca procesa estas consultas. Todas las consultas pasadas a las simulaciones provienen de la segunda mitad del registro de consultas de excita (la primera mitad se usa para calcular puntajes independientes de consulta), y se detienen, se detienen y tienen sus términos ordenados alfabéticamente. Esta alteración final simplemente permite que consultas como Red Dog y Dog Red devuelvan los mismos documentos, como sería el caso en un motor de búsqueda donde los operadores de frases explícitas se requerirían en la consulta para hacer cumplir el orden y la proximidad. La Figura 4 muestra el porcentaje de acceso al documento que alcanzó el caché utilizando los dos esquemas de almacenamiento en caché, con Q 0 o 10,000, en 535,276 consultas de excite en WT100G. El xaxis muestra el porcentaje de documentos que se mantienen en el caché, por lo que el 1.0% corresponde a aproximadamente 185,000 documentos. A partir de esta cifra, está claro que el almacenamiento en caché incluso un pequeño porcentaje de los documentos tiene un gran impacto en reducir el tiempo de búsqueda para la generación de fragmentos. Con el 1% de los documentos almacenados en caché, se evitan alrededor de 222 MB para la colección WT100G, se evitan alrededor del 80% de las buscas de disco. El caché estático funciona sorprendentemente bien (cuadrados en la Figura 4), pero lo supera el caché LRU (círculos). Sin embargo, en una implementación real de LRU, puede haber fragmentación del caché a medida que los documentos se intercambian dentro y fuera. La razón del gran impacto del caché del documento es 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Tamaño de la caché ( % de la colección) % de los accesorioscachehits lru q = 0 lru q = 10,000 estáticos q = 0 estática q = 10,000 Figura 4: porcentaje de laTiempo en el que el motor del fragmento no tiene que ir al disco para generar un fragmento trazado contra el tamaño del caché del documento como porcentaje de todos los documentos en la colección. Los resultados son de una simulación en WT100G con 535,276 consultas excitadas.Eso, para una colección en particular, es mucho más probable que aparezcan algunos documentos en las listas de resultados que en otros. Este efecto ocurre en parte debido a la distribución de frecuencia de consulta aproximadamente Zipfian, y en parte porque la mayoría de los motores de búsqueda web emplean métodos de clasificación que combinan puntajes basados en la consulta con puntajes estáticos (a priori) determinados a partir de factores como medidas de gráficos de enlace, características de URL, puntajes de spam y spam y spam.Entonces en [17]. Es mucho más probable que los documentos con puntajes estáticos altos se recuperen que otros. Además del caché del documento, la RAM del motor del fragmento también debe contener la tabla de decodificación de CTS que mapea los enteros en las cuerdas, que está limitada por un parámetro en el tiempo de compresión (1 GB en nuestros experimentos aquí). Esto es más que compensado por el tamaño reducido de cada documento, lo que permite más documentos en el caché del documento. Por ejemplo, el uso de CTS reduce el tamaño promedio del documento de 5.7 kb a 1.2 kb (como se muestra en la Tabla 1), por lo que una RAM de 2 GB podría tener 368,442 documentos sin comprimir (2% de la colección), o 850,691 documentos más una descompresión de 1 GBTabla (5% de la colección). De hecho, una mayor experimentación con el tamaño del modelo revela que el modelo puede ser muy pequeño y aún CTS ofrece una buena compresión y tiempos de puntuación rápidas. Esto se evidencia en la Figura 5, donde el tamaño comprimido de WT50G se muestra en los símbolos sólidos. Tenga en cuenta que cuando no se usa compresión (el tamaño del modelo es de 0 MB), la colección es de solo 31 GB como marcado HTML, JavaScript, y la puntuación repetida se ha descartado como se describe en la Sección 4.1. Con un modelo de 5 MB, el tamaño de la colección cae en más de la mitad a 14 GB, y aumentar el tamaño del modelo a 750 MB solo provoca una caída de 2 GB en el tamaño de la recolección. La Figura 5 también muestra el tiempo promedio de anotar y decodificar un fragmento (excluyendo el tiempo de búsqueda) con los diferentes tamaños del modelo (símbolos abiertos). Una vez más, hay una gran velocidad cuando se usa un modelo de 5 MB, pero poco 0 200 400 600 15202530 Tamaño del modelo (MB) de colección (GB) o Tiempo (MSEC) Tiempo (GB) (MSEC) Figura 5: Tamaño de recolección de la recolección de la recolección de la recolección de la recolección deLa colección WT50G cuando se comprime con CTS utilizando diferentes límites de memoria en el modelo, y el tiempo promedio para generar un fragmento único, excluyendo el tiempo de búsqueda en consultas de excite 20000 utilizando esos modelos.Mejora con modelos más grandes. Resultados similares se mantienen para la colección WT100G, donde un modelo de aproximadamente 10 MB ofrece un espacio sustancial y ahorros de tiempo en ningún modelo, pero los retornos disminuyen a medida que aumenta el tamaño del modelo. Además de la compresión, hay otro enfoque para reducir el tamaño de cada documento en el caché: no almacene el documento completo en caché. Más bien, almacenan oraciones que probablemente se usen en fragmentos en el caché, y si durante la generación de fragmentos en un documento en caché, las puntuaciones de las oraciones no alcanzan un cierto umbral, luego recupere todo el documento del disco. Esto plantea preguntas sobre cómo elegir oraciones de documentos para poner en caché y qué dejar en el disco, que abordamos en la siguiente sección.6. Las oraciones de reordenamiento de oraciones dentro de cada documento se pueden reordenar para que las oraciones que muy probablemente aparezcan en los fragmentos estén en la parte delantera del documento, por lo tanto, se procesan primero en el momento de la consulta, mientras que las oraciones menos probables se relegan a la parte posterior del documento. Luego, durante el tiempo de consulta, si se encuentran k oraciones con un puntaje que excede algún umbral antes de procesarse todo el documento, se ignora el resto del documento. Además, para mejorar el almacenamiento en caché, solo la cabeza de cada documento puede almacenarse en el caché, con la cola que reside en el disco. Tenga en cuenta que suponemos que el motor de búsqueda debe proporcionar copias en caché de un documento, es decir, el texto exacto del documento tal como estaba indexado, entonces esto sería atendido por otro subsistema en la Figura 1, y no por el alteradoCopiar, almacenamos en el motor del fragmento. Ahora presentamos cuatro enfoques de reordenamiento de oraciones.1. Ordene natural Las primeras oraciones de un documento bien escrita generalmente describen mejor el contenido del documento [12]. Por lo tanto, simplemente procesar un documento para producir un fragmento de calidad. Desafortunadamente, sin embargo, los documentos web a menudo no se escriben bien, con pocas habilidades de escritura editorial o profesional de la creación de una obra de mérito literario. Más importante aún, tal vez, es que estamos produciendo fragmentos con sesgo de consultas, y no hay garantía de que aparezcan términos de consulta en las oraciones hacia el frente de un documento.2. Términos significativos (ST) Luhn introdujeron el concepto de una oración significativa que contiene un grupo de términos significativos [12], un concepto que funciona bien por Tombros y Sanderson [20]. Sea FD, T la frecuencia del término T en el documento D, entonces se determina que el término T es significativo si FD, t ≥ 8 <: 7 - 0.1 × (25 - SD), si SD <25 7, si 25 ≤ SD≤ 40 7 + 0.1 × (SD - 40), de lo contrario, donde SD es el número de oraciones en el documento d.Una sección entre corchetes se define como un grupo de términos en los que los términos más a la izquierda y a la derecha son términos significativos, y no se dividen términos significativos en la sección entre paréntesis por más de cuatro términos no significativos. La puntuación de una sección entre corchetes es el cuadrado del número de palabras significativas que caen en la sección, dividida por el número total de palabras en toda la oración. El puntaje a priori para una oración se calcula como el máximo de todos los puntajes para las secciones entre corchetes de la oración. Luego ordenamos las oraciones por este puntaje.3. Consulta Registro basado (QLT) Muchas consultas web se repiten, y un pequeño número de consultas constituyen un gran volumen de búsquedas totales [9]. Para aprovechar este sesgo, las oraciones que contienen muchos términos de consulta pasados deben promoverse al frente de un documento, mientras que las oraciones que contienen pocos términos de consulta deben ser degradados. En este esquema, las oraciones están ordenadas por el número de términos de oraciones que ocurren en el registro de consulta. Para garantizar que las oraciones largas no dominen las oraciones cualitativas más cortas, el puntaje asignado a cada oración se divide por el número de términos en esa oración, lo que le da a cada oración un puntaje entre 0 y 1. 4. Consulta Registro basado (QLU) Este esquema es como para QLT, pero los términos repetidos en la oración solo se cuentan una vez. Al reordenar las oraciones utilizando esquemas ST, QLT o QLU, nuestro objetivo es finalizar la generación de fragmentos antes de que se use el orden natural, pero aún así producir oraciones con el mismo número de términos de consulta únicos (d en la Figura 2), número total de consultaTérminos (c), la misma puntuación posicional (H+) y el mismo tramo máximo (k). En consecuencia, realizamos experimentos que compararon los métodos, el primer 80% del registro de consulta de excita se usó para reordenar las oraciones cuando sea necesario, y el 20% final para las pruebas. La Figura 6 muestra las diferencias en los componentes de puntuación del fragmento utilizando cada uno de los tres métodos sobre el método de orden natural. Está claro que las oraciones de clasificación que usan el método de términos significativos (ST) conducen al cambio más pequeño en los componentes de puntuación de oraciones. El mayor cambio sobre todos los métodos está en el componente de la posición de oración (H +) de la puntuación, lo que es de esperar ya que no se garantiza que las oraciones de liderazgo y encabezado se procesen después de que se reordenen las oraciones. El segundo componente más afectado es el número de términos de consulta distintos en una oración devuelta, pero si solo el primer 50% del documento se procesa con el método ST, hay una caída de solo el 8% en el número de términos de consulta distintos encontradosen fragmentos. Dependiendo de cómo estos diversos componentes se ponderen para calcular una puntuación de fragmento general, se puede argumentar que hay poco efecto general en las puntuaciones al procesar solo la mitad del documento utilizando el método ST. CONTRO (K) Recuento de término (c) Posición de oración (H + L) Términos distintos (d) 40% 50% 60% 70% ST QLT QLU ST QLT QLU ST QLT QLU ST QLT QLU ST QLT QLU RELATIVEDIFENDIFERENTONATONATURALORDER Tamaño del tamaño del 90% utilizado 90%80% 70% 60% 50% 0% 10% 20% 30% Figura 6: Diferencia relativa en los componentes de la puntuación del fragmento en comparación con los documentos ordenados naturales cuando se reduce la cantidad de documentos procesados, y las oraciones en el documento se reordenan mediante consultaRegistros (QLT, QLU) o términos significativos (ST).7. Discusión En este documento, hemos descrito el esquema de algoritmos y compresión que harían un buen subsistema de motor de fragmento para generar fragmentos de texto del tipo que se muestra en las páginas de resultados de los conocidos motores de búsqueda web. Nuestros experimentos no solo muestran que nuestro esquema es más de un 50% más rápido que la línea de base obvia, sino que también revelan algunos aspectos muy importantes del problema de la generación del fragmento. Principalmente, los documentos de almacenamiento en caché evitan la búsqueda de costos para la memoria secundaria para cada documento que se resumirá y es vital para la generación de fragmentos rápidos. Nuestras simulaciones de almacenamiento en caché muestran que si tan solo el 1% de los documentos se pueden almacenar en caché en RAM como parte del motor de fragmento, posiblemente distribuido en muchas máquinas, entonces se puede evitar alrededor del 75% de las buscas. Nuestro segundo resultado principal es que mantener solo la mitad de cada documento en RAM, duplicando efectivamente el tamaño del caché, tiene poco efecto en la calidad de los fragmentos finales generados a partir de esos medios documentos, siempre que las oraciones que se mantienen en la memoria se eligen utilizandoEl algoritmo de término significativo de Luhn [12]. Tanto nuestro documento de compresión como los esquemas de compactación reducen drásticamente el tiempo necesario para generar fragmentos. Tenga en cuenta que estos resultados se generan utilizando un subconjunto de 100 GB de la web, y el registro de consulta Excite recopilado a partir del mismo período que se creó ese subconjunto. Asumimos, ya que no hay evidencia de lo contrario, que esta colección y registro son representativos de la entrada del motor de búsqueda en otros dominios. En particular, podemos escalar nuestros resultados para examinar qué recursos se requerirían, utilizando nuestro esquema, para proporcionar un motor de fragmento para toda la red mundial. Asumiremos que el motor del fragmento se distribuye en las máquinas M, y que hay N páginas web en la colección para ser indexadas y atendidas por el motor de búsqueda. También asumimos una carga equilibrada para cada máquina, por lo que cada máquina sirve sobre documentos N/M, que se logra fácilmente en la práctica. Cada máquina, por lo tanto, requiere que RAM sostenga lo siguiente.• El modelo CTS, que debe ser 1/1000 del tamaño de la colección sin comprimir (utilizando los resultados en la Figura 5 y Williams et al. [23]). Suponiendo un tamaño promedio de documento sin comprimir de 8 kb [11], esto requeriría N/M × 8.192 bytes de memoria.• Un caché del 1% de todos los documentos N/M. Cada documento requiere 2 kb cuando se comprime con CTS (Tabla 1), y solo se requiere la mitad de cada documento utilizando el reordenamiento de las oraciones ST, lo que requiere un total de bytes N/M × 0.01 × 1024.• La matriz de compensación que proporciona la posición de inicio de cada documento en el archivo comprimido único: 8 bytes por documentos N/M. La cantidad total de RAM requerida por una sola máquina, por lo tanto, sería N/M (8.192 + 10.24 + 8) bytes. Suponiendo que cada máquina tiene 8 GB de RAM, y que hay 20 mil millones de páginas para indexar en la web, se requeriría un total de M = 62 máquinas para el motor de fragmento. Por supuesto, en la práctica, se puede requerir más máquinas para administrar el sistema distribuido, para proporcionar servicios de respaldo para máquinas fallidas y otros servicios de red. Estas máquinas también necesitarían acceso a 37 TB de disco para almacenar las representaciones de documentos comprimidas que no estaban en caché. En este trabajo, hemos evitado deliberadamente comprometerse con un método de puntuación particular para oraciones en documentos. Más bien, hemos informado resultados de precisión en términos de los cuatro componentes que previamente se han demostrado que son importantes para determinar fragmentos útiles [20]. El método CTS puede incorporar cualquier métrica nueva que pueda surgir en el futuro que se calcule en palabras completas. Sin embargo, las técnicas de compactación del documento que utilizan la reordenamiento de oraciones eliminan la relación espacial entre las oraciones, por lo que si una técnica de puntuación se basa en la posición de una oración dentro de un documento, las técnicas agresivas de compactación informadas aquí no se pueden usar. Una variación en el enfoque de compresión semiestática que hemos adoptado en este trabajo se ha utilizado con éxito en el diseño anterior del motor de búsqueda [24], pero hay esquemas de compresión alternativos que permiten la coincidencia directa en el texto comprimido (ver Navarro y M¨akinen [15] para una encuesta reciente). Como buscar el tiempo domina el proceso de generación del fragmento, no nos hemos centrado en esta parte de la generación del fragmento en detalle en este documento. Exploraremos esquemas de compresión alternativos en el trabajo futuro. Agradecimientos Este trabajo fue apoyado en parte por el Proyecto ARC Discovery DP0558916 (AT). Gracias a Nick Lester y Justin Zobel por valiosas discusiones.8. Referencias [1] S. Brin y L. Page. La anatomía de un motor de búsqueda web hipertextual a gran escala. En WWW7, páginas 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J. A. Tomlin y D. P. Williamson. En WWW2003, Budapest, Hungría, mayo de 2003. [3] T. Fagni, R. Perego, F. Silvestri y S. Orlando. Aumentando el rendimiento de los motores de búsqueda web: almacenamiento en caché y captación de los resultados de consultas mediante la explotación de datos de uso histórico. Syst., 24 (1): 51-78, 2006. [4] J-L Gailly y M. Adler. Biblioteca de compresión Zlib.www.zlib.net. Consultado en enero de 2007. [5] S. García, H.E. En V. Estivill-Castro, editor, Proc. Conferencia de Ciencias de la Computación de Australasia, páginas 7-14, Dunedin, Nueva Zelanda, 2004. [6] S. Ghemawat, H. Gobioff y S. Leung. En Sosp 03: Proc.del 19º Simposio ACM sobre principios de sistemas operativos, páginas 29-43, Nueva York, NY, EE. UU., 2003. ACM Press.[7] J. Goldstein, M. Kantrowitz, V. Mittal y J. Carbonell. Resumen de documentos de texto: selección de oraciones y métricas de evaluación. En Sigir99, páginas 121-128, 1999. [8] D. Hawking, Nick C. y Paul Thistlewaite. Descripción general de TREC-7 Pista de colección muy grande. En Proc.de Trec-7, páginas 91-104, noviembre de 1998. [9] B. J. Jansen, A. Spink y J. Pedersen. Una comparación temporal de la búsqueda web de Altavista. Tecnología.(Jasist), 56 (6): 559-570, abril de 2005. [10] J. Kupiec, J. Pedersen y F. Chen. Un resumen de documentos capacitables. En Sigir95, páginas 68-73, 1995. [11] S. Lawrence y C.L. Accesibilidad de la información en la web. Nature, 400: 107-109, julio de 1999. [12] H.P. La creación automática de resúmenes de literatura. IBM Journal, páginas 159-165, abril de 1958. [13] I. Mani. Resumen automático, volumen 3 de procesamiento del lenguaje natural. John Benjamins Publishing Company, Amsterdam/Filadelfia, 2001. [14] A. Moffat, J. Zobel y N. Sharman. Compresión de texto para bases de datos de documentos dinámicos. Conocimiento e ingeniería de datos, 9 (2): 302-313, 1997. [15] G. Navarro y V. M¨akinen. A aparecer.[16] D. R. Radev, E. Hovy y K. McKeown. Introducción al número especial sobre resumen. Lingüista., 28 (4): 399-408, 2002. [17] M. Richardson, A. Prakash y E. Brill. Beyond PageRank: aprendizaje automático para clasificación estática. En WWW06, páginas 707-715, 2006. [18] T. Sakai y K. Sparck-Jones. Resúmenes genéricos para la indexación en la recuperación de la información. En Sigir01, páginas 190-198, 2001. [19] H. G. Silber y K. F. McCoy. Las cadenas léxicas calculadas de manera eficiente como una representación intermedia para el resumen de texto automático. Lingüista., 28 (4): 487-496, 2002. [20] A. Tombros y M. Sanderson. Ventajas de resúmenes sesgados de consulta en la recuperación de información. En Sigir98, páginas 2-10, Melbourne, Aust., Agosto de 1998. [21] R. W. White, I. Ruthven y J. M. José. Encontrar documentos relevantes utilizando oraciones de clasificación superior: una evaluación de dos esquemas alternativos. En Sigir02, páginas 57-64, 2002. [22] H. E. Williams y J. Zobel. Comprimir enteros para el acceso rápido a los archivos. J., 42 (3): 193-201, 1999. [23] H.E. International Journal on Digital Bibliotecas, 5 (2): 99-105, abril de 2005. [24] I. H. Witten, A. Moffat y T. C. Bell. Gestión de gigabytes: compresión e indexación de documentos e imágenes. Morgan Kaufmann Publishing, San Francisco, segunda edición, mayo de 1999. [25] El motor de búsqueda de Zettair.www.seg.rmit.edu.au/zettair.",
    "original_sentences": [
        "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
        "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
        "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
        "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
        "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
        "Using simulation, we examine snippet generation performance for different size RAM caches.",
        "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
        "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
        "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
        "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
        "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
        "Snippets are short fragments of text extracted from the document content (or its metadata).",
        "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
        "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
        "The addition of informative snippets to search results may substantially increase their value to searchers.",
        "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
        "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
        "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
        "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
        "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
        "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
        "Special-purpose filesystems have been built to address these problems [6].",
        "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
        "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
        "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
        "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
        "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
        "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
        "However, this is only true if there is no caching of documents in RAM.",
        "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
        "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
        "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
        "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
        "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
        "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
        "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
        "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
        "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
        "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
        "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
        "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
        "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
        "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
        "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
        "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
        "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
        "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
        "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
        "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
        "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
        "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
        "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
        "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
        "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
        "The Lexicon Engine maps query terms to integers. 2.",
        "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
        "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
        "The Meta Data Engine fetches other information about each document to construct the results page.",
        "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
        "OUT Remove the number of sentences required from the heap to form the summary.",
        "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
        "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
        "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
        "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
        "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
        "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
        "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
        "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
        "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
        "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
        "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
        "Both are limited to a maximum of 50 characters.",
        "Adjacent, repeating characters are removed from the punctuation.",
        "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
        "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
        "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
        "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
        "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
        "For example, The most wonderful company on earth.",
        "Products.",
        "Service.",
        "About us.",
        "Contact us.",
        "Try before you buy.",
        "Similar, but often not identical, navigational material is typically presented on every page within a site.",
        "This material tends to lower the quality of summaries and slow down summary generation.",
        "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
        "In wt100g, the average web page size is more than half the current Web average [11].",
        "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
        "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
        "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
        "Each document is stored in a single file.",
        "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
        "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
        "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
        "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
        "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
        "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
        "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
        "We use the vbyte coding scheme to code the word tokens [22].",
        "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
        "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
        "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
        "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
        "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
        "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
        "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
        "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
        "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
        "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
        "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
        "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
        "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
        "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
        "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
        "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
        "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
        "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
        "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
        "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
        "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
        "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
        "RAM.",
        "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
        "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
        "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
        "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
        "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
        "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
        "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
        "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
        "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
        "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
        "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
        "It is 1024 Mb in these experiments.",
        "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
        "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
        "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
        "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
        "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
        "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
        "Only the data for wt10g is shown, but the other collections showed similar patterns.",
        "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
        "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
        "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
        "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
        "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
        "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
        "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
        "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
        "With disk caching turned off, the average time to generate snippets varies little.",
        "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
        "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
        "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
        "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
        "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
        "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
        "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
        "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
        "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
        "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
        "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
        "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
        "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
        "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
        "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
        "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
        "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
        "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
        "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
        "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
        "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
        "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
        "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
        "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
        "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
        "Documents with high static scores are much more likely to be retrieved than others.",
        "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
        "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
        "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
        "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
        "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
        "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
        "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
        "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
        "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
        "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
        "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
        "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
        "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
        "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
        "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
        "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
        "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
        "We now introduce four sentence reordering approaches. 1.",
        "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
        "Thus simply processing a document in order should yield a quality snippet.",
        "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
        "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
        "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
        "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
        "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
        "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
        "We then sort the sentences by this score. 3.",
        "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
        "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
        "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
        "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
        "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
        "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
        "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
        "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
        "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
        "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
        "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
        "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
        "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
        "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
        "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
        "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
        "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
        "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
        "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
        "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
        "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
        "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
        "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
        "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
        "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
        "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
        "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
        "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
        "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
        "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
        "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
        "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
        "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
        "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
        "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
        "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
        "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
        "We will explore alternate compression schemes in future work.",
        "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
        "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
        "REFERENCES [1] S. Brin and L. Page.",
        "The anatomy of a large-scale hypertextual Web search engine.",
        "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
        "A. Tomlin, and D. P. Williamson.",
        "Searching the workplace web.",
        "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
        "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
        "ACM Trans.",
        "Inf.",
        "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
        "Zlib Compression Library. www.zlib.net.",
        "Accessed January 2007. [5] S. Garcia, H.E.",
        "Williams, and A. Cannane.",
        "Access-ordered indexes.",
        "In V. Estivill-Castro, editor, Proc.",
        "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
        "The google file system.",
        "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
        "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
        "Summarizing text documents: sentence selection and evaluation metrics.",
        "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
        "Overview of TREC-7 Very Large Collection Track.",
        "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
        "A temporal comparison of altavista web searching.",
        "J.",
        "Am.",
        "Soc.",
        "Inf.",
        "Sci.",
        "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
        "A trainable document summarizer.",
        "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
        "Giles.",
        "Accessibility of information on the web.",
        "Nature, 400:107-109, July 1999. [12] H.P.",
        "Luhn.",
        "The automatic creation of literature abstracts.",
        "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
        "Automatic Summarization, volume 3 of Natural Language Processing.",
        "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
        "Text compression for dynamic document databases.",
        "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
        "Compressed full text indexes.",
        "ACM Computing Surveys, 2007.",
        "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
        "Introduction to the special issue on summarization.",
        "Comput.",
        "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
        "Beyond pagerank: machine learning for static ranking.",
        "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
        "Generic summaries for indexing in information retrieval.",
        "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
        "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
        "Comput.",
        "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
        "Advantages of query biased summaries in information retrieval.",
        "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
        "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
        "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
        "Compressing integers for fast file access.",
        "Comp.",
        "J., 42(3):193-201, 1999. [23] H.E.",
        "Williams and J. Zobel.",
        "Searchable words on the Web.",
        "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
        "Managing Gigabytes: Compressing and Indexing Documents and Images.",
        "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
        "Accessed January 2007."
    ],
    "error_count": 0,
    "keys": {
        "search engine": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of <br>search engine</br> users.",
                "In this paper we explore the algorithms and data structures required as part of a <br>search engine</br> to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb <br>search engine</br> to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "<br>search engine</br> ARCHITECTURES A <br>search engine</br> must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the <br>search engine</br> indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a <br>search engine</br>.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web <br>search engine</br> may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair <br>search engine</br> [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web <br>search engine</br>, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a <br>search engine</br> where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the <br>search engine</br> is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of <br>search engine</br> input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the <br>search engine</br>.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous <br>search engine</br> design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web <br>search engine</br>.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair <br>search engine</br>. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "hughw@microsoft.com Resumen La presentación de los fragmentos de documentos sesgados de consulta como parte de las páginas de resultados presentadas por los motores de búsqueda se ha convertido en una expectativa de los usuarios de \"motor de búsqueda\".buscador",
                "En este artículo exploramos los algoritmos y las estructuras de datos requeridas como parte de un \"motor de búsqueda\" para permitir una generación eficiente de fragmentos sesgados de consulta.buscador",
                "Hasta donde sabemos, Google fue el primer \"motor de búsqueda\" de todo tuweb en proporcionar resúmenes sesgados de consulta, pero Brin y la página [1] solo se encuentran bajo el encabezado del trabajo futuro.buscador",
                "Arquitecturas de \"motor de búsqueda\" Un \"motor de búsqueda\" debe realizar una variedad de actividades, y se compone de muchos subsistemas, como se muestra en las cajas de la Figura 1. Motor de búsqueda",
                "Dependiendo de la cantidad de documentos que los indexan el \"motor de búsqueda\", los datos y los procesos para cada motor de clasificación que se arrastra el motor del motor del motor del motor Léxico Léxico Meta Data Engine Snippet Term & Doc#S Fnippetperdoc Web Consuly Resultados Página Página Término#S#S Invertedlists InvertedlistDOCS Título Perdoc, URL, etc. Doc documento Meta Data Términos de consulta Término de consulta#S Figura 1: Una abstracción de algunos de los subsistemas en un \"motor de búsqueda\".buscador",
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Motor de búsqueda",
                "Si bien es manejable para nuestras pequeñas colecciones de pruebas o pequeñas empresas con millones de documentos, un \"motor de búsqueda\" web completo puede requerir múltiples documentos para habitar archivos únicos, o un sistema de archivos de propósito especial [6].buscador",
                "El \"motor de búsqueda\" de Zettair [25] se utilizó para producir una lista de documentos para resumir para cada consulta.buscador",
                "Por ejemplo, en un \"motor de búsqueda\" completo, el motor del fragmento se distribuiría a través de muchas estaciones de trabajo, cada una con al menos 4 GB de RAM.buscador",
                "Esta alteración final simplemente permite que consultas como Red Dog y Dog Red devuelvan los mismos documentos, como sería el caso en un \"motor de búsqueda\" en el que se requerirían operadores de frases explícitas en la consulta para aplicar el orden y la proximidad de los términos.buscador"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "snippet generation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces <br>snippet generation</br> time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast <br>snippet generation</br> process.",
                "Using simulation, we examine <br>snippet generation</br> performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of <br>snippet generation</br>.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK <br>snippet generation</br> is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the <br>snippet generation</br> algorithms themselves are not isolated, so it is difficult to infer <br>snippet generation</br> time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to <br>snippet generation</br>.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For <br>snippet generation</br>, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for <br>snippet generation</br>, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in <br>snippet generation</br> times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of <br>snippet generation</br> time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for <br>snippet generation</br>.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during <br>snippet generation</br> on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate <br>snippet generation</br> earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the <br>snippet generation</br> problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast <br>snippet generation</br>.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the <br>snippet generation</br> process, we have not focused on this portion of the <br>snippet generation</br> in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Comenzamos proponiendo y analizando un método de compresión de documentos que reduzca el tiempo de \"generación de fragmentos\" en un 58% sobre una línea de base utilizando la biblioteca de compresión ZLIB.generación de fragmentos",
                "Estos experimentos revelan que encontrar documentos sobre el almacenamiento secundario domina el costo total de generar fragmentos y, por lo tanto, los documentos de almacenamiento en caché en RAM son esenciales para un proceso rápido de \"generación de fragmentos\".generación de fragmentos",
                "Usando la simulación, examinamos el rendimiento de la \"generación de fragmentos\" para los cachés RAM de diferentes tamaños.generación de fragmentos",
                "El control de la RAM de los sistemas físicos para la experimentación es difícil, por lo tanto, usamos la simulación para mostrar que los documentos de almacenamiento en caché mejoran drásticamente el rendimiento de la \"generación de fragmentos\".generación de fragmentos",
                "El trabajo relacionado \"Generación de fragmentos\" es un tipo especial de resumen de documentos extractivos, en el que las oraciones o fragmentos de oraciones se seleccionan para su inclusión en el resumen sobre el grado en que coinciden con la consulta de búsqueda.generación de fragmentos",
                "White et al.3. Generación de fragmentos",
                "Tenga en cuenta que puede haber varios otros subsistemas, como el motor de publicidad o el motor de análisis que se podría agregar fácilmente al diagrama, pero nos hemos concentrado en los subsistemas que son relevantes para la \"generación de fragmentos\".generación de fragmentos",
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Generación del fragmento",
                "Para la \"generación del fragmento\", los documentos requeridos se descomprimen uno a la vez, y se emplea una búsqueda lineal de los términos de consulta proporcionados.generación de fragmentos",
                "Observamos que esta compresión sin pérdidas de no palabras es aceptable cuando los documentos se usan para la \"generación de fragmentos\", pero puede no ser aceptable para una base de datos de documentos.generación de fragmentos"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "document caching": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from <br>document caching</br>.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "<br>document caching</br> In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Documento de almacenamiento en caché del documento",
                "Parte de esta ganancia se debe a la localidad espacial de las referencias de disco generadas por la transmisión de consulta: las consultas repetidas ya tendrán sus archivos de documentos almacenados en caché en la memoria;y similares consultas diferentes que devuelven los mismos documentos se beneficiarán del \"almacenamiento en caché de documentos\".almacenamiento en caché de documentos",
                "\"Documento en caché\" en la Sección 3 Observamos que el motor del fragmento tendría su propia RAM en proporción al tamaño de la recopilación de documentos.almacenamiento en caché de documentos"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "link graph measure": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Medida del gráfico de enlace"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "performance": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation <br>performance</br> for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-<br>performance</br> evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, <br>performance</br> 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time <br>performance</br> against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the <br>performance</br> of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the <br>performance</br> of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the <br>performance</br> of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Usando la simulación, examinamos el \"rendimiento\" de la generación del fragmento para los cachés de RAM de diferentes tamaños.actuación",
                "Categorías y descriptores de sujetos H.3.3 [Almacenamiento y recuperación de información]: Búsqueda y recuperación de información;H.3.4 [Almacenamiento y recuperación de información]: Sistemas y evaluación de \"rendimiento\" (eficiencia y efectividad);Algoritmos de términos generales, experimentación, medición, \"rendimiento\" 1. rendimiento",
                "Presentamos un nuevo algoritmo y una estructura compacta de un solo archivo diseñada para una generación rápida de fragmentos de alta calidad y comparamos su \"rendimiento\" de espacio/tiempo con una línea de base obvia basada en el compresor ZLIB en varios conjuntos de datos.actuación",
                "El control de la RAM de los sistemas físicos para la experimentación es difícil, por lo tanto, usamos la simulación para mostrar que los documentos de almacenamiento en caché mejoran drásticamente el \"rendimiento\" de la generación de fragmentos.actuación",
                "Silber y McKoy [19] describen un algoritmo de encadenamiento léxico de tiempo lineal para su uso en resúmenes genéricos, pero no ofrecen datos empíricos para el \"rendimiento\" de su algoritmo.actuación",
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones.",
                "Aumentando el \"rendimiento\" de los motores de búsqueda web: almacenamiento en caché y captación de los resultados de consultas mediante la explotación de datos de uso histórico.actuación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "web summary": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Resumen web"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "special-purpose filesystem": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Sistema de archivos de propósito especial"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "ram": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in <br>ram</br> is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size <br>ram</br> caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in <br>ram</br> is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in <br>ram</br>.",
                "Controlling the <br>ram</br> of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of <br>ram</br> or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "<br>ram</br>.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies <br>ram</br>.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own <br>ram</br> in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of <br>ram</br>.",
                "In a small enterprise, the Snippet Engine may be sharing <br>ram</br> with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the <br>ram</br> of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb <br>ram</br> could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in <br>ram</br> as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in <br>ram</br>, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires <br>ram</br> to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of <br>ram</br> required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of <br>ram</br>, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Estos experimentos revelan que encontrar documentos sobre el almacenamiento secundario domina el costo total de generar fragmentos y, por lo tanto, los documentos de almacenamiento en caché en \"RAM\" es esencial para un proceso de generación de fragmentos rápidos.RAM",
                "Usando la simulación, examinamos el rendimiento de la generación del fragmento para cachés de \"RAM\" de diferentes tamaños.RAM",
                "Como el momento de procesar un documento en \"RAM\" es pequeño en comparación con la ubicación y la lectura del documento en la memoria, puede parecer que no se requiere compresión.RAM",
                "Sin embargo, esto solo es cierto si no hay caché de documentos en \"RAM\".RAM",
                "El control de la \"RAM\" de los sistemas físicos para la experimentación es difícil, por lo tanto, usamos la simulación para mostrar que los documentos de almacenamiento en caché mejoran drásticamente el rendimiento de la generación de fragmentos.RAM",
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que \"etiquetas HTML como\" y <p> terminan las oraciones. RAM. RAM.",
                "Si almacenamos todas las palabras y no palabras que aparecen en la colección, y su frecuencia asociada, se requerirían muchos gigabytes de \"RAM\" o un árbol B o una estructura similar en el disco [23].RAM",
                "\"RAM\". RAM",
                "Tenga en cuenta que las cifras para CT no incluyen la asignación inversa de token entero a cadena que se requiere para producir los fragmentos finales, ya que eso ocupa \"RAM\".RAM",
                "El almacenamiento en caché de documentos en la Sección 3 Observamos que el motor de fragmento tendría su propia \"RAM\" en proporción al tamaño de la recopilación de documentos.RAM"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "document compaction": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine <br>document compaction</br>, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The <br>document compaction</br> techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como la presentación de la memoria caché del documento es importante, examinamos la \"compactación del documento\", a diferencia de la compresión, los esquemas imponiendo un pedido a priori de oraciones dentro de un documento, y luego solo permitiendo oraciones líderes en caché para cada documento.compactación de documentos",
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Compactación del documento",
                "Sin embargo, las técnicas de \"compactación de documentos\" utilizando reordenamiento de oraciones eliminan la relación espacial entre las oraciones, por lo que si una técnica de puntuación se basa en la posición de una oración dentro de un documento, las técnicas agresivas de compactación informadas aquí no se pueden utilizar.compactación de documentos"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "text fragment": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo tanto, se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Fragmento de texto"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "precomputed final result page": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo tanto, se supone que las etiquetas HTML como \"y <p> terminan las oraciones."
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "vbyte coding scheme": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the <br>vbyte coding scheme</br> to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo tanto, se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Esquema de codificación VByte",
                "Usamos el \"Esquema de codificación VByte\" para codificar las tokens de palabra [22].esquema de codificación vbyte"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "semi-static compression": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of document cache hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a <br>semi-static compression</br> method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the document cache is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the document cache as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the document cache, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the document cache.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the <br>semi-static compression</br> approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A menudo faltan estos caracteres de puntuación explícitos, por lo tanto, se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Compresión semiestática",
                "El primero es emplear un método de \"compresión semiestática\" en toda la recopilación de documentos, que permitirá una descompresión más rápida con una pérdida de compresión mínima [24].compresión semiestática",
                "Una variación en el enfoque de \"compresión semiestática\" que hemos adoptado en este trabajo se ha utilizado con éxito en el diseño anterior del motor de búsqueda [24], pero existen esquemas de compresión alternativos que permiten una coincidencia directa en texto comprimido (ver Navarro y M¨akinen[15] para una encuesta reciente.) Compresión semiestática"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "document cache": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Fast Generation of Result Snippets in Web Search Andrew Turpin & Yohannes Tsegay RMIT University Melbourne, Australia aht@cs.rmit.edu.au ytsegay@cs.rmit.edu.au David Hawking CSIRO ICT Centre Canberra, Australia david.hawking@acm.org Hugh E. Williams Microsoft Corporation One Microsoft Way Redmond, WA.",
                "hughw@microsoft.com ABSTRACT The presentation of query biased document snippets as part of results pages presented by search engines has become an expectation of search engine users.",
                "In this paper we explore the algorithms and data structures required as part of a search engine to allow efficient generation of query biased snippets.",
                "We begin by proposing and analysing a document compression method that reduces snippet generation time by 58% over a baseline using the zlib compression library.",
                "These experiments reveal that finding documents on secondary storage dominates the total cost of generating snippets, and so caching documents in RAM is essential for a fast snippet generation process.",
                "Using simulation, we examine snippet generation performance for different size RAM caches.",
                "Finally we propose and analyse document reordering and compaction, revealing a scheme that increases the number of <br>document cache</br> hits with only a marginal affect on snippet quality.",
                "This scheme effectively doubles the number of documents that can fit in a fixed size cache.",
                "Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; H.3.4 [Information Storage and Retrieval]: Systems and Software-performance evaluation (efficiency and effectiveness); General Terms Algorithms, Experimentation, Measurement, Performance 1.",
                "INTRODUCTION Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.",
                "In addition, one or more snippets are usually presented, giving the searcher a sneak preview of the document contents.",
                "Snippets are short fragments of text extracted from the document content (or its metadata).",
                "They may be static (for example, always show the first 50 words of the document, or the content of its description metadata, or a description taken from a directory site such as dmoz.org) or query-biased [20].",
                "A query-biased snippet is one selectively extracted on the basis of its relation to the searchers query.",
                "The addition of informative snippets to search results may substantially increase their value to searchers.",
                "Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.",
                "In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searchers real information need, such as the contact details of a person or an organization.",
                "Generation of query-biased snippets by Web search engines indexing of the order of ten billion web pages and handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).",
                "The simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.",
                "The overhead of opening and reading ten files per query on top of accessing the index structure to locate them, would be manifestly excessive under heavy query load.",
                "Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.",
                "Special-purpose filesystems have been built to address these problems [6].",
                "Note that the utility of snippets is by no means restricted to whole-of-Web search applications.",
                "Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).",
                "Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.",
                "We present a new algorithm and compact single-file structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.",
                "We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.",
                "As the time to process a document in RAM is small in comparison to locating and reading the document into memory, it may seem that compression is not required.",
                "However, this is only true if there is no caching of documents in RAM.",
                "Controlling the RAM of physical systems for experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of snippet generation.",
                "In turn, the more documents can be compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].",
                "As hitting the <br>document cache</br> is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a document, and then only allowing leading sentences into cache for each document.",
                "This leads to further time savings, with only marginal impact on the quality of the snippets returned. 2.",
                "RELATED WORK Snippet generation is a special type of extractive document summarization, in which sentences, or sentence fragments, are selected for inclusion in the summary on the basis of the degree to which they match the search query.",
                "This process was given the name of query-biased summarization by Tombros and Sanderson [20] The reader is referred to Mani [13] and to Radev et al. [16] for overviews of the very many different applications of summarization and for the equally diverse methods for producing summaries.",
                "Early Web search engines presented query-independent snippets consisting of the first k bytes of the result document.",
                "Generating these is clearly much simpler and much less computationally expensive than processing documents to extract query biased summaries, as there is no need to search the document for text fragments containing query terms.",
                "To our knowledge, Google was the first whole-ofWeb search engine to provide query biased summaries, but summarization is listed by Brin and Page [1] only under the heading of future work.",
                "Most of the experimental work using query-biased summarization has focused on comparing their value to searchers relative to other types of summary [20, 21], rather than efficient generation of summaries.",
                "Despite the importance of efficient summary generation in Web search, few algorithms appear in the literature.",
                "Silber and McKoy [19] describe a linear-time lexical chaining algorithm for use in generic summaries, but offer no empirical data for the performance of their algorithm.",
                "White et al [21] report some experimental timings of their WebDocSum system, but the snippet generation algorithms themselves are not isolated, so it is difficult to infer snippet generation time comparable to the times we report in this paper. 3.",
                "SEARCH ENGINE ARCHITECTURES A search engine must perform a variety of activities, and is comprised of many sub-systems, as depicted by the boxes in Figure 1.",
                "Note that there may be several other sub-systems such as the Advertising Engine or the Parsing Engine that could easily be added to the diagram, but we have concentrated on the sub-systems that are relevant to snippet generation.",
                "Depending on the number of documents that the search engine indexes, the data and processes for each Ranking Engine Crawling Engine Indexing Engine Engine Lexicon Meta Data Engine Engine Snippet Term&Doc#s Snippetperdoc WEB Query Engine Query Results Page Term#s Doc#s Invertedlists Docs perdoc Title,URL,etc Doc#s Document meta data Terms Querystring Term#s Figure 1: An abstraction of some of the sub-systems in a search engine.",
                "Depending on the number of documents indexed, each sub-system could reside on a single machine, be distributed across thousands of machines, or a combination of both. sub-system could be distributed over many machines, or all occupy a single server and filesystem, competing with each other for resources.",
                "Similarly, it may be more efficient to combine some sub-systems in an implementation of the diagram.",
                "For example, the meta-data such as document title and URL requires minimal computation apart from highlighting query words, but we note that disk seeking is likely to be minimized if title, URL and fixed summary information is stored contiguously with the text from which query biased summaries are extracted.",
                "Here we ignore the fixed text and consider only the generation of query biased summaries: we concentrate on the Snippet Engine.",
                "In addition to data and programs operating on that data, each sub-system also has its own memory management scheme.",
                "The memory management system may simply be the memory hierarchy provided by the operating system used on machines in the sub-system, or it may be explicitly coded to optimise the processes in the sub-system.",
                "There are many papers on caching in search engines (see [3] and references therein for a current summary), but it seems reasonable that there is a query cache in the Query Engine that stores precomputed final result pages for very popular queries.",
                "When one of the popular queries is issued, the result page is fetched straight from the query cache.",
                "If the issued query is not in the query cache, then the Query Engine uses the four sub-systems in turn to assemble a results page. 1.",
                "The Lexicon Engine maps query terms to integers. 2.",
                "The Ranking Engine retrieves inverted lists for each term, using them to get a ranked list of documents. 3.",
                "The Snippet Engine uses those document numbers and query term numbers to generate snippets. 4.",
                "The Meta Data Engine fetches other information about each document to construct the results page.",
                "IN A document broken into one sentence per line, and a sequence of query terms. 1 For each line of the text, L = [w1, w2, . . . , wm] 2 Let h be 1 if L is a heading, 0 otherwise. 3 Let be 2 if L is the first line of a document, 1 if it is the second line, 0 otherwise. 4 Let c be the number of wi that are query terms, counting repetitions. 5 Let d be the number of distinct query terms that match some wi. 6 Identify the longest contiguous run of query terms in L, say wj . . . wj+k. 7 Use a weighted combination of c, d, k, h and to derive a score s. 8 Insert L into a max-heap using s as the key.",
                "OUT Remove the number of sentences required from the heap to form the summary.",
                "Figure 2: Simple sentence ranker that operates on raw text with one sentence per line. 4.",
                "THE SNIPPET ENGINE For each document identifier passed to the Snippet Engine, the engine must generate text, preferably containing query terms, that attempts to summarize that document.",
                "Previous work on summarization identifies the sentence as the minimal unit for extraction and presentation to the user [12].",
                "Accordingly, we also assume a web snippet extraction process will extract sentences from documents.",
                "In order to construct a snippet, all sentences in a document should be ranked against the query, and the top two or three returned as the snippet.",
                "The scoring of sentences against queries has been explored in several papers [7, 12, 18, 20, 21], with different features of sentences deemed important.",
                "Based on these observations, Figure 2, shows the general algorithm for scoring sentences in relevant documents, with the highest scoring sentences making the snippet for each document.",
                "The final score of a sentence, assigned in Step 7, can be derived in many different ways.",
                "In order to avoid bias towards any particular scoring mechanism, we compare sentence quality later in the paper using the individual components of the score, rather than an arbitrary combination of the components. 4.1 Parsing Web Documents Unlike well edited text collections that are often the target for summarization systems, Web data is often poorly structured, poorly punctuated, and contains a lot of data that do not form part of valid sentences that would be candidates for parts of snippets.",
                "We assume that the documents passed to the Snippet Engine by the Indexing Engine have all HTML tags and JavaScript removed, and that each document is reduced to a series of word tokens separated by non-word tokens.",
                "We define a word token as a sequence of alphanumeric characters, while a non-word is a sequence of non-alphanumeric characters such as whitespace and the other punctuation symbols.",
                "Both are limited to a maximum of 50 characters.",
                "Adjacent, repeating characters are removed from the punctuation.",
                "Included in the punctuation set is a special end of sentence marker which replaces the usual three sentence terminators ? !..",
                "Often these explicit punctuation characters are missing, and so HTML tags such as <br> and <p> are assumed to terminate sentences.",
                "In addition, a sentence must contain at least five words and no more than twenty words, with longer or shorter sentences being broken and joined as required to meet these criteria [10].",
                "Unterminated HTML tags-that is, tags with an open brace, but no close brace-cause all text from the open brace to the next open brace to be discarded.",
                "A major problem in summarizing web pages is the presence of large amounts of promotional and navigational material (navbars) visually above and to the left of the actual page content.",
                "For example, The most wonderful company on earth.",
                "Products.",
                "Service.",
                "About us.",
                "Contact us.",
                "Try before you buy.",
                "Similar, but often not identical, navigational material is typically presented on every page within a site.",
                "This material tends to lower the quality of summaries and slow down summary generation.",
                "In our experiments we did not use any particular heuristics for removing navigational information as the test collection in use (wt100g) pre-dates the widespread take up of the current style of web publishing.",
                "In wt100g, the average web page size is more than half the current Web average [11].",
                "Anecdotally, the increase is due to inclusion of sophisticated navigational and interface elements and the JavaScript functions to support them.",
                "Having defined the format of documents that are presented to the Snippet Engine, we now define our Compressed Token System (CTS) document storage scheme, and the baseline system used for comparison. 4.2 Baseline Snippet Engine An obvious document representation scheme is to simply compress each document with a well known adaptive compressor, and then decompress the document as required [1], using a string matching algorithm to effect the algorithm in Figure 2.",
                "Accordingly, we implemented such a system, using zlib [4] with default parameters to compress every document after it has been parsed as in Section 4.1.",
                "Each document is stored in a single file.",
                "While manageable for our small test collections or small enterprises with millions of documents, a full Web search engine may require multiple documents to inhabit single files, or a special purpose filesystem [6].",
                "For snippet generation, the required documents are decompressed one at a time, and a linear search for provided query terms is employed.",
                "The search is optimized for our specific task, which is restricted to matching whole words and the sentence terminating token, rather than general pattern matching. 4.3 The CTS Snippet Engine Several optimizations over the baseline are possible.",
                "The first is to employ a semi-static compression method over the entire document collection, which will allow faster decompression with minimal compression loss [24].",
                "Using a semistatic approach involves mapping words and non-words produced by the parser to single integer tokens, with frequent symbols receiving small integers, and then choosing a coding scheme that assigns small numbers a small number of bits.",
                "Words and non-words strictly alternate in the compressed file, which always begins with a word.",
                "In this instance we simply assign each symbol its ordinal number in a list of symbols sorted by frequency.",
                "We use the vbyte coding scheme to code the word tokens [22].",
                "The set of non-words is limited to the 64 most common punctuation sequences in the collection itself, and are encoded with a flat 6-bit binary code.",
                "The remaining 2 bits of each punctuation symbol are used to store capitalization information.",
                "The process of computing the semi-static model is complicated by the fact that the number of words and non-words appearing in large web collections is high.",
                "If we stored all words and non-words appearing in the collection, and their associated frequency, many gigabytes of RAM or a B-tree or similar on-disk structure would be required [23].",
                "Moffat et al. [14] have examined schemes for pruning models during compression using large alphabets, and conclude that rarely occurring terms need not reside in the model.",
                "Rather, rare terms are spelt out in the final compressed file, using a special word token (escape symbol), to signal their occurrence.",
                "During the first pass of encoding, two move-to-front queues are kept; one for words and one for non-words.",
                "Whenever the available memory is consumed and a new symbol is discovered in the collection, an existing symbol is discarded from the end of the queue.",
                "In our implementation, we enforce the stricter condition on eviction that, where possible, the evicted symbol should have a frequency of one.",
                "If there is no symbol with frequency one in the last half of the queue, then we evict symbols of frequency two, and so on until enough space is available in the model for the new symbol.",
                "The second pass of encoding replaces each word with its vbyte encoded number, or the escape symbol and an ASCII representation of the word if it is not in the model.",
                "Similarly each non-word sequence is replaced with its codeword, or the codeword for a single space character if it is not in the model.",
                "We note that this lossless compression of non-words is acceptable when the documents are used for snippet generation, but may not be acceptable for a document database.",
                "We assume that a separate sub-system would hold cached documents for other purposes where exact punctuation is important.",
                "While this semi-static scheme should allow faster decompression than the baseline, it also readily allows direct matching of query terms as compressed integers in the compressed file.",
                "That is, sentences can be scored without having to decompress a document, and only the sentences returned as part of a snippet need to be decoded.",
                "The CTS system stores all documents contiguously in one file, and an auxiliary table of 64 bit integers indicating the start offset of each document in the file.",
                "Further, it must have access to the reverse mapping of term numbers, allowing those words not spelt out in the document to be recovered and returned to the Query Engine as strings.",
                "The first of these data structures can be readily partitioned and distributed if the Snippet Engine occupies multiple machines; the second, however, is not so easily partitioned, as any document on a remote machine might require access to the whole integer-to-string mapping.",
                "This is the second reason for employing the model pruning step during construction of the semi-static code: it limits the size of the reverse mapping table that should be present on every machine implementing the Snippet Engine. 4.4 Experimental assessment of CTS All experiments reported in this paper were run on a Sun Fire V210 Server running Solaris 10.",
                "The machine consists of dual 1.34 GHz UltraSPARC IIIi processors and 4GB of wt10g wt50g wt100g No.",
                "Docs. (×106 ) 1.7 10.1 18.5 Raw Text 10, 522 56, 684 102, 833 Baseline(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) CTS 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) Table 1: Total storage space (Mb) for documents for the three test collections both compressed, and uncompressed. 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) 0 20 40 60 0.00.20.40.60.8 Queries grouped in 100s Time(seconds) Baseline CTS with caching CTS without caching Figure 3: Time to generate snippets for 10 documents per query, averaged over buckets of 100 queries, for the first 7000 Excite queries on wt10g.",
                "RAM.",
                "All source code was compiled using gcc4.1.1 with -O9 optimisation.",
                "Timings were run on an otherwise unoccupied machine and were averaged over 10 runs, with memory flushed between runs to eliminate any caching of data files.",
                "In the absence of evidence to the contrary, we assume that it is important to model realistic query arrival sequences and the distribution of query repetitions for our experiments.",
                "Consequently, test collections which lack real query logs, such as TREC ad-hoc and .GOV2 were not considered suitable.",
                "Obtaining extensive query logs and associated result doc-ids for a corresponding large collection is not easy.",
                "We have used two collections (wt10g and wt100g) from the TREC Web Track [8] coupled with queries from Excite logs from the same (c. 1997) period.",
                "Further, we also made use of a medium sized collection wt50g, obtained by randomly sampling half of the documents from wt100g.",
                "The first two rows of Table 1 give the number of documents and the size in Mb of these collections.",
                "The final two rows of Table 1 show the size of the resulting document sets after compression with the baseline and CTS schemes.",
                "As expected, CTS admits a small compression loss over zlib, but both substantially reduce the size of the text to about 20% of the original, uncompressed size.",
                "Note that the figures for CTS do not include the reverse mapping from integer token to string that is required to produce the final snippets as that occupies RAM.",
                "It is 1024 Mb in these experiments.",
                "The Zettair search engine [25] was used to produce a list of documents to summarize for each query.",
                "For the majority of the experiments the Okapi BM25 scoring scheme was used to determine document rankings.",
                "For the static caching experiments reported in Section 5, the score of each document wt10g wt50g wt100g Baseline 75 157 183 CTS 38 70 77 Reduction in time 49% 56% 58% Table 2: Average time (msec) for the final 7000 queries in the Excite logs using the baseline and CTS systems on the 3 test collections. is a 50:50 weighted average of the BM25 score (normalized by the top scoring document for each query) and a score for each document independent of any query.",
                "This is to simulate effects of ranking algorithms like PageRank [1] on the distribution of document requests to the Snippet Engine.",
                "In our case we used the normalized Access Count [5] computed from the top 20 documents returned to the first 1 million queries from the Excite log to determine the query independent score component.",
                "Points on Figure 3 indicate the mean running time to generate 10 snippets for each query, averaged in groups of 100 queries, for the first 7000 queries in the Excite query log.",
                "Only the data for wt10g is shown, but the other collections showed similar patterns.",
                "The x-axis indicates the group of 100 queries; for example, 20 indicates the queries 2001 to 2100.",
                "Clearly there is a caching effect, with times dropping substantially after the first 1000 or so queries are processed.",
                "All of this is due to the operating system caching disk blocks and perhaps pre-fetching data ahead of specific read requests.",
                "This is evident because the baseline system has no large internal data structures to take advantage of non-disk based caching, it simply opens and processes files, and the speed up is evident for the baseline system.",
                "Part of this gain is due to the spatial locality of disk references generated by the query stream: repeated queries will already have their document files cached in memory; and similarly different queries that return the same documents will benefit from document caching.",
                "But when the log is processed after removing all but the first request for each document, the pronounced speed-up is still evident as more queries are processed (not shown in figure).",
                "This suggests that the operating system (or the disk itself) is reading and buffering a larger amount of data than the amount requested and that this brings benefit often enough to make an appreciable difference in snippet generation times.",
                "This is confirmed by the curve labeled CTS without caching, which was generated after mounting the filesystem with a no-caching option (directio in Solaris).",
                "With disk caching turned off, the average time to generate snippets varies little.",
                "The time to generate ten snippets for a query, averaged over the final 7000 queries in the Excite log as caching effects have dissipated, are shown in Table 2.",
                "Once the system has stabilized, CTS is over 50% faster than the Baseline system.",
                "This is primarily due to CTS matching single integers for most query words, rather than comparing strings in the baseline system.",
                "Table 3 shows a break down of the average time to generate ten snippets over the final 7000 queries of the Excite log on the wt50g collection when entire documents are processed, and when only the first half of each document is processed.",
                "As can be seen, the majority of time spent generating a snippet is in locating the document on disk (Seek): 64% for whole documents, and 75% for half documents.",
                "Even if the amount of processing a document must % of doc processed Seek Read Score & Decode 100% 45 4 21 50% 45 4 11 Table 3: Time to generate 10 snippets for a single query (msec) for the wt50g collection averaged over the final 7000 Excite queries when either all of each document is processed (100%) or just the first half of each document (50%). undergo is halved, as in the second row of the Table, there is only a 14% reduction in the total time required to generate a snippet.",
                "As locating documents in secondary storage occupies such a large proportion of snippet generation time, it seems logical to try and reduce its impact through caching. 5.",
                "DOCUMENT CACHING In Section 3 we observed that the Snippet Engine would have its own RAM in proportion to the size of the document collection.",
                "For example, on a whole-of-Web search engine, the Snippet Engine would be distributed over many workstations, each with at least 4 Gb of RAM.",
                "In a small enterprise, the Snippet Engine may be sharing RAM with all other sub-systems on a single workstation, hence only have 100 Mb available.",
                "In this section we use simulation to measure the number of cache hits in the Snippet Engine as memory size varies.",
                "We compare two caching policies: a static cache, where the cache is loaded with as many documents as it can hold before the system begins answering queries, and then never changes; and a least-recently-used cache, which starts out as for the static cache, but whenever a document is accessed it moves to the front of a queue, and if a document is fetched from disk, the last item in the queue is evicted.",
                "Note that documents are first loaded into the caches in order of decreasing query independent score, which is computed as described in Section 4.4.",
                "The simulations also assume a query cache exists for the top Q most frequent queries, and that these queries are never processed by the Snippet Engine.",
                "All queries passed into the simulations are from the second half of the Excite query log (the first half being used to compute query independent scores), and are stemmed, stopped, and have their terms sorted alphabetically.",
                "This final alteration simply allows queries such as red dog and dog red to return the same documents, as would be the case in a search engine where explicit phrase operators would be required in the query to enforce term order and proximity.",
                "Figure 4 shows the percentage of document access that hit cache using the two caching schemes, with Q either 0 or 10,000, on 535,276 Excite queries on wt100g.",
                "The xaxis shows the percentage of documents that are held in the cache, so 1.0% corresponds to about 185,000 documents.",
                "From this figure it is clear that caching even a small percentage of the documents has a large impact on reducing seek time for snippet generation.",
                "With 1% of documents cached, about 222 Mb for the wt100g collection, around 80% of disk seeks are avoided.",
                "The static cache performs surprisingly well (squares in Figure 4), but is outperformed by the LRU cache (circles).",
                "In an actual implementation of LRU, however, there may be fragmentation of the cache as documents are swapped in and out.",
                "The reason for the large impact of the <br>document cache</br> is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Cache size (% of collection) %ofaccessesascachehits LRU Q=0 LRU Q=10,000 Static Q=0 Static Q=10,000 Figure 4: Percentage of the time that the Snippet Engine does not have to go to disk in order to generate a snippet plotted against the size of the <br>document cache</br> as a percentage of all documents in the collection.",
                "Results are from a simulation on wt100g with 535,276 Excite queries. that, for a particular collection, some documents are much more likely to appear in results lists than others.",
                "This effect occurs partly because of the approximately Zipfian query frequency distribution, and partly because most Web search engines employ ranking methods which combine query based scores with static (a priori) scores determined from factors such as link graph measures, URL features, spam scores and so on [17].",
                "Documents with high static scores are much more likely to be retrieved than others.",
                "In addition to the <br>document cache</br>, the RAM of the Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).",
                "This is more than compensated for by the reduced size of each document, allowing more documents into the <br>document cache</br>.",
                "For example, using CTS reduces the average document size from 5.7 Kb to 1.2 Kb (as shown in Table 1), so a 2 Gb RAM could hold 368,442 uncompressed documents (2% of the collection), or 850,691 documents plus a 1 Gb decompression table (5% of the collection).",
                "In fact, further experimentation with the model size reveals that the model can in fact be very small and still CTS gives good compression and fast scoring times.",
                "This is evidenced in Figure 5, where the compressed size of wt50g is shown in the solid symbols.",
                "Note that when no compression is used (Model Size is 0Mb), the collection is only 31 Gb as HTML markup, JavaScript, and repeated punctuation has been discarded as described in Section 4.1.",
                "With a 5 Mb model, the collection size drops by more than half to 14 Gb, and increasing the model size to 750 Mb only elicits a 2 Gb drop in the collection size.",
                "Figure 5 also shows the average time to score and decode a a snippet (excluding seek time) with the different model sizes (open symbols).",
                "Again, there is a large speed up when a 5 Mb model is used, but little 0 200 400 600 15202530 Model Size (Mb) CollectionSize(Gb)orTime(msec) Size (Gb) Time (msec) Figure 5: Collection size of the wt50g collection when compressed with CTS using different memory limits on the model, and the average time to generate single snippet excluding seek time on 20000 Excite queries using those models. improvement with larger models.",
                "Similar results hold for the wt100g collection, where a model of about 10 Mb offers substantial space and time savings over no model at all, but returns diminish as the model size increases.",
                "Apart from compression, there is another approach to reducing the size of each document in the cache: do not store the full document in cache.",
                "Rather store sentences that are likely to be used in snippets in the cache, and if during snippet generation on a cached document the sentence scores do not reach a certain threshold, then retrieve the whole document from disk.",
                "This raises questions on how to choose sentences from documents to put in cache, and which to leave on disk, which we address in the next section. 6.",
                "SENTENCE REORDERING Sentences within each document can be re-ordered so that sentences that are very likely to appear in snippets are at the front of the document, hence processed first at query time, while less likely sentences are relegated to the rear of the document.",
                "Then, during query time, if k sentences with a score exceeding some threshold are found before the entire document is processed, the remainder of the document is ignored.",
                "Further, to improve caching, only the head of each document can be stored in the cache, with the tail residing on disk.",
                "Note that we assume that the search engine is to provide cached copies of a document-that is, the exact text of the document as it was indexed-then this would be serviced by another sub-system in Figure 1, and not from the altered copy we store in the Snippet Engine.",
                "We now introduce four sentence reordering approaches. 1.",
                "Natural order The first few sentences of a well authored document usually best describe the document content [12].",
                "Thus simply processing a document in order should yield a quality snippet.",
                "Unfortunately, however, web documents are often not well authored, with little editorial or professional writing skills brought to bear on the creation of a work of literary merit.",
                "More importantly, perhaps, is that we are producing query-biased snippets, and there is no guarantee that query terms will appear in sentences towards the front of a document. 2.",
                "Significant terms (ST) Luhn introduced the concept of a significant sentence as containing a cluster of significant terms [12], a concept found to work well by Tombros and Sanderson [20].",
                "Let fd,t be the frequency of term t in document d, then term t is determined to be significant if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwise, where sd is the number of sentences in document d. A bracketed section is defined as a group of terms where the leftmost and rightmost terms are significant terms, and no significant terms in the bracketed section are divided by more than four non-significant terms.",
                "The score of a bracketed section is the square of the number of significant words falling in the section, divided by the total number of words in the entire sentence.",
                "The a priori score for a sentence is computed as the maximum of all scores for the bracketed sections of the sentence.",
                "We then sort the sentences by this score. 3.",
                "Query log based (QLt) Many Web queries repeat, and a small number of queries make up a large volume of total searches [9].",
                "In order to take advantage of this bias, sentences that contain many past query terms should be promoted to the front of a document, while sentences that contain few query terms should be demoted.",
                "In this scheme, the sentences are sorted by the number of sentence terms that occur in the query log.",
                "To ensure that long sentences do not dominate over shorter qualitative sentences the score assigned to each sentence is divided by the number of terms in that sentence giving each sentence a score between 0 and 1. 4.",
                "Query log based (QLu) This scheme is as for QLt, but repeated terms in the sentence are only counted once.",
                "By re-ordering sentences using schemes ST, QLt or QLu, we aim to terminate snippet generation earlier than if Natural Order is used, but still produce sentences with the same number of unique query terms (d in Figure 2), total number of query terms (c), the same positional score (h+ ) and the same maximum span (k).",
                "Accordingly, we conducted experiments comparing the methods, the first 80% of the Excite query log was used to reorder sentences when required, and the final 20% for testing.",
                "Figure 6 shows the differences in snippet scoring components using each of the three methods over the Natural Order method.",
                "It is clear that sorting sentences using the Significant Terms (ST) method leads to the smallest change in the sentence scoring components.",
                "The greatest change over all methods is in the sentence position (h + ) component of the score, which is to be expected as their is no guarantee that leading and heading sentences are processed at all after sentences are re-ordered.",
                "The second most affected component is the number of distinct query terms in a returned sentence, but if only the first 50% of the document is processed with the ST method, there is a drop of only 8% in the number of distinct query terms found in snippets.",
                "Depending how these various components are weighted to compute an overall snippet score, one can argue that there is little overall affect on scores when processing only half the document using the ST method.",
                "Span (k) Term Count (c) Sentence Position (h + l) Distinct Terms (d) 40% 50% 60% 70% ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu ST QLt QLu RelativedifferencetoNaturalOrder Documents size used 90% 80% 70% 60% 50% 0% 10% 20% 30% Figure 6: Relative difference in the snippet score components compared to Natural Ordered documents when the amount of documents processed is reduced, and the sentences in the document are reordered using Query Logs (QLt, QLu) or Significant Terms (ST). 7.",
                "DISCUSSION In this paper we have described the algorithms and compression scheme that would make a good Snippet Engine sub-system for generating text snippets of the type shown on the results pages of well known Web search engines.",
                "Our experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very important aspects of the snippet generation problem.",
                "Primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.",
                "Our caching simulations show that if as little as 1% of the documents can be cached in RAM as part of the Snippet Engine, possibly distributed over many machines, then around 75% of seeks can be avoided.",
                "Our second major result is that keeping only half of each document in RAM, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the Significant Term algorithm of Luhn [12].",
                "Both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.",
                "Note that these results are generated using a 100Gb subset of the Web, and the Excite query log gathered from the same period as that subset was created.",
                "We are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.",
                "In particular, we can scale our results to examine what resources would be required, using our scheme, to provide a Snippet Engine for the entire World Wide Web.",
                "We will assume that the Snippet Engine is distributed across M machines, and that there are N web pages in the collection to be indexed and served by the search engine.",
                "We also assume a balanced load for each machine, so each machine serves about N/M documents, which is easily achieved in practice.",
                "Each machine, therefore, requires RAM to hold the following. • The CTS model, which should be 1/1000 of the size of the uncompressed collection (using results in Figure 5 and Williams et al. [23]).",
                "Assuming an average uncompressed document size of 8 Kb [11], this would require N/M × 8.192 bytes of memory. • A cache of 1% of all N/M documents.",
                "Each document requires 2 Kb when compressed with CTS (Table 1), and only half of each document is required using ST sentence reordering, requiring a total of N/M ×0.01× 1024 bytes. • The offset array that gives the start position of each document in the single, compressed file: 8 bytes per N/M documents.",
                "The total amount of RAM required by a single machine, therefore, would be N/M(8.192 + 10.24 + 8) bytes.",
                "Assuming that each machine has 8 Gb of RAM, and that there are 20 billion pages to index on the Web, a total of M = 62 machines would be required for the Snippet Engine.",
                "Of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.",
                "These machines would also need access to 37 Tb of disk to store the compressed document representations that were not in cache.",
                "In this work we have deliberately avoided committing to one particular scoring method for sentences in documents.",
                "Rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].",
                "The CTS method can incorporate any new metrics that may arise in the future that are calculated on whole words.",
                "The document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a sentence within a document, the aggressive compaction techniques reported here cannot be used.",
                "A variation on the semi-static compression approach we have adopted in this work has been used successfully in previous search engine design [24], but there are alternate compression schemes that allow direct matching in compressed text (see Navarro and M¨akinen [15] for a recent survey.)",
                "As seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in detail in this paper.",
                "We will explore alternate compression schemes in future work.",
                "Acknowledgments This work was supported in part by ARC Discovery Project DP0558916 (AT).",
                "Thanks to Nick Lester and Justin Zobel for valuable discussions. 8.",
                "REFERENCES [1] S. Brin and L. Page.",
                "The anatomy of a large-scale hypertextual Web search engine.",
                "In WWW7, pages 107-117, 1998. [2] R. Fagin, Ravi K., K. S. McCurley, J. Novak, D. Sivakumar, J.",
                "A. Tomlin, and D. P. Williamson.",
                "Searching the workplace web.",
                "In WWW2003, Budapest, Hungary, May 2003. [3] T. Fagni, R. Perego, F. Silvestri, and S. Orlando.",
                "Boosting the performance of web search engines: Caching and prefetching query results by exploiting historical usage data.",
                "ACM Trans.",
                "Inf.",
                "Syst., 24(1):51-78, 2006. [4] J-L Gailly and M. Adler.",
                "Zlib Compression Library. www.zlib.net.",
                "Accessed January 2007. [5] S. Garcia, H.E.",
                "Williams, and A. Cannane.",
                "Access-ordered indexes.",
                "In V. Estivill-Castro, editor, Proc.",
                "Australasian Computer Science Conference, pages 7-14, Dunedin, New Zealand, 2004. [6] S. Ghemawat, H. Gobioff, and S. Leung.",
                "The google file system.",
                "In SOSP 03: Proc. of the 19th ACM Symposium on Operating Systems Principles, pages 29-43, New York, NY, USA, 2003.",
                "ACM Press. [7] J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.",
                "Summarizing text documents: sentence selection and evaluation metrics.",
                "In SIGIR99, pages 121-128, 1999. [8] D. Hawking, Nick C., and Paul Thistlewaite.",
                "Overview of TREC-7 Very Large Collection Track.",
                "In Proc. of TREC-7, pages 91-104, November 1998. [9] B. J. Jansen, A. Spink, and J. Pedersen.",
                "A temporal comparison of altavista web searching.",
                "J.",
                "Am.",
                "Soc.",
                "Inf.",
                "Sci.",
                "Tech. (JASIST), 56(6):559-570, April 2005. [10] J. Kupiec, J. Pedersen, and F. Chen.",
                "A trainable document summarizer.",
                "In SIGIR95, pages 68-73, 1995. [11] S. Lawrence and C.L.",
                "Giles.",
                "Accessibility of information on the web.",
                "Nature, 400:107-109, July 1999. [12] H.P.",
                "Luhn.",
                "The automatic creation of literature abstracts.",
                "IBM Journal, pages 159-165, April 1958. [13] I. Mani.",
                "Automatic Summarization, volume 3 of Natural Language Processing.",
                "John Benjamins Publishing Company, Amsterdam/Philadelphia, 2001. [14] A. Moffat, J. Zobel, and N. Sharman.",
                "Text compression for dynamic document databases.",
                "Knowledge and Data Engineering, 9(2):302-313, 1997. [15] G. Navarro and V. M¨akinen.",
                "Compressed full text indexes.",
                "ACM Computing Surveys, 2007.",
                "To appear. [16] D. R. Radev, E. Hovy, and K. McKeown.",
                "Introduction to the special issue on summarization.",
                "Comput.",
                "Linguist., 28(4):399-408, 2002. [17] M. Richardson, A. Prakash, and E. Brill.",
                "Beyond pagerank: machine learning for static ranking.",
                "In WWW06, pages 707-715, 2006. [18] T. Sakai and K. Sparck-Jones.",
                "Generic summaries for indexing in information retrieval.",
                "In SIGIR01, pages 190-198, 2001. [19] H. G. Silber and K. F. McCoy.",
                "Efficiently computed lexical chains as an intermediate representation for automatic text summarization.",
                "Comput.",
                "Linguist., 28(4):487-496, 2002. [20] A. Tombros and M. Sanderson.",
                "Advantages of query biased summaries in information retrieval.",
                "In SIGIR98, pages 2-10, Melbourne, Aust., August 1998. [21] R. W. White, I. Ruthven, and J. M. Jose.",
                "Finding relevant documents using top ranking sentences: an evaluation of two alternative schemes.",
                "In SIGIR02, pages 57-64, 2002. [22] H. E. Williams and J. Zobel.",
                "Compressing integers for fast file access.",
                "Comp.",
                "J., 42(3):193-201, 1999. [23] H.E.",
                "Williams and J. Zobel.",
                "Searchable words on the Web.",
                "International Journal on Digital Libraries, 5(2):99-105, April 2005. [24] I. H. Witten, A. Moffat, and T. C. Bell.",
                "Managing Gigabytes: Compressing and Indexing Documents and Images.",
                "Morgan Kaufmann Publishing, San Francisco, second edition, May 1999. [25] The Zettair Search Engine. www.seg.rmit.edu.au/zettair.",
                "Accessed January 2007."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Finalmente, proponemos y analizamos la reordenamiento de documentos y la compactación, revelando un esquema que aumenta el número de \"caché de documentos\" con solo un efecto marginal en la calidad del fragmento.caché de documentos",
                "A medida que alcanzar el \"caché de documentos\" es importante, examinamos la compactación del documento, a diferencia de la compresión, los esquemas imponiendo un pedido a priori de oraciones dentro de un documento, y luego solo permitiendo oraciones líderes en caché para cada documento.caché de documentos",
                "A menudo faltan estos caracteres de puntuación explícitos, por lo que se supone que las etiquetas HTML como \"y <p> terminan las oraciones. Documento caché de documentos",
                "El motivo del gran impacto del \"caché de documento\" es 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 Tamaño de caché ( % de la colección) % de los accesorioscachehits lru q = 0 lru q = 10,000 estáticos q = 0 estática Q = 10,000 Figura 4: Porcentaje de porcentajedel momento en que el motor del fragmento no tiene que ir al disco para generar un fragmento trazado contra el tamaño del \"caché de documentos\" como un porcentaje de todos los documentos en la colección.caché de documentos",
                "Además del \"caché de documentos\", la RAM del motor del fragmento también debe contener la tabla de decodificación de CTS que mapea enteros a las cadenas, que está limitada por un parámetro en el tiempo de compresión (1 GB en nuestros experimentos aquí).caché de documentos",
                "Esto es más que compensado por el tamaño reducido de cada documento, lo que permite más documentos en el \"caché de documentos\".caché de documentos"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}