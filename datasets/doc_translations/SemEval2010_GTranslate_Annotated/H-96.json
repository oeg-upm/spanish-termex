{
    "id": "H-96",
    "original_text": "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland. G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland. G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system. IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly. In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search. Our findings suggest that all three of these factors contribute to the utility of IRF. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1. INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems. In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection. However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers. As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6]. Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification. However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10]. Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact. IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5]. IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7]. In this paper we present a study into the use and effectiveness of IRF in an online search environment. The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages? This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use. The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13]. In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2. STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user. One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material. Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time. We used the Web as the test collection in this study and Google1 as the underlying search engine. Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence). Each summary sentence and top-ranking sentence is regarded as a representation of the document. The default display contains the list of top-ranking sentences and the list of the first ten document titles. Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document. This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16]. In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information. Both systems provide an interactive query expansion feature by suggesting new query terms to the user. The searcher has the responsibility for choosing which, if any, of these terms to add to the query. The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF. Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant. Only the representations marked relevant by the user are used for suggesting new query terms. This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact. As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document. To the searcher this is a way they can find out more information from a potentially interesting source. To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant. The query modification terms are selected using the same algorithm as in the Explicit RF system. Therefore the only difference between the systems is how relevance is communicated to the system. The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects. The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance. We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects. These subjects were not involved in the main experiment. For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity. As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks. By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms. Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task. In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task. Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 . They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task. Figure 1 shows the task statements for three levels of task complexity for one of the six search topics. HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source. Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide. MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it. Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK. LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol. However, as you have not been driving for long, you are unaware of any major changes in price. You decide to find out how the price of petrol has changed in the UK in recent years. Figure 1. Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers). Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use. The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science. The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience. The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed. All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity. Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface. Each subject carried out three tasks, one on each system. We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once. The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design. Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4]. System logging was also used to record subject interaction. A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system. Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires. Subjects were offered a 5 minute break after the first hour. In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms. This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire. This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet. No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once. Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search. The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview. Subjects were told that their interaction may be used by the IRF system to help them as they searched. They were not told which behaviours would be used or how it would be used. We now describe the findings of our analysis. 3. FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF. We present our findings per research question. Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated. We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate. All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement. The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively. The highest, or most positive, values in each table are shown in bold. Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system. In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities. We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks. In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control. The average obtained differential values are shown in Table 1 for IRF and each task category. The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement. This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely. The values for ERF are included for reference in this table and all other tables and figures in the Findings section. Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback. Table 1. Subject perceptions of RF method (lower = better). Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc). Kruskal-Wallis Tests were applied to each differential for each type of RF3 . Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials. This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true. Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 . Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 . To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses. The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 . On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 . Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding. In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task. We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided. To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess. In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant. In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher. This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document. There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context. Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess. Table 2 shows proportion of representations provided as RF by subjects. Table 2. Feedback and documents viewed. Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document. This suggests a pattern where users are investigating retrieved documents in more depth. It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task. Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide. This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected. Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document). This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change. In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16]. We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks. To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful. Table 3 presents average responses grouped by search task. Table 3. Subject perceptions of system terms (lower = better). Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF. The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task. For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 . This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity. Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 . As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested. Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query. In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF. Table 4. Term Acceptance (percentage of top six terms). Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 . As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks . Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer. This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases. For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 . Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted. Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF. From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks. Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers. As such, levels of search experience may affect searchers use and perceptions of IRF. In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed. In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects. The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…). These differentials elicited opinion from experimental subjects about the RF method used. In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5. Subject perceptions of RF method (lower = better). The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant. For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 . Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 . As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction. Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 . It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process. In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems. Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects. Table 6 shows the average differential responses obtained from both subject groups. Table 6. Subject perceptions of system terms (lower = better). Explicit RF Implicit RF Differential Inexp. Exp. Inexp. Exp. Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 . Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp. Exp. Inexp. Exp. Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects. This finding was supported by the proportion of query modification terms these subjects accepted. In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects. Table 7 shows the average number of accepted terms per subject group. Table 7. Term Acceptance (percentage of top six terms). Explicit RF Implicit RFProportion of terms Inexp. Exp. Inexp. Exp. Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF. However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF. The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful. We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries. Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search. To test this, our third research question concerned the use and usefulness of IRF during the course of a search. In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are. For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes. We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes. In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices. This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results. Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end. In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search. The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%). To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2. Distribution of RF provision per search task. Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm. These are essentially differences in the way users are assessing documents. In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 . When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations. At this stage the subjects are perhaps concentrating more on reading the retrieved results. Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search. This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings. Figure 2 also shows the proportion of feedback for tasks of different complexity. The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task. More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point. This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items. That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification. Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search. In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search. The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects. Table 8. Term Acceptance (proportion of top six terms). Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept. Search stage affects term acceptance in IRF but not in ERF26 . The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ). A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user. Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4. DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks. From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest). When the search was more complex subjects rarely found results they regarded as completely relevant. Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system. In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do. The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves. It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias. Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher. For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents. Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF. Our analysis revealed a general preference across all subjects for IRF over ERF. That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF. However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations). All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 . These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF. Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method. The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end. The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point. Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour. The findings suggest that searchers interact differently for IRF and ERF. Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF. The development of such a system represents part of our ongoing work in this area. 5. CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF). We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF. These factors were search task complexity, the subjects search experience and the stage in the search. Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant. Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity. Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not. It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6. REFERENCES [1] Bell, D.J. and Ruthven, I. (2004). Searchers assessments of task complexity for web searching. Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000). Experimental components for the evaluation of interactive information retrieval systems. Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002). Strategic help for user interfaces for information retrieval. Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980). Research methods in librarianship: Techniques and interpretation. Library and information science series. New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996). The ostensive model of developing information needs. Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992). Relevance feedback and other query modification techniques. In Information retrieval: Data structures and algorithms. New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003). Implicit feedback for inferring user preference. SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996). A case for interaction: A study of interactive information retrieval behavior and effectiveness. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984). Statistics using ranks: A unified approach. Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994). Information filtering based on user behavior analysis and best match text retrieval. Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990). Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988). Nonparametric statistics for the behavioural sciences. 2nd ed. Singapore: McGraw-Hill. [13] White, R.W. (2004). Implicit feedback for interactive information retrieval. Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005). An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004). A simulated study of implicit feedback models. Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000). The impact of fluid documents on reading and browsing: An observational study. Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256. Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system. Appendix A. Interface to Implicit RF system. 1. Top-Ranking Sentence 2. Title 3. Summary 4. Summary Sentence 5. Sentence in Context 2 3 4 5 1",
    "original_translation": "Un estudio de factores que afectan la utilidad de la retroalimentación de relevancia implícita Ryen W. Instituto de Laboratorio de Interacción Humana Humana Blanca para los Estudios de Computación Avanzada de la Universidad de Maryland College, MD 20742, EE. UU. Ryen@umd.edu Ian Ruthven Departamento de Informática e Ciencias de la Información Universidad deStrathclyde Glasgow, Escocia. G1 1xH.ir@cis.strath.ac.uk Joemon M. José Departamento de Ciencias de la Computación Universidad de Glasgow Glasgow, Escocia. G12 8RZ.jj@dcs.gla.ac.uk Resumen La retroalimentación de relevancia implícita (IRF) es el proceso por el cual un sistema de búsqueda reúne con evidencia discretas sobre los intereses del buscador de su interacción con el sistema. IRF es un nuevo método para recopilar información sobre el interés del usuario y, si se va a utilizar IRF en los sistemas operativos de IR, es importante establecer cuándo funciona bien y cuándo funciona mal. En este artículo investigamos cómo el uso y la efectividad de IRF se ve afectado por tres factores: la complejidad de la tarea de búsqueda, la experiencia de búsqueda del usuario y la etapa en la búsqueda. Nuestros hallazgos sugieren que estos tres factores contribuyen a la utilidad de IRF. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información] Experimentación de términos generales, factores humanos.1. Introducción Los sistemas de recuperación de información (IR) están diseñados para ayudar a los buscadores a resolver problemas. En la metáfora de interacción tradicional empleada por sistemas de búsqueda web como Yahoo!Y la búsqueda de MSN, el sistema generalmente solo respalda la recuperación de documentos potencialmente relevantes de la colección. Sin embargo, también es posible ofrecer soporte a los buscadores de diferentes actividades de búsqueda, como seleccionar los términos a presentar al sistema o elegir qué estrategia de búsqueda adoptar [3, 8];Ambos pueden ser problemáticos para los buscadores. Como la calidad de la consulta presentada al sistema afecta directamente la calidad de los resultados de búsqueda, el problema de cómo mejorar las consultas de búsqueda se ha estudiado ampliamente en la investigación IR [6]. Se han propuesto técnicas como la retroalimentación de relevancia (RF) [11] como una forma en que el sistema IR puede respaldar el desarrollo iterativo de una consulta de búsqueda al sugerir términos alternativos para la modificación de la consulta. Sin embargo, en la práctica, las técnicas de RF se han subutilizado a medida que colocan una mayor carga cognitiva para los buscadores para indicar directamente los resultados relevantes [10]. La retroalimentación de relevancia implícita (IRF) [7] se ha propuesto como una forma en que las consultas de búsqueda pueden mejorarse observando pasivamente a los buscadores a medida que interactúan. El IRF se ha implementado mediante el uso de medidas sustitutas basadas en la interacción con documentos (como el tiempo de lectura, el desplazamiento o la retención de documentos) [7] o el uso de la interacción con las interfaces de resultados basadas en la navegación [5]. Se ha demostrado que IRF muestra una efectividad mixta porque los factores que son buenos indicadores de interés del usuario a menudo son erráticos y las inferencias extraídas de la interacción del usuario no siempre son válidos [7]. En este artículo presentamos un estudio sobre el uso y la efectividad de IRF en un entorno de búsqueda en línea. El estudio tiene como objetivo investigar los factores que afectan el IRF, en particular tres preguntas de investigación: (i) ¿El uso y la calidad percibida de los términos generados por el IRF afectados por la tarea de búsqueda?(ii) ¿El uso y la calidad percibida de los términos generados por el IRF afectado por el nivel de experiencia de búsqueda de los usuarios del sistema?(iii) ¿Se usa IRF igualmente y genera términos que son igualmente útiles en todas las etapas de búsqueda? Este estudio tiene como objetivo establecer cuándo, y en qué circunstancias, IRF funciona bien en términos de su uso y los términos de modificación de consulta seleccionados como resultado de su uso. El experimento principal del que se toman los datos se diseñó para probar técnicas para seleccionar términos y técnicas de modificación de consultas para mostrar resultados de recuperación [13]. En este artículo usamos datos derivados de ese experimento para estudiar factores que afectan la utilidad de IRF.2. Estudio en esta sección describimos el estudio de usuario realizado para abordar nuestras preguntas de investigación.2.1 Sistemas Nuestro estudio utilizaron dos sistemas que sugirieron nuevos términos de consulta al usuario. Un sistema sugirió términos basados en la interacción de los usuarios (IRF), el otro usó RF explícito (ERF) pidiéndole al usuario que indique explícitamente material relevante. Ambos sistemas utilizaron el algoritmo de sugerencia del mismo término, [15], y utilizaron una interfaz común.2.1.1 Descripción general de la interfaz En ambos sistemas, los documentos recuperados están representados en la interfaz por su texto completo y una variedad de representaciones más pequeñas y relevantes para la consulta, creadas en el momento de la recuperación. Utilizamos la web como la colección de pruebas en este estudio y Google1 como el motor de búsqueda subyacente. Las representaciones de documentos incluyen el título del documento y un resumen del documento;Una lista de oraciones de alto rango (TRS) extraídas de los documentos superiores recuperados, obtenidos en relación con la consulta, una oración en el resumen del documento y cada oración sumaria en el contexto que ocurre en el documento (es decir, con el precedente y el precedente yla siguiente frase). Cada oración de resumen y oración de alto rango se considera una representación del documento. La pantalla predeterminada contiene la lista de oraciones de alto rango y la lista de los primeros diez títulos de documentos. Interactuar con una representación guía a los buscadores a una representación diferente del mismo documento, por ejemplo, mover el mouse sobre un título de documento muestra un resumen del documento. Se ha demostrado que esta presentación de más información progresivamente de documentos para ayudar a las evaluaciones de relevancia es efectiva en trabajos anteriores [14, 16]. En el Apéndice A, mostramos la interfaz completa al sistema IRF con las representaciones de documentos marcadas y en el Apéndice B mostramos un fragmento de la interfaz ERF con las casillas de verificación utilizadas por los buscadores para indicar información relevante. Ambos sistemas proporcionan una función de expansión de consulta interactiva al sugerir nuevos términos de consulta al usuario. El buscador tiene la responsabilidad de elegir cuáles, si los hay, de estos términos se agregarán a la consulta. El buscador también puede agregar o eliminar los términos de la consulta a voluntad.2.1.2 Sistema de RF explícito Esta versión del sistema implementa RF explícito. Junto a cada representación del documento hay casillas de verificación que permiten a los buscadores marcar representaciones individuales como relevantes;Marcar una representación es una indicación de que su contenido es relevante. Solo las representaciones marcadas relevantes por el usuario se utilizan para sugerir nuevos términos de consulta. Este sistema se usó como una línea de base contra la cual se podía comparar el sistema IRF.2.1.3 Sistema de RF implícito Este sistema hace inferencias sobre los intereses del buscador en función de la información con la que interactúan. Como se describe en la Sección 2.1.1, interactuar con una representación destaca una nueva representación del mismo documento. Para el buscador, esta es una forma en que pueden obtener más información de una fuente potencialmente interesante. Para el sistema RF implícito, cada interacción con una representación se interpreta como una indicación implícita de interés en esa representación;Se supone que interactuar con una representación es una indicación de que su contenido es relevante. Los términos de modificación de la consulta se seleccionan utilizando el mismo algoritmo que en el sistema RF explícito. Por lo tanto, la única diferencia entre los sistemas es cómo se comunica la relevancia al sistema. Los resultados del experimento principal [13] indicaron que estos dos sistemas eran comparables en términos de efectividad.2.2 Tareas Las tareas de búsqueda fueron diseñadas para fomentar el comportamiento de búsqueda realista por parte de nuestros sujetos. Las tareas se redujeron en forma de situaciones de tareas de trabajo simuladas [2], es decir, escenarios de búsqueda breves que fueron diseñados para reflejar situaciones de búsqueda de la vida real y permitir a los sujetos desarrollar evaluaciones personales de relevancia. Diseñamos seis temas de búsqueda (es decir, aplicando a la universidad, alergias en el lugar de trabajo, galerías de arte en Roma, teléfonos móviles de tercera generación, piratería de música de Internet y precios de gasolina) basados en pruebas piloto con un pequeño grupo representativo de temas. Estos sujetos no participaron en el experimento principal. Para cada tema, se idearon tres versiones de cada situación de tarea de trabajo, cada versión que difiere en su nivel de complejidad de tareas predicho. Como se describe en [1], la complejidad de la tarea es una variable que afecta las percepciones de la sujeto de una tarea y su comportamiento interactivo, por ejemplo, los sujetos realizan más actividades de filtrado con tareas de búsqueda altamente complejas. Al desarrollar tareas de diferente complejidad, podemos evaluar cómo la naturaleza de la tarea afecta el comportamiento interactivo de los sujetos y, por lo tanto, la evidencia suministrada a los algoritmos IRF. La complejidad de la tarea se varió de acuerdo con la metodología descrita en [1], específicamente al variar el número de posibles fuentes de información y tipos de información requeridos, para completar una tarea. En nuestras pruebas piloto (y en un análisis posteriori de los principales resultados del experimento) verificamos que los sujetos que informan sobre la complejidad de la tarea individual coincidían con nuestra estimación de la complejidad de la tarea. Los sujetos intentaron tres tareas de búsqueda: una alta complejidad, una complejidad moderada y una baja complejidad2. Se les pidió que leyeran la tarea, se ubicaran en la situación que describió y encontró la información que sentían que se requería para completar la tarea. La Figura 1 muestra las declaraciones de tareas para tres niveles de complejidad de la tarea para uno de los seis temas de búsqueda. Tarea HC: alta complejidad Mientras cenan con un colega estadounidense, comentan el alto precio de la gasolina en el Reino Unido en comparación con otros países, a pesar de los grandes volúmenes provienen de la misma fuente. Sin darse cuenta de las diferencias importantes, decide averiguar cómo y por qué los precios de la gasolina varían en todo el mundo. Tarea MC: Complejidad moderada mientras cena una noche, uno de los invitados de sus amigos se queja del precio de la gasolina y los factores que lo causan. Durante toda la noche parecen estar quejándose de todo lo que pueden, reduciendo la credibilidad de sus declaraciones anteriores para que decida investigar qué factores son realmente importantes para determinar el precio de la gasolina en el Reino Unido. Tarea LC: baja complejidad mientras cena una noche, su amigo se queja del aumento del precio de la gasolina. Sin embargo, como no ha estado conduciendo por mucho tiempo, no tiene conocimiento de ningún cambio importante en el precio. Usted decide averiguar cómo ha cambiado el precio de la gasolina en el Reino Unido en los últimos años. Figura 1. Variando complejidad de la tarea (tema de precios de la gasolina).2.3 Sujetos 156 Voluntarios expresaron su interés en participar en nuestro estudio.Se seleccionaron 48 sujetos de este conjunto con el objetivo de poblar dos grupos, cada uno con 24 sujetos: inexpertos (buscadores infrecuentes/ inexpertos) y experimentados (buscadores frecuentes/ experimentados). Los sujetos no fueron elegidos y clasificados en sus grupos hasta que completaron un cuestionario de entrada que les hizo sobre su experiencia de búsqueda y uso de la computadora. La edad promedio de los sujetos fue de 22.83 años (máximo 51, mínimo 18, σ = 5.23 años) y el 75% tenía un diploma universitario o un título más alto.El 47,91% de los sujetos tenían, o buscaban, una calificación en una disciplina relacionada con la informática. Las asignaturas eran una mezcla de estudiantes, investigadores, personal académico y otros, con diferentes niveles de computadora y experiencia de búsqueda. Los sujetos se dividieron en los dos grupos dependiendo de su experiencia de búsqueda, con qué frecuencia buscaban y los tipos de búsquedas que realizaron. Todos estaban familiarizados con la búsqueda en la web, y algunos con la búsqueda en otros dominios.2.4 Metodología El experimento tenía un diseño factorial;Con 2 niveles de experiencia en búsqueda, 3 sistemas experimentales (aunque solo informamos sobre los hallazgos de los sistemas ERF e IRF) y 3 niveles de complejidad de tareas de búsqueda. Los sujetos intentaron una tarea de cada complejidad, 2 El experimento principal del que se dibujan estos resultados tuvieron un tercer sistema de comparación que tenía una interfaz diferente. Cada sujeto llevó a cabo tres tareas, una en cada sistema. Solo informamos sobre los resultados de los sistemas ERF e IRF, ya que estos son los únicos pertinentes para este documento.Sistemas conmutados después de cada tarea y usé cada sistema una vez. El orden en el que se utilizaron los sistemas y las tareas de búsqueda intentadas se asignaron al azar de acuerdo con un diseño experimental de cuadrado latino. Los cuestionarios utilizaron escamas Likert, diferenciales semánticos y preguntas abiertas para obtener opiniones de temas [4]. El registro del sistema también se utilizó para registrar la interacción del sujeto. Un tutorial llevado a cabo antes del experimento permitió a los sujetos usar una versión que no es de retroalimentación del sistema para intentar una tarea de práctica antes de usar el primer sistema experimental. Los experimentos duraron entre uno y la mitad y dos horas, dependiendo de variables como el tiempo que pasa completando cuestionarios. A los sujetos se les ofreció un descanso de 5 minutos después de la primera hora. En cada experimento: i.El sujeto fue bienvenido y se le pidió que leyera una introducción a los experimentos y firme los formularios de consentimiento. Este conjunto de instrucciones se escribió para garantizar que cada sujeto recibiera precisamente la misma información.II.Se le pidió al sujeto que completara un cuestionario introductorio. Esto contenía preguntas sobre la educación de las asignaturas, la experiencia de búsqueda general, la experiencia de la computadora y la experiencia de búsqueda web.iii.El sujeto recibió un tutorial sobre la interfaz, seguido de un tema de capacitación sobre una versión de la interfaz sin RF.IV.El sujeto recibió tres hojas de tareas y se le pidió que elija una tarea de los seis temas en cada hoja. No se dieron pautas a los sujetos al elegir una tarea distinta de que no pudieron elegir una tarea de ningún tema más de una vez. El experimentador rotó la complejidad de la tarea, por lo que cada sujeto intentó una tarea de alta complejidad, una tarea moderada de complejidad y una tarea de baja complejidad.v. Se le pidió al sujeto que realizara la búsqueda y se le dio 15 minutos para buscar. El sujeto podría terminar una búsqueda temprano si no pudieron encontrar más información que sintieron que les ayudó a completar la tarea.VI.Después de completar la búsqueda, se le pidió al sujeto que completara un cuestionario posterior a la búsqueda.vii.Las tareas restantes fueron intentadas por el sujeto, siguiendo los pasos v. y VI.viii.El sujeto completó un cuestionario posterior al experimento y participó en una entrevista posterior al experimento. Se les dijo a los sujetos que el sistema IRF puede usar su interacción para ayudarlos mientras buscaban. No se les dijo qué comportamientos se utilizarían o cómo se usaría. Ahora describimos los hallazgos de nuestro análisis.3. Hallazgos En esta sección utilizamos los datos derivados del experimento para responder nuestras preguntas de investigación sobre el efecto de la complejidad de la tarea de búsqueda, la experiencia de búsqueda y la etapa en la búsqueda sobre el uso y la efectividad del IRF. Presentamos nuestros hallazgos por pregunta de investigación. Debido a la naturaleza ordinal de gran parte de los datos, las pruebas estadísticas no paramétricas se usan en este análisis y el nivel de significación se establece en P <.05, a menos que se indique lo contrario. Utilizamos el método propuesto por [12] para determinar la importancia de las diferencias en las comparaciones múltiples y el de [9] para probar los efectos de interacción entre las variables experimentales, cuya aparición informamos cuando corresponda. Todas las escalas Likert y los diferenciales semánticos estaban en una escala de 5 puntos donde una calificación más cercana a 1 significa más acuerdo con la declaración de actitud. Las etiquetas de categoría HC, MC y LC se utilizan para denotar las tareas de alta, moderada y de baja complejidad respectivamente. Los valores más altos o más positivos en cada tabla se muestran en negrita. Nuestro análisis utiliza datos de cuestionarios, entrevistas posteriores al experimento y el registro de sistemas de antecedentes en los sistemas ERF e IRF.3.1 Los buscadores de tareas de búsqueda intentaron tres tareas de búsqueda de complejidad variable, cada una en un sistema experimental diferente. En esta sección presentamos un análisis sobre el uso y la utilidad de IRF para tareas de búsqueda de diferentes complejidades. Presentamos nuestros hallazgos en términos de la RF proporcionada por los sujetos y los términos recomendados por los sistemas.3.1.1 Comentarios Utilizamos cuestionarios y registros del sistema para recopilar datos sobre las percepciones de los sujetos y la provisión de RF para diferentes tareas de búsqueda. En el cuestionario posterior a la investigación, se preguntó a los sujetos sobre cómo se transmitió RF utilizando diferenciales para obtener su opinión sobre: 1. El valor de la técnica de retroalimentación: cómo transmitió relevancia para el sistema (es decir, casillas o información de visualización) fue: fácil / difícil,efectivo/ ineficaz, útil/ no útil.2. El proceso de proporcionar la retroalimentación: cómo transmitió relevancia para el sistema lo hizo sentir: cómodo/incómodo, en control/no en control. Los valores diferenciales obtenidos promedio se muestran en la Tabla 1 para IRF y cada categoría de tareas. El valor correspondiente al diferencial representa la media de todos los diferenciales para una declaración de actitud particular. Esto proporciona una comprensión general de los sentimientos de los sujetos que pueden ser útiles ya que los sujetos pueden no responder a los diferenciales individuales con mucha precisión. Los valores para ERF se incluyen como referencia en esta tabla y todas las demás tablas y figuras en la sección de resultados. Dado que el objetivo del documento es investigar situaciones en las que IRF podría funcionar bien, no una comparación directa entre IRF y ERF, solo hacemos comparaciones limitadas entre estos dos tipos de comentarios. Tabla 1. Percepciones de sujeto del método RF (más bajo = mejor). Cada celda de la Tabla 1 resume las respuestas de los sujetos para 16 pares de sistemas de tareas (16 sujetos que realizaron una tarea de alta complejidad (HC) en el sistema ERF, 16 sujetos que ejecutaban una tarea de complejidad media (MC) en el sistema ERF, etc.). Se aplicaron pruebas de Kruskal-Wallis a cada diferencial para cada tipo de RF3. Las respuestas de los sujetos sugirieron que 3 Dado que este análisis involucraba muchos diferenciales, utilizamos una corrección de Bonferroni para controlar la tasa de error en términos de experimento y establecer el nivel alfa (α) en .0167 y .0250 para ambas declaraciones 1. y 2. respectivamente, es decir, es decir, es decir., .05 dividido por el número de diferenciales. Esta corrección reduce el número de errores de tipo I, es decir, rechazando hipótesis nulas que son verdaderas. Explícito RF implícito RF Diferencial HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Efectivo 2.94 2.68 2.44 2.04 2.41 2.66 Útil 2.76 2.51 2.16 1.91 2.37 2.56 Todos (1) 2.83 2.55 2.24 1.94 2.38 Cómodos 2.27 2.28 2.28Control 2.01 1.97 1.93 2.73 2.68 2.61 Todos (2) 2.14 2.13 2.14 2.42 2.42 2.39 El IRF fue más efectivo y útil para tareas de búsqueda más complejas4 y que las diferencias en todas las comparaciones de pares entre las tareas fueron significativas5. Las percepciones de los sujetos de IRF provocadas utilizando los otros diferenciales no parecían verse afectadas por la complejidad de la tarea de búsqueda 6. Para determinar si existe una relación entre la efectividad y la utilidad del proceso IRF y la complejidad de la tarea, aplicamos el coeficiente de correlación de orden de clasificación a las respuestas de los participantes. Los resultados de este análisis sugieren que la efectividad del IRF y la utilidad de IRF están relacionadas con la complejidad de la tarea;A medida que la complejidad de la tarea aumenta la preferencia del sujeto por IRF, también aumenta el7. Por otro lado, los sujetos sintieron que ERF era más efectivo y útil para tareas de baja complejidad8. Su informe verbal de ERF, donde la utilidad y la efectividad percibidas aumentaron a medida que disminuyó la complejidad de la tarea, respalda este hallazgo. En tareas de menor complejidad, los sujetos sintieron que estaban en mejores condiciones para proporcionar comentarios sobre si los documentos eran relevantes o no para la tarea. Analizamos registros de interacción generados por ambas interfaces para investigar la cantidad de sujetos de RF proporcionados. Para hacer esto, utilizamos una medida de precisión de búsqueda que es la proporción de todas las representaciones de documentos posibles que evaluó un buscador, dividido por el número total que podrían evaluar. En ERF, esta es la proporción de todas las representaciones posibles que fueron marcadas relevantes por el buscador, es decir, esas representaciones marcadas explícitamente relevantes. En IRF, esta es la proporción de representaciones vistas por un buscador sobre todas las representaciones posibles que el buscador podría haber visto. Esta proporción mide el nivel de interacción de los buscadores con un documento, lo tomamos para medir el interés de los usuarios en el documento: cuantas más representaciones de documentos se vean, más interesado suponemos que un usuario está en el contenido del documento. Hay un máximo de 14 representaciones por documento: 4 oraciones de topranque, 1 título, 1 resumen, 4 oraciones resumidas y 4 oraciones sumarias en el contexto del documento. Dado que la interfaz muestra representaciones de documentos de los 30 principales documentos, hay 420 representaciones que un buscador puede evaluar. La Tabla 2 muestra la proporción de representaciones proporcionadas como RF por sujetos. Tabla 2. Comentarios y documentos vistos. Medida de RF implícita explícita HC MC LC HC HC MC LC Comentarios de la proporción 2.14 2.39 2.65 21.50 19.36 15.32 Documentos Visilados 10.63 10.43 10.81 10.84 12.19 14.81 Para IRF hay un patrón claro: a medida que la complejidad aumenta los senos vistas menos documentos más vistas para cada documento para cada documento para cada documento para cada documento para cada documento.. Esto sugiere un patrón donde los usuarios están investigando documentos recuperados con más profundidad. También significa que la cantidad de 4 efectiva: χ2 (2) = 11.62, p = .003;Útil: χ2 (2) = 12.43, p = .002 5 pruebas post-hoc Dunns (comparación múltiple usando sumas de rango);Todos z ≥ 2.88, todos p ≤ .002 6 Todos χ2 (2) ≤ 2.85, todos p ≥ .24 (pruebas de Kruskal-Wallis) 7 Efectivo: todos r ≥ 0.644, p ≤ .002;útil: todos r ≥ 0.541, p ≤ .009 8 efectivo: χ2 (2) = 7.01, p = .03;útil: χ2 (2) = 6.59, p = .037 (prueba de Kruskal-Wallis);Todas las diferencias de pares significativas, todas las z ≥ 2.34, todas las p ≤ .01 (pruebas post-hoc de DUNN) La retroalimentación varía según la complejidad de la tarea de búsqueda. Dado que IRF se basa en la interacción del buscador, cuanto más interactúen, más comentarios brindan. Esto no tiene efecto en el número de términos de RF elegidos, pero puede afectar la calidad de los términos seleccionados. El análisis de correlación reveló una fuerte correlación negativa entre el número de documentos vistos y la cantidad de retroalimentación proporcionan 9;A medida que el número de documentos vistos aumenta la proporción de retroalimentación (los buscadores ven menos representaciones de cada documento). Esto puede ser una consecuencia natural de que tengan menos tiempo para ver los documentos en un entorno de tareas limitados por el tiempo, pero como mostraremos como cambios en la complejidad, la naturaleza de los buscadores de información interactúa también parece cambiar. En la siguiente sección investigamos el efecto de la complejidad de la tarea en los términos elegidos como resultado de IRF.3.1.2 Términos El mismo algoritmo de RF se usó para seleccionar términos de modificación de consultas en todos los sistemas [16]. Utilizamos opiniones de sujetos de los términos recomendados por los sistemas como una medida de la efectividad de IRF con respecto a los términos generados para diferentes tareas de búsqueda. Para probar esto, se pidió a los sujetos que completaron dos diferenciales semánticos que completaron la declaración: las palabras elegidas por el sistema fueron: relevantes/irrelevantes y útiles/no útiles. La Tabla 3 presenta respuestas promedio agrupadas por tarea de búsqueda. Tabla 3. Percepciones de sujeto de los términos del sistema (más bajo = mejor). RF explícito RF Diferencial RF HC MC LC HC MC LC Relevante 2.50 2.46 2.41 1.94 2.35 2.68 Útil 2.61 2.61 2.59 2.06 2.54 2.70 Se aplicaron pruebas de Kruskal-Wallis dentro de cada tipo de RF. Los resultados indican que la relevancia y la utilidad de los términos elegidos por IRF se ve afectado por la complejidad de la tarea de búsqueda;Los términos elegidos son más relevantes y útiles cuando la tarea de búsqueda es más compleja.10 Relevante aquí, se explicó como relacionado con su tarea, mientras que útiles fueron los términos que se consideraron útiles en la tarea de búsqueda. Para ERF, los resultados indican que los términos generados se perciben como más relevantes y útiles para tareas de búsqueda menos complejas;aunque las diferencias entre las tareas no fueron significativas11. Esto sugiere que las percepciones del sujeto de los términos elegidos para la modificación de la consulta se ven afectadas por la complejidad de la tarea. La comparación entre ERF e IRF muestra que las percepciones de los sujetos también varían para diferentes tipos de RF12. Además de usar datos sobre relevancia y utilidad de los términos elegidos, utilizamos datos sobre la aceptación del término para medir el valor percibido de los términos sugeridos. Los sistemas RF explícitos e implícitos hicieron recomendaciones sobre qué términos podrían agregarse a la consulta de búsqueda original. En la Tabla 4 mostramos la proporción de los seis términos principales 9 r = −0.696, p = .001 (coeficiente de correlación de Pearsons) 10 relevante: χ2 (2) = 13.82, p = .001;útil: χ2 (2) = 11.04, p = .004;\"al buscador que se agregaron a la consulta de búsqueda, para cada tipo de tarea y cada tipo de RF. Tabla 4. Aceptación a plazo (porcentaje de los seis términos principales). RF explícito RFProporción de términos HC MC LC HC MC LC aceptó 65.31 67.32 68.65 67.45 67.24 67.59 El número promedio de términos aceptados de IRF es aproximadamente el mismo en todas las tareas de búsqueda y generalmente el mismo que el de ERF14. Como muestra la Tabla 2, los sujetos marcaron menos documentos relevantes para tareas altamente complejas. Por lo tanto, cuando aumenta la complejidad de la tarea, el sistema ERF tiene menos ejemplos de documentos relevantes y los términos de expansión generados pueden ser más pobres. Esto podría explicar la diferencia en la proporción de términos recomendados aceptados en ERF a medida que aumenta la complejidad de la tarea. Para IRF, hay poca diferencia en cuántos de los términos recomendados fueron elegidos por sujetos para cada nivel de complejidad de tareas15. Los sujetos pueden haber percibido los términos del IRF como más útiles para tareas de alta complejidad, pero esto no se reflejó en la proporción de términos de IRF aceptados. Las diferencias pueden residir en la naturaleza de los términos aceptados;El trabajo futuro investigará este problema.3.1.3 Resumen En esta sección hemos presentado una investigación sobre el efecto de la complejidad de la tarea de búsqueda en la utilidad de IRF. A partir de los resultados, parece haber una fuerte relación entre la complejidad de la tarea y la interacción del sujeto: los sujetos que prefieren IRF para tareas altamente complejas. La complejidad de la tarea no afectó la proporción de términos aceptados en ninguno de los métodos de RF, a pesar de haber una diferencia en cómo los sujetos relevantes y útiles percibieron que los términos son para diferentes complejidades;La complejidad puede afectar la selección de términos de formas distintas de la proporción de términos aceptados.3.2 Experiencia de búsqueda Los buscadores experimentados pueden interactuar de manera diferente y dar diferentes tipos de evidencia a RF que los buscadores sin experiencia. Como tal, los niveles de experiencia de búsqueda pueden afectar el uso y las percepciones de los buscadores de IRF. En nuestro experimento, los sujetos se dividieron en dos grupos en función de su nivel de experiencia de búsqueda, la frecuencia con la que buscaron y los tipos de búsquedas que realizaron. En esta sección utilizamos sus percepciones y registro para abordar la próxima pregunta de investigación;La relación entre la utilidad y el uso de IRF y la experiencia de búsqueda de sujetos experimentales. Los datos son los mismos que los analizados en la sección anterior, pero aquí nos centramos en la experiencia de búsqueda en lugar de la tarea de búsqueda.3.2.1 Comentarios Analizamos los resultados de las declaraciones de actitud descritas al comienzo de la Sección 3.1.1.(es decir, cómo transmitió la relevancia para el sistema ... y cómo transmitió relevancia para el sistema lo hizo sentir ...). Estos diferenciales provocaron la opinión de los sujetos experimentales sobre el método de RF utilizado. En la Tabla 5 mostramos las respuestas promedio media para grupos sujetos sin experiencia y experimentado en ERF e IRF;24 sujetos por celda.13 Este fue el menor número de términos de modificación de consultas que se ofrecieron en ambos sistemas.14 Todos t (16) ≥ 80, todos p ≤ .31, (prueba de rango firmado de Wilcoxon) 15 ERF: χ2 (2) = 3.67, p = .16;IRF: χ2 (2) = 2.55, p = .28 (pruebas de Kruskalwallis) Tabla 5. Percepciones de sujeto del método RF (más bajo = mejor). Los resultados demuestran una fuerte preferencia en los sujetos inexpertos para IRF;Lo encontraron más fácil y efectivo que los sujetos experimentados.16 Las diferencias para todos los demás diferenciales IRF no fueron estadísticamente significativas. Para todos los diferenciales, aparte del control, los sujetos sin experiencia generalmente prefieren el IRF sobre ERF17. Los sujetos sin experiencia también sintieron que IRF era más difícil de controlar que los sujetos experimentados18. Como estos sujetos tienen menos experiencia de búsqueda, pueden ser menos capaces de comprender los procesos de RF y pueden sentirse más cómodos con el sistema recolectando comentarios implícitamente de su interacción. A los sujetos experimentados tendían a gustarle más que los sujetos sin experiencia y se sintieron más cómodos con este método de retroalimentación19. A partir de estos resultados, aparece que los sujetos experimentados encontraron ERF más útil y estaban más a gusto con el proceso ERF. De manera similar a la Sección 3.1.1, analizamos la proporción de comentarios que los buscadores proporcionaron a los sistemas experimentales. Nuestro análisis sugirió que la experiencia de búsqueda no afecta la cantidad de retroalimentación que proporcionan los sujetos20.3.2.2 Términos utilizamos respuestas del cuestionario para medir la opinión del sujeto sobre la relevancia y la utilidad de los términos desde la perspectiva de los sujetos experimentados e inexpertos. La Tabla 6 muestra las respuestas diferenciales promedio obtenidas de ambos grupos sujetos. Tabla 6. Percepciones de sujeto de los términos del sistema (más bajo = mejor). RF explícito RF Diferencial de RF. Exp. Bajoestación. Exp. Relevante 2.58 2.44 2.33 2.21 útil 2.88 2.63 2.33 2.23 Las diferencias entre los grupos sujetos fueron significativas21. Los sujetos experimentados generalmente reaccionaron a los términos de modificación de la consulta elegidos por el sistema de manera más positiva que la inexperta 16 fácil: u (24) = 391, p = .016;Efectivo: u (24) = 399, p = .011;α = .0167 (pruebas de Mann-Whitney) 17 Todos t (24) ≥ 231, todos p ≤ .001 (prueba de rango firmado de Wilcoxon) 18 U (24) = 390, p = .018;α = .0250 (prueba de Mann-Whitney) 19 t (24) = 222, p = .020 (prueba de rango firmado de Wilcoxon) 20 ERF: Todos U (24) ≤ 319, P ≥ .26, IRF: Todos U (24) ≤ 313, p ≥ .30 (pruebas de Mannwhitney) 21 ERF: Todos U (24) ≥ 388, P ≤ .020, IRF: Todos U (24) ≥ 384, P ≤ .024 RF explícito RF Diferencial de RF implícito. Exp. Bajoestación. Exp. Fácil 2.46 2.46 1.84 1.98 Efectivo 2.75 2.63 2.32 2.43 Útil 2.50 2.46 2.28 2.27 Todos (1) 2.57 2.52 2.14 2.23 cómodo 2.46 2.14 2.05 2.24 en control 1.96 1.98 2.73 2.64 all (2) 2.21 2.06 2.39 2.44 Sujetos. Este hallazgo fue respaldado por la proporción de términos de modificación de consultas que estos sujetos aceptaron. De la misma manera que en la Sección 3.1.2, analizamos el número de términos de modificación de consultas recomendados por el sistema que fueron utilizados por sujetos experimentales. La Tabla 7 muestra el número promedio de términos aceptados por grupo sujeto. Tabla 7. Aceptación a plazo (porcentaje de los seis términos principales). RF Explícito RFProportion de términos Increz. Exp. Bajoestación. Exp. Aceptado 63.76 70.44 64.43 71.35 Nuestro análisis de los datos muestra que las diferencias entre los grupos de sujetos para cada tipo de RF son significativas;Los sujetos experimentados aceptaron más términos de expansión independientemente del tipo de RF. Sin embargo, las diferencias entre los mismos grupos para diferentes tipos de RF no son significativas;Los sujetos eligieron aproximadamente el mismo porcentaje de términos de expansión ofrecidos independientemente del tipo de RF22.3.2.3 Resumen En esta sección hemos analizado los datos recopilados de dos grupos de sujetos, buscadores sin experiencia y buscadores experimentados, sobre cómo perciben y usan IRF. Los resultados indican que los sujetos sin experiencia encontraron que el IRF era más fácil y efectivo que los sujetos experimentados, quienes a su vez encontraron los términos elegidos como resultado de IRF más relevante y útil. También mostramos que los sujetos sin experiencia generalmente aceptaban términos menos recomendados que los sujetos experimentados, tal vez porque estaban menos cómodos con RF o generalmente presentaban consultas de búsqueda más cortas. La experiencia de búsqueda parece afectar cómo los sujetos usan los términos recomendados como resultado del proceso de RF.3.3 Etapa de búsqueda de nuestras observaciones de sujetos experimentales mientras buscaban conjeturamos que RF puede usarse de manera diferente en diferentes momentos durante una búsqueda. Para probar esto, nuestra tercera pregunta de investigación se refería al uso y la utilidad de IRF durante el curso de una búsqueda. En esta sección investigamos si la cantidad de RF proporcionada por los buscadores o la proporción de términos aceptados se ven afectados por cuán lejos están a través de su búsqueda. A los efectos de este análisis, una búsqueda comienza cuando un sujeto plantea la primera consulta al sistema y progresa hasta que termine la búsqueda o alcance el tiempo máximo permitido para una tarea de búsqueda de 15 minutos. No dividimos las tareas basadas en este límite, ya que los sujetos a menudo terminaron su búsqueda en menos de 15 minutos. En esta sección, utilizamos datos recopilados a partir de registros de interacción y opiniones de sujetos para investigar en qué medida se usó RF y la medida en que parecía beneficiar a nuestros sujetos experimentales en diferentes etapas en su búsqueda 3.3.1 Comentarios Los registros de interacción para todas las búsquedasEn el RF explícito y la RF implícita se analizaron y cada búsqueda se divide en nueve cortes de tiempo de igual longitud. Este número de cortes nos dio un número igual por etapa y fue un nivel suficiente de granularidad para identificar las tendencias en los resultados. Las rebanadas 1 - 3 corresponden al inicio de la búsqueda, 4 - 6 hasta el medio de la búsqueda y 7 - 9 hasta el final. En la Figura 2 trazamos la medida de precisión descrita en la Sección 3.1.1 (es decir, la proporción de todas las representaciones posibles que se proporcionaron como RF) en cada uno de los 22 IRF: U (24) = 403, p = .009, ERF: U (24) = 396, p = .013 nueve rebanadas, por tarea de búsqueda, promediado en todos los sujetos;Esto nos permite ver cómo se distribuyó la provisión de RF durante una búsqueda. La cantidad total de retroalimentación para un solo método de RF/emparejamiento de complejidad de tareas en las nueve cortes corresponde al valor registrado en la primera fila de la Tabla 2 (por ejemplo, la suma de la RF para IRF/HC en las nueve cortes de la Figura 2 IS IS21.50%). Para simplificar el análisis estadístico y la comparación, utilizamos la agrupación de Start, Middle and End.0 1 2 3 3 4 5 6 7 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Searchprecision de corte (%OftotalRepSProvidedAsrf) Explícito RF/HC RF/MC Explícito RF/LC RF/HC Implicit RF/Mc implicit RF/LCFigura 2. Distribución de la provisión de RF por tarea de búsqueda. La Figura 2 parece mostrar la existencia de una relación entre la etapa en la búsqueda y la cantidad de información de relevancia proporcionada a los diferentes tipos de algoritmo de retroalimentación. Estas son esencialmente diferencias en la forma en que los usuarios evalúan documentos. En el caso de los sujetos ERF proporcionan evaluaciones de relevancia explícita durante la mayor parte de la búsqueda, pero generalmente hay un fuerte aumento en la fase final para completar el Search23. Al usar el sistema IRF, los datos indican que al comienzo de los sujetos de búsqueda proporcionan poca información de relevancia24, que corresponde a interactuar con pocas representaciones de documentos. En esta etapa, los sujetos tal vez se concentran más en leer los resultados recuperados. La información de relevancia implícita generalmente se ofrece ampliamente en el medio de la búsqueda a medida que interactúan con los resultados y luego se aleja hacia el final de la búsqueda. Esto parecería corresponder a las etapas de exploración inicial, análisis detallado de representaciones de documentos y almacenamiento y presentación de hallazgos. La Figura 2 también muestra la proporción de retroalimentación para tareas de diferente complejidad. Los resultados parecen mostrar una diferencia25 en cómo se usa IRF que se relaciona con la complejidad de la tarea de búsqueda. Más específicamente, a medida que aumenta la complejidad, parece que los sujetos tardan más en alcanzar su punto más interactivo. Esto sugiere que la complejidad de la tarea afecta cómo se distribuye IRF durante la búsqueda y que pueden pasar más tiempo interpretando inicialmente los resultados de búsqueda para tareas más complejas.23 IRF: Todos Z ≥ 1.87, P ≤ .031, ERF: Inicio vs. End Z = 2.58, P = .005 (pruebas post-hoc de DUNNS).24 aunque aumenta hacia el final de la etapa de inicio.25 Aunque no es estadísticamente significativo;χ2 (2) = 3.54, p = .17 (prueba de suma de rango de Friedman) 3.3.2 Términos Los términos recomendados por el sistema se eligen en función de la frecuencia de su ocurrencia en los elementos relevantes. Es decir, los términos no cuidados que no sean cuidantes que ocurren con frecuencia en los resultados de búsqueda considerados relevantes pueden recomendarse al buscador para la modificación de la consulta. Dado que existe una asociación directa entre la RF y los términos seleccionados, utilizamos el número de términos aceptados por los buscadores en diferentes puntos de la búsqueda como una indicación de cuán efectivo ha sido el RF hasta el punto actual en la búsqueda. En esta sección analizamos el número promedio de términos de los seis términos principales recomendados por RF explícito y RF implícito en el transcurso de una búsqueda. La proporción promedio de los seis términos recomendados principales que se aceptaron en cada etapa se muestran en la Tabla 8;Cada celda contiene datos de los 48 sujetos. Tabla 8. Término de aceptación (proporción de los seis términos principales). RF explícito RFProportion de los términos Inicio del extremo medio del extremo medio Aceptado 66.87 66.98 67.34 61.85 68.54 73.22 Los resultados muestran una asociación aparente entre la etapa en la búsqueda y el número de sujetos de términos de retroalimentación aceptan. La etapa de búsqueda afecta la aceptación del término en IRF pero no en ERF26. Cuanto más avanza en una búsqueda que un buscador progresa, más probabilidades tendrá de aceptar los términos recomendados a través de IRF (significativamente más que ERF27). Un análisis de correlación entre la proporción de términos aceptados en cada etapa de búsqueda y RF acumulada (es decir, la suma de toda la precisión en cada corte de la Figura 2 hasta el final de la etapa de búsqueda) sugiere que en ambos tipos de RF la calidad de calidadLos términos del sistema mejoran a medida que se proporciona más RF28.3.3.3 Resumen Los resultados de esta sección indican que la ubicación en una búsqueda afecta la cantidad de comentarios dados por el usuario al sistema, y por lo tanto la cantidad de información que el mecanismo de RF debe decidir qué términos ofrecer al usuario. Además, las tendencias en los datos sugieren que la complejidad de la tarea afecta cómo los sujetos proporcionan IRF y la proporción de términos del sistema aceptados.4. Discusión e implicaciones en esta sección discutimos las implicaciones de los hallazgos presentados en la sección anterior para cada pregunta de investigación.4.1 Tarea de búsqueda Los resultados de nuestro estudio mostraron que ERF era preferido para tareas menos complejas e IRF para tareas más complejas. De las observaciones y los comentarios de los sujetos, percibimos que cuando el uso de los sujetos de sistemas ERF generalmente olvidó proporcionar la retroalimentación, pero también empleó diferentes criterios durante el proceso ERF (es decir, estaban evaluando la relevancia en lugar de expresar un interés). Cuando la búsqueda era más compleja, los sujetos rara vez encontraron resultados que consideraban completamente relevantes. Por lo tanto, lucharon por encontrar 26 ERF relevantes: χ2 (2) = 2.22, p = .33;IRF: χ2 (2) = 7.73, p = .021 (pruebas de suma de rango de Friedman);IRF: Todas las comparaciones por pares significativas en z ≥ 1.77, todas p ≤ .038 (pruebas post-hoc de DUNN) 27 todas t (48) ≥ 786, todas p ≤ .002, (prueba de rango firmado de Wilcoxon) 28 IRF:R = .712, p <.001, ERF: R = .695, p = .001 (coeficiente de correlación de Pearson) y no pudieron comunicar RF al sistema de búsqueda. En estas situaciones, los sujetos parecían preferir IRF, ya que no necesitan tomar una decisión de relevancia para obtener los beneficios de RF, es decir, sugerencias de término, mientras que en ERF lo hacen. La asociación entre el método de RF y la complejidad de la tarea tiene implicaciones para el diseño de estudios de usuarios de los sistemas de RF y los mismos sistemas de RF. Implica que en el diseño de estudios de usuarios que involucren la atención de sistemas ERF o IRF se debe tomar para incluir tareas de complejidades variables, para evitar el sesgo de tareas. Además, en el diseño de los sistemas de búsqueda implica que, dado que los diferentes tipos de RF pueden ser apropiados para diferentes complejidades de tareas, entonces un sistema que podría detectar automáticamente la complejidad podría usar tanto ERF como IRF simultáneamente para beneficiar al buscador. Por ejemplo, en el sistema IRF notamos que a medida que la complejidad de la tarea cae, el comportamiento de búsqueda cambia de la interfaz de resultados a los documentos recuperados. El monitoreo de dicha interacción en varios estudios puede conducir a un conjunto de criterios que podrían ayudar a los sistemas IR a detectar automáticamente la complejidad de las tareas y adaptar el soporte para adaptarse.4.2 Experiencia de búsqueda Analizamos el efecto de la experiencia de búsqueda en la utilidad de IRF. Nuestro análisis reveló una preferencia general entre todos los sujetos para IRF sobre ERF. Es decir, las calificaciones promedio asignadas a IRF fueron generalmente más positivas que las asignadas a ERF. Sin embargo, ambos grupos de sujetos le gustaban generalmente el IRF (tal vez porque eliminaba la carga de proporcionar información de relevancia) y los sujetos experimentados fueron preferidos por sujetos experimentados más que sujetos inexpertos (tal vez porque les permitía especificar qué resultados fueron utilizados por el sistema cuando el sistemaGeneración de recomendaciones de término). Todos los sujetos se sintieron más en control con ERF que IRF, pero para los sujetos sin experiencia, esto no parecía afectar sus preferencias generales29. Estos sujetos pueden comprender menos el proceso de RF, pero pueden estar más dispuestos a sacrificar el control sobre la retroalimentación a favor de IRF, un proceso que perciben de manera más positiva.4.3 Etapa de búsqueda También analizamos los efectos de la etapa de búsqueda en el uso y la utilidad de IRF. A través del análisis de esta naturaleza, podemos construir una imagen más completa de cómo los buscadores usaron RF y cómo esto varía en función del método de RF. Los resultados sugieren que el IRF se usa más en el medio de la búsqueda que al principio o al final, mientras que ERF se usa más hacia el final. Los resultados también muestran los efectos de la complejidad de la tarea en el proceso IRF y qué tan rápido los sujetos alcanzan su punto más interactivo. Sin un análisis de este tipo, no hubiera sido posible establecer la existencia de tales patrones de comportamiento. Los resultados sugieren que los buscadores interactúan de manera diferente para IRF y ERF. Dado que ERF no se usa tradicionalmente hasta el final de la búsqueda, puede ser posible incorporar IRF y ERF en el mismo sistema IR, y se usa IRF para reunir evidencia hasta que los sujetos decidan usar ERF. El desarrollo de dicho sistema representa parte de nuestro trabajo continuo en esta área.5. Conclusiones En este artículo hemos presentado una investigación de la retroalimentación de relevancia implícita (IRF). Nuestro objetivo fue responder tres preguntas de investigación sobre factores que pueden afectar la provisión y utilidad de IRF. Estos factores fueron la complejidad de la tarea de búsqueda, la experiencia de búsqueda de los sujetos y la etapa en la búsqueda. Nuestra conclusión general fue que todos los factores 29 esto también pueden ser cierto para los sujetos experimentados, pero los datos que tenemos son insuficientes para sacar esta conclusión.Parece tener algún efecto sobre el uso y la efectividad de IRF, aunque los efectos de interacción entre los factores no son estadísticamente significativos. Nuestras conclusiones por cada pregunta de investigación son: (i) El IRF es generalmente más útil para tareas de búsqueda complejas, donde los buscadores desean centrarse en la tarea de búsqueda y obtener nuevas ideas para su búsqueda del sistema, (ii) IRF se prefiere a ERF en generaly generalmente preferidos por los sujetos sin experiencia que desean reducir la carga de proporcionar RF, y (iii) dentro de una sola sesión de búsqueda, el IRF se ve afectado por la ubicación temporal en una búsqueda (es decir, se usa en el medio, no al principio o al final) yComplejidad de la tarea. Los estudios de esta naturaleza son importantes para establecer las circunstancias en las que una técnica prometedora como IRF es útil y las que no lo es. Es solo después de que tales estudios se hayan ejecutado y analizado de esta manera, ¿podemos desarrollar una comprensión de IRF que permita implementarlo con éxito en los sistemas operativos de IR?6. Referencias [1] Bell, D.J.y Ruthven, I. (2004). Evaluaciones de búsqueda de la complejidad de tareas para la búsqueda web. Actas de la 26ª Conferencia Europea sobre Recuperación de Información, 57-71.[2] Borlund, P. (2000). Componentes experimentales para la evaluación de sistemas de recuperación de información interactiva. Revista de documentación.56 (1): 71-90.[3] Brajnik, G., Mizzaro, S., Tasso, C. y Venuti, F. (2002). Ayuda estratégica para interfaces de usuario para la recuperación de información. Revista de la Sociedad Americana de Ciencia y Tecnología de la Información.53 (5): 343-358.[4] Busha, C.H.y Harter, S.P., (1980). Métodos de investigación en biblioteconomía: técnicas e interpretación. Serie de Biblioteca y Ciencias de la Información. Nueva York: Academic Press.[5] Campbell, I. y Van Rijsbergen, C.J. (1996). El modelo ostensivo de desarrollar necesidades de información. Actas de la tercera conferencia internacional sobre concepciones de la biblioteca y ciencias de la información, 251-268.[6] Harman, D., (1992). Comentarios de relevancia y otras técnicas de modificación de consultas. En recuperación de información: estructuras de datos y algoritmos. Nueva York: Prentice-Hall.[7] Kelly, D. y Teevan, J. (2003). Comentarios implícitos para inferir la preferencia del usuario. Foro Sigir.37 (2): 18-28.[8] Koenemann, J. y Belkin, N.J. (1996). Un caso para la interacción: un estudio del comportamiento y efectividad de recuperación de información interactiva. Actas de la Conferencia ACM Sigchi sobre factores humanos en sistemas informáticos, 205-212.[9] Meddis, R., (1984). Estadísticas utilizando rangos: un enfoque unificado. Oxford: Basil Blackwell, 303-308.[10] Morita, M. y Shinoda, Y. (1994). Filtrado de información basado en el análisis de comportamiento del usuario y la mejor recuperación de texto coincidir. Actas de la 17ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, 272-281.[11] Salton, G. y Buckley, C. (1990). Mejora del rendimiento de la recuperación por retroalimentación relevante. Revista de la Sociedad Americana de Ciencias de la Información.41 (4): 288-297.[12] Siegel, S. y Castellan, N.J. (1988). Estadísticas no paramétricas para las ciencias del comportamiento.2ª ed. Singapur: McGraw-Hill.[13] White, R.W. (2004). Comentarios implícitos para la recuperación de información interactiva. Tesis doctoral no publicada, Universidad de Glasgow, Glasgow, Reino Unido.[14] White, R.W., Jose, J.M. y Ruthven, I. (2005). Un enfoque de retroalimentación implícita para la recuperación de información interactiva, procesamiento de información y gestión, en la prensa.[15] White, R.W., Jose, J.M., Ruthven, I. y Van Rijsbergen, C.J. (2004). Un estudio simulado de modelos de retroalimentación implícitos. Actas de la 26ª Conferencia Europea sobre Recuperación de Información, 311-326.[16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D. y Chang, B.-W.(2000). El impacto de los documentos fluidos en la lectura y la navegación: un estudio de observación. Actas de la Conferencia ACM Sigchi sobre factores humanos en sistemas informáticos, 249-256. Apéndice B. casillas de verificación para marcar los títulos de documentos relevantes en el sistema RF explícito. Apéndice A. Interfaz al sistema RF implícito.1. Frase de alto rango 2. Título 3. Resumen 4. Sentencia resumida 5. Oración en el contexto 2 3 4 5 1",
    "original_sentences": [
        "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
        "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
        "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
        "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
        "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
        "Our findings suggest that all three of these factors contribute to the utility of IRF.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
        "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
        "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
        "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
        "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
        "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
        "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
        "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
        "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
        "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
        "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
        "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
        "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
        "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
        "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
        "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
        "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
        "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
        "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
        "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
        "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
        "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
        "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
        "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
        "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
        "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
        "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
        "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
        "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
        "Only the representations marked relevant by the user are used for suggesting new query terms.",
        "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
        "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
        "To the searcher this is a way they can find out more information from a potentially interesting source.",
        "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
        "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
        "Therefore the only difference between the systems is how relevance is communicated to the system.",
        "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
        "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
        "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
        "These subjects were not involved in the main experiment.",
        "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
        "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
        "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
        "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
        "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
        "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
        "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
        "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
        "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
        "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
        "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
        "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
        "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
        "However, as you have not been driving for long, you are unaware of any major changes in price.",
        "You decide to find out how the price of petrol has changed in the UK in recent years.",
        "Figure 1.",
        "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
        "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
        "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
        "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
        "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
        "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
        "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
        "Each subject carried out three tasks, one on each system.",
        "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
        "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
        "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
        "System logging was also used to record subject interaction.",
        "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
        "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
        "Subjects were offered a 5 minute break after the first hour.",
        "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
        "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
        "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
        "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
        "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
        "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
        "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
        "They were not told which behaviours would be used or how it would be used.",
        "We now describe the findings of our analysis. 3.",
        "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
        "We present our findings per research question.",
        "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
        "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
        "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
        "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
        "The highest, or most positive, values in each table are shown in bold.",
        "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
        "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
        "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
        "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
        "The average obtained differential values are shown in Table 1 for IRF and each task category.",
        "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
        "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
        "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
        "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
        "Table 1.",
        "Subject perceptions of RF method (lower = better).",
        "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
        "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
        "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
        "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
        "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
        "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
        "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
        "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
        "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
        "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
        "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
        "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
        "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
        "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
        "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
        "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
        "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
        "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
        "Table 2 shows proportion of representations provided as RF by subjects.",
        "Table 2.",
        "Feedback and documents viewed.",
        "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
        "This suggests a pattern where users are investigating retrieved documents in more depth.",
        "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
        "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
        "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
        "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
        "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
        "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
        "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
        "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
        "Table 3 presents average responses grouped by search task.",
        "Table 3.",
        "Subject perceptions of system terms (lower = better).",
        "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
        "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
        "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
        "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
        "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
        "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
        "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
        "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
        "Table 4.",
        "Term Acceptance (percentage of top six terms).",
        "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
        "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
        "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
        "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
        "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
        "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
        "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
        "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
        "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
        "As such, levels of search experience may affect searchers use and perceptions of IRF.",
        "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
        "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
        "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
        "These differentials elicited opinion from experimental subjects about the RF method used.",
        "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
        "Subject perceptions of RF method (lower = better).",
        "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
        "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
        "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
        "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
        "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
        "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
        "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
        "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
        "Table 6 shows the average differential responses obtained from both subject groups.",
        "Table 6.",
        "Subject perceptions of system terms (lower = better).",
        "Explicit RF Implicit RF Differential Inexp.",
        "Exp.",
        "Inexp.",
        "Exp.",
        "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
        "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
        "Exp.",
        "Inexp.",
        "Exp.",
        "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
        "This finding was supported by the proportion of query modification terms these subjects accepted.",
        "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
        "Table 7 shows the average number of accepted terms per subject group.",
        "Table 7.",
        "Term Acceptance (percentage of top six terms).",
        "Explicit RF Implicit RFProportion of terms Inexp.",
        "Exp.",
        "Inexp.",
        "Exp.",
        "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
        "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
        "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
        "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
        "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
        "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
        "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
        "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
        "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
        "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
        "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
        "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
        "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
        "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
        "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
        "Distribution of RF provision per search task.",
        "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
        "These are essentially differences in the way users are assessing documents.",
        "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
        "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
        "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
        "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
        "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
        "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
        "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
        "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
        "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
        "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
        "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
        "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
        "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
        "Table 8.",
        "Term Acceptance (proportion of top six terms).",
        "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
        "Search stage affects term acceptance in IRF but not in ERF26 .",
        "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
        "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
        "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
        "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
        "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
        "When the search was more complex subjects rarely found results they regarded as completely relevant.",
        "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
        "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
        "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
        "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
        "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
        "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
        "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
        "Our analysis revealed a general preference across all subjects for IRF over ERF.",
        "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
        "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
        "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
        "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
        "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
        "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
        "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
        "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
        "The findings suggest that searchers interact differently for IRF and ERF.",
        "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
        "The development of such a system represents part of our ongoing work in this area. 5.",
        "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
        "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
        "These factors were search task complexity, the subjects search experience and the stage in the search.",
        "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
        "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
        "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
        "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
        "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
        "Searchers assessments of task complexity for web searching.",
        "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
        "Experimental components for the evaluation of interactive information retrieval systems.",
        "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
        "Strategic help for user interfaces for information retrieval.",
        "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
        "Research methods in librarianship: Techniques and interpretation.",
        "Library and information science series.",
        "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
        "The ostensive model of developing information needs.",
        "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
        "Relevance feedback and other query modification techniques.",
        "In Information retrieval: Data structures and algorithms.",
        "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
        "Implicit feedback for inferring user preference.",
        "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
        "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
        "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
        "Statistics using ranks: A unified approach.",
        "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
        "Information filtering based on user behavior analysis and best match text retrieval.",
        "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
        "Improving retrieval performance by relevance feedback.",
        "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
        "Nonparametric statistics for the behavioural sciences. 2nd ed.",
        "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
        "Implicit feedback for interactive information retrieval.",
        "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
        "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
        "A simulated study of implicit feedback models.",
        "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
        "The impact of fluid documents on reading and browsing: An observational study.",
        "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
        "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
        "Appendix A. Interface to Implicit RF system. 1.",
        "Top-Ranking Sentence 2.",
        "Title 3.",
        "Summary 4.",
        "Summary Sentence 5.",
        "Sentence in Context 2 3 4 5 1"
    ],
    "error_count": 0,
    "keys": {
        "implicit relevance feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of <br>implicit relevance feedback</br> Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT <br>implicit relevance feedback</br> (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "<br>implicit relevance feedback</br> (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of <br>implicit relevance feedback</br> (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un estudio de factores que afectan la utilidad de la \"retroalimentación de relevancia implícita\" Ryen W. Instituto de Laboratorio de Interacción Humana Humana Blanca para los Estudios de Computación Avanzada de la Universidad de Maryland College Park, MD 20742, EE. UU. Ryen@umd.edu Ian Ruthven Departamento de Ciencias de la Información e InformaciónUniversidad de Strathclyde Glasgow, Escocia.retroalimentación de relevancia implícita",
                "G12 8RZ.jj@dcs.gla.ac.uk Resumen \"Comentarios de relevancia implícita\" (IRF) es el proceso por el cual un sistema de búsqueda reúne con evidencia discreta sobre los intereses del buscador de su interacción con el sistema.retroalimentación de relevancia implícita",
                "Se ha propuesto \"retroalimentación de relevancia implícita\" (IRF) [7] como una forma en que las consultas de búsqueda pueden mejorarse observando pasivamente a los buscadores a medida que interactúan.retroalimentación de relevancia implícita",
                "Conclusiones En este artículo hemos presentado una investigación de \"retroalimentación de relevancia implícita\" (IRF).retroalimentación de relevancia implícita"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "search task complexity": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: <br>search task complexity</br>, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of <br>search task complexity</br>.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of <br>search task complexity</br>, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of <br>search task complexity</br> on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were <br>search task complexity</br>, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En este artículo investigamos cómo el uso y la efectividad de IRF se ve afectado por tres factores: \"complejidad de la tarea de búsqueda\", la experiencia de búsqueda del usuario y la etapa en la búsqueda.Complejidad de la tarea de búsqueda",
                "Todos estaban familiarizados con la búsqueda en la web, y algunos con la búsqueda en otros dominios.2.4 Metodología El experimento tenía un diseño factorial;Con 2 niveles de experiencia en búsqueda, 3 sistemas experimentales (aunque solo informamos sobre los hallazgos de los sistemas ERF e IRF) y 3 niveles de \"complejidad de tareas de búsqueda\".Complejidad de la tarea de búsqueda",
                "Hallazgos En esta sección utilizamos los datos derivados del experimento para responder nuestras preguntas de investigación sobre el efecto de la \"complejidad de tareas de búsqueda\", la experiencia de búsqueda y la etapa en la búsqueda sobre el uso y la efectividad del IRF.Complejidad de la tarea de búsqueda",
                "Las diferencias pueden residir en la naturaleza de los términos aceptados;El trabajo futuro investigará este problema.3.1.3 Resumen En esta sección hemos presentado una investigación sobre el efecto de la \"complejidad de la tarea de búsqueda\" en la utilidad de IRF.Complejidad de la tarea de búsqueda",
                "Estos factores fueron \"complejidad de tareas de búsqueda\", la experiencia de búsqueda de los sujetos y la etapa de la búsqueda.Complejidad de la tarea de búsqueda"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "relevance feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit <br>relevance feedback</br> Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit <br>relevance feedback</br> (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as <br>relevance feedback</br> (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit <br>relevance feedback</br> (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit <br>relevance feedback</br> (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "<br>relevance feedback</br> and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by <br>relevance feedback</br>.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un estudio de factores que afectan la utilidad de la \"retroalimentación de relevancia\" implícita Ryen W. Instituto de Laboratorio de Interacción Humana Humana Blanca para los Estudios de Computación Avanzado de la Universidad de Maryland College Park, MD 20742, EE. UU. Ryen@umd.edu Ian Ruthven Departamento de Ciencias de la Información e InformaciónUniversidad de Strathclyde Glasgow, Escocia.Comentarios de relevancia",
                "G12 8RZ.jj@dcs.gla.ac.uk Resumen implícito \"Comentarios de relevancia\" (IRF) es el proceso por el cual un sistema de búsqueda reúne discretamente evidencia sobre los intereses del buscador de su interacción con el sistema.Comentarios de relevancia",
                "Se han propuesto técnicas como \"retroalimentación de relevancia\" (RF) [11] como una forma en que el sistema IR puede respaldar el desarrollo iterativo de una consulta de búsqueda al sugerir términos alternativos para la modificación de la consulta.Comentarios de relevancia",
                "La \"retroalimentación de relevancia\" implícita (IRF) [7] se ha propuesto como una forma en que las consultas de búsqueda pueden mejorarse observando pasivamente a los buscadores a medida que interactúan.Comentarios de relevancia",
                "Conclusiones En este artículo hemos presentado una investigación de la \"retroalimentación de relevancia\" implícita (IRF).Comentarios de relevancia",
                "\"Comentarios de relevancia\" y otras técnicas de modificación de consultas.Comentarios de relevancia",
                "Mejora del rendimiento de la recuperación por \"retroalimentación de relevancia\".Comentarios de relevancia"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "browse-based result interface": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with <br>browse-based result interface</br>s [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El IRF se ha implementado mediante el uso de medidas sustitutas basadas en la interacción con documentos (como el tiempo de lectura, el desplazamiento o la retención de documentos) [7] o utilizando la interacción con la \"interfaz de resultados basada en la navegación\" s [5].interfaz de resultados basada en navegación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "top-ranking sentence": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and <br>top-ranking sentence</br> is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "<br>top-ranking sentence</br> 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cada oración resumida y \"oración de alto rango\" se considera una representación del documento.oración de alto rango",
                "\"Frase de rango superior\" 2. Frase de alto rango"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "query modification term": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the <br>query modification term</br>s selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting <br>query modification term</br>s and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The <br>query modification term</br>s are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select <br>query modification term</br>s in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of <br>query modification term</br>s that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the <br>query modification term</br>s chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of <br>query modification term</br>s these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of <br>query modification term</br>s recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este estudio tiene como objetivo establecer cuándo, y en qué circunstancias, el IRF funciona bien en términos de su uso y el \"término de modificación de consulta\" seleccionado como resultado de su uso.término de modificación de la consulta",
                "El experimento principal del que se toman los datos se diseñó para probar técnicas para seleccionar el \"término de modificación de consultas\" y las técnicas para mostrar resultados de recuperación [13].término de modificación de la consulta",
                "El \"Término de modificación de la consulta\" se seleccionan utilizando el mismo algoritmo que en el sistema RF explícito.término de modificación de la consulta",
                "En la siguiente sección investigamos el efecto de la complejidad de la tarea en los términos elegidos como resultado de IRF.3.1.2 Términos El mismo algoritmo de RF se usó para seleccionar el \"término de modificación de consulta\" s en todos los sistemas [16].término de modificación de la consulta",
                "En la Tabla 5 mostramos las respuestas promedio media para grupos sujetos sin experiencia y experimentado en ERF e IRF;24 sujetos por celda.13 Este fue el menor número de \"término de modificación de consultas\" que se ofrecieron en ambos sistemas.14 Todos t (16) ≥ 80, todos p ≤ .31, (prueba de rango firmado de Wilcoxon) 15 ERF: χ2 (2) = 3.67, p = .16;IRF: χ2 (2) = 2.55, p = .28 (pruebas de Kruskalwallis) Tabla 5. Término de modificación de la consulta",
                "Los sujetos experimentados generalmente reaccionaron al \"término de modificación de consulta\" s elegido por el sistema de manera más positiva que inexperida 16 fácil: u (24) = 391, p = .016;Efectivo: u (24) = 399, p = .011;α = .0167 (pruebas de Mann-Whitney) 17 Todos t (24) ≥ 231, todos p ≤ .001 (prueba de rango firmado de Wilcoxon) 18 U (24) = 390, p = .018;α = .0250 (prueba de Mann-Whitney) 19 t (24) = 222, p = .020 (prueba de rango firmado de Wilcoxon) 20 ERF: Todos U (24) ≤ 319, P ≥ .26, IRF: Todos U (24) ≤ 313, p ≥ .30 (pruebas de Mannwhitney) 21 ERF: Todos U (24) ≥ 388, P ≤ .020, IRF: Todos U (24) ≥ 384, P ≤ .024 RF explícito RF Diferencial de RF implícito.término de modificación de la consulta",
                "Este hallazgo fue respaldado por la proporción del \"término de modificación de la consulta\" s estos sujetos aceptados.término de modificación de la consulta",
                "De la misma manera que en la Sección 3.1.2, analizamos el número de \"Término de modificación de consultas\" recomendado por el sistema que fue utilizado por sujetos experimentales.término de modificación de la consulta"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "explicit rf system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 <br>explicit rf system</br> This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the <br>explicit rf system</br>.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the <br>explicit rf system</br>.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El buscador también puede agregar o eliminar los términos de la consulta a voluntad.2.1.2 \"Sistema de RF explícito\" Esta versión del sistema implementa RF explícito.sistema de RF explícito",
                "Los términos de modificación de la consulta se seleccionan utilizando el mismo algoritmo que en el \"sistema de RF explícito\".sistema de RF explícito",
                "Apéndice B. casillas de verificación para marcar los títulos de documentos relevantes en el \"Sistema de RF explícito\".sistema de RF explícito"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "interactive query expansion feature": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an <br>interactive query expansion feature</br> by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Ambos sistemas proporcionan una \"función de expansión de consulta interactiva\" al sugerir nuevos términos de consulta al usuario.función de expansión de consultas interactivas"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "high complexity whilst": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: <br>high complexity whilst</br> having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Tarea de HC: \"alta complejidad mientras\" cena con un colega estadounidense, comentan el alto precio de la gasolina en el Reino Unido en comparación con otros países, a pesar de los grandes volúmenes provienen de la misma fuente.Alta complejidad mientras"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "moderate complexity whilst": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: <br>moderate complexity whilst</br> out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Tarea MC: \"Complejidad moderada mientras\" se cena una noche, uno de los invitados de sus amigos se queja del precio de la gasolina y los factores que lo causan.complejidad moderada mientras"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "introductory questionnaire": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an <br>introductory questionnaire</br>.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este conjunto de instrucciones se escribió para garantizar que cada sujeto recibiera precisamente la misma información.II.Se le pidió al sujeto que completara un \"cuestionario introductorio\".cuestionario introductorio"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "varying complexity": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of <br>varying complexity</br>, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuestro análisis utiliza datos de cuestionarios, entrevistas posteriores al experimento y el registro de sistemas de antecedentes en los sistemas ERF e IRF.3.1 Los buscadores de tareas de búsqueda intentaron tres tareas de búsqueda de \"complejidad variable\", cada una en un sistema experimental diferente.complejidad variable"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "medium complexity": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a <br>medium complexity</br> (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cada celda de la Tabla 1 resume las respuestas de sujetos para 16 pares de sistemas de tareas (16 sujetos que realizaron una tarea de alta complejidad (HC) en el sistema ERF, 16 sujetos que ejecutaban una tarea de \"complejidad media\" (MC) en el sistema ERF, etc.).complejidad media"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "search precision": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of <br>search precision</br> that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC Proportion Feedback 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para hacer esto, utilizamos una medida de \"precisión de búsqueda\" que es la proporción de todas las representaciones de documentos posibles que evaluó un buscador, dividido por el número total que podrían evaluar.Precisión de búsqueda"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "proportion feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Study of Factors Affecting the Utility of Implicit Relevance Feedback Ryen W. White Human-Computer Interaction Laboratory Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA ryen@umd.edu Ian Ruthven Department of Computer and Information Sciences University of Strathclyde Glasgow, Scotland.",
                "G1 1XH. ir@cis.strath.ac.uk Joemon M. Jose Department of Computing Science University of Glasgow Glasgow, Scotland.",
                "G12 8RZ. jj@dcs.gla.ac.uk ABSTRACT Implicit relevance feedback (IRF) is the process by which a search system unobtrusively gathers evidence on searcher interests from their interaction with the system.",
                "IRF is a new method of gathering information on user interest and, if IRF is to be used in operational IR systems, it is important to establish when it performs well and when it performs poorly.",
                "In this paper we investigate how the use and effectiveness of IRF is affected by three factors: search task complexity, the search experience of the user and the stage in the search.",
                "Our findings suggest that all three of these factors contribute to the utility of IRF.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval] General Terms Experimentation, Human Factors. 1.",
                "INTRODUCTION Information Retrieval (IR) systems are designed to help searchers solve problems.",
                "In the traditional interaction metaphor employed by Web search systems such as Yahoo! and MSN Search, the system generally only supports the retrieval of potentially relevant documents from the collection.",
                "However, it is also possible to offer support to searchers for different search activities, such as selecting the terms to present to the system or choosing which search strategy to adopt [3, 8]; both of which can be problematic for searchers.",
                "As the quality of the query submitted to the system directly affects the quality of search results, the issue of how to improve search queries has been studied extensively in IR research [6].",
                "Techniques such as Relevance Feedback (RF) [11] have been proposed as a way in which the IR system can support the iterative development of a search query by suggesting alternative terms for query modification.",
                "However, in practice RF techniques have been underutilised as they place an increased cognitive burden on searchers to directly indicate relevant results [10].",
                "Implicit Relevance Feedback (IRF) [7] has been proposed as a way in which search queries can be improved by passively observing searchers as they interact.",
                "IRF has been implemented either through the use of surrogate measures based on interaction with documents (such as reading time, scrolling or document retention) [7] or using interaction with browse-based result interfaces [5].",
                "IRF has been shown to display mixed effectiveness because the factors that are good indicators of user interest are often erratic and the inferences drawn from user interaction are not always valid [7].",
                "In this paper we present a study into the use and effectiveness of IRF in an online search environment.",
                "The study aims to investigate the factors that affect IRF, in particular three research questions: (i) is the use of and perceived quality of terms generated by IRF affected by the search task? (ii) is the use of and perceived quality of terms generated by IRF affected by the level of search experience of system users? (iii) is IRF equally used and does it generate terms that are equally useful at all search stages?",
                "This study aims to establish when, and under what circumstances, IRF performs well in terms of its use and the query modification terms selected as a result of its use.",
                "The main experiment from which the data are taken was designed to test techniques for selecting query modification terms and techniques for displaying retrieval results [13].",
                "In this paper we use data derived from that experiment to study factors affecting the utility of IRF. 2.",
                "STUDY In this section we describe the user study conducted to address our research questions. 2.1 Systems Our study used two systems both of which suggested new query terms to the user.",
                "One system suggested terms based on the users interaction (IRF), the other used Explicit RF (ERF) asking the user to explicitly indicate relevant material.",
                "Both systems used the same term suggestion algorithm, [15], and used a common interface. 2.1.1 Interface Overview In both systems, retrieved documents are represented at the interface by their full-text and a variety of smaller, query-relevant representations, created at retrieval time.",
                "We used the Web as the test collection in this study and Google1 as the underlying search engine.",
                "Document representations include the document title and a summary of the document; a list of top-ranking sentences (TRS) extracted from the top documents retrieved, scored in relation to the query, a sentence in the document summary, and each summary sentence in the context it occurs in the document (i.e., with the preceding and following sentence).",
                "Each summary sentence and top-ranking sentence is regarded as a representation of the document.",
                "The default display contains the list of top-ranking sentences and the list of the first ten document titles.",
                "Interacting with a representation guides searchers to a different representation from the same document, e.g., moving the mouse over a document title displays a summary of the document.",
                "This presentation of progressively more information from documents to aid relevance assessments has been shown to be effective in earlier work [14, 16].",
                "In Appendix A we show the complete interface to the IRF system with the document representations marked and in Appendix B we show a fragment from the ERF interface with the checkboxes used by searchers to indicate relevant information.",
                "Both systems provide an interactive query expansion feature by suggesting new query terms to the user.",
                "The searcher has the responsibility for choosing which, if any, of these terms to add to the query.",
                "The searcher can also add or remove terms from the query at will. 2.1.2 Explicit RF system This version of the system implements explicit RF.",
                "Next to each document representation are checkboxes that allow searchers to mark individual representations as relevant; marking a representation is an indication that its contents are relevant.",
                "Only the representations marked relevant by the user are used for suggesting new query terms.",
                "This system was used as a baseline against which the IRF system could be compared. 2.1.3 Implicit RF system This system makes inferences about searcher interests based on the information with which they interact.",
                "As described in Section 2.1.1 interacting with a representation highlights a new representation from the same document.",
                "To the searcher this is a way they can find out more information from a potentially interesting source.",
                "To the implicit RF system each interaction with a representation is interpreted as an implicit indication of interest in that representation; interacting with a representation is assumed to be an indication that its contents are relevant.",
                "The query modification terms are selected using the same algorithm as in the Explicit RF system.",
                "Therefore the only difference between the systems is how relevance is communicated to the system.",
                "The results of the main experiment [13] indicated that these two systems were comparable in terms of effectiveness. 2.2 Tasks Search tasks were designed to encourage realistic search behaviour by our subjects.",
                "The tasks were phrased in the form of simulated work task situations [2], i.e., short search scenarios that were designed to reflect real-life search situations and allow subjects to develop personal assessments of relevance.",
                "We devised six search topics (i.e., applying to university, allergies in the workplace, art galleries in Rome, Third Generation mobile phones, Internet music piracy and petrol prices) based on pilot testing with a small representative group of subjects.",
                "These subjects were not involved in the main experiment.",
                "For each topic, three versions of each work task situation were devised, each version differing in their predicted level of task complexity.",
                "As described in [1] task complexity is a variable that affects subject perceptions of a task and their interactive behaviour, e.g., subjects perform more filtering activities with highly complex search tasks.",
                "By developing tasks of different complexity we can assess how the nature of the task affects the subjects interactive behaviour and hence the evidence supplied to IRF algorithms.",
                "Task complexity was varied according to the methodology described in [1], specifically by varying the number of potential information sources and types of information required, to complete a task.",
                "In our pilot tests (and in a posteriori analysis of the main experiment results) we verified that subjects reporting of individual task complexity matched our estimation of the complexity of the task.",
                "Subjects attempted three search tasks: one high complexity, one moderate complexity and one low complexity2 .",
                "They were asked to read the task, place themselves in the situation it described and find the information they felt was required to complete the task.",
                "Figure 1 shows the task statements for three levels of task complexity for one of the six search topics.",
                "HC Task: High Complexity Whilst having dinner with an American colleague, they comment on the high price of petrol in the UK compared to other countries, despite large volumes coming from the same source.",
                "Unaware of any major differences, you decide to find out how and why petrol prices vary worldwide.",
                "MC Task: Moderate Complexity Whilst out for dinner one night, one of your friends guests is complaining about the price of petrol and the factors that cause it.",
                "Throughout the night they seem to be complaining about everything they can, reducing the credibility of their earlier statements so you decide to research which factors actually are important in determining the price of petrol in the UK.",
                "LC Task: Low Complexity While out for dinner one night, your friend complains about the rising price of petrol.",
                "However, as you have not been driving for long, you are unaware of any major changes in price.",
                "You decide to find out how the price of petrol has changed in the UK in recent years.",
                "Figure 1.",
                "Varying task complexity (Petrol Prices topic). 2.3 Subjects 156 volunteers expressed an interest in participating in our study. 48 subjects were selected from this set with the aim of populating two groups, each with 24 subjects: inexperienced (infrequent/ inexperienced searchers) and experienced (frequent/ experienced searchers).",
                "Subjects were not chosen and classified into their groups until they had completed an entry questionnaire that asked them about their search experience and computer use.",
                "The average age of the subjects was 22.83 years (maximum 51, minimum 18, σ = 5.23 years) and 75% had a university diploma or a higher degree. 47.91% of subjects had, or were pursuing, a qualification in a discipline related to Computer Science.",
                "The subjects were a mixture of students, researchers, academic staff and others, with different levels of computer and search experience.",
                "The subjects were divided into the two groups depending on their search experience, how often they searched and the types of searches they performed.",
                "All were familiar with Web searching, and some with searching in other domains. 2.4 Methodology The experiment had a factorial design; with 2 levels of search experience, 3 experimental systems (although we only report on the findings from the ERF and IRF systems) and 3 levels of search task complexity.",
                "Subjects attempted one task of each complexity, 2 The main experiment from which these results are drawn had a third comparator system which had a different interface.",
                "Each subject carried out three tasks, one on each system.",
                "We only report on the results from the ERF and IRF systems as these are the only pertinent ones for this paper. switched systems after each task and used each system once.",
                "The order in which systems were used and search tasks attempted was randomised according to a Latin square experimental design.",
                "Questionnaires used Likert scales, semantic differentials and openended questions to elicit subject opinions [4].",
                "System logging was also used to record subject interaction.",
                "A tutorial carried out prior to the experiment allowed subjects to use a non-feedback version of the system to attempt a practice task before using the first experimental system.",
                "Experiments lasted between oneand-a-half and two hours, dependent on variables such as the time spent completing questionnaires.",
                "Subjects were offered a 5 minute break after the first hour.",
                "In each experiment: i. the subject was welcomed and asked to read an introduction to the experiments and sign consent forms.",
                "This set of instructions was written to ensure that each subject received precisely the same information. ii. the subject was asked to complete an introductory questionnaire.",
                "This contained questions about the subjects education, general search experience, computer experience and Web search experience. iii. the subject was given a tutorial on the interface, followed by a training topic on a version of the interface with no RF. iv. the subject was given three task sheets and asked to choose one task from the six topics on each sheet.",
                "No guidelines were given to subjects when choosing a task other than they could not choose a task from any topic more than once.",
                "Task complexity was rotated by the experimenter so each subject attempted one high complexity task, one moderate complexity task and one low complexity task. v. the subject was asked to perform the search and was given 15 minutes to search.",
                "The subject could terminate a search early if they were unable to find any more information they felt helped them complete the task. vi. after completion of the search, the subject was asked to complete a post-search questionnaire. vii. the remaining tasks were attempted by the subject, following steps v. and vi. viii. the subject completed a post-experiment questionnaire and participated in a post-experiment interview.",
                "Subjects were told that their interaction may be used by the IRF system to help them as they searched.",
                "They were not told which behaviours would be used or how it would be used.",
                "We now describe the findings of our analysis. 3.",
                "FINDINGS In this section we use the data derived from the experiment to answer our research questions about the effect of search task complexity, search experience and stage in search on the use and effectiveness of IRF.",
                "We present our findings per research question.",
                "Due to the ordinal nature of much of the data non-parametric statistical testing is used in this analysis and the level of significance is set to p < .05, unless otherwise stated.",
                "We use the method proposed by [12] to determine the significance of differences in multiple comparisons and that of [9] to test for interaction effects between experimental variables, the occurrence of which we report where appropriate.",
                "All Likert scales and semantic differentials were on a 5-point scale where a rating closer to 1 signifies more agreement with the attitude statement.",
                "The category labels HC, MC and LC are used to denote the high, moderate and low complexity tasks respectively.",
                "The highest, or most positive, values in each table are shown in bold.",
                "Our analysis uses data from questionnaires, post-experiment interviews and background system logging on the ERF and IRF systems. 3.1 Search Task Searchers attempted three search tasks of varying complexity, each on a different experimental system.",
                "In this section we present an analysis on the use and usefulness of IRF for search tasks of different complexities.",
                "We present our findings in terms of the RF provided by subjects and the terms recommended by the systems. 3.1.1 Feedback We use questionnaires and system logs to gather data on subject perceptions and provision of RF for different search tasks.",
                "In the postsearch questionnaire subjects were asked about how RF was conveyed using differentials to elicit their opinion on: 1. the value of the feedback technique: How you conveyed relevance to the system (i.e. ticking boxes or viewing information) was: easy / difficult, effective/ ineffective, useful/not useful. 2. the process of providing the feedback: How you conveyed relevance to the system made you feel: comfortable/uncomfortable, in control/not in control.",
                "The average obtained differential values are shown in Table 1 for IRF and each task category.",
                "The value corresponding to the differential All represents the mean of all differentials for a particular attitude statement.",
                "This gives some overall understanding of the subjects feelings which can be useful as the subjects may not answer individual differentials very precisely.",
                "The values for ERF are included for reference in this table and all other tables and figures in the Findings section.",
                "Since the aim of the paper is to investigate situations in which IRF might perform well, not a direct comparison between IRF and ERF, we make only limited comparisons between these two types of feedback.",
                "Table 1.",
                "Subject perceptions of RF method (lower = better).",
                "Each cell in Table 1 summarises the subject responses for 16 tasksystem pairs (16 subjects who ran a high complexity (HC) task on the ERF system, 16 subjects who ran a medium complexity (MC) task on the ERF system, etc).",
                "Kruskal-Wallis Tests were applied to each differential for each type of RF3 .",
                "Subject responses suggested that 3 Since this analysis involved many differentials, we use a Bonferroni correction to control the experiment-wise error rate and set the alpha level (α) to .0167 and .0250 for both statements 1. and 2. respectively, i.e., .05 divided by the number of differentials.",
                "This correction reduces the number of Type I errors i.e., rejecting null hypotheses that are true.",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Easy 2.78 2.47 2.12 1.86 1.81 1.93 Effective 2.94 2.68 2.44 2.04 2.41 2.66 Useful 2.76 2.51 2.16 1.91 2.37 2.56 All (1) 2.83 2.55 2.24 1.94 2.20 2.38 Comfortable 2.27 2.28 2.35 2.11 2.15 2.16 In control 2.01 1.97 1.93 2.73 2.68 2.61 All (2) 2.14 2.13 2.14 2.42 2.42 2.39 IRF was most effective and useful for more complex search tasks4 and that the differences in all pair-wise comparisons between tasks were significant5 .",
                "Subject perceptions of IRF elicited using the other differentials did not appear to be affected by the complexity of the search task6 .",
                "To determine whether a relationship exists between the effectiveness and usefulness of the IRF process and task complexity we applied Spearmans Rank Order Correlation Coefficient to participant responses.",
                "The results of this analysis suggest that the effectiveness of IRF and usefulness of IRF are both related to task complexity; as task complexity increases subject preference for IRF also increases7 .",
                "On the other hand, subjects felt ERF was more effective and useful for low complexity tasks8 .",
                "Their verbal reporting of ERF, where perceived utility and effectiveness increased as task complexity decreased, supports this finding.",
                "In tasks of lower complexity the subjects felt they were better able to provide feedback on whether or not documents were relevant to the task.",
                "We analyse interaction logs generated by both interfaces to investigate the amount of RF subjects provided.",
                "To do this we use a measure of search precision that is the proportion of all possible document representations that a searcher assessed, divided by the total number they could assess.",
                "In ERF this is the proportion of all possible representations that were marked relevant by the searcher, i.e., those representations explicitly marked relevant.",
                "In IRF this is the proportion of representations viewed by a searcher over all possible representations that could have been viewed by the searcher.",
                "This proportion measures the searchers level of interaction with a document, we take it to measure the users interest in the document: the more document representations viewed the more interested we assume a user is in the content of the document.",
                "There are a maximum of 14 representations per document: 4 topranking sentences, 1 title, 1 summary, 4 summary sentences and 4 summary sentences in document context.",
                "Since the interface shows document representations from the top-30 documents, there are 420 representations that a searcher can assess.",
                "Table 2 shows proportion of representations provided as RF by subjects.",
                "Table 2.",
                "Feedback and documents viewed.",
                "Explicit RF Implicit RF Measure HC MC LC HC MC LC <br>proportion feedback</br> 2.14 2.39 2.65 21.50 19.36 15.32 Documents Viewed 10.63 10.43 10.81 10.84 12.19 14.81 For IRF there is a clear pattern: as complexity increases the subjects viewed fewer documents but viewed more representations for each document.",
                "This suggests a pattern where users are investigating retrieved documents in more depth.",
                "It also means that the amount of 4 effective: χ2 (2) = 11.62, p = .003; useful: χ2 (2) = 12.43, p = .002 5 Dunns post-hoc tests (multiple comparison using rank sums); all Z ≥ 2.88, all p ≤ .002 6 all χ2 (2) ≤ 2.85, all p ≥ .24 (Kruskal-Wallis Tests) 7 effective: all r ≥ 0.644, p ≤ .002; useful: all r ≥ 0.541, p ≤ .009 8 effective: χ2 (2) = 7.01, p = .03; useful: χ2 (2) = 6.59, p = .037 (Kruskal-Wallis Test); all pair-wise differences significant, all Z ≥ 2.34, all p ≤ .01 (Dunns post-hoc tests) feedback varies based on the complexity of the search task.",
                "Since IRF is based on the interaction of the searcher, the more they interact, the more feedback they provide.",
                "This has no effect on the number of RF terms chosen, but may affect the quality of the terms selected.",
                "Correlation analysis revealed a strong negative correlation between the number of documents viewed and the amount of feedback searchers provide9 ; as the number of documents viewed increases the proportion of feedback falls (searchers view less representations of each document).",
                "This may be a natural consequence of their being less time to view documents in a time constrained task environment but as we will show as complexity changes, the nature of information searchers interact with also appears to change.",
                "In the next section we investigate the effect of task complexity on the terms chosen as a result of IRF. 3.1.2 Terms The same RF algorithm was used to select query modification terms in all systems [16].",
                "We use subject opinions of terms recommended by the systems as a measure of the effectiveness of IRF with respect to the terms generated for different search tasks.",
                "To test this, subjects were asked to complete two semantic differentials that completed the statement: The words chosen by the system were: relevant/irrelevant and useful/not useful.",
                "Table 3 presents average responses grouped by search task.",
                "Table 3.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential HC MC LC HC MC LC Relevant 2.50 2.46 2.41 1.94 2.35 2.68 Useful 2.61 2.61 2.59 2.06 2.54 2.70 Kruskal-Wallis Tests were applied within each type of RF.",
                "The results indicate that the relevance and usefulness of the terms chosen by IRF is affected by the complexity of the search task; the terms chosen are more relevant and useful when the search task is more complex. 10 Relevant here, was explained as being related to their task whereas useful was for terms that were seen as being helpful in the search task.",
                "For ERF, the results indicate that the terms generated are perceived to be more relevant and useful for less complex search tasks; although differences between tasks were not significant11 .",
                "This suggests that subject perceptions of the terms chosen for query modification are affected by task complexity.",
                "Comparison between ERF and IRF shows that subject perceptions also vary for different types of RF12 .",
                "As well as using data on relevance and utility of the terms chosen, we used data on term acceptance to measure the perceived value of the terms suggested.",
                "Explicit and Implicit RF systems made recommendations about which terms could be added to the original search query.",
                "In Table 4 we show the proportion of the top six terms 9 r = −0.696, p = .001 (Pearsons Correlation Coefficient) 10 relevant: χ2 (2) = 13.82, p = .001; useful: χ2 (2) = 11.04, p = .004; α = .025 11 all χ2 (2) ≤ 2.28, all p ≥ .32 (Kruskal-Wallis Test) 12 all T(16) ≥ 102, all p ≤ .021, (Wilcoxon Signed-Rank Test) 13 that were shown to the searcher that were added to the search query, for each type of task and each type of RF.",
                "Table 4.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms HC MC LC HC MC LC Accepted 65.31 67.32 68.65 67.45 67.24 67.59 The average number of terms accepted from IRF is approximately the same across all search tasks and generally the same as that of ERF14 .",
                "As Table 2 shows, subjects marked fewer documents relevant for highly complex tasks .",
                "Therefore, when task complexity increases the ERF system has fewer examples of relevant documents and the expansion terms generated may be poorer.",
                "This could explain the difference in the proportion of recommended terms accepted in ERF as task complexity increases.",
                "For IRF there is little difference in how many of the recommended terms were chosen by subjects for each level of task complexity15 .",
                "Subjects may have perceived IRF terms as more useful for high complexity tasks but this was not reflected in the proportion of IRF terms accepted.",
                "Differences may reside in the nature of the terms accepted; future work will investigate this issue. 3.1.3 Summary In this section we have presented an investigation on the effect of search task complexity on the utility of IRF.",
                "From the results there appears to be a strong relation between the complexity of the task and the subject interaction: subjects preferring IRF for highly complex tasks.",
                "Task complexity did not affect the proportion of terms accepted in either RF method, despite there being a difference in how relevant and useful subjects perceived the terms to be for different complexities; complexity may affect term selection in ways other than the proportion of terms accepted. 3.2 Search Experience Experienced searchers may interact differently and give different types of evidence to RF than inexperienced searchers.",
                "As such, levels of search experience may affect searchers use and perceptions of IRF.",
                "In our experiment subjects were divided into two groups based on their level of search experience, the frequency with which they searched and the types of searches they performed.",
                "In this section we use their perceptions and logging to address the next research question; the relationship between the usefulness and use of IRF and the search experience of experimental subjects.",
                "The data are the same as that analysed in the previous section, but here we focus on search experience rather than the search task. 3.2.1 Feedback We analyse the results from the attitude statements described at the beginning of Section 3.1.1. (i.e., How you conveyed relevance to the system was… and How you conveyed relevance to the system made you feel…).",
                "These differentials elicited opinion from experimental subjects about the RF method used.",
                "In Table 5 we show the mean average responses for inexperienced and experienced subject groups on ERF and IRF; 24 subjects per cell. 13 This was the smallest number of query modification terms that were offered in both systems. 14 all T(16) ≥ 80, all p ≤ .31, (Wilcoxon Signed-Rank Test) 15 ERF: χ2 (2) = 3.67, p = .16; IRF: χ2 (2) = 2.55, p = .28 (KruskalWallis Tests) Table 5.",
                "Subject perceptions of RF method (lower = better).",
                "The results demonstrate a strong preference in inexperienced subjects for IRF; they found it more easy and effective than experienced subjects. 16 The differences for all other IRF differentials were not statistically significant.",
                "For all differentials, apart from in control, inexperienced subjects generally preferred IRF over ERF17 .",
                "Inexperienced subjects also felt that IRF was more difficult to control than experienced subjects18 .",
                "As these subjects have less search experience they may be less able to understand RF processes and may be more comfortable with the system gathering feedback implicitly from their interaction.",
                "Experienced subjects tended to like ERF more than inexperienced subjects and felt more comfortable with this feedback method19 .",
                "It appears from these results that experienced subjects found ERF more useful and were more at ease with the ERF process.",
                "In a similar way to Section 3.1.1 we analysed the proportion of feedback that searchers provided to the experimental systems.",
                "Our analysis suggested that search experience does not affect the amount of feedback subjects provide20 . 3.2.2 Terms We used questionnaire responses to gauge subject opinion on the relevance and usefulness of the terms from the perspective of experienced and inexperienced subjects.",
                "Table 6 shows the average differential responses obtained from both subject groups.",
                "Table 6.",
                "Subject perceptions of system terms (lower = better).",
                "Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Relevant 2.58 2.44 2.33 2.21 Useful 2.88 2.63 2.33 2.23 The differences between subject groups were significant21 .",
                "Experienced subjects generally reacted to the query modification terms chosen by the system more positively than inexperienced 16 easy: U(24) = 391, p = .016; effective: U(24) = 399, p = .011; α = .0167 (Mann-Whitney Tests) 17 all T(24) ≥ 231, all p ≤ .001 (Wilcoxon Signed-Rank Test) 18 U(24) = 390, p = .018; α = .0250 (Mann-Whitney Test) 19 T(24) = 222, p = .020 (Wilcoxon Signed-Rank Test) 20 ERF: all U(24) ≤ 319, p ≥ .26, IRF: all U(24) ≤ 313, p ≥ .30 (MannWhitney Tests) 21 ERF: all U(24) ≥ 388, p ≤ .020, IRF: all U(24) ≥ 384, p ≤ .024 Explicit RF Implicit RF Differential Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Easy 2.46 2.46 1.84 1.98 Effective 2.75 2.63 2.32 2.43 Useful 2.50 2.46 2.28 2.27 All (1) 2.57 2.52 2.14 2.23 Comfortable 2.46 2.14 2.05 2.24 In control 1.96 1.98 2.73 2.64 All (2) 2.21 2.06 2.39 2.44 subjects.",
                "This finding was supported by the proportion of query modification terms these subjects accepted.",
                "In the same way as in Section 3.1.2, we analysed the number of query modification terms recommended by the system that were used by experimental subjects.",
                "Table 7 shows the average number of accepted terms per subject group.",
                "Table 7.",
                "Term Acceptance (percentage of top six terms).",
                "Explicit RF Implicit RFProportion of terms Inexp.",
                "Exp.",
                "Inexp.",
                "Exp.",
                "Accepted 63.76 70.44 64.43 71.35 Our analysis of the data show that differences between subject groups for each type of RF are significant; experienced subjects accepted more expansion terms regardless of type of RF.",
                "However, the differences between the same groups for different types of RF are not significant; subjects chose roughly the same percentage of expansion terms offered irrespective of the type of RF22 . 3.2.3 Summary In this section we have analysed data gathered from two subject groups - inexperienced searchers and experienced searchers - on how they perceive and use IRF.",
                "The results indicate that inexperienced subjects found IRF more easy and effective than experienced subjects, who in turn found the terms chosen as a result of IRF more relevant and useful.",
                "We also showed that inexperienced subjects generally accepted less recommended terms than experienced subjects, perhaps because they were less comfortable with RF or generally submitted shorter search queries.",
                "Search experience appears to affect how subjects use the terms recommended as a result of the RF process. 3.3 Search Stage From our observations of experimental subjects as they searched we conjectured that RF may be used differently at different times during a search.",
                "To test this, our third research question concerned the use and usefulness of IRF during the course of a search.",
                "In this section we investigate whether the amount of RF provided by searchers or the proportion of terms accepted are affected by how far through their search they are.",
                "For the purposes of this analysis a search begins when a subject poses the first query to the system and progresses until they terminate the search or reach the maximum allowed time for a search task of 15 minutes.",
                "We do not divide tasks based on this limit as subjects often terminated their search in less than 15 minutes.",
                "In this section we use data gathered from interaction logs and subject opinions to investigate the extent to which RF was used and the extent to which it appeared to benefit our experimental subjects at different stages in their search 3.3.1 Feedback The interaction logs for all searches on the Explicit RF and Implicit RF were analysed and each search is divided up into nine equal length time slices.",
                "This number of slices gave us an equal number per stage and was a sufficient level of granularity to identify trends in the results.",
                "Slices 1 - 3 correspond to the start of the search, 4 - 6 to the middle of the search and 7 - 9 to the end.",
                "In Figure 2 we plot the measure of precision described in Section 3.1.1 (i.e., the proportion of all possible representations that were provided as RF) at each of the 22 IRF: U(24) = 403, p = .009, ERF: U(24) = 396, p = .013 nine slices, per search task, averaged across all subjects; this allows us to see how the provision of RF was distributed during a search.",
                "The total amount of feedback for a single RF method/task complexity pairing across all nine slices corresponds to the value recorded in the first row of Table 2 (e.g., the sum of the RF for IRF/HC across all nine slices of Figure 2 is 21.50%).",
                "To simplify the statistical analysis and comparison we use the grouping of start, middle and end. 0 1 2 3 4 5 6 7 8 9 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 Slice Searchprecision(%oftotalrepsprovidedasRF) Explicit RF/HC Explicit RF/MC Explicit RF/LC Implicit RF/HC Implicit RF/MC Implicit RF/LC Figure 2.",
                "Distribution of RF provision per search task.",
                "Figure 2 appears to show the existence of a relationship between the stage in the search and the amount of relevance information provided to the different types of feedback algorithm.",
                "These are essentially differences in the way users are assessing documents.",
                "In the case of ERF subjects provide explicit relevance assessments throughout most of the search, but there is generally a steep increase in the end phase towards the completion of the search23 .",
                "When using the IRF system, the data indicates that at the start of the search subjects are providing little relevance information24 , which corresponds to interacting with few document representations.",
                "At this stage the subjects are perhaps concentrating more on reading the retrieved results.",
                "Implicit relevance information is generally offered extensively in the middle of the search as they interact with results and it then tails off towards the end of the search.",
                "This would appear to correspond to stages of initial exploration, detailed analysis of document representations and storage and presentation of findings.",
                "Figure 2 also shows the proportion of feedback for tasks of different complexity.",
                "The results appear to show a difference25 in how IRF is used that relates to the complexity of the search task.",
                "More specifically, as complexity increases it appears as though subjects take longer to reach their most interactive point.",
                "This suggests that task complexity affects how IRF is distributed during the search and that they may be spending more time initially interpreting search results for more complex tasks. 23 IRF: all Z ≥ 1.87, p ≤ .031, ERF: start vs. end Z = 2.58, p = .005 (Dunns post-hoc tests). 24 Although increasing toward the end of the start stage. 25 Although not statistically significant; χ2 (2) = 3.54, p = .17 (Friedman Rank Sum Test) 3.3.2 Terms The terms recommended by the system are chosen based on the frequency of their occurrence in the relevant items.",
                "That is, nonstopword, non-query terms occurring frequently in search results regarded as relevant are likely to be recommended to the searcher for query modification.",
                "Since there is a direct association between the RF and the terms selected we use the number of terms accepted by searchers at different points in the search as an indication of how effective the RF has been up until the current point in the search.",
                "In this section we analysed the average number of terms from the top six terms recommended by Explicit RF and Implicit RF over the course of a search.",
                "The average proportion of the top six recommended terms that were accepted at each stage are shown in Table 8; each cell contains data from all 48 subjects.",
                "Table 8.",
                "Term Acceptance (proportion of top six terms).",
                "Explicit RF Implicit RFProportion of terms start middle end start middle end Accepted 66.87 66.98 67.34 61.85 68.54 73.22 The results show an apparent association between the stage in the search and the number of feedback terms subjects accept.",
                "Search stage affects term acceptance in IRF but not in ERF26 .",
                "The further into a search a searcher progresses, the more likely they are to accept terms recommended via IRF (significantly more than ERF27 ).",
                "A correlation analysis between the proportion of terms accepted at each search stage and cumulative RF (i.e., the sum of all precision at each slice in Figure 2 up to and including the end of the search stage) suggests that in both types of RF the quality of system terms improves as more RF is provided28 . 3.3.3 Summary The results from this section indicate that the location in a search affects the amount of feedback given by the user to the system, and hence the amount of information that the RF mechanism has to decide which terms to offer the user.",
                "Further, trends in the data suggest that the complexity of the task affects how subjects provide IRF and the proportion of system terms accepted. 4.",
                "DISCUSSION AND IMPLICATIONS In this section we discuss the implications of the findings presented in the previous section for each research question. 4.1 Search Task The results of our study showed that ERF was preferred for less complex tasks and IRF for more complex tasks.",
                "From observations and subject comments we perceived that when using ERF systems subjects generally forgot to provide the feedback but also employed different criteria during the ERF process (i.e., they were assessing relevance rather than expressing an interest).",
                "When the search was more complex subjects rarely found results they regarded as completely relevant.",
                "Therefore they struggled to find relevant 26 ERF: χ2 (2) = 2.22, p = .33; IRF: χ2 (2) = 7.73, p = .021 (Friedman Rank Sum Tests); IRF: all pair-wise comparisons significant at Z ≥ 1.77, all p ≤ .038 (Dunns post-hoc tests) 27 all T(48) ≥ 786, all p ≤ .002, (Wilcoxon Signed-Rank Test) 28 IRF: r = .712, p < .001, ERF: r = .695, p = .001 (Pearson Correlation Coefficient) information and were unable to communicate RF to the search system.",
                "In these situations subjects appeared to prefer IRF as they do not need to make a relevance decision to obtain the benefits of RF, i.e., term suggestions, whereas in ERF they do.",
                "The association between RF method and task complexity has implications for the design of user studies of RF systems and the RF systems themselves.",
                "It implies that in the design of user studies involving ERF or IRF systems care should be taken to include tasks of varying complexities, to avoid task bias.",
                "Also, in the design of search systems it implies that since different types of RF may be appropriate for different task complexities then a system that could automatically detect complexity could use both ERF and IRF simultaneously to benefit the searcher.",
                "For example, on the IRF system we noticed that as task complexity falls search behaviour shifts from results interface to retrieved documents.",
                "Monitoring such interaction across a number of studies may lead to a set of criteria that could help IR systems automatically detect task complexity and tailor support to suit. 4.2 Search Experience We analysed the affect of search experience on the utility of IRF.",
                "Our analysis revealed a general preference across all subjects for IRF over ERF.",
                "That is, the average ratings assigned to IRF were generally more positive than those assigned to ERF.",
                "However, IRF was generally liked by both subject groups (perhaps because it removed the burden of providing relevance information) and ERF was generally preferred by experienced subjects more than inexperienced subjects (perhaps because it allowed them to specify which results were used by the system when generating term recommendations).",
                "All subjects felt more in control with ERF than IRF, but for inexperienced subjects this did not appear to affect their overall preferences29 .",
                "These subjects may understand the RF process less, but may be more willing to sacrifice control over feedback in favour of IRF, a process that they perceive more positively. 4.3 Search Stage We also analysed the effects of search stage on the use and usefulness of IRF.",
                "Through analysis of this nature we can build a more complete picture of how searchers used RF and how this varies based on the RF method.",
                "The results suggest that IRF is used more in the middle of the search than at the beginning or end, whereas ERF is used more towards the end.",
                "The results also show the effects of task complexity on the IRF process and how rapidly subjects reach their most interactive point.",
                "Without an analysis of this type it would not have been possible to establish the existence of such patterns of behaviour.",
                "The findings suggest that searchers interact differently for IRF and ERF.",
                "Since ERF is not traditionally used until toward the end of the search it may be possible to incorporate both IRF and ERF into the same IR system, with IRF being used to gather evidence until subjects decide to use ERF.",
                "The development of such a system represents part of our ongoing work in this area. 5.",
                "CONCLUSIONS In this paper we have presented an investigation of Implicit Relevance Feedback (IRF).",
                "We aimed to answer three research questions about factors that may affect the provision and usefulness of IRF.",
                "These factors were search task complexity, the subjects search experience and the stage in the search.",
                "Our overall conclusion was that all factors 29 This may also be true for experienced subjects, but the data we have is insufficient to draw this conclusion. appear to have some effect on the use and effectiveness of IRF, although the interaction effects between factors are not statistically significant.",
                "Our conclusions per each research question are: (i) IRF is generally more useful for complex search tasks, where searchers want to focus on the search task and get new ideas for their search from the system, (ii) IRF is preferred to ERF overall and generally preferred by inexperienced subjects wanting to reduce the burden of providing RF, and (iii) within a single search session IRF is affected by temporal location in a search (i.e., it is used in the middle, not the beginning or end) and task complexity.",
                "Studies of this nature are important to establish the circumstances where a promising technique such as IRF are useful and those when it is not.",
                "It is only after such studies have been run and analysed in this way can we develop an understanding of IRF that allow it to be successfully implemented in operational IR systems. 6.",
                "REFERENCES [1] Bell, D.J. and Ruthven, I. (2004).",
                "Searchers assessments of task complexity for web searching.",
                "Proceedings of the 26th European Conference on Information Retrieval, 57-71. [2] Borlund, P. (2000).",
                "Experimental components for the evaluation of interactive information retrieval systems.",
                "Journal of Documentation. 56(1): 71-90. [3] Brajnik, G., Mizzaro, S., Tasso, C., and Venuti, F. (2002).",
                "Strategic help for user interfaces for information retrieval.",
                "Journal of the American Society for Information Science and Technology. 53(5): 343-358. [4] Busha, C.H. and Harter, S.P., (1980).",
                "Research methods in librarianship: Techniques and interpretation.",
                "Library and information science series.",
                "New York: Academic Press. [5] Campbell, I. and Van Rijsbergen, C.J. (1996).",
                "The ostensive model of developing information needs.",
                "Proceedings of the 3rd International Conference on Conceptions of Library and Information Science, 251-268. [6] Harman, D., (1992).",
                "Relevance feedback and other query modification techniques.",
                "In Information retrieval: Data structures and algorithms.",
                "New York: Prentice-Hall. [7] Kelly, D. and Teevan, J. (2003).",
                "Implicit feedback for inferring user preference.",
                "SIGIR Forum. 37(2): 18-28. [8] Koenemann, J. and Belkin, N.J. (1996).",
                "A case for interaction: A study of interactive information retrieval behavior and effectiveness.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 205-212. [9] Meddis, R., (1984).",
                "Statistics using ranks: A unified approach.",
                "Oxford: Basil Blackwell, 303-308. [10] Morita, M. and Shinoda, Y. (1994).",
                "Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the 17th Annual ACM SIGIR Conference on Research and Development in Information Retrieval, 272-281. [11] Salton, G. and Buckley, C. (1990).",
                "Improving retrieval performance by relevance feedback.",
                "Journal of the American Society for Information Science. 41(4): 288-297. [12] Siegel, S. and Castellan, N.J. (1988).",
                "Nonparametric statistics for the behavioural sciences. 2nd ed.",
                "Singapore: McGraw-Hill. [13] White, R.W. (2004).",
                "Implicit feedback for interactive information retrieval.",
                "Unpublished Doctoral Dissertation, University of Glasgow, Glasgow, United Kingdom. [14] White, R.W., Jose, J.M. and Ruthven, I. (2005).",
                "An implicit feedback approach for interactive information retrieval, Information Processing and Management, in press. [15] White, R.W., Jose, J.M., Ruthven, I. and Van Rijsbergen, C.J. (2004).",
                "A simulated study of implicit feedback models.",
                "Proceedings of the 26th European Conference on Information Retrieval, 311-326. [16] Zellweger, P.T., Regli, S.H., Mackinlay, J.D., and Chang, B.-W. (2000).",
                "The impact of fluid documents on reading and browsing: An observational study.",
                "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems, 249-256.",
                "Appendix B. Checkboxes to mark relevant document titles in the Explicit RF system.",
                "Appendix A. Interface to Implicit RF system. 1.",
                "Top-Ranking Sentence 2.",
                "Title 3.",
                "Summary 4.",
                "Summary Sentence 5.",
                "Sentence in Context 2 3 4 5 1"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Medida de RF implícita explícita HC MC LC HC MC LC \"Comentarios de proporción\" 2.14 2.39 2.65 21.50 19.36 15.32 Documentos Visilados 10.63 10.43 10.81 10.84 12.19 14.81 Para IRF hay un patrón claro: a medida que la complejidad aumenta los sujetos vistas de pocos documentos más vistos para más representaciones para las representaciones para más representaciones paracada documento.retroalimentación de proporción"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}