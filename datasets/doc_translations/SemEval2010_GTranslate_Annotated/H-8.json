{
    "id": "H-8",
    "original_text": "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort. Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems. Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1. INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task. She has built a system to perform the task and wants to evaluate it. Since the task is new, it is unlikely that there are any extant relevance judgments. She does not have the time or resources to judge every document, or even every retrieved document. She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions. But what happens when she develops a new system and needs to evaluate it? Or another research group decides to implement a system to perform the task? Can they reliably reuse the original judgments? Can they evaluate without more relevance judgments? Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them. The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged. This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21]. This solution is not adequate for our hypothetical researcher. The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time. As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others. As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems. Returning to our hypothetical resesarcher, can she reuse her relevance judgments? First we must formally define what it means to be reusable. In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems. While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one. We need a more careful definition of reusability. Specifically, the question of reusability is not how accurately we can evaluate new systems. A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents. The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence. Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence. Any set of judgments, no matter how small, becomes reusable to some degree. Small, reusable test collections could have a huge impact on information retrieval research. Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics. The amount of data available to researchers would grow exponentially over time. 2. ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation. By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents. Our evaluation should be robust to missing judgments. In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8]. This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure. Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence. We therefore see confidence as a probability estimate. One of the questions we must ask about a probability estimate is what it means. What does it mean to have 75% confidence that system A is better than system B? As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change. If this is what it means, we can trust the confidence estimates. But do we know it has that meaning? Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant. This assumption is almost certainly not realistic in most IR applications. As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted. Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall). It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i. Let Xi be a random variable indicating the relevance of document i. If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj . Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}. Using aij instead of 1/i allows us to number the documents arbitrarily. To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2. Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1. Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation. We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents. This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value. All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1). Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7. We can then compute e.g. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056. Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2. As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking. Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero. Since topics are independent, we can easily extend this to mean average precision (MAP). MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ). Let Z be the set of all pairs of ranked results for a common set of topics. Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence. Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm . For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document. If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments. If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate. The assumptions they are based on are the probabilities of relevance pi. We need these to be realistic. We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions. This is known as the principle of maximum entropy [13]. The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i). This has found a wide array of uses in computer science and information retrieval. The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form. The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence. Theorem 1. If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now). We forgo the proof for the time being, but it is quite simple. This says that the better the estimates of relevance, the more accurate the evaluation. The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents. The theorem and its proof say nothing whatsoever about the evaluation metric. The probability estimates are entirely indepedent of the measure we are interested in. This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc. Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1. If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate. The task therefore becomes the imputation of the missing values of relevance. The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3. PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified. One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance. A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on. If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation. This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking. Aslam et al. [3] previously identified a connection between evaluation and metasearch. Our problem has two key differences: 1. We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2. We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment. In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1). The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik). The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi). As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5]. Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression. Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters. We include a beta prior for p(λj) with parameters α, β. This can be seen as a type of smoothing to account for the fact that the training data is highly biased. This model has the advantage of including the statistical dependence between the experts. A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10]. A similar maximumentropy-motivated approach has been used for expert aggregation [15]. Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts. Where do the qij s come from? Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics. A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query. We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence. Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well. We could use a hierarchical model [12], but that will not generalize to unseen topics. Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic. Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document. We need to convert these to probabilities. A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance. The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference. Let q∗ ij be expert js self-reported probability that document i is relevant. Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant). The pairwise preference model can handle these two requirements easily, so we will use it. Let θrj (i) be the relevance coefficient of the document at rank rj(i). We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively. Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents. After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) . We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0. Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD. Therefore we only have to solve this once for each topic. The above model gives topic-independent probabilities for each document. But suppose an expert who reports 90% probability is only right 50% of the time. Its opinion should be discounted based on its observed performance. Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point. Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) . Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert. Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance. The calibrated probabilities are plugged into model (2) to find the document probabilities. Figure 1: Conceptual diagram of our aggregation model. Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2. The first step is to obtain q∗ ij. Next is calibration to true performance to find qij . Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output. A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic). This gives us q∗ ij, expert js self-reported probability of the relevance of document i. This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system). This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i. This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities. This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij . This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents. Although the model appears rather complex, it is really just three successive applications of logistic regression. As such, it can be implemented in a statistical programming language such as R in a few lines of code. The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line. Our code is available at http://ciir.cs.umass.edu/~carteret/. 4. EXPERIMENTS Three hypotheses are under consideration. The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool. The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8]. These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8. Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4). Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14. These are the tracks that have replaced the ad-hoc track since its end in 1999. Statistics are shown in Table 1. We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing. We use the qrels files assembled by NIST as truth. The number of relevance judgments made and relevant documents found for each track are listed in Table 1. For computational reasons, we truncate ranked lists at 100 documents. There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming. Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments. The baseline is a variation of TREC pooling that we will call incremental pooling (IP). This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged. It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant. The second algorithm is that presented in Carterette et al. [8] (Algorithm 1). Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists. For this approach pi = 0.5 for all i; there is no estimation of probabilities. We will call this MTC for minimal test collection. The third algorithm augments MTC with updated estimates of probabilities of relevance. We will call this RTC for robust test collection. It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3. RTC has smoothing (prior distribution) parameters that must be set. We trained using the ad-hoc 95 set. We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation. Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance. For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems. For each experimental trial: 1. Pick a random subset of k runs. 2. From those k, pick an initial c < k to evaluate. 3. Run RTC to 95% confidence on the initial c. 4. Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5. Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs. We do the same for MTC, but omit step 4. Note that after evaluating the first c systems, we make no additional relevance judgments. To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems. We will then generalize to a set of k = 10 (of which those two are a subset). As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence. One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin. We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher. Since summary statistics are useful, we devised the following metric. Suppose we are a bookmaker taking bets on whether ΔMAP < 0. We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) . Suppose a bettor wagers $1 on ΔMAP ≥ 0. If it turns out that ΔMAP < 0, we win the dollar. Otherwise, we pay out O. If our confidence estimates are perfectly accurate, we break even. If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence. Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary. However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good. The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0). The summary statistic is W, the mean of Wi. Note that as Pi increases, we lose more for being wrong. This is as it should be: the penalty should be great for missing the high probability predictions. However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100. For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems. The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand. The distribution is therefore highly skewed, and the mean strongly affected by those outliers. Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments. Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study. It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped. As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability. We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms. Using the same sets of systems allows the use of paired tests. As we stated above, we are more interested in the median number of judgments than the mean. A test for difference in median is the Wilcoxon sign rank test. We can also use a paired t-test to test for a difference in mean. For rank correlation, we can use a paired t-test to test for a difference in τ. 5. RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2. With MTC and uniform probabilities of relevance, the results are far from robust. We cannot reuse the relevance judgments with much confidence. But with RTC, the results are very robust. There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy. Mean Wi shows that RTC is much closer to 0 than MTC. The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates. The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems. Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems. More detailed results for both algorithms are shown in Figure 2. The solid line is the ideal result that would give W = 0. RTC is on or above this line at all points until confidence reaches about 0.97. After that there is a slight dip in accuracy which we discuss below. Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC. Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets. RTC is much more robust than MTC. W is defined in Section 4.4; closer to 0 is better. Median judged is the number of judgments to reach 95% confidence on the first two systems. Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC. The solid line is the perfect result that would give W = 0; performance should be on or above this line. Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7. This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7]. Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic. The median required by RTC is 235, about 4.7 per topic. Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001). For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair. The difference in means is much greater. MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.) This difference is significant by a paired t-test (p < 0.0001). Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic). Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9. This shows that even tiny collections can be reusable. For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9. Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates. The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC. This difference is significant by a paired t-test (p < 0.0001). Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments. It is more important that we estimate confidence in each pairwise comparison correctly. We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant). We calculated the τ correlation to the true ranking. The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC. Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP. Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable. We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%. In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction. Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences. Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other. Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs. Table 3 shows confidence vs. accuracy results for each of these three groups. Interestingly, performance is worst when comparing one of the original runs to one of the additional runs. This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP. Nevertheless, performance is quite good on all three subsets. Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common. If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments. A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2]. We calculated this for all pairs, then looked at performance on pairs with low similarity. Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs. RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%). RTC is robust in all three cases. Table 4. Performance is in fact very robust even when similarity is low. When the two runs share very few documents in common, W is actually positive. MTC and IP both performed quite poorly in these cases. When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC. By Data Set: All the previous results have only been on the ad-hoc collections. We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies. The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems. The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6. CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments. Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation. The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons. In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets. The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems. As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task. We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing. It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18]. The model we presented in Section 3 is by no means the only possibility for creating a robust test collection. A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed). In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents. This is an obvious area for future exploration. Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it. In the meantime, capping confidence estimates at 95% is a hack that solves the problem. We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems. Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7. REFERENCES [1] J. Aslam and M. Montague. Models for Metasearch. In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell. On the effectiveness of evaluating retrieval systems in the absence of relevance judgments. In Proceedings of SIGIR, pages 361-362, 2003. [3] J. A. Aslam, V. Pavlu, and R. Savell. A unified model for metasearch, pooling, and system evaluation. In Proceedings of CIKM, pages 484-491, 2003. [4] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower. An easy derivation of logistic regression from the bayesian and maximum entropy perspective. In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan. Research methodology in studies of assessor effort for retrieval evaluation. In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman. Minimal test collections for retrieval evaluation. In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova. Learning a ranking from pairwise preferences. In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler. Unanimity and compromise among probability forecasters. Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke. Efficient Construction of Large Test Collections. In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian Data Analysis. Chapman & Hall/CRC, 2004. [13] E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge University Press, 2003. [14] R. Manmatha and H. Sever. A Formal Approach to Score Normalization for Metasearch. In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily. Maximum entropy aggregation of expert predictions. Management Science, 42(10):1420-1436, October 1996. [16] J. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho. Forming test collections with no system pooling. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff. Dynamic test collections: measuring search effectiveness on the live web. In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen. Information Retrieval Test Collections. Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005. [21] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In Proceedings of SIGIR, pages 307-314, 1998.",
    "original_translation": "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro para la información inteligente Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu Resumen Los métodos de bajo costo para adquirir juicios de relevancia pueden ser un boon para los investigadores que necesitan evaluarNuevas tareas o temas de recuperación, pero no tienen los recursos para hacer miles de juicios. Si bien estos juicios son muy útiles para una evaluación única, no está claro que se pueda confiar en ellos cuando se reutilizan para evaluar nuevos sistemas. En este trabajo, definimos formalmente lo que significa que los juicios sean reutilizables: la confianza en una evaluación de nuevos sistemas puede evaluarse con precisión a partir de un conjunto existente de juicios de relevancia. Luego presentamos un método para aumentar un conjunto de juicios de relevancia con estimaciones de relevancia que no requieren un esfuerzo de evaluador adicional. El uso de este método prácticamente garantiza la reutilización: con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas. Incluso los conjuntos más pequeños de juicios pueden ser útiles para la evaluación de nuevos sistemas. Categorías y descriptores de sujetos: H.3 Almacenamiento y recuperación de información;H.3.4 Sistemas y software: Evaluación de rendimiento Términos generales: experimentación, medición, confiabilidad 1. Introducción Considere un investigador de recuperación de información que ha inventado una nueva tarea de recuperación. Ella ha creado un sistema para realizar la tarea y quiere evaluarla. Dado que la tarea es nueva, es poco probable que haya juicios de relevancia existentes. Ella no tiene el tiempo o los recursos para juzgar cada documento, o incluso cada documento recuperado. Ella solo puede juzgar los documentos que parecen ser los más informativos y detenerse cuando tiene un grado razonable de confianza en sus conclusiones. Pero, ¿qué sucede cuando desarrolla un nuevo sistema y necesita evaluarlo? ¿O otro grupo de investigación decide implementar un sistema para realizar la tarea? ¿Pueden reutilizar de manera confiable los juicios originales? ¿Pueden evaluar sin más juicios de relevancia? La evaluación es un aspecto importante de la investigación de recuperación de información, pero es solo un problema semi-resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento;Simplemente hay demasiados de ellos. La solución utilizada por NIST en TREC (Conferencia de recuperación de texto) es el método de agrupación [19, 20]: todos los sistemas competitivos contribuyen con n documentos a un grupo, y cada documento en ese grupo se juzga. Este método crea grandes conjuntos de juicios que son reutilizables para capacitar o evaluar nuevos sistemas que no contribuyeron al grupo [21]. Esta solución no es adecuada para nuestro investigador hipotético. El método de agrupación ofrece miles de juicios de relevancia, pero requiere muchas horas de tiempo del anotador (pagado). Como resultado, ha habido una serie de documentos recientes sobre la reducción del esfuerzo del anotador en la producción de colecciones de pruebas: Cormack et al.[11], Zobel [21], Sanderson y Joho [17], Carterette et al.[8], y Aslam et al.[4], entre otros. Como veremos, los juicios que producen estos métodos pueden sesgar significativamente la evaluación de un nuevo conjunto de sistemas. Volviendo a nuestro resasquero hipotético, ¿puede reutilizar sus juicios de relevancia? Primero debemos definir formalmente lo que significa ser reutilizable. En trabajos anteriores, la reutilización se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia para evaluar los sistemas invisibles. Si bien podemos decir que fue correcto el 75% del tiempo, o que tenía una correlación de rango de 0.8, estos números no tienen ningún poder predictivo: no nos dicen qué sistemas probablemente estén mal o qué tan seguros debemosestar en cualquiera. Necesitamos una definición más cuidadosa de reutilización. Específicamente, la cuestión de la reutilización no es cuán exactamente podemos evaluar nuevos sistemas. Un adversario malicioso siempre puede producir una nueva lista clasificada que no ha recuperado ninguno de los documentos juzgados. La verdadera pregunta es cuánta confianza tenemos en nuestras evaluaciones y, lo que es más importante, si podemos confiar en nuestras estimaciones de confianza. Incluso si la confianza no es alta, siempre que podamos confiar en ella, podemos identificar qué sistemas necesitan más juicios para aumentar la confianza. Cualquier conjunto de juicios, no importa cuán pequeño sea, se vuelve reutilizable hasta cierto punto. Las pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de recuperación de información. Los grupos de investigación podrían compartir los juicios de relevancia que han realizado internamente para estudios piloto, nuevas tareas o nuevos temas. La cantidad de datos disponibles para los investigadores crecería exponencialmente con el tiempo.2. Evaluación robusta anterior damos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación. Con eso queremos decir que si hemos realizado algunos juicios de relevancia y, por ejemplo, el 75% de la confianza de que el Sistema A es mejor que el Sistema B, nos gustaría que no haya más del 25% de nuestra evaluación de la calidad relativa deLos sistemas cambiarán a medida que continuemos juzgando documentos. Nuestra evaluación debe ser robusta para los juicios faltantes. En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de evaluación calculada para dos sistemas sea inferior a cero [8]. Esta noción de confianza se define en el contexto de una tarea de evaluación particular que llamamos evaluación comparativa: determinar el signo de la diferencia en una medida de evaluación. Se podrían definir otras tareas de evaluación;Estimación de la magnitud de la diferencia o los valores de las medidas en sí mismas son ejemplos que implican diferentes nociones de confianza. Por lo tanto, vemos la confianza como una estimación de probabilidad. Una de las preguntas que debemos hacer sobre una estimación de probabilidad es lo que significa. ¿Qué significa tener un 75% de confianza en que el sistema A es mejor que el sistema B? Como se describió anteriormente, queremos que signifique que si continuamos juzgando documentos, solo habrá un 25% de posibilidades de que nuestra evaluación cambie. Si esto es lo que significa, podemos confiar en las estimaciones de confianza. ¿Pero sabemos que tiene ese significado? Nuestro cálculo de la confianza se basó en una suposición sobre la probabilidad de relevancia de los documentos injudicados, específicamente que cada documento injustificado era igualmente relevante o no relevante. Esta suposición es casi seguro que no es realista en la mayoría de las aplicaciones IR. Como resultado, es esta suposición la que determina si las estimaciones de confianza pueden confiar en EB. Antes de elaborar esto, definimos formalmente la confianza.2.1 Estimación de confianza Precisión promedio (AP) es una métrica de evaluación estándar que captura tanto la capacidad de un sistema para clasificar los documentos relevantes (precisión) como su capacidad para recuperar documentos relevantes (retiro). Por lo general, se escribe como la precisión media en las filas de documentos relevantes: AP = 1 | R |i∈R prec@r (i) donde r es el conjunto de documentos relevantes y r (i) es el rango de documento i. Sea Xi una variable aleatoria que indica la relevancia del documento i. Si los documentos se ordenan por rango, podemos expresar precisión como prec@i = 1/i i j = 1 xj. La precisión promedio se convierte en la ecuación cuadrática AP = 1 xi n i = 1 xi/ i i j = 1 xj = 1 xi n i = 1 j≥i aijxixj donde aij = 1/ max {r (i), r (j)}. El uso de AIJ en lugar de 1/I nos permite numerar los documentos arbitrariamente. Para ver por qué esto es cierto, considere un ejemplo de juguete: una lista de 3 documentos con documentos relevantes B, C en los rangos 1 y 3 y el documento no relevante A en el rango 2. La precisión promedio será 1 2 (1 1 x2 b + 1 2 xbxa + 1 3 xbxc + 1 2 x2 a + 1 3 xaxc + 1 3 x2 c) = 1 2 1 + 2 3 porque xa = 0, xb = 1, xc = 1 1. Aunque el orden B, A, C es diferente del etiquetado A, B, C, no afecta el cálculo. Ahora podemos ver que la precisión promedio en sí es una variable aleatoria con una distribución sobre todas las tareas posibles de relevancia para todos los documentos. Esta variable aleatoria tiene una expectativa, una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado. Todos estos dependen de la probabilidad de que el documento I sea relevante: PI = P (xi = 1). Supongamos que en nuestro ejemplo anterior no conocemos los juicios de relevancia, pero creemos PA = 0.4, Pb = 0.8, PC = 0.7. Entonces podemos calcular, p. P (AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, o P (AP = 1 2) = 0.2 · 0.4 · 0.7 = 0.056. Sumando sobre todas las posibilidades, podemos calcular la expectativa y la varianza: E [AP] ≈ 1 Pi Aiipi + J> I aij pipj v ar [ap] ≈ 1 (pi) 2 n i a2 iipiqi + j> i a2 ijpipj (1 − pipj +) + i = J 2aiiaijpipj (1 - pi) + k> j = i 2aijaikpipjpk (1 - pi) aparguear asintóticamente a una distribución normal con expectativa y varianza como se define anteriormente.1 Para nuestra tarea de evaluación comparativa, estamos interesados en el signode la diferencia en dos precisiones promedio: ΔAP = AP1 - AP2. Como mostramos en nuestro trabajo anterior, ΔAP tiene una forma cerrada cuando los documentos se ordenan arbitrariamente: ΔAP = 1 xi n i = 1 j≥i cij xixj cij = aij - bij donde bijJ se define análoga a AIJ para la segunda clasificación. Dado que AP es normal, ΔAP también es normal, lo que significa que podemos usar la función de densidad acumulativa normal para determinar la confianza de que una diferencia en AP es inferior a cero. Dado que los temas son independientes, podemos extender fácilmente esto como una precisión promedio (MAP). El mapa también se distribuye normalmente;Su expectativa y varianza son: EMAP = 1 t t∈T e [apt] (1) vmap = 1 t2 t∈T v ar [apt] Δmap = MAP1 - MAP2 La confianza se puede estimar calculando la expectativa y la varianza y utilizando la normalidadfunción de densidad para encontrar p (Δmap <0).2.2 Confianza y robustez Al haber definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección con los juicios faltantes.1 Estas son realmente aproximaciones a la verdadera expectativa y varianza, pero el error es una O (N2 - N) insignificante. Sea Z el conjunto de todos los pares de resultados clasificados para un conjunto común de temas. Supongamos que tenemos un conjunto de juicios de relevancia M xm = {x1, x2, ..., xm} (usando X pequeña en lugar de capital X para distinguir entre documentos juzgados y sin juzgar);Estos son los juicios contra los cuales calculamos la confianza. Deje que Zα sea el subconjunto de pares en z para los cuales predecimos que Δmap = −1 con confianza α dada los juicios xm. Para que las estimaciones de confianza sean precisas, necesitamos al menos α · | Zα |de estos pares para tener Δmap = −1 después de haber juzgado cada documento. Si lo hacen, podemos confiar en las estimaciones de confianza;Nuestra evaluación será sólida para los juicios faltantes. Si nuestras estimaciones de confianza se basan en suposiciones poco realistas, no podemos esperar que sean precisas. Los supuestos en los que se basan son las probabilidades de relevancia PI. Necesitamos que estos sean realistas. Argumentamos que la mejor distribución posible de relevancia P (XI) es la que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hace suposiciones injustificadas. Esto se conoce como el principio de la entropía máxima [13]. La entropía de una variable aleatoria x con distribución p (x) se define como h (p) = - i p (x = i) log p (x = i). Esto ha encontrado una amplia gama de usos en informática y recuperación de información. La distribución máxima de entropía es la que maximiza a H. Esta distribución es única y tiene una forma exponencial. El siguiente teorema muestra la utilidad de una distribución de entropía máxima para relevancia al estimar la confianza. Teorema 1. Si p (xn | i, xm) = argmaxph (p), las estimaciones de confianza serán precisas.Cuando XM es el conjunto de juicios de relevancia definidos anteriormente, Xn es el conjunto completo de documentos del que deseamos estimar la relevancia, y yo es información sobre los documentos (no especificados a partir de ahora). Renunciamos a la prueba por el momento, pero es bastante simple. Esto dice que cuanto mejores serán las estimaciones de relevancia, más precisa será la evaluación. La tarea de crear una recopilación de pruebas reutilizables se convierte en la tarea de estimar la relevancia de los documentos injudgados. El teorema y su prueba no dicen nada sobre la métrica de evaluación. Las estimaciones de probabilidad son completamente independientes de la medida que nos interesa. Esto significa que las mismas estimaciones de probabilidad pueden decirnos sobre la precisión promedio, así como la precisión, el recuerdo, BPREF, etc. Además, podríamos suponer que la relevancia de los documentos I y J es independiente y logran el mismo resultado, que declaramos como corolario: Corolario 1. Si p (xi | i, xm) = argmaxph (p), las estimaciones de confianza serán precisas. Por lo tanto, la tarea se convierte en la imputación de los valores faltantes de relevancia. El teorema implica que cuanto más nos acercamos a la distribución de relevancia máxima de entropía, más cerca llegaremos a la robustez.3. Predecir la relevancia en nuestra declaración del Teorema 1, dejamos la naturaleza de la información que no especificé. Una de las ventajas de nuestras estimaciones de confianza es que admiten información de una amplia variedad de fuentes;Esencialmente, cualquier cosa que pueda modelarse puede usarse como información para predecir la relevancia. Una fuente natural de información son los sistemas de recuperación en sí mismos: cómo clasificaron los documentos juzgados, con qué frecuencia no pudieron clasificar los documentos relevantes, cómo se desempeñan entre los temas, y así sucesivamente. Si tratamos a cada sistema como un experto en recuperación de información que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en una agregación de opinión experta. Esto es similar al problema de MetaSearch o Fusion de datos en el que la tarea es tomar K sistemas de entrada y fusionarlos en una sola clasificación. Aslam et al.[3] Identificó previamente una conexión entre evaluación y metasearch. Nuestro problema tiene dos diferencias clave: 1. Necesitamos explícitamente probabilidades de relevancia que podamos conectarnos a la ecuación.1;Los algoritmos de metasearch no tienen tal requisito.2. Estamos acumulando juicios de relevancia a medida que avanzamos con la evaluación y podemos restablecer la relevancia dada cada nuevo juicio. A la luz de (1) arriba, presentamos un modelo probabilístico para la combinación de expertos.3.1 Un modelo para la agregación de opiniones expertas suponga que cada experto J proporciona una probabilidad de relevancia qij = pj (xi = 1). La información sobre la relevancia del documento I será el conjunto de k opiniones de expertos i = qi = (qi1, qi2, · · ·, qik). La distribución de probabilidad que deseamos encontrar es la que maximiza la entropía de Pi = P (xi = 1 | qi). Como resultado, encontrar el modelo de entropía máxima es equivalente a encontrar los parámetros que maximizan la probabilidad [5]. El soplador [6] muestra explícitamente que encontrar el modelo de entropía máxima para una variable binaria es equivalente a resolver una regresión logística. Entonces Pi = P (xi = 1 | qi) = exp k j = 1 λjqij 1 + exp k j = 1 λj qij (2) donde λ1, · · ·, λk son los parámetros de regresión. Incluimos una beta anterior para p (λj) con parámetros α, β. Esto puede verse como un tipo de suavizado para tener en cuenta el hecho de que los datos de capacitación son altamente sesgados. Este modelo tiene la ventaja de incluir la dependencia estadística entre los expertos. Clemen & Winkler demostró que un modelo de la misma forma era el mejor para agregar probabilidades de expertos [10]. Se ha utilizado un enfoque similar motivado por MaximumentRopy para la agregación de expertos [15]. Aslam y Montague [1] utilizaron un modelo similar para Metasearch, pero asumieron la independencia entre los expertos. ¿De dónde vienen los Qij s? El uso de puntajes crudos y no calibrados como predictores no funcionará porque las distribuciones de puntaje varían demasiado entre los temas. Un marcador de modelado de idiomas, por ejemplo, generalmente dará una puntuación mucho más alta al documento recuperado para una consulta corta que al documento recuperado para una consulta larga. Podríamos capacitar a un modelo de predicción separado para cada tema, pero eso no aprovecha toda la información que tenemos: es posible que solo tengamos un puñado de juicios para un tema, no lo suficiente como para capacitar a un modelo a cualquier confianza. Además, parece razonable suponer que si un experto hace buenas predicciones para un tema, también hará buenas predicciones para otros temas. Podríamos usar un modelo jerárquico [12], pero eso no se generalizará a temas invisibles. En cambio, calibraremos los puntajes de cada experto individualmente para que los puntajes se puedan comparar tanto dentro del tema como entre el tema. Por lo tanto, nuestro modelo tiene en cuenta no solo la dependencia entre los expertos, sino también la dependencia entre el rendimiento de los expertos en diferentes tareas (temas).3.2 Expertos de calibración Cada experto nos da una puntuación y un rango para cada documento. Necesitamos convertirlos en probabilidades. Un método como el utilizado por Manmatha et al.[14] podría usarse para convertir los puntajes en probabilidades de relevancia. El método de preferencia por pares de Carterette y Petkova [9] también podría usarse, intercalando la clasificación de un documento sobre otro como una expresión de preferencia. Deje que Q ∗ sea experto en la probabilidad autoinformada de que el documento I sea relevante. Intuitivamente parece claro que Q ∗ ij debería disminuir con el rango, y debería ser cero si el documento I no está unido (el experto no creía que fuera relevante). El modelo de preferencia por pares puede manejar estos dos requisitos fácilmente, por lo que lo usaremos. Sea θrj (i) el coeficiente de relevancia del documento en rango RJ (i). Queremos encontrar los θs que maximicen la función de probabilidad: ljt (θ) = rj (i) <rj (k) exp (θrj (i) - θrj (k)) 1 + exp (θrj (i) - θrj (k)) Nuevamente incluimos una beta anterior en p (θrj (i)) con parámetros | RT |+ 1 y | nt |+ 1, el tamaño de los conjuntos de documentos relevantes y no relevantes juzgados respectivamente. El uso de estos como parámetros anteriores asegura que las probabilidades resultantes se concentrarán en torno a la relación de documentos relevantes que se han descubierto para el Tema T.Esto significa que las estimaciones de probabilidad disminuyen por rango y son más altas para los temas que tienen documentos más relevantes. Después de encontrar el θ que maximiza la probabilidad, tenemos q ∗ ij = exp (θrj (i)) 1+exp (θrj (i)). Definimos θ∞ = −∞, de modo que la probabilidad de que un documento no lo tenga sea relevante es 0. Dado que Q ∗ IJ se basa en el rango en el que se recupera un documento en lugar de la identidad del documento en sí, las probabilidades son idénticas de experto a experto, p.Si Expert E pone el documento A en el rango 1, y el experto d pone el documento B en el rango 1, tendremos Q ∗ ae = q ∗ bd. Por lo tanto, solo tenemos que resolver esto una vez para cada tema. El modelo anterior ofrece probabilidades independientes del tema para cada documento. Pero suponga que un experto que informa que el 90% de probabilidad es solo el 50% del tiempo. Su opinión debe descartarse en función de su rendimiento observado. Específicamente, queremos aprender una función de calibración Qij = CJ (Q ∗ ij) que garantice que las probabilidades predichas se ajusten a la capacidad de los expertos para recuperar documentos relevantes dados los juicios que se han realizado hasta este punto. El método de calibración de Platts SVM [16] se ajusta a una función sigmoide entre Q ∗ ij y los juicios de relevancia para obtener qij = cj (q ∗ ij) = exp (aj +bjq ∗ ij) 1 +exp (aj +bj q ∗ ij). Dado que q ∗ ij es independiente del tema, solo necesitamos aprender una función de calibración para cada experto. Una vez que tenemos la función de calibración, se aplica para ajustar las predicciones de los expertos a su rendimiento real. Las probabilidades calibradas se conectan al modelo (2) para encontrar las probabilidades de documentos. Figura 1: Diagrama conceptual de nuestro modelo de agregación. Los expertos E1, E2 han clasificado los documentos A, B, C para el Tema T1 y los Documentos D, E, F para el Tema T2. El primer paso es obtener Q ∗ ij. Lo siguiente es la calibración al rendimiento verdadero para encontrar QIJ. Finalmente obtenemos Pi = P (xi = 1 | qi1, qi2), · · ·.3.3 Resumen del modelo Nuestro modelo tiene tres componentes que difieren por los datos que toman como entrada y lo que producen como salida. Se muestra un diagrama conceptual en la Figura 1. 1. Rangos → Probabilidades (por sistema por tema). Esto nos da q ∗ ij, la probabilidad autoinformada de expertos de la relevancia del documento i. Esto no está supervisado;No requiere datos etiquetados (aunque si tenemos algunos, los usamos para establecer parámetros anteriores).2. Probabilidades → Probabilidades calibradas (por sistema). Esto nos da qij = cj (q ∗ ij), probabilidad de experto js calibrada de la relevancia del documento i. Esto es semisupervisado;Tenemos juicios de relevancia en algunos rangos que usamos para imputar la probabilidad de relevancia en otros rangos.3. Probabilidades calibradas → Probabilidades de documentos. Esto nos da pi = p (xi = 1 | qi), la probabilidad de relevancia del documento que dio probabilidades de expertos calibradas qij. Esto se supervisa;Aprendemos coeficientes de un conjunto de documentos juzgados y los usamos para estimar la relevancia de los documentos injustificados. Aunque el modelo parece bastante complejo, en realidad son solo tres aplicaciones sucesivas de regresión logística. Como tal, se puede implementar en un lenguaje de programación estadística como R en algunas líneas de código. El uso de antecedentes beta (conjugados) asegura que no sean necesarios métodos computacionales costosos como MCMC [12], por lo que el modelo se capacita y se aplica lo suficientemente rápido como para usarse en línea. Nuestro código está disponible en http://ciir.cs.umass.edu/~carteret/.4. Experimentos Se están considerando tres hipótesis. El primero, y lo más importante, es que el uso de nuestro modelo de agregación de expertos para predecir la relevancia produce colecciones de pruebas que son lo suficientemente robustas como para ser reutilizables;Es decir, podemos confiar en las estimaciones de confianza cuando evaluamos sistemas que no contribuyeron con ningún juicio al grupo. Las otras dos hipótesis se relacionan con la mejora que vemos mediante el uso de mejores estimaciones de relevancia que en nuestro trabajo anterior [8]. Estos son que (a) requiere menos rastreo de relevancia no.Temas no.Corre no.juzgado no.rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web04 225 74 88,566 1,763 robusto 05 50 74 37,798 6,561 Terabyte 05 50 58 45,291 10,407 Tabla 1: Número de temas, número de ejecuciones, número de documentos juzgados y el número encontrado relevante para cada uno de nuestros conjuntos de datos.Los juicios para alcanzar la confianza del 95% y (b) la precisión de las predicciones es más alta que si simplemente asumiéramos PI = 0.5 para todos los documentos no juzgados.4.1 Datos obtuvimos ejecuciones ad-hoc completas enviadas a TRECS 3 a 8. Cada ejecución se ubica en la mayoría de los 1000 documentos para 50 temas (49 temas para TREC 4). Además, obtuvimos todas las ejecuciones de la pista web de TREC 13, la pista robusta2 de TREC 14 y la pista de Terabyte (ad-hoc) de TREC 14. Estas son las pistas que han reemplazado la pista ad-hoc desde su final en 1999. Las estadísticas se muestran en la Tabla 1. Dejamos de lado el TREC 4 (AD-Hoc 95) establecido para el entrenamiento, TRECS 3 y 5-8 (AD-Hoc 94 y 96-99) para pruebas primarias, y los conjuntos restantes para pruebas adicionales. Usamos los archivos Qrels ensamblados por NIST como verdad. El número de juicios de relevancia realizados y documentos relevantes encontrados para cada pista se enumeran en la Tabla 1. Por razones computacionales, truncamos listas clasificadas en 100 documentos. No hay razón para que no pudiéramos profundizar, pero la varianza de cálculo es O (N3) y, por lo tanto, es muy de tiempo. Debido a la naturaleza de rango recíproco de AP, no perdemos mucha información truncando en el rango 100. 4.2 Algoritmos compararemos tres algoritmos para adquirir juicios de relevancia. La línea de base es una variación de la agrupación de TREC que llamaremos agrupación incremental (IP). Este algoritmo toma un número K como entrada y presenta los primeros K documentos en orden de rango (sin tener en cuenta el tema) a juzgar. No estimula la relevancia de los documentos innecesarios;Simplemente supone que cualquier documento insuficiente no es relevante. El segundo algoritmo es el presentado en Carterette et al.[8] (Algoritmo 1). Los documentos se seleccionan en función de lo interesantes que son para determinar si existe una diferencia en la precisión promedio media. Para este enfoque PI = 0.5 para todo i;No hay estimación de las probabilidades. Llamaremos a esto MTC para una colección de pruebas mínimas. El tercer algoritmo aumenta MTC con estimaciones actualizadas de probabilidades de relevancia. Llamaremos a este RTC para una colección de prueba robusta. Es idéntico al Algoritmo 1, excepto que cada décima iteración estimamos PI para todos los documentos injustificados que utilizo el modelo de agregación experta de la Sección 3. RTC tiene parámetros de suavizado (distribución previa) que deben establecerse. Entrenamos usando el set Ad-Hoc 95. Limitamos 2 robustos aquí significa una recuperación robusta;Esto es diferente de nuestro objetivo de una evaluación robusta. Algoritmo 1 (MTC) dada dos listas clasificadas y nivel de confianza α, predice el signo de ΔMAP.1: Pi ← 0.5 Para todos los documentos I 2: mientras que P (Δmap <0) <α do 3: Calcule el peso WI para todos los documentos inequívocos I (ver Carterette et al. [8] para detalles) 4: J ← Argmaxiwi 5:XJ ← 1 Si el documento J es relevante, 0 de lo contrario 6: PJ ← XJ 7: Fin mientras la búsqueda de antecedentes uniformes con una variación relativamente alta. Para la agregación de expertos, los parámetros anteriores son α = β = 1. 4.3 Diseño experimental primero, queremos saber si podemos aumentar un conjunto de juicios de relevancia con un conjunto de probabilidades de relevancia para reutilizar los juicios para evaluar un nuevo conjunto desistemas. Para cada ensayo experimental: 1. Elija un subconjunto aleatorio de K corridas.2. De esos k, elija una c <k inicial para evaluar.3. Ejecute RTC a un 95% de confianza en la c.4. Usando el modelo de la Sección 3, estime las probabilidades de relevancia para todos los documentos recuperados por todas las ejecuciones K.5. Calcule EMAP para todas las corridas K, y P (ΔMap <0) para todos los pares de corridas. Hacemos lo mismo para MTC, pero omitir el paso 4. Tenga en cuenta que después de evaluar los primeros sistemas C, no realizamos juicios de relevancia adicionales. Para poner a prueba nuestro método, seleccionamos C = 2: construiremos un conjunto de juicios para evaluar solo dos sistemas iniciales. Luego generalizaremos a un conjunto de k = 10 (de los cuales esos dos son un subconjunto). A medida que ejecutamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis.4.4 Evaluación experimental Recuerde que un conjunto de juicios es robusto si la precisión de las predicciones que hace es al menos su confianza estimada. Una forma de evaluar la robustez es a los pares de contenido por su confianza, luego calcular la precisión sobre todos los pares en cada contenedor. Nos gustaría que la precisión sea no menos que el puntaje de confianza más bajo en el contenedor, pero preferiblemente mayor. Dado que las estadísticas resumidas son útiles, ideamos la siguiente métrica. Supongamos que somos un corredor de apuestas que apuesta por si Δmap <0. Usamos RTC o MTC para establecer las probabilidades O = P (ΔMap <0) 1 - P (ΔMap <0). Suponga que un apostador apuesta $ 1 en Δmap ≥ 0. Si resulta que Δmap <0, ganamos el dólar. De lo contrario, pagamos a O. Si nuestras estimaciones de confianza son perfectamente precisas, alcanzamos el punto de equilibrio. Si la confianza es mayor que la precisión, perdemos dinero;Ganamos si la precisión es mayor que la confianza. Contraintuitivamente, el resultado más deseable es llegar a un punto de equilibrio: si perdemos dinero, no podemos confiar en las estimaciones de confianza, pero si ganamos dinero, hemos subestimado la confianza o juzgamos más documentos de los necesarios. Sin embargo, el costo de no poder confiar en las estimaciones de confianza es mayor que el costo de los juicios de relevancia adicional, por lo que trataremos los resultados positivos como buenos. La cantidad que ganamos en cada comparación por pares I es: wi = yi - (1 - yi) pi 1 - pi = yi - pi 1 - pi yi = 1 si Δmap <0 y 0 de lo contrario, y pi = p (Δmap <0). La estadística resumida es w, la media de WI. Tenga en cuenta que a medida que PI aumenta, perdemos más por estar equivocado. Esto es como debería ser: la penalización debe ser excelente por perder las predicciones de alta probabilidad. Sin embargo, dado que nuestras pérdidas crecen sin limitados a medida que se acercan a la certeza de las predicciones, Cap -Wi a los 100. Para nuestra hipótesis de que RTC requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar la confianza del 95% en el primer par de sistemas. La mediana es más interesante que la media: la mayoría de los pares requieren unos pocos cientos de juicios, pero algunos pares requieren varios miles. Por lo tanto, la distribución es muy sesgada y la media es fuertemente afectada por esos valores atípicos. Finalmente, para nuestra hipótesis de que RTC es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de los sistemas K por un pequeño conjunto de juicios y la clasificación verdadera utilizando el conjunto completo de juicios. Kendalls τ, un estadístico no paramétrico basado en swaps por pares entre dos listas, es una evaluación estándar para este tipo de estudio. Varía de −1 (perfectamente correlacionado) a 1 (rankings idéntico), con 0, lo que significa que la mitad de los pares se intercambian. Sin embargo, cuando mencionamos en la introducción, una medida de precisión como la correlación de rango no es una buena evaluación de la reutilización. Lo incluimos para completar.4.4.1 Pruebas de hipótesis que ejecutan múltiples ensayos permiten el uso de pruebas de hipótesis estadística para comparar algoritmos. El uso de los mismos conjuntos de sistemas permite el uso de pruebas emparejadas. Como dijimos anteriormente, estamos más interesados en el número medio de juicios que en la media. Una prueba de diferencia en la mediana es la prueba de rango de signo de Wilcoxon. También podemos usar una prueba t emparejada para probar una diferencia en la media. Para la correlación de rango, podemos usar una prueba t emparejada para probar una diferencia en τ.5. Resultados y análisis La comparación entre MTC y RTC se muestra en la Tabla 2. Con MTC y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos. No podemos reutilizar los juicios de relevancia con mucha confianza. Pero con RTC, los resultados son muy robustos. Hay una ligera caída en la precisión cuando la confianza supere 0.95;No obstante, las predicciones de confianza son confiables. WI medio muestra que RTC está mucho más cerca de 0 que MTC. La distribución de los puntajes de confianza muestra que se logra al menos el 80% de confianza más del 35% del tiempo, lo que indica que ninguno de los algoritmo está siendo demasiado conservador en sus estimaciones de confianza. Las estimaciones de confianza son bastante bajas en general;Esto se debe a que hemos construido una colección de pruebas a partir de solo dos sistemas iniciales. Recuerde de la Sección 1 que no podemos requerir (o incluso esperar) un nivel mínimo de confianza cuando generalizamos a nuevos sistemas. Los resultados más detallados para ambos algoritmos se muestran en la Figura 2. La línea continua es el resultado ideal que daría w = 0. RTC está en o por encima de esta línea en todos los puntos hasta que la confianza alcanza aproximadamente 0.97. Después de eso, hay una ligera caída en la precisión que discutimos a continuación. Tenga en cuenta que tanto el% de confianza de MTC RTC en la precisión del contenedor en la precisión del contenedor 0.5 - 0.6 33.7% 61.7% 28.6% 61.9% 0.6 - 0.7 18.1% 73.1% 20.1% 76.3% 0.7 - 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.99.4% 69.0% 12.1% 84.9% 0.9 - 0.95 7.3% 78.0% 6.6% 93.1% 0.95 - 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% w −5.34 −0.39 medios de 251 235 τ τ τ τ τ τ τ τ τ τ τ τ τ τ τ τ τ media τ °0.393 0.555 Tabla 2: Confía de que P (ΔMap <0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y RTC. Cada contenedor contiene más de 1,000 ensayos de los conjuntos ADHOC 3, 5-8. RTC es mucho más robusto que MTC. W se define en la Sección 4.4;Más cerca de 0 es mejor. La mediana de juicio es el número de juicios para alcanzar la confianza del 95% en los dos primeros sistemas. La media τ es la correlación de rango promedio para los 10 sistemas.0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 Precisión Confianza Breakevenven RTC MTC Figura 2: Confianza versus precisión de MTC y RTC. La línea continua es el resultado perfecto que daría w = 0;El rendimiento debe estar en o por encima de esta línea. Cada punto representa al menos 500 comparaciones por pares.Los algoritmos están muy por encima de la línea hasta la confianza 0.7. Esto se debe a que el rendimiento de línea de base en estos conjuntos de datos es alto;Es bastante fácil lograr una precisión del 75% haciendo muy poco trabajo [7]. Número de juicios: el número medio de juicios requeridos por MTC para alcanzar el 95% de confianza en los dos primeros sistemas es 251, un promedio de 5 por tema. La mediana requerida por RTC es 235, aproximadamente 4.7 por tema. Aunque los números están cerca, la mediana de RTCS es significativamente menor por una prueba de wilcoxon emparejada (P <0,0001). A modo de comparación, un grupo de profundidad 100 daría como resultado un mínimo de 5,000 juicios para cada par. La diferencia en las medias es mucho mayor. MTC requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema.(Recuerde que eso significa que están fuertemente sesgados por unos pocos pares que toman miles de juicios). Esta diferencia es significativa por una prueba t emparejada (P <0,0001). El diez por ciento de los conjuntos dieron como resultado 100 o menos juicios (menos de dos por tema). El rendimiento en estos es muy alto: W = 0.41 y una precisión del 99.7% cuando la confianza es al menos 0.9. Esto muestra que incluso pequeñas colecciones pueden ser reutilizables. Para el 50% de los sets con más de 235 juicios, la precisión es del 93% cuando la confianza es al menos 0.9. Correlación de rango: MTC y RTC clasifican los 10 sistemas por EMAP (ecuación (1)) calculados utilizando sus respectivas estimaciones de probabilidad. La correlación media de rango τ entre el mapa verdadero y el EMAP es 0.393 para MTC y 0.555 para RTC. Esta diferencia es significativa por una prueba t emparejada (P <0,0001). Tenga en cuenta que no esperamos que las correlaciones τ sean altas, ya que estamos clasificando los sistemas con tan pocos juicios de relevancia. Es más importante que estimemos la confianza en cada comparación por pares correctamente. Ejecutamos IP para el mismo número de juicios que MTC tomó para cada par, luego clasificamos los sistemas por mapa utilizando solo aquellos juicios (todos los documentos injustificados asumidos no relevantes). Calculamos la correlación τ con la clasificación verdadera. La correlación media τ es 0.398, que no es significativamente diferente de MTC, pero es significativamente menor que RTC. El uso de estimaciones uniformes de la probabilidad es indistinguible de la línea de base, mientras que la estimación de la relevancia por parte de la agregación experta aumenta el rendimiento mucho: casi el 40% sobre MTC e IP. Overecita: es posible sobrepacharse: si demasiados juicios provienen de los dos primeros sistemas, la varianza en ΔMAP se reduce y las estimaciones de confianza se vuelven poco confiables. Vimos esto en la Tabla 2 y la Figura 2, donde RTC exhibe una caída en la precisión cuando la confianza es de alrededor del 97%. De hecho, el número de juicios realizados antes de una predicción incorrecta es más del 50% mayor que el número realizado antes de una predicción correcta. El sobreajuste es difícil de cuantificar exactamente, porque hacer juicios de más relevancia no siempre lo causa: a niveles de confianza más altos, se realizan más juicios de relevancia, y como muestra la Tabla 2, la precisión es mayor en esas mayores confidencias. Obviamente, tener más juicios de relevancia debería aumentar tanto la confianza como la precisión;La diferencia parece ser cuando un sistema tiene mucho más juicios que el otro. Comparaciones por pares: nuestras comparaciones por pares se dividen en uno de los tres grupos: 1. Las dos corridas originales a partir de las cuales se adquieren juicios de relevancia;2. Una de las carreras originales frente a una de las nuevas carreras;3. Dos nuevas carreras. La Tabla 3 muestra la confianza frente a los resultados de precisión para cada uno de estos tres grupos. Curiosamente, el rendimiento es peor al comparar una de las carreras originales con una de las carreras adicionales. Esto probablemente se deba a una gran diferencia en el número de juicios que afectan la varianza de ΔMAP. Sin embargo, el rendimiento es bastante bueno en los tres subconjuntos. El peor de los casos: el caso intuitivamente es más probable que produzca un error es cuando los dos sistemas que se comparan han recuperado muy pocos documentos en común. Si queremos que los juicios sean reutilizables, debemos poder generalizar incluso a corridas que son muy diferentes de las que se usan para adquirir los juicios de relevancia. Una simple medida de similitud de dos corridas es el porcentaje promedio de documentos que recuperaron en común para cada tema [2]. Calculamos esto para todos los pares, luego observamos el rendimiento en pares con baja similitud. Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 - 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza versus precisión de RTC cuando se compara las dos carreras originales, una carrera original y una nueva carrera,y dos nuevas carreras. RTC es robusto en los tres casos.precisión cuando una confianza similar 0 - 0.1 0.1 - 0.2 0.2 - 0.3 0.5 - 0.6 68.4% 63.1% 61.4% 0.6 - 0.7 84.2% 78.6% 76.6% 0.7 - 0.8 82.0% 79.8% 78.9% 0.8 - 0.9 93.6% 83.3% 82.111-0.95 99.3% 92.7% 92.4% 0.95-0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza versus precisión de RTC cuando un par de sistemas retrateado 0-30% documentosen común (desencadenado en 0%-10%, 10%-20%y 20%30%). RTC es robusto en los tres casos. Tabla 4. El rendimiento es de hecho muy robusto incluso cuando la similitud es baja. Cuando las dos ejecuciones comparten muy pocos documentos en común, W es realmente positivo. MTC e IP tuvieron un desempeño bastante mal en estos casos. Cuando la similitud fue entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC. Por conjunto de datos: todos los resultados anteriores solo han estado en las colecciones ad-hoc. Hicimos los mismos experimentos en nuestros conjuntos de datos adicionales y rompimos los resultados mediante el conjunto de datos para ver cómo varía el rendimiento. Los resultados en la Tabla 5 muestran todo sobre cada conjunto, incluida la precisión agrupada, W, la media τ y el número medio de juicios para alcanzar la confianza del 95% en los dos primeros sistemas. Los resultados son altamente consistentes de la recopilación a la recopilación, lo que sugiere que nuestro método no es excesivo a ningún conjunto de datos en particular.6. Conclusiones y trabajo futuro En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una colección de pruebas y presentó un modelo que puede lograr una reutilización con conjuntos de juicios de relevancia muy pequeños. La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos precisa que RTC, que puede eliminar el sesgo para dar una evaluación robusta. Las estimaciones de confianza de RTC, además de ser precisas, proporcionan una guía para obtener juicios adicionales: concéntrese en juzgar documentos a partir de las comparaciones de menor confianza. A la larga, vemos pequeños conjuntos de relevancia Confianza de juicio ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robusto 05 terabyte 05 0.5-0.6 64.1% 61.8% 62.2% 62.0% 62.0% 62.0% 62.0%59.4% 64.3% 61.5% 61.6% 0.6 - 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 - 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 - 0.9.2.2% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 - 0.9.% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 - 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 - 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.93.93.93.93.% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.0.07 -0.41 -0.67 Median juzgó 235 276 243 179 448 310 3200.573 0.556 0.579 0.532 0.596 0.565 0.574 Tabla 5: Precisión, W, media τ y número medio de juicios para los 8 conjuntos de pruebas. Los resultados son muy consistentes entre los conjuntos de datos.Los investigadores comparten los investigadores, cada grupo contribuye con algunos juicios más para ganar más confianza sobre sus sistemas particulares. A medida que pasa el tiempo, el número de juicios crece hasta que haya un 100% de confianza en cada evaluación, y hay una recopilación de prueba completa para la tarea. Vemos más uso para este método en escenarios como la recuperación web en la que el corpus está cambiando con frecuencia. Podría aplicarse a la evaluación en una colección de pruebas dinámicas según lo definido por Soboroff [18]. El modelo que presentamos en la Sección 3 de ninguna manera es la única posibilidad para crear una colección de prueba robusta. Un modelo de agregación de expertos más simple puede funcionar tan bien o mejor (aunque todos nuestros esfuerzos para simplificar fracasaron). Además de la agregación de expertos, podríamos estimar las probabilidades observando similitudes entre documentos. Esta es un área obvia para la exploración futura. Además, valdrá la pena investigar el problema del sobreajuste: las circunstancias en las que ocurre y lo que se puede hacer para prevenirlo. Mientras tanto, limitar las estimaciones de confianza al 95% es un truco que resuelve el problema. Tenemos muchos más resultados experimentales para los que desafortunadamente no teníamos espacio, pero que refuerzan la noción de que RTC es muy robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación por pares de sistemas. Agradecimientos Este trabajo fue apoyado en parte por el Centro para la Recuperación de Información Inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023. Cualquier opinión, hallazgos y conclusiones o recomendaciones expresadas en este material son las del autor y no reflejan necesariamente las del patrocinador.7. Referencias [1] J. Aslam y M. Montague. Modelos para MetaSearch. En Actas de Sigir, páginas 275-285, 2001. [2] J. Aslam y R. Savell. Sobre la efectividad de la evaluación de los sistemas de recuperación en ausencia de juicios de relevancia. En Actas de Sigir, páginas 361-362, 2003. [3] J. A. Aslam, V. Pavlu y R. Savell. Un modelo unificado para metasearch, agrupación y evaluación del sistema. En Actas de CIKM, páginas 484-491, 2003. [4] J. A. Aslam, V. Pavlu y E. Yilmaz. Un método estadístico para la evaluación del sistema utilizando juicios incompletos. En Actas de Sigir, páginas 541-548, 2006. [5] A. L. Berger, S. D. Pietra y V. J. D. Pietra. Un enfoque de entropía máxima para el procesamiento del lenguaje natural. Computational Linguistics, 22 (1): 39-71, 1996. [6] D. J. Blower. Una fácil derivación de la regresión logística desde la perspectiva bayesiana y máxima de entropía. En Actas del 23º trabajo internacional sobre inferencia bayesiana y métodos de entropía máxima en ciencia e ingeniería, páginas 30-43, 2004. [7] B. Carterette y J. Allan. Metodología de investigación en estudios de esfuerzo del asesor para la evaluación de la recuperación. En Actas de Riao, 2007. [8] B. Carterette, J. Allan y R. K. Sitaraman. Colecciones de pruebas mínimas para la evaluación de recuperación. En Actas de Sigir, páginas 268-275, 2006. [9] B. Carterette y D. I. Petkova. Aprender una clasificación de preferencias por pares. En Actas de Sigir, 2006. [10] R. T. Clemen y R. L. Winkler. Unanimidad y compromiso entre los pronosticadores de probabilidad. Management Science, 36 (7): 767-779, julio de 1990. [11] G. V. Cormack, C. R. Palmer y C. L. Clarke. Construcción eficiente de grandes colecciones de pruebas. En Actas de Sigir, páginas 282-289, 1998. [12] A. Gelman, J. B. Carlin, H. S. Stern y D. B. Rubin. Análisis de datos bayesianos. Chapman y Hall/CRC, 2004. [13] E. T. Jaynes. Teoría de la probabilidad: la lógica de la ciencia. Cambridge University Press, 2003. [14] R. Manmatha y H. Sever. Un enfoque formal para la normalización de la puntuación para MetaSearch. En Actas de HLT, páginas 88-93, 2002. [15] I. J. Myung, S. Ramamoorti y J. Andrew D. Baily. Agregación máxima de entropía de predicciones expertas. Management Science, 42 (10): 1420-1436, octubre de 1996. [16] J. Platt. Salidas probabilísticas para máquinas de vectores de soporte y comparación con métodos de probabilidad regularizados.Páginas 61-74, 2000. [17] M. Sanderson y H. Joho. Formando colecciones de pruebas sin agrupación de sistemas. En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 33-40, 2004. [18] I. Soboroff. Colecciones de pruebas dinámicas: medir la efectividad de la búsqueda en la web en vivo. En Actas de Sigir, páginas 276-283, 2006. [19] K. Sparck Jones y C. J. Van Rijsbergen. Colecciones de pruebas de recuperación de información. Journal of Documation, 32 (1): 59-75, 1976. [20] E. M. Voorhees y D. K. Harman, editores. TREC: Experimento y evaluación en la recuperación de la información. MIT Press, 2005. [21] J. Zobel. ¿Qué tan confiables son los resultados de los experimentos de recuperación de información a gran escala? En Actas de Sigir, páginas 307-314, 1998.",
    "original_sentences": [
        "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
        "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
        "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
        "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
        "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
        "Even the smallest sets of judgments can be useful for evaluation of new systems.",
        "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
        "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
        "She has built a system to perform the task and wants to evaluate it.",
        "Since the task is new, it is unlikely that there are any extant relevance judgments.",
        "She does not have the time or resources to judge every document, or even every retrieved document.",
        "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
        "But what happens when she develops a new system and needs to evaluate it?",
        "Or another research group decides to implement a system to perform the task?",
        "Can they reliably reuse the original judgments?",
        "Can they evaluate without more relevance judgments?",
        "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
        "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
        "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
        "This solution is not adequate for our hypothetical researcher.",
        "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
        "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
        "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
        "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
        "First we must formally define what it means to be reusable.",
        "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
        "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
        "We need a more careful definition of reusability.",
        "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
        "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
        "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
        "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
        "Any set of judgments, no matter how small, becomes reusable to some degree.",
        "Small, reusable test collections could have a huge impact on information retrieval research.",
        "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
        "The amount of data available to researchers would grow exponentially over time. 2.",
        "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
        "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
        "Our evaluation should be robust to missing judgments.",
        "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
        "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
        "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
        "We therefore see confidence as a probability estimate.",
        "One of the questions we must ask about a probability estimate is what it means.",
        "What does it mean to have 75% confidence that system A is better than system B?",
        "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
        "If this is what it means, we can trust the confidence estimates.",
        "But do we know it has that meaning?",
        "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
        "This assumption is almost certainly not realistic in most IR applications.",
        "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
        "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
        "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
        "Let Xi be a random variable indicating the relevance of document i.",
        "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
        "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
        "Using aij instead of 1/i allows us to number the documents arbitrarily.",
        "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
        "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
        "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
        "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
        "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
        "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
        "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
        "We can then compute e.g.",
        "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
        "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
        "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
        "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
        "Since topics are independent, we can easily extend this to mean average precision (MAP).",
        "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
        "Let Z be the set of all pairs of ranked results for a common set of topics.",
        "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
        "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
        "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
        "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
        "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
        "The assumptions they are based on are the probabilities of relevance pi.",
        "We need these to be realistic.",
        "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
        "This is known as the principle of maximum entropy [13].",
        "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
        "This has found a wide array of uses in computer science and information retrieval.",
        "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
        "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
        "Theorem 1.",
        "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
        "We forgo the proof for the time being, but it is quite simple.",
        "This says that the better the estimates of relevance, the more accurate the evaluation.",
        "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
        "The theorem and its proof say nothing whatsoever about the evaluation metric.",
        "The probability estimates are entirely indepedent of the measure we are interested in.",
        "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
        "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
        "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
        "The task therefore becomes the imputation of the missing values of relevance.",
        "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
        "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
        "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
        "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
        "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
        "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
        "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
        "Our problem has two key differences: 1.",
        "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
        "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
        "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
        "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
        "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
        "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
        "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
        "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
        "We include a beta prior for p(λj) with parameters α, β.",
        "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
        "This model has the advantage of including the statistical dependence between the experts.",
        "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
        "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
        "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
        "Where do the qij s come from?",
        "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
        "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
        "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
        "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
        "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
        "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
        "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
        "We need to convert these to probabilities.",
        "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
        "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
        "Let q∗ ij be expert js self-reported probability that document i is relevant.",
        "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
        "The pairwise preference model can handle these two requirements easily, so we will use it.",
        "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
        "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
        "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
        "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
        "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
        "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
        "Therefore we only have to solve this once for each topic.",
        "The above model gives topic-independent probabilities for each document.",
        "But suppose an expert who reports 90% probability is only right 50% of the time.",
        "Its opinion should be discounted based on its observed performance.",
        "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
        "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
        "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
        "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
        "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
        "Figure 1: Conceptual diagram of our aggregation model.",
        "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
        "The first step is to obtain q∗ ij.",
        "Next is calibration to true performance to find qij .",
        "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
        "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
        "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
        "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
        "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
        "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
        "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
        "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
        "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
        "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
        "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
        "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
        "EXPERIMENTS Three hypotheses are under consideration.",
        "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
        "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
        "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
        "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
        "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
        "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
        "Statistics are shown in Table 1.",
        "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
        "We use the qrels files assembled by NIST as truth.",
        "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
        "For computational reasons, we truncate ranked lists at 100 documents.",
        "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
        "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
        "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
        "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
        "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
        "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
        "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
        "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
        "We will call this MTC for minimal test collection.",
        "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
        "We will call this RTC for robust test collection.",
        "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
        "RTC has smoothing (prior distribution) parameters that must be set.",
        "We trained using the ad-hoc 95 set.",
        "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
        "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
        "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
        "For each experimental trial: 1.",
        "Pick a random subset of k runs. 2.",
        "From those k, pick an initial c < k to evaluate. 3.",
        "Run RTC to 95% confidence on the initial c. 4.",
        "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
        "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
        "We do the same for MTC, but omit step 4.",
        "Note that after evaluating the first c systems, we make no additional relevance judgments.",
        "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
        "We will then generalize to a set of k = 10 (of which those two are a subset).",
        "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
        "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
        "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
        "Since summary statistics are useful, we devised the following metric.",
        "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
        "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
        "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
        "If it turns out that ΔMAP < 0, we win the dollar.",
        "Otherwise, we pay out O.",
        "If our confidence estimates are perfectly accurate, we break even.",
        "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
        "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
        "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
        "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
        "The summary statistic is W, the mean of Wi.",
        "Note that as Pi increases, we lose more for being wrong.",
        "This is as it should be: the penalty should be great for missing the high probability predictions.",
        "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
        "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
        "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
        "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
        "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
        "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
        "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
        "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
        "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
        "Using the same sets of systems allows the use of paired tests.",
        "As we stated above, we are more interested in the median number of judgments than the mean.",
        "A test for difference in median is the Wilcoxon sign rank test.",
        "We can also use a paired t-test to test for a difference in mean.",
        "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
        "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
        "With MTC and uniform probabilities of relevance, the results are far from robust.",
        "We cannot reuse the relevance judgments with much confidence.",
        "But with RTC, the results are very robust.",
        "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
        "Mean Wi shows that RTC is much closer to 0 than MTC.",
        "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
        "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
        "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
        "More detailed results for both algorithms are shown in Figure 2.",
        "The solid line is the ideal result that would give W = 0.",
        "RTC is on or above this line at all points until confidence reaches about 0.97.",
        "After that there is a slight dip in accuracy which we discuss below.",
        "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
        "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
        "RTC is much more robust than MTC.",
        "W is defined in Section 4.4; closer to 0 is better.",
        "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
        "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
        "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
        "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
        "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
        "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
        "The median required by RTC is 235, about 4.7 per topic.",
        "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
        "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
        "The difference in means is much greater.",
        "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
        "This difference is significant by a paired t-test (p < 0.0001).",
        "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
        "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
        "This shows that even tiny collections can be reusable.",
        "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
        "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
        "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
        "This difference is significant by a paired t-test (p < 0.0001).",
        "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
        "It is more important that we estimate confidence in each pairwise comparison correctly.",
        "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
        "We calculated the τ correlation to the true ranking.",
        "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
        "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
        "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
        "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
        "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
        "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
        "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
        "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
        "Table 3 shows confidence vs. accuracy results for each of these three groups.",
        "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
        "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
        "Nevertheless, performance is quite good on all three subsets.",
        "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
        "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
        "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
        "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
        "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
        "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
        "RTC is robust in all three cases.",
        "Table 4.",
        "Performance is in fact very robust even when similarity is low.",
        "When the two runs share very few documents in common, W is actually positive.",
        "MTC and IP both performed quite poorly in these cases.",
        "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
        "By Data Set: All the previous results have only been on the ad-hoc collections.",
        "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
        "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
        "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
        "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
        "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
        "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
        "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
        "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
        "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
        "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
        "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
        "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
        "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
        "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
        "This is an obvious area for future exploration.",
        "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
        "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
        "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
        "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
        "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
        "REFERENCES [1] J. Aslam and M. Montague.",
        "Models for Metasearch.",
        "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
        "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
        "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
        "A. Aslam, V. Pavlu, and R. Savell.",
        "A unified model for metasearch, pooling, and system evaluation.",
        "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
        "A. Aslam, V. Pavlu, and E. Yilmaz.",
        "A statistical method for system evaluation using incomplete judgments.",
        "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
        "A maximum entropy approach to natural language processing.",
        "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
        "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
        "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
        "Research methodology in studies of assessor effort for retrieval evaluation.",
        "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
        "Minimal test collections for retrieval evaluation.",
        "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
        "Learning a ranking from pairwise preferences.",
        "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
        "Unanimity and compromise among probability forecasters.",
        "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
        "Efficient Construction of Large Test Collections.",
        "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
        "B. Carlin, H. S. Stern, and D. B. Rubin.",
        "Bayesian Data Analysis.",
        "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
        "Probability Theory: The Logic of Science.",
        "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
        "A Formal Approach to Score Normalization for Metasearch.",
        "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
        "Maximum entropy aggregation of expert predictions.",
        "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
        "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
        "Forming test collections with no system pooling.",
        "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
        "Dynamic test collections: measuring search effectiveness on the live web.",
        "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
        "Information Retrieval Test Collections.",
        "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
        "TREC: Experiment and Evaluation in Information Retrieval.",
        "MIT Press, 2005. [21] J. Zobel.",
        "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
        "In Proceedings of SIGIR, pages 307-314, 1998."
    ],
    "error_count": 0,
    "keys": {
        "information retrieval": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent <br>information retrieval</br> Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an <br>information retrieval</br> researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of <br>information retrieval</br> research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on <br>information retrieval</br> research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and <br>information retrieval</br>.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an <br>information retrieval</br> expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent <br>information retrieval</br> and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in <br>information retrieval</br>, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "<br>information retrieval</br> Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in <br>information retrieval</br>.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale <br>information retrieval</br> Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Colecciones de pruebas robustas para la evaluación de recuperación Ben Carterette Centro para la \"Recuperación de la información\" inteligente de la Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu Resumen Los métodos de bajo costo para adquirir juicios de relevancia pueden ser un boon para los investigadores que necesitanevaluar nuevas tareas o temas de recuperación, pero no tiene los recursos para hacer miles de juicios.recuperación de información",
                "Introducción Considere un investigador de \"recuperación de información\" que ha inventado una nueva tarea de recuperación.recuperación de información",
                "La evaluación es un aspecto importante de la investigación de \"recuperación de información\", pero es solo un problema semi-resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento;Simplemente hay demasiados de ellos.recuperación de información",
                "Las pequeñas colecciones de pruebas reutilizables podrían tener un gran impacto en la investigación de \"recuperación de información\".recuperación de información",
                "Esto ha encontrado una amplia gama de usos en informática y \"recuperación de información\".recuperación de información",
                "Si tratamos a cada sistema como un experto en \"recuperación de información\" que proporciona una opinión sobre la relevancia de cada documento, el problema se convierte en una agregación de opinión experta.recuperación de información",
                "Agradecimientos Este trabajo fue apoyado en parte por el Centro para la \"Recuperación de información\" inteligente y en parte por la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA) bajo el número de contrato HR0011-06-C-0023.recuperación de información",
                "En Actas de la 27ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en \"Recuperación de información\", páginas 33-40, 2004. [18] I. Soboroff.recuperación de información",
                "Colecciones de pruebas de \"recuperación de información\".recuperación de información",
                "TREC: Experimento y evaluación en \"recuperación de información\".recuperación de información",
                "¿Qué tan confiables son los resultados de los experimentos a gran escala de \"recuperación de información\"?recuperación de información"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "evaluation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval <br>evaluation</br> Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time <br>evaluation</br>, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an <br>evaluation</br> of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for <br>evaluation</br> of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance <br>evaluation</br> General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "<br>evaluation</br> is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the <br>evaluation</br> of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST <br>evaluation</br> Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an <br>evaluation</br>.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our <br>evaluation</br> should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an <br>evaluation</br> measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular <br>evaluation</br> task that we call comparative <br>evaluation</br>: determining the sign of the difference in an evaluation measure.",
                "Other <br>evaluation</br> tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard <br>evaluation</br> metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative <br>evaluation</br> task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our <br>evaluation</br> will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the <br>evaluation</br>.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the <br>evaluation</br> metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between <br>evaluation</br> and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the <br>evaluation</br> and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust <br>evaluation</br>.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental <br>evaluation</br> Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard <br>evaluation</br> for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good <br>evaluation</br> of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust <br>evaluation</br>.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every <br>evaluation</br>-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to <br>evaluation</br> on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system <br>evaluation</br>.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system <br>evaluation</br> using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval <br>evaluation</br>.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval <br>evaluation</br>.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and <br>evaluation</br> in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Colecciones de pruebas robustas para la \"evaluación\" de recuperación Ben Carterette para la información inteligente Recuperación Departamento de Ciencias de la Computación Universidad de Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu Resumen Los métodos de bajo costo para adquirir juicios de relevancia pueden ser un boon para los investigadores que necesitanevaluar nuevas tareas o temas de recuperación, pero no tiene los recursos para hacer miles de juicios.evaluación",
                "Si bien estos juicios son muy útiles para una \"evaluación\" única, no está claro que se pueda confiar en ellos cuando se reutilizan para evaluar nuevos sistemas.evaluación",
                "En este trabajo, definimos formalmente lo que significa que los juicios sean reutilizables: la confianza en una \"evaluación\" de nuevos sistemas puede evaluarse con precisión a partir de un conjunto existente de juicios de relevancia.evaluación",
                "Incluso los conjuntos más pequeños de juicios pueden ser útiles para la \"evaluación\" de nuevos sistemas.evaluación",
                "Categorías y descriptores de sujetos: H.3 Almacenamiento y recuperación de información;H.3.4 Sistemas y software: \"Evaluación\" de rendimiento Términos generales: experimentación, medición, confiabilidad 1. Evaluación",
                "La \"evaluación\" es un aspecto importante de la investigación de recuperación de información, pero es solo un problema semi-resuelto: para la mayoría de las tareas de recuperación, es imposible juzgar la relevancia de cada documento;Simplemente hay demasiados de ellos.evaluación",
                "Como veremos, los juicios que producen estos métodos pueden sesgar significativamente la \"evaluación\" de un nuevo conjunto de sistemas.evaluación",
                "\"Evaluación\" robusta anterior damos una definición intuitiva de reutilización: una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una \"evaluación\".evaluación",
                "Nuestra \"evaluación\" debería ser robusta para los juicios faltantes.evaluación",
                "En nuestro trabajo anterior, definimos la confianza como la probabilidad de que la diferencia en una medida de \"evaluación\" calculada para dos sistemas sea inferior a cero [8].evaluación",
                "Esta noción de confianza se define en el contexto de una tarea particular de \"evaluación\" que llamamos \"evaluación\" comparativa: determinar el signo de la diferencia en una medida de evaluación.evaluación",
                "Se podrían definir otras tareas de \"evaluación\";Estimación de la magnitud de la diferencia o los valores de las medidas en sí mismas son ejemplos que implican diferentes nociones de confianza.evaluación",
                "Antes de elaborar esto, definimos formalmente la confianza.2.1 Estimación de la confianza La precisión promedio (AP) es una métrica estándar de \"evaluación\" que captura tanto la capacidad de un sistema para clasificar los documentos relevantes (precisión) como su capacidad para recuperar documentos relevantes (retiro).evaluación",
                "Sumando sobre todas las posibilidades, podemos calcular la expectativa y la varianza: E [AP] ≈ 1 Pi Aiipi + J> I aij pipj v ar [ap] ≈ 1 (pi) 2 n i a2 iipiqi + j> i a2 ijpipj (1 − pipj +) + i = j 2aiiaijpipj (1 - pi) + k> j = i 2aijaikpipjpk (1 - pi) AP converge asintóticamente a una distribución normal con expectativa y varianza como se define anteriormente.1 Para nuestra tarea comparativa de \"evaluación\" en la que estamos interesadosEl signo de la diferencia en dos precisiones promedio: ΔAP = AP1 - AP2.evaluación",
                "Si lo hacen, podemos confiar en las estimaciones de confianza;Nuestra \"evaluación\" será robusta para los juicios faltantes.evaluación",
                "Esto dice que cuanto mejores serán las estimaciones de relevancia, más precisa es la \"evaluación\".evaluación",
                "El teorema y su prueba no dicen nada sobre la métrica de \"evaluación\".evaluación",
                "Aslam et al.[3] Identificó previamente una conexión entre \"evaluación\" y MetaSearch.evaluación",
                "Estamos acumulando juicios de relevancia a medida que avanzamos con la \"evaluación\" y podemos restablecer la relevancia dada cada nuevo juicio.evaluación",
                "Limitamos 2 robustos aquí significa una recuperación robusta;Esto es diferente de nuestro objetivo de una \"evaluación\" robusta.evaluación",
                "A medida que ejecutamos más pruebas, obtenemos los datos que necesitamos para probar las tres hipótesis.4.4 La \"evaluación\" experimental recuerda que un conjunto de juicios es robusto si la precisión de las predicciones que hace es al menos su confianza estimada.evaluación",
                "Kendalls τ, un estadístico no paramétrico basado en swaps por pares entre dos listas, es una \"evaluación\" estándar para este tipo de estudio.evaluación",
                "Sin embargo, cuando mencionamos en la introducción, una medida de precisión como la correlación de rango no es una buena \"evaluación\" de reutilización.evaluación",
                "La Tabla 2 y la Figura 2 juntos muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos precisa que RTC, que puede eliminar el sesgo para dar una \"evaluación\" sólida.evaluación",
                "A medida que pasa el tiempo, el número de juicios crece hasta que haya un 100% de confianza en cada \"evaluación\", y hay una recopilación de prueba completa para la tarea.evaluación",
                "Podría aplicarse a la \"evaluación\" en una colección de pruebas dinámicas según lo definido por Soboroff [18].evaluación",
                "Un modelo unificado para metasearch, agrupación y \"evaluación\" del sistema.evaluación",
                "Un método estadístico para la \"evaluación\" del sistema utilizando juicios incompletos.evaluación",
                "Metodología de investigación en estudios de esfuerzo de evaluadores para la \"evaluación\" de recuperación.evaluación",
                "Colecciones de pruebas mínimas para la \"evaluación\" de recuperación.evaluación",
                "TREC: Experimento y \"evaluación\" en la recuperación de la información.evaluación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "relevance judgement": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "reusability": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees <br>reusability</br>: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, <br>reusability</br> has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of <br>reusability</br>.",
                "Specifically, the question of <br>reusability</br> is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of <br>reusability</br>: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of <br>reusability</br>.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of <br>reusability</br> of a test collection and presented a model that is able to achieve <br>reusability</br> with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El uso de este método prácticamente garantiza la \"reutilización\": con tan solo cinco juicios por tema tomados de solo dos sistemas, podemos evaluar de manera confiable un conjunto más grande de diez sistemas.reutilización",
                "En trabajos anteriores, la \"reutilización\" se ha probado simplemente evaluando la precisión de un conjunto de juicios de relevancia para evaluar los sistemas invisibles.reutilización",
                "Necesitamos una definición más cuidadosa de \"reutilización\".reutilización",
                "Específicamente, la cuestión de la \"reutilización\" no es cuán exactamente podemos evaluar nuevos sistemas.reutilización",
                "Evaluación robusta anterior damos una definición intuitiva de \"reutilización\": una colección es reutilizable si podemos confiar en nuestras estimaciones de confianza en una evaluación.reutilización",
                "Sin embargo, cuando mencionamos en la introducción, una medida de precisión como la correlación de rango no es una buena evaluación de \"reutilización\".reutilización",
                "Conclusiones y trabajo futuro en este trabajo Hemos ofrecido la primera definición formal de la idea común de \"reutilización\" de una colección de pruebas y presentó un modelo que puede lograr \"reutilización\" con conjuntos de juicios de relevancia muy pequeños.reutilización"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "lowerest-confidence comparison": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "mtc": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this <br>mtc</br> for minimal test collection.",
                "The third algorithm augments <br>mtc</br> with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (<br>mtc</br>) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for <br>mtc</br>, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or <br>mtc</br> to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than <br>mtc</br>, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than <br>mtc</br>, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between <br>mtc</br> and RTC is shown in Table 2.",
                "With <br>mtc</br> and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than <br>mtc</br>.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both <br>mtc</br> RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using <br>mtc</br> and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than <br>mtc</br>.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC <br>mtc</br> Figure 2: Confidence vs. accuracy of <br>mtc</br> and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by <br>mtc</br> to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "<br>mtc</br> required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: <br>mtc</br> and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for <br>mtc</br> and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that <br>mtc</br> took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from <br>mtc</br>, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both <br>mtc</br> and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "<br>mtc</br> and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both <br>mtc</br> and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: <br>mtc</br> is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Llamaremos a esto \"MTC\" para una colección de pruebas mínimas.MTC",
                "El tercer algoritmo aumenta \"MTC\" con estimaciones actualizadas de probabilidades de relevancia.MTC",
                "Algoritmo 1 (\"MTC\") Dadas dos listas clasificadas y el nivel de confianza α, predice el signo de ΔMAP.1: Pi ← 0.5 Para todos los documentos I 2: mientras que P (Δmap <0) <α do 3: Calcule el peso WI para todos los documentos inequívocos I (ver Carterette et al. [8] para detalles) 4: J ← Argmaxiwi 5:XJ ← 1 Si el documento J es relevante, 0 de lo contrario 6: PJ ← XJ 7: Fin mientras la búsqueda de antecedentes uniformes con una variación relativamente alta.MTC",
                "Hacemos lo mismo para \"MTC\", pero omitir el paso 4. MTC",
                "Usamos RTC o \"MTC\" para establecer las probabilidades O = P (ΔMap <0) 1 - P (ΔMap <0).MTC",
                "Para nuestra hipótesis de que RTC requiere menos juicios que \"MTC\", estamos interesados en el número de juicios necesarios para alcanzar la confianza del 95% en el primer par de sistemas.MTC",
                "Finalmente, para nuestra hipótesis de que RTC es más preciso que \"MTC\", analizaremos la correlación de Kendalls τ entre una clasificación de K sistemas por un pequeño conjunto de juicios y la verdadera clasificación utilizando el conjunto completo de juicios.MTC",
                "Resultados y análisis La comparación entre \"MTC\" y RTC se muestra en la Tabla 2. MTC",
                "Con \"MTC\" y probabilidades uniformes de relevancia, los resultados están lejos de ser robustos.MTC",
                "WI medio muestra que RTC está mucho más cerca de 0 que \"MTC\".MTC",
                "Tenga en cuenta que tanto el% de confianza de \"MTC\" RTC en la precisión del contenedor en la precisión del contenedor 0.5 - 0.6 33.7% 61.7% 28.6% 61.9% 0.6 - 0.7 18.1% 73.1% 20.1% 76.3% 0.7 - 0.8 10.4% 70.1% 15.5% 78.0% 0.88- 0.9 9.4% 69.0% 12.1% 84.9% 0.9 - 0.95 7.3% 78.0% 6.6% 93.1% 0.95 - 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% WMedia τ 0.393 0.555 Tabla 2: Confía de que P (ΔMap <0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando \"MTC\" y RTC.MTC",
                "RTC es mucho más robusto que \"MTC\".MTC",
                "La media τ es la correlación de rango promedio para los 10 sistemas.0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 Precisión Confianza Breakevenven RTC \"MTC\" Figura 2: Confianza versus precisión de \"MTC\" y RTC.MTC",
                "Número de juicios: El número medio de juicios requeridos por \"MTC\" para alcanzar el 95% de confianza en los dos primeros sistemas es 251, un promedio de 5 por tema.MTC",
                "\"MTC\" requirió una media de 823 juicios, 16 por tema, mientras que RTC requirió una media de 502, 10 por tema.(Recuerde que eso significa que están fuertemente sesgados por unos pocos pares que toman miles de juicios). MTC",
                "Correlación de rango: \"MTC\" y RTC clasifican los 10 sistemas por EMAP (Ec. (1)) calculados utilizando sus respectivas estimaciones de probabilidad.MTC",
                "La correlación media de rango τ entre el mapa verdadero y el EMAP es 0.393 para \"MTC\" y 0.555 para RTC.MTC",
                "Ejecutamos IP para el mismo número de juicios que \"MTC\" tomó para cada par, luego clasificamos los sistemas por mapa utilizando solo aquellos juicios (todos los documentos injustificados asumidos no relevantes).MTC",
                "La correlación media τ es 0.398, que no es significativamente diferente de \"MTC\", pero es significativamente menor que RTC.MTC",
                "El uso de estimaciones uniformes de probabilidad es indistinguible de la línea de base, mientras que la estimación de relevancia por parte de la agregación experta aumenta el rendimiento mucho: casi el 40% sobre \"MTC\" e IP.MTC",
                "\"MTC\" e IP tuvieron un desempeño bastante mal en estos casos.MTC",
                "Cuando la similitud fue entre 0 y 10%, tanto \"MTC\" como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para RTC.MTC",
                "La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: \"MTC\" está sobreestimando drásticamente la confianza y es mucho menos precisa que RTC, lo que puede eliminar el sesgo para dar una evaluación robusta.MTC"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "rtc": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this <br>rtc</br> for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "<br>rtc</br> has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run <br>rtc</br> to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use <br>rtc</br> or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that <br>rtc</br> requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that <br>rtc</br> is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and <br>rtc</br> is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with <br>rtc</br>, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that <br>rtc</br> is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "<br>rtc</br> is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC <br>rtc</br> confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and <br>rtc</br>.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "<br>rtc</br> is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven <br>rtc</br> MTC Figure 2: Confidence vs. accuracy of MTC and <br>rtc</br>.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by <br>rtc</br> is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while <br>rtc</br> required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and <br>rtc</br> both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for <br>rtc</br>.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than <br>rtc</br>.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where <br>rtc</br> exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of <br>rtc</br> when comparing the two original runs, one original run and one new run, and two new runs.",
                "<br>rtc</br> is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of <br>rtc</br> when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "<br>rtc</br> is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for <br>rtc</br>.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than <br>rtc</br>, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of <br>rtc</br>, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that <br>rtc</br> is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Llamaremos a esto \"RTC\" para una colección de prueba robusta.RTC",
                "\"RTC\" tiene parámetros de suavizado (distribución previa) que deben establecerse.RTC",
                "Ejecute \"RTC\" a un 95% de confianza en la c.4. RTC",
                "Usamos \"RTC\" o MTC para establecer las probabilidades O = P (ΔMap <0) 1 - P (ΔMap <0).RTC",
                "Para nuestra hipótesis de que \"RTC\" requiere menos juicios que MTC, estamos interesados en el número de juicios necesarios para alcanzar la confianza del 95% en el primer par de sistemas.RTC",
                "Finalmente, para nuestra hipótesis de que \"RTC\" es más preciso que MTC, analizaremos la correlación de Kendalls τ entre una clasificación de K sistemas por un pequeño conjunto de juicios y la verdadera clasificación utilizando el conjunto completo de juicios.RTC",
                "Resultados y análisis La comparación entre MTC y \"RTC\" se muestra en la Tabla 2. RTC",
                "Pero con \"RTC\", los resultados son muy robustos.RTC",
                "WI medio muestra que \"RTC\" está mucho más cerca de 0 que MTC.RTC",
                "\"RTC\" está en o por encima de esta línea en todos los puntos hasta que la confianza alcanza aproximadamente 0.97.RTC",
                "Tenga en cuenta que tanto MTC \"RTC\" de confianza en el% de precisión del contenedor en la precisión del contenedor 0.5 - 0.6 33.7% 61.7% 28.6% 61.9% 0.6 - 0.7 18.1% 73.1% 20.1% 76.3% 0.7 - 0.8 10.4% 70.1% 15.5% 78.0% 0.88- 0.9 9.4% 69.0% 12.1% 84.9% 0.9 - 0.95 7.3% 78.0% 6.6% 93.1% 0.95 - 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% WMedia τ 0.393 0.555 Tabla 2: Confía de que P (ΔMap <0) y precisión de la predicción al generalizar un conjunto de juicios de relevancia adquiridos utilizando MTC y \"RTC\".RTC",
                "\"RTC\" es mucho más robusto que MTC.RTC",
                "La media τ es la correlación de rango promedio para los 10 sistemas.0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 Precisión Confianza Breakevenen \"RTC\" MTC Figura 2: Confianza versus precisión de MTC y \"RTC\".RTC",
                "La mediana requerida por \"RTC\" es 235, aproximadamente 4.7 por tema.RTC",
                "MTC requería una media de 823 juicios, 16 por tema, mientras que \"RTC\" requería una media de 502, 10 por tema.(Recuerde que eso significa que están fuertemente sesgados por unos pocos pares que toman miles de juicios). RTC",
                "Correlación de rango: MTC y \"RTC\" clasifican los 10 sistemas por EMAP (Ec. (1)) calculados utilizando sus respectivas estimaciones de probabilidad.RTC",
                "La correlación media de rango τ entre el mapa verdadero y el EMAP es 0.393 para MTC y 0.555 para \"RTC\".RTC",
                "La correlación media τ es 0.398, que no es significativamente diferente de MTC, pero es significativamente menor que \"RTC\".RTC",
                "Vimos esto en la Tabla 2 y la Figura 2, donde \"RTC\" exhibe una caída en la precisión cuando la confianza es de alrededor del 97%.RTC",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 - 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Tabla 3: Confianza versus precisión de \"RTC\" al comparar las dos carreras originales, una carrera original y una nuevacorrer y dos nuevas carreras.RTC",
                "\"RTC\" es robusto en los tres casos.precisión cuando una confianza similar 0 - 0.1 0.1 - 0.2 0.2 - 0.3 0.5 - 0.6 68.4% 63.1% 61.4% 0.6 - 0.7 84.2% 78.6% 76.6% 0.7 - 0.8 82.0% 79.8% 78.9% 0.8 - 0.9 93.6% 83.3% 82.111-0.95 99.3% 92.7% 92.4% 0.95-0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Tabla 4: Confianza versus precisión de \"RTC\" cuando un par de sistemas recuperado 0-30%Documentos en común (dividido en 0%-10%, 10%-20%y 20%30%).RTC",
                "\"RTC\" es robusto en los tres casos.RTC",
                "Cuando la similitud fue entre 0 y 10%, tanto MTC como IP predijeron correctamente ΔMAP solo el 60% del tiempo, en comparación con una tasa de éxito del 87.6% para \"RTC\".RTC",
                "La Tabla 2 y la Figura 2 juntas muestran cuán sesgado puede ser un pequeño conjunto de juicios: MTC está sobreestimando drásticamente la confianza y es mucho menos precisa que \"RTC\", que puede eliminar el sesgo para dar una evaluación robusta.RTC",
                "Las estimaciones de confianza de \"RTC\", además de ser precisas, proporcionan una guía para obtener juicios adicionales: centrarse en juzgar documentos a partir de las comparaciones con la confianza más baja.RTC",
                "Tenemos muchos más resultados experimentales para los que desafortunadamente no teníamos espacio, pero que refuerzan la noción de que \"RTC\" es muy robusto: con solo unos pocos juicios por tema, podemos evaluar con precisión la confianza en cualquier comparación por pares de sistemas.RTC"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "expectation": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an <br>expectation</br>, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute <br>expectation</br> and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with <br>expectation</br> and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its <br>expectation</br> and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the <br>expectation</br> and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esta variable aleatoria tiene una \"expectativa\", una varianza, intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado.expectativa",
                "Sumando sobre todas las posibilidades, podemos calcular \"expectativa\" y varianza: E [AP] ≈ 1 Pi Aiipi + J> I aij pipj v ar [ap] ≈ 1 (pi) 2 n i a2 iipiqi + j> i a2 ijpipj (1− Pipj) + i = j 2aiiaijpipj (1 - pi) + k> j = i 2aijaikpipjpk (1 - pi) ap asintóticamente converge a una distribución normal con \"expectativa\" y varianza como se definió anteriormente.1 Para nuestra tarea de evaluación comparativa somosInteresado en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 - AP2.expectativa",
                "El mapa también se distribuye normalmente;Su \"expectativa\" y varianza son: EMAP = 1 T T∈T e [apt] (1) vmap = 1 t2 t∈T v ar [apt] Δmap = map1 - map2 la confianza puede estimarse calculando la \"expectativa\" y la varianzay usando la función de densidad normal para encontrar p (Δmap <0).2.2 Confianza y robustez Al haber definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección con los juicios faltantes.1 Estas son realmente aproximaciones a la verdadera expectativa y varianza, pero el error es una O (N2 - N) insignificante.expectativa"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "variance": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a <br>variance</br>, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and <br>variance</br>: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and <br>variance</br> as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and <br>variance</br> are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and <br>variance</br> and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating <br>variance</br> is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high <br>variance</br>.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the <br>variance</br> in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the <br>variance</br> of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esta variable aleatoria tiene una expectativa, una \"varianza\", intervalos de confianza y una cierta probabilidad de ser menor o igual a un valor dado.diferencia",
                "En su resumen de todas las posibilidades, podemos calcular la expectativa y \"varianza\": E [ap] ≈ 1 pi aiipi + j> i aij pipj v ar [ap] ≈ 1 (pi) 2 n i a2 iipiqi + j> i a2 ijpipj (1− Pipj) + i = j 2aiiaijpipj (1 - pi) + k> j = i 2aijaikpipjpk (1 - pi) AP converge asintóticamente a una distribución normal con expectativa y \"varianza\" como se definió anteriormente.1 Para nuestra tarea de evaluación comparativa somosInteresado en el signo de la diferencia en dos precisiones promedio: ΔAP = AP1 - AP2.diferencia",
                "El mapa también se distribuye normalmente;Su expectativa y \"varianza\" son: EMAP = 1 t t∈T e [apt] (1) vmap = 1 t2 t∈T v ar [apt] Δmap = mAp1 - MAP2 La confianza puede estimarse calculando la expectativa y la \"varianza\"y usando la función de densidad normal para encontrar p (Δmap <0).2.2 Confianza y robustez Al haber definido la confianza, volvemos al tema de la confianza en las estimaciones de confianza, y mostramos cómo se relaciona con la robustez de la colección con los juicios faltantes.1 Estas son realmente aproximaciones a la verdadera expectativa y varianza, pero el error es una O (N2 - N) insignificante.diferencia",
                "No hay razón para que no pudiéramos profundizar, pero calcular la \"varianza\" es O (N3) y, por lo tanto, muy tiempo de tiempo.diferencia",
                "Algoritmo 1 (MTC) dada dos listas clasificadas y nivel de confianza α, predice el signo de ΔMAP.1: Pi ← 0.5 Para todos los documentos I 2: mientras que P (Δmap <0) <α do 3: Calcule el peso WI para todos los documentos inequívocos I (ver Carterette et al. [8] para detalles) 4: J ← Argmaxiwi 5:XJ ← 1 Si el documento J es relevante, 0 de lo contrario 6: PJ ← XJ 7: Fin mientras la búsqueda de antecedentes uniformes con \"varianza\" relativamente alta.diferencia",
                "Overecking: es posible sobrepasar: si demasiados juicios provienen de los dos primeros sistemas, la \"varianza\" en Δmap se reduce y las estimaciones de confianza se vuelven poco confiables.diferencia",
                "Esto probablemente se deba a una gran diferencia en el número de juicios que afectan la \"varianza\" de Δmap.diferencia"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "distribution of relevance": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible <br>distribution of relevance</br> p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy <br>distribution of relevance</br>, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Argumentamos que la mejor \"distribución de relevancia\" P (XI) es la que explica todos los datos (todas las observaciones realizadas sobre los sistemas de recuperación) al mismo tiempo que no hacen suposiciones injustificadas.Distribución de relevancia",
                "El teorema implica que cuanto más cerca llegamos a la \"distribución de relevancia\" de la entropía máxima, más cerca llegaremos a la robustez.3. Distribución de relevancia"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "relevance distribution": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable test collection thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal test collection.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust test collection.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a test collection from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a test collection and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full test collection for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic test collection as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust test collection.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "test collection": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Robust Test Collections for Retrieval Evaluation Ben Carterette Center for Intelligent Information Retrieval Computer Science Department University of Massachusetts Amherst Amherst, MA 01003 carteret@cs.umass.edu ABSTRACT Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments.",
                "While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems.",
                "In this work, we formally define what it means for judgments to be reusable: the confidence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments.",
                "We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor effort.",
                "Using this method practically guarantees reusability: with as few as five judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems.",
                "Even the smallest sets of judgments can be useful for evaluation of new systems.",
                "Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Performance Evaluation General Terms: Experimentation, Measurement, Reliability 1.",
                "INTRODUCTION Consider an information retrieval researcher who has invented a new retrieval task.",
                "She has built a system to perform the task and wants to evaluate it.",
                "Since the task is new, it is unlikely that there are any extant relevance judgments.",
                "She does not have the time or resources to judge every document, or even every retrieved document.",
                "She can only judge the documents that seem to be the most informative and stop when she has a reasonable degree of confidence in her conclusions.",
                "But what happens when she develops a new system and needs to evaluate it?",
                "Or another research group decides to implement a system to perform the task?",
                "Can they reliably reuse the original judgments?",
                "Can they evaluate without more relevance judgments?",
                "Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.",
                "The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.",
                "This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].",
                "This solution is not adequate for our hypothetical researcher.",
                "The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.",
                "As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al. [11], Zobel [21], Sanderson and Joho [17], Carterette et al. [8], and Aslam et al. [4], among others.",
                "As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.",
                "Returning to our hypothetical resesarcher, can she reuse her relevance judgments?",
                "First we must formally define what it means to be reusable.",
                "In previous work, reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.",
                "While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.",
                "We need a more careful definition of reusability.",
                "Specifically, the question of reusability is not how accurately we can evaluate new systems.",
                "A malicious adversary can always produce a new ranked list that has not retrieved any of the judged documents.",
                "The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.",
                "Even if confidence is not high, as long as we can trust it, we can identify which systems need more judgments in order to increase confidence.",
                "Any set of judgments, no matter how small, becomes reusable to some degree.",
                "Small, reusable test collections could have a huge impact on information retrieval research.",
                "Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.",
                "The amount of data available to researchers would grow exponentially over time. 2.",
                "ROBUST EVALUATION Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of confidence in an evaluation.",
                "By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our assessment of the relative quality of the systems will change as we continue to judge documents.",
                "Our evaluation should be robust to missing judgments.",
                "In our previous work, we defined confidence as the probability that the difference in an evaluation measure calculated for two systems is less than zero [8].",
                "This notion of confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.",
                "Other evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.",
                "We therefore see confidence as a probability estimate.",
                "One of the questions we must ask about a probability estimate is what it means.",
                "What does it mean to have 75% confidence that system A is better than system B?",
                "As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.",
                "If this is what it means, we can trust the confidence estimates.",
                "But do we know it has that meaning?",
                "Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.",
                "This assumption is almost certainly not realistic in most IR applications.",
                "As it turns out, it is this assumption that determines whether the confidence estimates can eb trusted.",
                "Before elaborating on this, we formally define confidence. 2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).",
                "It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.",
                "Let Xi be a random variable indicating the relevance of document i.",
                "If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .",
                "Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.",
                "Using aij instead of 1/i allows us to number the documents arbitrarily.",
                "To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and nonrelevant document A at rank 2.",
                "Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.",
                "Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.",
                "We can now see average precision itself is a random variable with a distribution over all possible assignments of relevance to all documents.",
                "This random variable has an expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.",
                "All of these are dependent on the probability that document i is relevant: pi = p(Xi = 1).",
                "Suppose in our previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.",
                "We can then compute e.g.",
                "P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.",
                "Summing over all possibilities, we can compute expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.",
                "As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second ranking.",
                "Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to determine the confidence that a difference in AP is less than zero.",
                "Since topics are independent, we can easily extend this to mean average precision (MAP).",
                "MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the expectation and variance and using the normal density function to find P(ΔMAP < 0). 2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments. 1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).",
                "Let Z be the set of all pairs of ranked results for a common set of topics.",
                "Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged documents); these are the judgments against which we compute confidence.",
                "Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .",
                "For the confidence estimates to be accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.",
                "If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.",
                "If our confidence estimates are based on unrealistic assumptions, we cannot expect them to be accurate.",
                "The assumptions they are based on are the probabilities of relevance pi.",
                "We need these to be realistic.",
                "We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.",
                "This is known as the principle of maximum entropy [13].",
                "The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).",
                "This has found a wide array of uses in computer science and information retrieval.",
                "The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.",
                "The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.",
                "Theorem 1.",
                "If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate. where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).",
                "We forgo the proof for the time being, but it is quite simple.",
                "This says that the better the estimates of relevance, the more accurate the evaluation.",
                "The task of creating a reusable <br>test collection</br> thus becomes the task of estimating the relevance of unjudged documents.",
                "The theorem and its proof say nothing whatsoever about the evaluation metric.",
                "The probability estimates are entirely indepedent of the measure we are interested in.",
                "This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.",
                "Furthermore, we could assume that the relevance of documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.",
                "If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.",
                "The task therefore becomes the imputation of the missing values of relevance.",
                "The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 3.",
                "PREDICTING RELEVANCE In our statement of Theorem 1, we left the nature of the information I unspecified.",
                "One of the advantages of our confidence estimates is that they admit information from a wide variety of sources; essentially anything that can be modeled can be used as information for predicting relevance.",
                "A natural source of information is the retrieval systems themselves: how they ranked the judged documents, how often they failed to rank relevant documents, how they perform across topics, and so on.",
                "If we treat each system as an information retrieval expert providing an opinion about the relevance of each document, the problem becomes one of expert opinion aggregation.",
                "This is similar to the metasearch or data fusion problem in which the task is to take k input systems and merge them into a single ranking.",
                "Aslam et al. [3] previously identified a connection between evaluation and metasearch.",
                "Our problem has two key differences: 1.",
                "We explicitly need probabilities of relevance that we can plug into Eq. 1; metasearch algorithms have no such requirement. 2.",
                "We are accumulating relevance judgments as we proceed with the evaluation and are able to re-estimate relevance given each new judgment.",
                "In light of (1) above, we introduce a probabilistic model for expert combination. 3.1 A Model for Expert Opinion Aggregation Suppose that each expert j provides a probability of relevance qij = pj(Xi = 1).",
                "The information about the relevance of document i will then be the set of k expert opinions I = qi = (qi1, qi2, · · · , qik).",
                "The probability distribution we wish to find is the one that maximizes the entropy of pi = p(Xi = 1|qi).",
                "As it turns out, finding the maximum entropy model is equivalent to finding the parameters that maximize the likelihood [5].",
                "Blower [6] explicitly shows that finding the maximum entropy model for a binary variable is equivalent to solving a logistic regression.",
                "Then pi = p(Xi = 1|qi) = exp k j=1 λjqij 1 + exp k j=1 λj qij (2) where λ1, · · · , λk are the regression parameters.",
                "We include a beta prior for p(λj) with parameters α, β.",
                "This can be seen as a type of smoothing to account for the fact that the training data is highly biased.",
                "This model has the advantage of including the statistical dependence between the experts.",
                "A model of the same form was shown by Clemen & Winkler to be the best for aggregating expert probabilities [10].",
                "A similar maximumentropy-motivated approach has been used for expert aggregation [15].",
                "Aslam & Montague [1] used a similar model for metasearch, but assumed independence among experts.",
                "Where do the qij s come from?",
                "Using raw, uncalibrated scores as predictors will not work because score distributions vary too much between topics.",
                "A language modeling ranker, for instance, will typically give a much higher score to the top retrieved document for a short query than to the top retrieved document for a long query.",
                "We could train a separate predicting model for each topic, but that does not take advantage of all of the information we have: we may only have a handful of judgments for a topic, not enough to train a model to any confidence.",
                "Furthermore, it seems reasonable to assume that if an expert makes good predictions for one topic, it will make good predictions for other topics as well.",
                "We could use a hierarchical model [12], but that will not generalize to unseen topics.",
                "Instead, we will calibrate the scores of each expert individually so that scores can be compared both within topic and between topic.",
                "Thus our model takes into account not only the dependence between experts, but also the dependence between experts performances on different tasks (topics). 3.2 Calibrating Experts Each expert gives us a score and a rank for each document.",
                "We need to convert these to probabilities.",
                "A method such as the one used by Manmatha et al. [14] could be used to convert scores into probabilities of relevance.",
                "The pairwise preference method of Carterette & Petkova [9] could also be used, interpeting the ranking of one document over another as an expression of preference.",
                "Let q∗ ij be expert js self-reported probability that document i is relevant.",
                "Intuitively it seems clear that q∗ ij should decrease with rank, and it should be zero if document i is unranked (the expert did not believe it to be relevant).",
                "The pairwise preference model can handle these two requirements easily, so we will use it.",
                "Let θrj (i) be the relevance coefficient of the document at rank rj(i).",
                "We want to find the θs that maximize the likelihood function: Ljt(Θ) = rj (i)<rj (k) exp(θrj (i) − θrj (k)) 1 + exp(θrj (i) − θrj (k)) We again include a beta prior on p(θrj(i)) with parameters |Rt| + 1 and |Nt| + 1, the size of the sets of judged relevant and nonrelevant documents respectively.",
                "Using these as prior parameters ensures that the resulting probabilities will be concentrated around the ratio of relevant documents that have been discovered for topic t. This means that the probability estimates decrease by rank and are higher for topics that have more relevant documents.",
                "After finding the Θ that maximizes the likelihood, we have q∗ ij = exp(θrj (i)) 1+exp(θrj (i)) .",
                "We define θ∞ = −∞, so that the probability that an unranked document is relevant is 0.",
                "Since q∗ ij is based on the rank at which a document is retrieved rather than the identity of the document itself, the probabilities are identical from expert to expert, e.g. if expert E put document A at rank 1, and expert D put document B at rank 1, we will have q∗ AE = q∗ BD.",
                "Therefore we only have to solve this once for each topic.",
                "The above model gives topic-independent probabilities for each document.",
                "But suppose an expert who reports 90% probability is only right 50% of the time.",
                "Its opinion should be discounted based on its observed performance.",
                "Specifically, we want to learn a calibration function qij = Cj(q∗ ij) that will ensure that the predicted probabilities are tuned to the experts ability to retrieve relevant documents given the judgments that have been made to this point.",
                "Platts SVM calibration method [16] fits a sigmoid function between q∗ ij and the relevance judgments to obtain qij = Cj (q∗ ij) = exp(Aj +Bjq∗ ij ) 1+exp(Aj +Bj q∗ ij ) .",
                "Since q∗ ij is topic-independent, we only need to learn one calibration function for each expert.",
                "Once we have the calibration function, it is applied to adjust the experts predictions to their actual performance.",
                "The calibrated probabilities are plugged into model (2) to find the document probabilities.",
                "Figure 1: Conceptual diagram of our aggregation model.",
                "Experts E1, E2 have ranked documents A, B, C for topic T1 and documents D, E, F for topic T2.",
                "The first step is to obtain q∗ ij.",
                "Next is calibration to true performance to find qij .",
                "Finally we obtain pi = p(Xi = 1|qi1, qi2), · · · . 3.3 Model Summary Our model has three components that differ by the data they take as input and what they produce as output.",
                "A conceptual diagram is shown in Figure 1. 1. ranks → probabilities (per system per topic).",
                "This gives us q∗ ij, expert js self-reported probability of the relevance of document i.",
                "This is unsupervised; it requires no labeled data (though if we have some, we use it to set prior parameters). 2. probabilities → calibrated probabilities (per system).",
                "This gives us qij = Cj (q∗ ij), expert js calibrated probability of the relevance of document i.",
                "This is semisupervised; we have relevance judgments at some ranks which we use to impute the probability of relevance at other ranks. 3. calibrated probabilities → document probabilities.",
                "This gives us pi = p(Xi = 1|qi), the probability of relevance of document i given calibrated expert probabilities qij .",
                "This is supervised; we learn coefficients from a set of judged documents and use those to estimate the relevance of the unjudged documents.",
                "Although the model appears rather complex, it is really just three successive applications of logistic regression.",
                "As such, it can be implemented in a statistical programming language such as R in a few lines of code.",
                "The use of beta (conjugate) priors ensures that no expensive computational methods such as MCMC are necessary [12], so the model is trained and applied fast enough to be used on-line.",
                "Our code is available at http://ciir.cs.umass.edu/~carteret/. 4.",
                "EXPERIMENTS Three hypotheses are under consideration.",
                "The first, and most important, is that using our expert aggregation model to predict relevance produces test collections that are robust enough to be reusable; that is, we can trust the estimates of confidence when we evaluate systems that did not contribute any judgments to the pool.",
                "The other two hypotheses relate to the improvement we see by using better estimates of relevance than we did in our previous work [8].",
                "These are that (a) it takes fewer relevance track no. topics no. runs no. judged no. rel ad-hoc 94 50 40 97,319 9,805 ad-hoc 95 49 33 87,069 6,503 ad-hoc 96 50 61 133,681 5,524 ad-hoc 97 50 74 72,270 4,611 ad-hoc 98 50 103 80,345 4,674 ad-hoc 99 50 129 86,830 4,728 web 04 225 74 88,566 1,763 robust 05 50 74 37,798 6,561 terabyte 05 50 58 45,291 10,407 Table 1: Number of topics, number of runs, number of documents judged, and number found relevant for each of our data sets. judgments to reach 95% confidence and (b) the accuracy of the predictions is higher than if we were to simply assume pi = 0.5 for all unjudged documents. 4.1 Data We obtained full ad-hoc runs submitted to TRECs 3 through 8.",
                "Each run ranks at most 1000 documents for 50 topics (49 topics for TREC 4).",
                "Additionally, we obtained all runs from the Web track of TREC 13, the Robust2 track of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.",
                "These are the tracks that have replaced the ad-hoc track since its end in 1999.",
                "Statistics are shown in Table 1.",
                "We set aside the TREC 4 (ad-hoc 95) set for training, TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing, and the remaining sets for additional testing.",
                "We use the qrels files assembled by NIST as truth.",
                "The number of relevance judgments made and relevant documents found for each track are listed in Table 1.",
                "For computational reasons, we truncate ranked lists at 100 documents.",
                "There is no reason that we could not go deeper, but calculating variance is O(n3 ) and thus very timeconsuming.",
                "Because of the reciprocal rank nature of AP, we do not lose much information by truncating at rank 100. 4.2 Algorithms We will compare three algorithms for acquiring relevance judgments.",
                "The baseline is a variation of TREC pooling that we will call incremental pooling (IP).",
                "This algorithm takes a number k as input and presents the first k documents in rank order (without regard to topic) to be judged.",
                "It does not estimate the relevance of unjudged documents; it simply assumes any unjudged document is nonrelevant.",
                "The second algorithm is that presented in Carterette et al. [8] (Algorithm 1).",
                "Documents are selected based on how interesting they are in determining whether a difference in mean average precision exists.",
                "For this approach pi = 0.5 for all i; there is no estimation of probabilities.",
                "We will call this MTC for minimal <br>test collection</br>.",
                "The third algorithm augments MTC with updated estimates of probabilities of relevance.",
                "We will call this RTC for robust <br>test collection</br>.",
                "It is identical to Algorithm 1, except that every 10th iteration we estimate pi for all unjudged documents i using the expert aggregation model of Section 3.",
                "RTC has smoothing (prior distribution) parameters that must be set.",
                "We trained using the ad-hoc 95 set.",
                "We limited 2 Robust here means robust retrieval; this is different from our goal of robust evaluation.",
                "Algorithm 1 (MTC) Given two ranked lists and confidence level α, predict the sign of ΔMAP. 1: pi ← 0.5 for all documents i 2: while P(ΔMAP < 0) < α do 3: calculate weight wi for all unjudged documents i (see Carterette et al. [8] for details) 4: j ← argmaxiwi 5: xj ← 1 if document j is relevant, 0 otherwise 6: pj ← xj 7: end while the search to uniform priors with relatively high variance.",
                "For expert aggregation, the prior parameters are α = β = 1. 4.3 Experimental Design First, we want to know whether we can augment a set of relevance judgments with a set of relevance probabilities in order to reuse the judgments to evaluate a new set of systems.",
                "For each experimental trial: 1.",
                "Pick a random subset of k runs. 2.",
                "From those k, pick an initial c < k to evaluate. 3.",
                "Run RTC to 95% confidence on the initial c. 4.",
                "Using the model from Section 3, estimate the probabilities of relevance for all documents retrieved by all k runs. 5.",
                "Calculate EMAP for all k runs, and P(ΔMAP < 0) for all pairs of runs.",
                "We do the same for MTC, but omit step 4.",
                "Note that after evaluating the first c systems, we make no additional relevance judgments.",
                "To put our method to the test, we selected c = 2: we will build a set of judgments from evaluating only two initial systems.",
                "We will then generalize to a set of k = 10 (of which those two are a subset).",
                "As we run more trials, we obtain the data we need to test all three of our hypotheses. 4.4 Experimental Evaluation Recall that a set of judgments is robust if the accuracy of the predictions it makes is at least its estimated confidence.",
                "One way to evaluate robustness is to bin pairs by their confidence, then calculate the accuracy over all the pairs in each bin.",
                "We would like the accuracy to be no less than the lowest confidence score in the bin, but preferably higher.",
                "Since summary statistics are useful, we devised the following metric.",
                "Suppose we are a bookmaker taking bets on whether ΔMAP < 0.",
                "We use RTC or MTC to set the odds O = P (ΔMAP <0) 1−P (ΔMAP <0) .",
                "Suppose a bettor wagers $1 on ΔMAP ≥ 0.",
                "If it turns out that ΔMAP < 0, we win the dollar.",
                "Otherwise, we pay out O.",
                "If our confidence estimates are perfectly accurate, we break even.",
                "If confidence is greater than accuracy, we lose money; we win if accuracy is greater than confidence.",
                "Counterintuitively, the most desirable outcome is breaking even: if we lose money, we cannot trust the confidence estimates, but if we win money, we have either underestimated confidence or judged more documents than necessary.",
                "However, the cost of not being able to trust the confidence estimates is higher than the cost of extra relevance judgments, so we will treat positive outcomes as good.",
                "The amount we win on each pairwise comparison i is: Wi = yi − (1 − yi) Pi 1 − Pi = yi − Pi 1 − Pi yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP < 0).",
                "The summary statistic is W, the mean of Wi.",
                "Note that as Pi increases, we lose more for being wrong.",
                "This is as it should be: the penalty should be great for missing the high probability predictions.",
                "However, since our losses grow without bound as predictions approach certainty, we cap −Wi at 100.",
                "For our hypothesis that RTC requires fewer judgments than MTC, we are interested in the number of judgments needed to reach 95% confidence on the first pair of systems.",
                "The median is more interesting than the mean: most pairs require a few hundred judgments, but a few pairs require several thousand.",
                "The distribution is therefore highly skewed, and the mean strongly affected by those outliers.",
                "Finally, for our hypothesis that RTC is more accurate than MTC, we will look at Kendalls τ correlation between a ranking of k systems by a small set of judgments and the true ranking using the full set of judgments.",
                "Kendalls τ, a nonparametric statistic based on pairwise swaps between two lists, is a standard evaluation for this type of study.",
                "It ranges from −1 (perfectly anti-correlated) to 1 (rankings identical), with 0 meaning that half of the pairs are swapped.",
                "As we touched on in the introduction, though, an accuracy measure like rank correlation is not a good evaluation of reusability.",
                "We include it for completeness. 4.4.1 Hypothesis Testing Running multiple trials allows the use of statistical hypothesis testing to compare algorithms.",
                "Using the same sets of systems allows the use of paired tests.",
                "As we stated above, we are more interested in the median number of judgments than the mean.",
                "A test for difference in median is the Wilcoxon sign rank test.",
                "We can also use a paired t-test to test for a difference in mean.",
                "For rank correlation, we can use a paired t-test to test for a difference in τ. 5.",
                "RESULTS AND ANALYSIS The comparison between MTC and RTC is shown in Table 2.",
                "With MTC and uniform probabilities of relevance, the results are far from robust.",
                "We cannot reuse the relevance judgments with much confidence.",
                "But with RTC, the results are very robust.",
                "There is a slight dip in accuracy when confidence gets above 0.95; nonetheless, the confidence predictions are trustworthy.",
                "Mean Wi shows that RTC is much closer to 0 than MTC.",
                "The distribution of confidence scores shows that at least 80% confidence is achieved more than 35% of the time, indicating that neither algorithm is being too conservative in its confidence estimates.",
                "The confidence estimates are rather low overall; that is because we have built a <br>test collection</br> from only two initial systems.",
                "Recall from Section 1 that we cannot require (or even expect) a minimum level of confidence when we generalize to new systems.",
                "More detailed results for both algorithms are shown in Figure 2.",
                "The solid line is the ideal result that would give W = 0.",
                "RTC is on or above this line at all points until confidence reaches about 0.97.",
                "After that there is a slight dip in accuracy which we discuss below.",
                "Note that both MTC RTC confidence % in bin accuracy % in bin accuracy 0.5 − 0.6 33.7% 61.7% 28.6% 61.9% 0.6 − 0.7 18.1% 73.1% 20.1% 76.3% 0.7 − 0.8 10.4% 70.1% 15.5% 78.0% 0.8 − 0.9 9.4% 69.0% 12.1% 84.9% 0.9 − 0.95 7.3% 78.0% 6.6% 93.1% 0.95 − 0.99 17.9% 70.4% 12.4% 93.4% 1.0 3.3% 68.3% 4.7% 98.9% W −5.34 −0.39 median judged 251 235 mean τ 0.393 0.555 Table 2: Confidence that P(ΔMAP < 0) and accuracy of prediction when generalizing a set of relevance judgments acquired using MTC and RTC.",
                "Each bin contains over 1,000 trials from the adhoc 3, 5-8 sets.",
                "RTC is much more robust than MTC.",
                "W is defined in Section 4.4; closer to 0 is better.",
                "Median judged is the number of judgments to reach 95% confidence on the first two systems.",
                "Mean τ is the average rank correlation for all 10 systems. 0.5 0.6 0.7 0.8 0.9 1 0.5 0.6 0.7 0.8 0.9 1 accuracy confidence breakeven RTC MTC Figure 2: Confidence vs. accuracy of MTC and RTC.",
                "The solid line is the perfect result that would give W = 0; performance should be on or above this line.",
                "Each point represents at least 500 pairwise comparisons. algorithms are well above the line up to around confidence 0.7.",
                "This is because the baseline performance on these data sets is high; it is quite easy to achieve 75% accuracy doing very little work [7].",
                "Number of Judgments: The median number of judgments required by MTC to reach 95% confidence on the first two systems is 251, an average of 5 per topic.",
                "The median required by RTC is 235, about 4.7 per topic.",
                "Although the numbers are close, RTCs median is significantly lower by a paired Wilcoxon test (p < 0.0001).",
                "For comparison, a pool of depth 100 would result in a minimum of 5,000 judgments for each pair.",
                "The difference in means is much greater.",
                "MTC required a mean of 823 judgments, 16 per topic, while RTC required a mean of 502, 10 per topic. (Recall that means are strongly skewed by a few pairs that take thousands of judgments.)",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Ten percent of the sets resulted in 100 or fewer judgments (less than two per topic).",
                "Performance on these is very high: W = 0.41, and 99.7% accuracy when confidence is at least 0.9.",
                "This shows that even tiny collections can be reusable.",
                "For the 50% of sets with more than 235 judgments, accuracy is 93% when confidence is at least 0.9.",
                "Rank Correlation: MTC and RTC both rank the 10 systems by EMAP (Eq. (1)) calculated using their respective probability estimates.",
                "The mean τ rank correlation between true MAP and EMAP is 0.393 for MTC and 0.555 for RTC.",
                "This difference is significant by a paired t-test (p < 0.0001).",
                "Note that we do not expect the τ correlations to be high, since we are ranking the systems with so few relevance judgments.",
                "It is more important that we estimate confidence in each pairwise comparison correctly.",
                "We ran IP for the same number of judgments that MTC took for each pair, then ranked the systems by MAP using only those judgments (all unjudged documents assumed nonrelevant).",
                "We calculated the τ correlation to the true ranking.",
                "The mean τ correlation is 0.398, which is not significantly different from MTC, but is significantly lower than RTC.",
                "Using uniform estimates of probability is indistinguishable from the baseline, whereas estimating relevance by expert aggregation boosts performance a great deal: nearly 40% over both MTC and IP.",
                "Overfitting: It is possible to overfit: if too many judgments come from the first two systems, the variance in ΔMAP is reduced and the confidence estimates become unreliable.",
                "We saw this in Table 2 and Figure 2 where RTC exhibits a dip in accuracy when confidence is around 97%.",
                "In fact, the number of judgments made prior to a wrong prediction is over 50% greater than the number made prior to a correct prediction.",
                "Overfitting is difficult to quantify exactly, because making more relevance judgments does not always cause it: at higher confidence levels, more relevance judgments are made, and as Table 2 shows, accuracy is greater at those higher confidences.",
                "Obviously having more relevance judgments should increase both confidence and accuracy; the difference seems to be when one system has a great deal more judgments than the other.",
                "Pairwise Comparisons: Our pairwise comparisons fall into one of three groups: 1. the two original runs from which relevance judgments are acquired; 2. one of the original runs vs. one of the new runs; 3. two new runs.",
                "Table 3 shows confidence vs. accuracy results for each of these three groups.",
                "Interestingly, performance is worst when comparing one of the original runs to one of the additional runs.",
                "This is most likely due to a large difference in the number of judgments affecting the variance of ΔMAP.",
                "Nevertheless, performance is quite good on all three subsets.",
                "Worst Case: The case intuitively most likely to produce an error is when the two systems being compared have retrieved very few documents in common.",
                "If we want the judgments to be reusable, we should to be able to generalize even to runs that are very different from the ones used to acquire the relevance judgments.",
                "A simple measure of similarity of two runs is the average percentage of documents they retrieved in common for each topic [2].",
                "We calculated this for all pairs, then looked at performance on pairs with low similarity.",
                "Results are shown in accuracy confidence two original one original no original 0.5 − 0.6 - 48.1% 62.8% 0.6 − 0.7 - 57.1% 79.2% 0.7 − 0.8 - 67.9% 81.7% 0.8 − 0.9 - 82.2% 86.3% 0.9 − 0.95 95.9% 93.7% 92.6% 0.95 − 0.99 96.2% 92.5% 93.1% 1.0 100% 98.0% 99.1% W −1.11 −0.87 −0.27 Table 3: Confidence vs. accuracy of RTC when comparing the two original runs, one original run and one new run, and two new runs.",
                "RTC is robust in all three cases. accuracy when similar confidence 0 − 0.1 0.1 − 0.2 0.2 − 0.3 0.5 − 0.6 68.4% 63.1% 61.4% 0.6 − 0.7 84.2% 78.6% 76.6% 0.7 − 0.8 82.0% 79.8% 78.9% 0.8 − 0.9 93.6% 83.3% 82.1% 0.9 − 0.95 99.3% 92.7% 92.4% 0.95 − 0.99 98.7% 93.4% 93.3% 1.0 99.9% 97.9% 98.1% W 0.44 −0.45 −0.49 Table 4: Confidence vs. accuracy of RTC when a pair of systems retrieved 0-30% documents in common (broken out into 0%-10%, 10%-20%, and 20%30%).",
                "RTC is robust in all three cases.",
                "Table 4.",
                "Performance is in fact very robust even when similarity is low.",
                "When the two runs share very few documents in common, W is actually positive.",
                "MTC and IP both performed quite poorly in these cases.",
                "When the similarity was between 0 and 10%, both MTC and IP correctly predicted ΔMAP only 60% of the time, compared to an 87.6% success rate for RTC.",
                "By Data Set: All the previous results have only been on the ad-hoc collections.",
                "We did the same experiments on our additional data sets, and broke out the results by data set to see how performance varies.",
                "The results in Table 5 show everything about each set, including binned accuracy, W, mean τ, and median number of judgments to reach 95% confidence on the first two systems.",
                "The results are highly consistent from collection to collection, suggesting that our method is not overfitting to any particular data set. 6.",
                "CONCLUSIONS AND FUTURE WORK In this work we have offered the first formal definition of the common idea of reusability of a <br>test collection</br> and presented a model that is able to achieve reusability with very small sets of relevance judgments.",
                "Table 2 and Figure 2 together show how biased a small set of judgments can be: MTC is dramatically overestimating confidence and is much less accurate than RTC, which is able to remove the bias to give a robust evaluation.",
                "The confidence estimates of RTC, in addition to being accurate, provide a guide for obtaining additional judgments: focus on judging documents from the lowest-confidence comparisons.",
                "In the long run, we see small sets of relevance judgaccuracy confidence ad-hoc 94 ad-hoc 96 ad-hoc 97 ad-hoc 98 ad-hoc 99 web 04 robust 05 terabyte 05 0.5 − 0.6 64.1% 61.8% 62.2% 62.0% 59.4% 64.3% 61.5% 61.6% 0.6 − 0.7 76.1% 77.8% 74.5% 78.2% 74.3% 78.1% 75.9% 75.9% 0.7 − 0.8 75.2% 78.9% 77.6% 80.0% 78.6% 82.6% 77.5% 80.4% 0.8 − 0.9 83.2% 85.5% 84.6% 84.9% 86.8% 84.5% 86.7% 87.7% 0.9 − 0.95 93.0% 93.6% 92.8% 93.7% 92.6% 94.2% 93.9% 94.2% 0.95 − 0.99 93.1% 94.3% 93.1% 93.7% 92.8% 95.0% 93.9% 91.6% 1.0 99.2% 96.8% 98.7% 99.5% 99.6% 100% 99.2% 98.3% W -0.34 -0.34 -0.48 -0.35 -0.44 -0.07 -0.41 -0.67 median judged 235 276 243 213 179 448 310 320 mean τ 0.538 0.573 0.556 0.579 0.532 0.596 0.565 0.574 Table 5: Accuracy, W, mean τ, and median number of judgments for all 8 testing sets.",
                "The results are highly consistent across data sets. ments being shared by researchers, each group contributing a few more judgments to gain more confidence about their particular systems.",
                "As time goes on, the number of judgments grows until there is 100% confidence in every evaluation-and there is a full <br>test collection</br> for the task.",
                "We see further use for this method in scenarios such as web retrieval in which the corpus is frequently changing.",
                "It could be applied to evaluation on a dynamic <br>test collection</br> as defined by Soboroff [18].",
                "The model we presented in Section 3 is by no means the only possibility for creating a robust <br>test collection</br>.",
                "A simpler expert aggregation model might perform as well or better (though all our efforts to simplify failed).",
                "In addition to expert aggregation, we could estimate probabilities by looking at similarities between documents.",
                "This is an obvious area for future exploration.",
                "Additionally, it will be worthwhile to investigate the issue of overfitting: the circumstances it occurs under and what can be done to prevent it.",
                "In the meantime, capping confidence estimates at 95% is a hack that solves the problem.",
                "We have many more experimental results that we unfortunately did not have space for but that reinforce the notion that RTC is highly robust: with just a few judgments per topic, we can accurately assess the confidence in any pairwise comparison of systems.",
                "Acknowledgments This work was supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023.",
                "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect those of the sponsor. 7.",
                "REFERENCES [1] J. Aslam and M. Montague.",
                "Models for Metasearch.",
                "In Proceedings of SIGIR, pages 275-285, 2001. [2] J. Aslam and R. Savell.",
                "On the effectiveness of evaluating retrieval systems in the absence of relevance judgments.",
                "In Proceedings of SIGIR, pages 361-362, 2003. [3] J.",
                "A. Aslam, V. Pavlu, and R. Savell.",
                "A unified model for metasearch, pooling, and system evaluation.",
                "In Proceedings of CIKM, pages 484-491, 2003. [4] J.",
                "A. Aslam, V. Pavlu, and E. Yilmaz.",
                "A statistical method for system evaluation using incomplete judgments.",
                "In Proceedings of SIGIR, pages 541-548, 2006. [5] A. L. Berger, S. D. Pietra, and V. J. D. Pietra.",
                "A maximum entropy approach to natural language processing.",
                "Computational Linguistics, 22(1):39-71, 1996. [6] D. J. Blower.",
                "An easy derivation of logistic regression from the bayesian and maximum entropy perspective.",
                "In Proceedings of the 23rd International Workship on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, pages 30-43, 2004. [7] B. Carterette and J. Allan.",
                "Research methodology in studies of assessor effort for retrieval evaluation.",
                "In Proceedings of RIAO, 2007. [8] B. Carterette, J. Allan, and R. K. Sitaraman.",
                "Minimal test collections for retrieval evaluation.",
                "In Proceedings of SIGIR, pages 268-275, 2006. [9] B. Carterette and D. I. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of SIGIR, 2006. [10] R. T. Clemen and R. L. Winkler.",
                "Unanimity and compromise among probability forecasters.",
                "Management Science, 36(7):767-779, July 1990. [11] G. V. Cormack, C. R. Palmer, and C. L. Clarke.",
                "Efficient Construction of Large Test Collections.",
                "In Proceedings of SIGIR, pages 282-289, 1998. [12] A. Gelman, J.",
                "B. Carlin, H. S. Stern, and D. B. Rubin.",
                "Bayesian Data Analysis.",
                "Chapman & Hall/CRC, 2004. [13] E. T. Jaynes.",
                "Probability Theory: The Logic of Science.",
                "Cambridge University Press, 2003. [14] R. Manmatha and H. Sever.",
                "A Formal Approach to Score Normalization for Metasearch.",
                "In Proceedings of HLT, pages 88-93, 2002. [15] I. J. Myung, S. Ramamoorti, and J. Andrew D. Baily.",
                "Maximum entropy aggregation of expert predictions.",
                "Management Science, 42(10):1420-1436, October 1996. [16] J. Platt.",
                "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. pages 61-74, 2000. [17] M. Sanderson and H. Joho.",
                "Forming test collections with no system pooling.",
                "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33-40, 2004. [18] I. Soboroff.",
                "Dynamic test collections: measuring search effectiveness on the live web.",
                "In Proceedings of SIGIR, pages 276-283, 2006. [19] K. Sparck Jones and C. J. van Rijsbergen.",
                "Information Retrieval Test Collections.",
                "Journal of Documentation, 32(1):59-75, 1976. [20] E. M. Voorhees and D. K. Harman, editors.",
                "TREC: Experiment and Evaluation in Information Retrieval.",
                "MIT Press, 2005. [21] J. Zobel.",
                "How Reliable are the Results of Large-Scale Information Retrieval Experiments?",
                "In Proceedings of SIGIR, pages 307-314, 1998."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La tarea de crear una \"recopilación de pruebas\" reutilizable se convierte en la tarea de estimar la relevancia de los documentos innecesarios.recolección de pruebas",
                "Llamaremos a este MTC para una \"colección de pruebas\" mínima.recolección de pruebas",
                "Llamaremos a este RTC para una \"colección de pruebas\" robusta.recolección de pruebas",
                "Las estimaciones de confianza son bastante bajas en general;Esto se debe a que hemos creado una \"colección de pruebas\" a partir de solo dos sistemas iniciales.recolección de pruebas",
                "Conclusiones y trabajo futuro En este trabajo hemos ofrecido la primera definición formal de la idea común de reutilización de una \"colección de pruebas\" y presentamos un modelo que puede lograr una reutilización con conjuntos de juicios de relevancia muy pequeños.recolección de pruebas",
                "A medida que pasa el tiempo, el número de juicios crece hasta que haya un 100% de confianza en cada evaluación, y hay una \"colección de pruebas\" completa para la tarea.recolección de pruebas",
                "Podría aplicarse a la evaluación en una \"colección de pruebas\" dinámica según lo definido por Soboroff [18].recolección de pruebas",
                "El modelo que presentamos en la Sección 3 de ninguna manera es la única posibilidad de crear una \"colección de pruebas\" robusta.recolección de pruebas"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}