{
    "id": "H-32",
    "original_text": "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a Human Interest Model from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1. DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003. The Definition questions, also called Other questions in recent years, are defined as follows. Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?. The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic. Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets. Each informative nugget is a sentence fragment that describe some factual information about the topic. Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic. From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets. Rather, these topic nuggets have a trivia-like quality associated with them. Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest. For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets. In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?. We now have two very different perspective as to what constitutes an answer to Definition questions. An answer can be some important factual information about the topic or some novel and interesting aspect about the topic. This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman. Certain answer nuggets are more informative while other nuggets are more interesting in nature. Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history. Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus. As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion. Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom. As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets. In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets. A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system. We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2. RELATED WORK There are currently two general methods for Definitional Question Answering. The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14]. Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets. For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system. Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created. A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains. Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to. As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information. This process requires a lot of manual labor, expertise and is not scalable. This lead to the development of the soft-pattern approach by Cui et al. [4, 11]. Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences. Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training. Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO. However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations. This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities. For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being. Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets. This leads to the exploration of the second relevance-based approach that has been used in definitional question answering. Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1]. A similar approach has also been used as a baseline system for TREC 2003 [14]. More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering. Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic. The baseline system in TREC 2003 simply uses the topic words as its definitional corpus. Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional. Chen et al. [3] collect snippets from Google to build its definitional corpus. From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected. This centroid vector or set of centroid words is taken to be highly indicative of the topic. Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic. BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality. Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid. As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus. However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences. Thus such methods identify relevant sentences and not sentences containing definitional nuggets. Yet, the TREC 2003 baseline system [14] outperformed all but one other system. The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach. At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7]. We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords. This may explain why relevance-based method can perform competitively in definitional question answering. However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner. Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets. We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3. HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order. However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic. Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in. Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics. We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand. This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents. This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified. Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight. This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task. In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm. We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus. Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of. Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge. For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic. Such articles are useful as they contain concise information about the topic. More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture. We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic. Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers. This resource has been used by many Question Answering system as a source of knowledge about each topic. We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus. NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies. For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus. Google Snippets are retrieved by issuing the topic as a query to the Google search engine. From the search results, we extracted the top 100 snippets. While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics. Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus. We also extracted documents from other resources. However, as these resources are more specific in nature, we do not always get any single relevant document. These resources are listed below. Biography.com is the website for the Biography television cable channel. The channels website contains searchable biographies on over 25,000 notable people. If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus. Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people. Like Biography.com, we include the most relevant biography we can find in the Interest Corpus. Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one. We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus. Figure 1: Human Interest Model Architecture. WordNet WordNet is an well-known electronic semantic lexicon for the English language. Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset. We add this short definition, if there is one, into our Interest Corpus. We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic. Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence. We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids. Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids. Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers. By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm). Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus. In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus. An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system. The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented. Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration. The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus. The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper. The module first builds the unigram language model, I, from the collected web documents. This language model will be used to weight the importance of terms within sentences. Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences. Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness. We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms. A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents. To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm. Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus. When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight. We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence. In this manner, every candidate sentence is ranked by interestingness. Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4. INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets. Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering. This is done in order to explore how interestingness plays a factor in definitional answers. In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11]. In order to ensure comparable results, both systems are provided identical input data. Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module. Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module. For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems. Both systems are evaluated the results using the standard scoring methodology for TREC definitions. TREC provides a list of vital and okay nuggets for each question topic. Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall. Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12]. The evaluation is automatically conducted using Pourpre v1.0c [10]. FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets. We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13]. Table 1 shows the F3 score the three systems for the TREC 2005 question set. The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303. The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9]. This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns. In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3. These categories conform to TRECs general division of question topics into 4 main entity types [13]. The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2. Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics. This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system. In general, it is harder to locate a single web article that describes an event or a general object. However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types. This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5. REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information. As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings. From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora. There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets. A standard unigram language model would not capture these low-frequency terms as important terms. To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms. The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8]. TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus. For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus. Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions. Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I. General English words are likely to have similar distributions in both language models I and A. Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English. In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence. DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A. While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words. These high frequency topic specific words occur very much more frequently in I than in A. As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare. For this reason, we explored another divergence measure as a possible term weighting scheme. Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3. As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model. However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4. The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has. DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above. As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms. Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model. TFIDF performed the worst as we had anticipated. The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms. This causes the IDF component to be the main factor in scoring sentences. As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms. This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus. Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights. We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets. Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments. From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list. Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms. However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required. However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain. We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model. For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary. W - Wikipedia: Text from the most relevant article found in Wikipedia. S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google. M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions. We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination. All runs were conducted on Human Interest Model using JS divergence as term weighting scheme. The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference. A consistent trend can be observed for each entity class. For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence. This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets. We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia. We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures. Human readers are likely to be interested in news events that spotlight these personalities. Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations. With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6. UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets. However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional. Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic. We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set. Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions. The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets. The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets. We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set. However, none of the ensemble learning methods we attempted could outperform our Human Interest Model. The reason is that both systems are picking up very different sentences as definitional answers. In essence, our two experts are disagreeing on which sentences are definitional. In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets. The remaining answers were completely different. Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%. Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score. There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content. This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different. Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets. To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems. When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%. While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets. We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets. It is also indication that in general, interesting and informative nuggets are quite different in nature. There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches. However, the differences between the two systems also cause issues when we attempt to combine both answer sets. Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements. We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5. When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score. When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems. Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2]. Using the approach described here, we achieve a F3 score of 0.3081. This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7. CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets. Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity. The notion of an average human reader is an important consideration in our approach. This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering. Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings. Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems. We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers. What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic. Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic. While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task. Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features. As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model. We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers. Although the methods we used are simple, they have been shown experimentally to be effective. Our approach may also provide some insight into a few anomalies in past definitional question answerings trials. For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets. We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S. Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest. In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8. REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer. A hybrid approach for qa track definitional questions. In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang. Reranking answers for definitional qa using language modeling. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006. Association for Computational Linguistics. [4] H. Cui, M.-Y. Kan, and T.-S. Chua. Generic soft pattern models for definitional question answering. In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005. ACM Press. [5] T. G. Dietterich. Ensemble methods in machine learning. Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. Employing two question answering systems at trec 2005. In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber. Experiments at the university of edinburgh for the trec 2006 qa track. In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006. National Institute of Standards and Technology. [8] J. Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu. A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my! In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman. Automatically evaluating answers to definition questions. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005. Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y. Kan. Using syntactic and semantic relation analysis in question answering. In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees. Overview of the trec 2003 question answering track. In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003. National Institute of Standards and Technology. [13] E. M. Voorhees. Overview of the trec 2005 question answering track. In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel. TREC 2003 QA at BBN: Answering definitional questions. In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee. A language modeling approach to passage question answering. In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003.",
    "original_translation": "Nuggets interesantes y su impacto en las preguntas de definición respondiendo a Kian-Wei Kor Departamento de Informática Escuela de Computación Universidad Nacional de Singapur dkor@comp.nus.edu.sg Tat-Seng Chua Departamento de Ciencias de la Computación Escuela de Computación de la Universidad Nacional de Singapur@comp.nus.edu.sg Enfoques actuales abstractos para identificar oraciones de definición en el contexto de la respuesta de preguntas implica principalmente el uso de patrones lingüísticos o sintácticos para identificar pepitas informativas. Esto es insuficiente ya que no abordan el factor de novedad que también debe poseer una pepita definitiva. Este documento propone abordar la deficiencia construyendo un modelo de interés humano a partir del conocimiento externo. Se espera que dicho modelo permita el cálculo del interés humano en la oración con respecto al tema. Comparamos y contrastamos nuestro modelo con los modelos de respuesta de preguntas de definición actuales para mostrar que la interesante juega un factor importante en la respuesta de las preguntas de definición. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: modelos de recuperación;H.1.2 [Sistemas de usuario/máquina]: Algoritmos de términos generales de factores humanos, factores humanos, experimentación 1. Pregunta de definición Respuesta de respuesta Definición La respuesta se introdujo por primera vez en la Conferencia de recuperación de texto de la Conferencia de respuesta de respuesta a la tarea principal en 2003. Las preguntas de definición, también llamadas otras preguntas en los últimos años, se definen de la siguiente manera. Dada un tema de pregunta X, la tarea de un sistema de control de calidad definitivo es similar a responder la pregunta ¿Qué es X?¿O quién es x?. El sistema de control de calidad definitivo es buscar a través de un corpus de noticias y devolver un conjunto de respuestas que mejor describe el tema de la pregunta. Cada respuesta debe ser una pepita única específica del tema que constituya una faceta en la definición del tema de la pregunta.1.1 Los dos aspectos de las pepitas de tema oficialmente, las pepitas de respuesta específicas del tema o simplemente las pepitas de temas se describen como pepitas informativas. Cada pepita informativa es un fragmento de oración que describe alguna información objetiva sobre el tema. Dependiendo del tipo de tema y el dominio, esto puede incluir propiedades del tema, relaciones que el tema tiene con una entidad estrechamente relacionada o eventos que le sucedieron al tema. Desde la observación de la respuesta establecida para la respuesta de preguntas de definición de TREC 2003 a 2005, parece que un número significativo de temas que las pepitas no pueden describirse simplemente como pepitas informativas. Más bien, estas pepitas de tema tienen una calidad de trivia asociada con ellas. Por lo general, estos están fuera de la información ordinaria sobre un tema que puede despertar un interés de los lectores humanos. Por esta razón, decidimos definir las pepitas de respuesta que pueden evocar el interés humano como pepitas interesantes. En esencia, las pepitas interesantes responden las preguntas por las que es X famosa?, ¿Qué define x?¿O de qué se trata extraordinariamente x?. Ahora tenemos dos perspectivas muy diferentes sobre lo que constituye una respuesta a las preguntas de definición. Una respuesta puede ser una información objetiva importante sobre el tema o algún aspecto novedoso e interesante sobre el tema. Esta dualidad de informatividad e interesante se puede observar claramente en las cinco pepitas de respuesta vitales para un tema TREC 2005 de George Foreman. Ciertas respuestas nuggets son más informativas, mientras que otras pepitas son más interesantes en la naturaleza. Nuggets informativos: se graduó de Job Corps.- Se convirtió en el campeón mundial más antiguo en la historia del boxeo. Nuggets interesantes: ha prestado su nombre a la línea de productos de preparación de alimentos.- Waveed American Flag después de ganar el campeonato de las Olímpicas de 1968.- Regresó al boxeo después de 10 años. Como boxeador profesional de peso pesado profesional afroamericano, un lector humano promedio encontraría que las últimas tres pepitas sobre George Foreman son interesantes porque los boxeadores generalmente no prestan sus nombres a los productos de preparación de alimentos, ni los boxeadores se retiran durante 10 años antes de regresar al ring y convertirse en el ring y convertirse enEl campeón de boxeo más antiguo del mundo. El onda de Foreman de la bandera estadounidense en los Juegos Olímpicos es interesante porque la acción inocente hizo que algunos afroamericanos acusaran a Foreman de ser un tío Tom. Como se ve aquí, Nuggets interesantes tiene un factor sorpresa o una calidad única que los hace interesantes para los lectores humanos.1.2 Identificación de pepitas interesantes Dado que la descripción oficial original de las definiciones comprende identificar pepitas informativas, la mayoría de las investigaciones se han centrado completamente en identificar pepitas informativas. En este artículo, nos centramos en explorar las propiedades de las pepitas interesantes y desarrollar formas de identificar pepitas tan interesantes. Un sistema de respuesta de preguntas de definición de modelo de interés humano se desarrolla con énfasis en la identificación de pepitas interesantes para evaluar el impacto de las pepitas interesantes en el desempeño de un sistema de respuesta de preguntas de definición. Además, experimentamos con la combinación del modelo de interés humano con un sistema de respuesta de preguntas de definición basado en patrones léxicos para capturar pepitas informativas e interesantes.2. Trabajo relacionado Actualmente hay dos métodos generales para la respuesta de preguntas de definición. El método más común utiliza un enfoque léxico basado en el patrón fue propuesto por primera vez por Blair-Goldensohn et al.[1] y Xu et al.[14]. Ambos grupos usaron predominantemente patrones como cópulas y apositivos, así como patrones lexicosintácticos diseñados manualmente para identificar oraciones que contienen pepitas informativas. Por ejemplo, Xu et al.Usó 40 patrones estructurados definidos manualmente en su sistema de respuesta de preguntas de definición de 2003. Desde entonces, en un intento por capturar una clase más amplia de pepitas informativas, se han creado muchos sistemas de complejidad creciente. Un sistema reciente de Harabagiu et al.[6] creó un sistema de respuesta de preguntas de definición que combina el uso de 150 patrones positivos y negativos definidos manualmente, las relaciones con la entidad nombradas y plantillas de extracción de información especialmente elaboradas para 33 dominios objetivo. Aquí, una plantilla de músicos puede contener patrones léxicos que identifican información como el estilo musical de los músicos, las canciones cantadas por el músico y la banda, si los hay, a los que pertenece el músico. Como se puede imaginar, este es un enfoque intensivo de conocimiento que requiere un lingüista experto para definir manualmente todos los patrones léxicos o sintácticos posibles necesarios para identificar tipos específicos de información. Este proceso requiere mucho trabajo manual, experiencia y no es escalable. Esto conduce al desarrollo del enfoque de patrón suave de Cui et al.[4, 11]. En lugar de codificar patrones manualmente, las respuestas a las evaluaciones de respuesta de preguntas de definición anteriores se convirtieron en patrones genéricos y se capacita un modelo probabilístico para identificar tales patrones en las oraciones. Dada una posible oración de respuesta, el modelo probabilístico genera una probabilidad que indica cómo es probable que la oración coincida con uno o más patrones que el modelo ha visto en el entrenamiento. Se ha demostrado que dicho enfoque de patrones léxicososintácticos es experto en identificar pepitas informativas objetivas, como una fecha de nacimiento de personas, o el nombre de un CEO de compañías. Sin embargo, estos patrones son aplicables a nivel mundial a todos los temas o a un conjunto específico de entidades como músicos u organizaciones. Esto contrasta directamente con las pepitas interesantes que son altamente específicas para los temas individuales y no para un conjunto de entidades. Por ejemplo, las pepitas interesantes para George Foreman son específicas, solo George Foreman y ningún otro boxeador o ser humano. La especificidad del tema o la relevancia del tema es, por lo tanto, un criterio importante que ayuda a identificar pepitas interesantes. Esto lleva a la exploración del segundo enfoque basado en la relevancia que se ha utilizado en la respuesta de las preguntas de definición. Predominantemente, este enfoque se ha utilizado como un método de respaldo para identificar oraciones de definición cuando el método primario de los patrones léxicosisintácticos no pudo encontrar un número suficiente de pepitas informativas [1]. Un enfoque similar también se ha utilizado como sistema de referencia para TREC 2003 [14]. Más recientemente, Chen et al.[3] Adaptó un modelo de lenguaje bi-gram o bi-término para la respuesta de preguntas de definición. En general, el enfoque basado en la relevancia requiere un corpus de definición que contenga documentos altamente relevantes para el tema. El sistema de referencia en TREC 2003 simplemente usa las palabras temáticas como su corpus de definición. Blair-Goldensohn et al.[1] utiliza un alumno de máquina para incluir en las oraciones de Corpus definitonales que probablemente sean definitionales. Chen et al.[3] Recoja fragmentos de Google para construir su corpus de definición. Desde el corpus de definición, se crea un vector centralide de definición o se selecciona un conjunto de palabras centroides. Este vector centralide o un conjunto de palabras centroides se considera muy indicativas del tema. Luego, los sistemas pueden usar este centroide para identificar respuestas de definición utilizando una variedad de métricas de distancia para comparar con las oraciones encontradas en el conjunto de documentos recuperados para el tema. Blairgoldensohn et al.[1] utiliza similitud de coseno para clasificar las oraciones por centralidad. Chen et al.[3] construye un modelo de lenguaje BigRam utilizando los 350 términos de fragmentos de Google que se producen más frecuentes, descritos en su documento como un centroide ordenado, para estimar la probabilidad de que una oración sea similar al centroide ordenado. Como se describe aquí, el enfoque basado en la relevancia es altamente específico para los temas individuales debido a su dependencia de un corpus de definición específico del tema. Sin embargo, si las oraciones individuales se ven como un documento, los enfoques basados en relevancia utilizan esencialmente las palabras centroides específicas del tema recopiladas como una forma de recuperación de documentos con expansión de consulta automatizada para identificar oraciones muy relevantes. Por lo tanto, dichos métodos identifican oraciones relevantes y no oraciones que contienen pepitas de definición. Sin embargo, el sistema de referencia TREC 2003 [14] superó a todos menos a otro sistema. El modelo de lenguaje bi-término [3] puede informar resultados que son altamente competitivos para los resultados de vanguardia utilizando este enfoque basado en la recuperación. En TREC 2006, una suma ponderada simple de todos los términos modelo con términos ponderados utilizando únicamente los fragmentos de Google superaron a todos los demás sistemas por un margen significativo [7]. Creemos que las pepitas interesantes a menudo vienen en forma de trivia, hechos novedosos o raros sobre el tema que tienden a coincidir fuertemente con la mención directa de las palabras clave del tema. Esto puede explicar por qué el método basado en la relevancia puede funcionar de manera competitiva en la respuesta de las preguntas de definición. Sin embargo, simplemente comparar con un solo vector centralide o un conjunto de palabras centroide puede haber enfatizado sobre relevancia del tema y solo ha identificado pepitas de definición interesantes de manera indirecta. Aún así, los métodos de recuperación basados en relevancia se pueden utilizar como punto de partida para identificar pepitas interesantes. Describiremos cómo ampliamos tales métodos para identificar pepitas interesantes en la siguiente sección.3. Modelo de interés humano Obtener un sistema informático para identificar oraciones que un lector humano encontraría interesante es una tarea difícil. Sin embargo, hay muchos documentos en la red mundial que contienen resúmenes concisos y escritos humanos sobre cualquier tema. Además, estos documentos se escriben explícitamente para los seres humanos y contendrán información sobre el tema que la mayoría de los lectores humanos estarían interesados. Suponiendo que podemos identificar dichos documentos relevantes en la web, podemos aprovecharlos para ayudar a identificar respuestas definitionales a dichos temas. Podemos suponer que la mayoría de las oraciones encontradas dentro de estos documentos web contendrán facetas interesantes sobre el tema en cuestión. Esto simplifica enormemente el problema al de encontrar dentro de las oraciones del corpus de Aquaint similares a las que se encuentran en los documentos web. Este enfoque se ha utilizado con éxito en varios sistemas de respuesta a preguntas de Factoid y Listas [11] y creemos que el uso de dicho enfoque para la respuesta de definición u otra respuesta está justificada. La identificación de pepitas interesantes requiere calcular maquinaria para comprender el conocimiento mundial y la visión humana. Esta sigue siendo una tarea muy desafiante y el uso de documentos escritos humanos simplifica drásticamente la complejidad de la tarea. En este documento, informamos sobre dicho enfoque experimentando con un algoritmo de comparación de términos ponderado de distancia de edición de edición de nivel simple simple. Utilizamos el algoritmo de distancia de edición para obtener la similitud de un par de oraciones, con una oración proveniente de recursos web y la otra oración seleccionada del Corpus de Aquaint. A través de una serie de experimentos, mostraremos que incluso un enfoque tan simple puede ser muy efectivo en la respuesta de preguntas definitivas.3.1 Recursos web Existe en los artículos de Internet sobre cualquier tema que un humano pueda pensar. Además, muchos de estos artículos están ubicados en varios sitios web prominentes, lo que los convierte en una fuente de conocimiento mundial fácilmente accesible. Para nuestro trabajo para identificar pepitas interesantes, nos centramos en encontrar artículos cortos de uno o dos páginas en Internet que sean muy relevantes para nuestro tema deseado. Dichos artículos son útiles ya que contienen información concisa sobre el tema. Más importante aún, los artículos están escritos por humanos, para lectores humanos y, por lo tanto, contienen el conocimiento crítico del mundo humano que actualmente no puede capturar un sistema informático. Aprovechamos este conocimiento mundial mediante la recopilación de artículos para cada tema de los siguientes recursos externos para construir nuestro corpus de interés para cada tema. Wikipedia es una enciclopedia de contenido libre basada en la web escrita en colaboración por voluntarios. Este recurso ha sido utilizado por muchos sistemas de respuesta de preguntas como fuente de conocimiento sobre cada tema. Utilizamos una instantánea de Wikipedia tomada en marzo de 2006 e incluimos el artículo más relevante en el Corpus de Interest. Newslibrary es un archivo de búsqueda de artículos de noticias de más de 100 agencias de periódicos diferentes. Para cada tema, descargamos los 50 artículos más relevantes e incluimos el título y el primer párrafo de cada artículo en el Corpus de Interest. Los fragmentos de Google se recuperan emitiendo el tema como una consulta para el motor de búsqueda de Google. De los resultados de la búsqueda, extrajimos los 100 fragmentos principales. Si bien los fragmentos de Google no son artículos, encontramos que proporcionan una amplia cobertura de información autorativa sobre la mayoría de los temas. Debido a su cobertura integral de una amplia variedad de temas, los recursos anteriores forman la mayor parte de nuestro corpus de interés. También extraemos documentos de otros recursos. Sin embargo, como estos recursos son de naturaleza más específica, no siempre obtenemos ningún documento relevante. Estos recursos se enumeran a continuación. Biography.com es el sitio web del canal de cable de la televisión de biografía. El sitio web de los canales contiene biografías de búsqueda en más de 25,000 personas notables. Si el tema es una persona y podemos encontrar una biografía relevante en la persona, la incluimos en nuestro corpus de interés. Bartleby.com contiene una copia de búsqueda de varios recursos, incluida la Enciclopedia de Columbia, el World Factbook y varios diccionarios ingleses.S9.com es un diccionario de biografía sobre más de 33,000 personas notables. Al igual que Biography.com, incluimos la biografía más relevante que podemos encontrar en el Corpus de Interest. Definiciones de Google Google Search Engine ofrece una característica llamada definiciones que proporciona la definición para una consulta, si tiene una. Utilizamos esta función y extraemos cualquier definición que el motor de búsqueda de Google haya encontrado para cada tema en el Corpus de Interest. Figura 1: Arquitectura del modelo de interés humano. Wordnet Wordnet es un conocido léxico semántico electrónico para el idioma inglés. Además de agrupar palabras en inglés en conjuntos de sinónimos llamados sinsets, también proporciona una definición breve sobre el significado de las palabras que se encuentran en cada synset. Agregamos esta definición corta, si hay una, en nuestro Corpus de Interest. Tenemos dos usos principales para este Corpus de interés específico del tema, como fuente de oraciones que contienen pepitas interesantes y como modelo de lenguaje unigram de términos temáticos, I. 3.2 Múltiples centroides interesantes que hemos visto que las pepitas interesantes son altamente específicas para un tema. Enfoques basados en relevancia, como el modelo de lenguaje BigRam utilizado por Chen et al.[3] se centran en identificar oraciones altamente relevantes y recoger las pepitas de respuesta de definición como una consecuencia indirecta. Creemos que el uso de una sola colección de palabras centroides ha enfatizado la relevancia del tema y elegir en su lugar usar múltiples centroides. Dado que las oraciones en el corpus de interés de los artículos que recopilamos de Internet probablemente contengan pepitas que son de interés para los lectores humanos, esencialmente podemos usar cada oración como pseudocentroides. Cada oración en el Corpus de intereses esencialmente plantea un aspecto diferente del tema para su consideración como una oración de interés para los lectores humanos. Al realizar una comparación de oraciones por pares entre oraciones en el corpus de intereses y oraciones candidatas recuperadas del Corpus Aquaint, aumentamos el número de comparaciones de oraciones de O (N) a O (NM). Aquí, n es el número de oraciones candidatas potenciales y M es el número de oraciones en el corpus de intereses. A cambio, obtenemos una lista diversa de respuestas clasificadas que son individualmente similares a varias oraciones que se encuentran en el Corpus de interés de los temas. Una respuesta solo puede estar altamente clasificada si es muy similar a una oración en el Corpus de Interest, y también es muy relevante para el tema.3.3 Implementación La Figura 1 muestra la arquitectura del sistema para el sistema de control de calidad de definición basado en el interés humano propuesto. El módulo de recuperación de Aquaint que se muestra en la Figura 1 reutiliza un módulo de recuperación de documentos de un sistema actual de contestadores de preguntas de factoid y de preguntas que hemos implementado. Dado un conjunto de palabras que describen el tema, el módulo de recuperación de Aquaint consulta la expansión de la expansión de Google y busca un índice de documentos de Aquaint para recuperar los 800 documentos más relevantes para su consideración. El módulo de recuperación web, por otro lado, busca los recursos en línea descritos en la Sección 3.1 para obtener documentos interesantes para completar el Corpus de Interés. El Him Ranker, o el Módulo de clasificación del Modelo de Interés Humano, es la implementación de lo que se describe en este documento. El módulo primero construye el modelo de idioma unigram, I, a partir de los documentos web recopilados. Este modelo de idioma se utilizará para soportar la importancia de los términos dentro de las oraciones. A continuación, se utiliza un frase de oraciones para segmentar los 800 documentos recuperados en oraciones individuales. Cada una de estas oraciones puede ser una frase de respuesta potencial que se clasificará independientemente por la interesante. Clasificamos oraciones por interesante utilizando oraciones tanto del Corpus de Interés de documentos externos como del modelo de idioma unigram que creamos anteriormente que usamos para peso. Una oración candidata en nuestros 800 documentos de Aquaint relevantes principales se considera interesante si es muy similar en contenido a una oración que se encuentra en nuestra colección de documentos web externos. Para lograr esto, realizamos una comparación de similitud por pares entre una oración candidata y oraciones en nuestros documentos externos utilizando un algoritmo de distancia de edición a plazo ponderado. Los pesos de término se utilizan para ajustar la importancia relativa de cada término único que se encuentra en el corpus de intereses. Cuando ambas oraciones comparten el mismo término, el puntaje de similitud se incrementa en las dos veces el peso de los términos y cada término diferente disminuye el puntaje de similitud por el peso de términos diferentes. Elegimos el puntaje de similitud más alto para una sentencia candidata como el puntaje del modelo de interés humano para la sentencia candidata. De esta manera, cada oración candidata está clasificada por la interesante. Finalmente, para obtener el conjunto de respuestas, seleccionamos las 12 oraciones más altas y no redundantes como respuestas de definición para el tema.4. Experimentos iniciales El sistema basado en intereses humanos descrito en la sección anterior está diseñado para identificar solo pepitas interesantes y no pepitas informativas. Por lo tanto, se puede describir como un sistema discapacitado que solo trata con la mitad del problema en la respuesta de las preguntas de definición. Esto se hace para explorar cómo la interesante juega un factor en las respuestas de definición. Para comparar y contrastar las diferencias entre las pepitas informativas e interesantes, también implementamos el modelo BigRam de patrón suave propuesto por Cui et al.[4, 11]. Para garantizar resultados comparables, ambos sistemas reciben datos de entrada idénticos. Dado que ambos sistemas requieren el uso de recursos externos, ambos se proporcionan los mismos artículos web recuperados por nuestro módulo de recuperación web. Ambos sistemas también clasifican el mismo conjunto de oraciones candidatas en forma de 800 documentos más relevantes que recuperan nuestro módulo de recuperación de Aquaint. Para los experimentos, utilizamos el conjunto de preguntas TREC 2004 para ajustar los parámetros del sistema y utilizar los conjuntos de preguntas TREC 2005 para probar los dos sistemas. Ambos sistemas se evalúan los resultados utilizando la metodología de puntuación estándar para las definiciones TREC. TREC proporciona una lista de pepitas vitales y bien para cada tema de la pregunta. Cada pregunta se califica en Nugget RetRink (NR) y Nugget Precision (NP) y se calcula una sola puntuación final utilizando la medición F (ver ecuación 1) con β = 3 para enfatizar el retiro de Nugget. Aquí, NR es el número de pepitas vitales devueltas divididas por el número total de pepitas vitales, mientras que NP se calcula utilizando una función de longitud de carácter mínima permitida definida en [12]. La evaluación se realiza automáticamente usando Pourpre V1.0C [10]. Fscore = β2 ∗ np ∗ nr (β2 + 1) np + nr (1) sistema f3-score mejor sistema trec 2005 0.2480 patrón suave (sp) 0.2872 modelo de interés humano (HIT) 0.3031 Tabla 1: rendimiento en trec 2005 set de preguntasFigura 2: rendimiento por tipos de entidades.4.1 Informatividad versus Interesante Nuestro primer experimento compara el rendimiento de identificar únicamente pepitas interesantes contra la identificación únicamente de pepitas informativas. Comparamos los resultados alcanzados por el modelo de interés humano que solo identifican pepitas interesantes con los resultados del modelo de búsqueda de patrones sintácticos, así como el resultado del sistema de definición de mayor rendimiento en TREC 2005 [13]. La Tabla 1 muestra la puntuación F3 de los tres sistemas para el conjunto de preguntas TREC 2005. El modelo de interés humano supera claramente tanto el patrón suave como el mejor sistema TREC 2005 con una puntuación F3 de 0.303. El resultado también es comparable con el resultado de una carrera manual humana, que alcanzó una puntuación F3 de 0.299 en el mismo conjunto de preguntas [9]. Este resultado es una confirmación de que las nuggets interesantes juegan un papel importante en la captura de respuestas de definición, y puede ser más vital que usar información para encontrar patrones léxicos. Para obtener una mejor perspectiva de qué tan bien se desempeña el modelo de interés humano para diferentes tipos de temas, dividimos manualmente los temas de TREC 2005 en cuatro amplias categorías de persona, organización, cosa y evento como se enumeran en la Tabla 3. Estas categorías se ajustan a la división general de TREC de temas de preguntas en 4 tipos principales de entidad [13]. El rendimiento del modelo de interés humano y el modelo BigRam de patrón suave para cada tipo de entidad se pueden ver en la Figura 2. Ambos sistemas exhiben un comportamiento consistente en todos los tipos de entidades, con el mejor rendimiento proveniente de temas de persona y organización y el peor rendimiento de las cosas y los temas de eventos. Esto se puede atribuir principalmente a nuestra selección de recursos basados en la web para el corpus de definición utilizado por ambos sistemas. En general, es más difícil localizar un solo artículo web que describa un evento o un objeto general. Sin embargo, dado el mismo conjunto de información basada en la web, el modelo de interés humano supera constantemente el modelo de patrón suave para los cuatro tipos de entidades. Esto sugiere que el modelo de interés humano está mejor capaz de aprovechar la información que se encuentra en los recursos web para identificar respuestas de definición.5. Refinamientos alentados por los resultados experimentales iniciales, exploramos dos optimización adicional del algoritmo básico.5.1 Ponderación de términos interesantes La palabra trivia se refiere a las cositas de información sin importancia o poco común. Como hemos señalado, las pepitas interesantes a menudo tienen una cualidad trivial que les hace interesarse a los seres humanos. A partir de esta descripción de pepitas y trivia interesantes, planteamos la hipótesis de que es probable que ocurran nuggets interesantes raramente en un corporaciones de texto. Existe la posibilidad de que algunos términos de baja frecuencia realmente sean importantes para identificar pepitas interesantes. Un modelo de lenguaje unigram estándar no capturaría estos términos de baja frecuencia como términos importantes. Para explorar esta posibilidad, experimentamos con tres esquemas de ponderación a término que pueden proporcionar más peso a ciertos términos de baja frecuencia. Los esquemas de ponderación que consideramos incluyen TFIDF de uso común, así como la divergencia teórica de Kullback-Leiber de la información y la divergencia de Jensen-Shannon [8]. TFIDF, o frecuencia de término × frecuencia de documento inverso, es un esquema de ponderación de recuperación de información estándar que equilibra la importancia de un término en un documento y en un corpus. Para nuestros experimentos, calculamos el peso de cada término como tf × log (n nt), donde tf es el término frecuencia, nt es el número de oraciones en el corpus de intereses que tiene el término y n es el número total de oraciones en elCorpus de interés. La divergencia de Kullback-Leibbler (Ecuación 2) también se llama divergencia de KL o entropía relativa, puede considerarse que mide la diferencia entre dos distribuciones de probabilidad. Aquí, tratamos al Corpus de Aquaint como un modelo de idioma unigram de inglés general [15], A, y el Corpus de Interest como un modelo de idioma unigram que consiste en términos específicos del tema y términos generales de inglés, I. Es probable que las palabras en inglés en general tengan distribuciones similares tanto en los modelos de idiomas I como en A. Por lo tanto, el uso de la divergencia de KL como un esquema de ponderación de término hará que se dan pesos fuertes a términos específicos de temas porque su distribución en el corpus de interés ocurren significativamente más a menudo o menos a menudo que en el inglés general. De esta manera, los términos centroides de alta frecuencia, así como los términos raros pero específicos del tema de baja frecuencia, se identifican y se ponderan altamente usando divergencia KL. Dkl (i a) = t i (t) log i (t) a (t) (2) Debido a la distribución de términos de la ley de potencia en el lenguaje natural, solo hay un pequeño número de términos muy frecuentes y una gran cantidad de rarosTérminos tanto en I como en A. Si bien los términos comunes en inglés consisten en palabras de parada, los términos comunes en el corpus específico del tema, I, consisten en palabras de parada y palabras temáticas relevantes. Estas palabras específicas del tema de alta frecuencia ocurren con mucha más frecuencia en I que en A. Como resultado, encontramos que la divergencia de KL tiene un sesgo hacia términos de tema altamente frecuentes, ya que estamos midiendo la disimilitud directa contra un modelo de inglés general donde tales términos temáticos son muy raros. Por esta razón, exploramos otra medida de divergencia como un posible esquema de ponderación de término. La divergencia o la divergencia de Jensen-Shannon se extiende sobre la divergencia de KL como se ve en la ecuación 3. Al igual que con la divergencia KL, también usamos la divergencia JS para medir la diferencia entre nuestros dos modelos de lenguaje, I y A. DJS (i a) = 1 2 ¢ dkl i i+a 2 ¡+dkl a i+a 2 ¡£ (3 (3) Figura 3: rendimiento por varios esquemas de ponderación a término en el modelo de interés humano. Sin embargo, la divergencia JS tiene propiedades adicionales1 de ser simétricos y no negativos como se ve en la ecuación 4. La propiedad simétrica ofrece una medida más equilibrada de disimilitud y evita el sesgo que tiene la divergencia KL. DJS (i a) = djs (a i) = 0 i = a> 0 i <> a (4) Realizamos otro experimento, sustituyendo el esquema de ponderación del modelo de lenguaje unigram que utilizamos en los experimentos iniciales con los esquemas de ponderación de tres términos descritos anteriormente. Como referencia de límite inferior, incluimos un esquema de ponderación de término que consiste en una constante 1 para todos los términos. La Figura 3 muestra el resultado de aplicar los cinco esquemas de ponderación a término en el modelo de interés humano. TFIDF realizó lo peor como habíamos anticipado. La razón es que la mayoría de los términos solo aparecen una vez dentro de cada oración, lo que resulta en una frecuencia de término de 1 para la mayoría de los términos. Esto hace que el componente de las FDI sea el factor principal en la puntuación de las oraciones. A medida que calculamos la frecuencia de documentos inversos para los términos en el Corpus de Intereses recopilado de los recursos web, las FDI sean de gran peso, términos de temas de alta frecuencia y términos relevantes. Esto da como resultado que TFIDF favorezca todos los términos de baja frecuencia en términos de alta frecuencia en el corpus de intereses. A pesar de esto, el esquema de ponderación TFIDF solo obtuvo un ligero 0.0085 inferior a nuestra referencia límite inferior de pesos constantes. Vemos esto como una indicación positiva de que los términos de baja frecuencia pueden ser útiles para encontrar pepitas interesantes. Tanto la divergencia KL como JS funcionó marginalmente mejor que el esquema probabilístico del modelo de lenguaje uniforme que utilizamos en nuestros experimentos iniciales. A partir de la inspección de la lista de términos ponderados, observamos que, si bien los términos relevantes de baja frecuencia aumentaron en resistencia, los términos relevantes de alta frecuencia aún dominan la parte superior de la lista de términos ponderado. Solo un puñado de términos de baja frecuencia se ponderaron tan fuertemente como las palabras clave de los temas y combinadas con su baja frecuencia, pueden haber limitado el impacto de la re-peso de dichos términos. Sin embargo, creemos que a pesar de esto, la divergencia de Jensen-Shannon proporciona un aumento pequeño pero medible en el rendimiento de nuestro modelo de interés humano.1 JS Divergencia también tiene la propiedad de estar limitado, lo que permite que los resultados se traten como una probabilidad si es necesario. Sin embargo, la propiedad limitada no se requiere aquí, ya que solo estamos tratando la divergencia calculada por la divergencia JS como Pesos de término 5.2 Selección de recursos web en uno de nuestros experimentos iniciales, observamos que la calidad de los recursos web incluidos en el Corpus de Interest puede tener unImpacto directo en los resultados que obtenemos. Queríamos determinar qué impacto tiene la elección de los recursos web en el desempeño de nuestro modelo de interés humano. Por esta razón, dividimos nuestra colección de recursos web en cuatro grupos principales enumerados aquí: N - Noticias: Título y primer párrafo de los 50 artículos más relevantes que se encuentran en el biblioteca de periódicos. W - Wikipedia: texto del artículo más relevante que se encuentra en Wikipedia. S - Fragmentos: fragmentos extraídos de los 100 enlaces más relevantes después de consultar Google. M - Fuentes misceláneas: combinación de contenido (cuando esté disponible) de fuentes secundarias, incluidas Biography.com, S9.com, Bartleby.com, definiciones de Google y definiciones de WordNet. Realizamos una gama de carreras en el conjunto de preguntas TREC 2005 utilizando todas las combinaciones posibles de los cuatro grupos de recursos web anteriores para identificar la mejor combinación posible. Todas las ejecuciones se realizaron en el modelo de interés humano utilizando la divergencia JS como esquema de ponderación del término. Las ejecuciones se clasificaron en puntaje F3 descendente y las 3 mejores ejecuciones de mejor rendimiento para cada clase de entidad se enumeran en la Tabla 2 junto con las puntuaciones F3 informadas anteriores de la Figura 2 como referencia de línea de base. Se puede observar una tendencia consistente para cada clase de entidad. Para los temas de persona y eventos, los artículos de biblioteca de periódicos son la principal fuente de pepitas interesantes con fragmentos de Google y artículos diversos que ofrecen evidencia de apoyo adicional. Esto parece intuitivo para los eventos, ya que los periódicos se centran predominantemente en informar eventos de ruptura y, por lo tanto, son excelentes fuentes de pepitas interesantes. Esperábamos que Wikipedia en lugar de los artículos de noticias fuera una mejor fuente de datos interesantes sobre las personas y nos sorprendió descubrir que los artículos de noticias superaron a Wikipedia. Creemos que la razón es porque las personas seleccionadas como temas hasta ahora han sido celebridades o figuras públicas bien conocidas. Es probable que los lectores humanos estén interesados en los eventos de noticias que destinen a estas personalidades. Por el contrario de los temas de organización y cosas, la mejor fuente de pepitas interesantes proviene del artículo más relevante de Wikipedias sobre el tema con Google Fnippets nuevamente proporcionando información adicional para las organizaciones. Con un Oracle que puede clasificar temas por clase de entidad con una precisión del 100% y mediante el uso de los mejores recursos web para cada clase de entidad como se muestra en la Tabla 2, podemos lograr una puntuación F3 de 0.3158.6. Uniendo la información con interesante, hasta ahora hemos estado comparando el modelo de interés humano con el modelo de patrón suave para comprender las diferencias entre pepitas interesantes e informativas. Sin embargo, desde la perspectiva de un lector humano, las pepitas informativas e interesantes son útiles y definitivamente. Las pepitas informativas presentan una descripción general del tema, mientras que las pepitas interesantes brindan a los lectores una profundidad y una visión adicionales al proporcionar aspectos novedosos y únicos sobre el tema. Creemos que un buen sistema de respuesta de preguntas de definición debería proporcionar al lector una mezcla combinada de ambos tipos de pepitas como un conjunto de respuestas de definición. Rango de persona orgía de la organización del evento Base Base Unigram Scheming, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S W+S W+S W+S W+S W+S W+S W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Tabla 2: Top 3 ejecutas usando diferentes recursos web para cada clase de entidad ahora ahora We Now We Wetener dos expertos muy diferentes para identificar definiciones. El modelo BigRam de patrón suave propuesto por Cui et al.es un experto en la identificación de pepitas informativas. El modelo de interés humano que hemos descrito en este documento, por otro lado, es un experto en encontrar pepitas interesantes. Inicialmente esperábamos unificar los dos sistemas de respuesta de preguntas de definición separadas aplicando un método de aprendizaje de conjunto [5], como votar o aumentar para lograr una buena mezcla de pepitas informativas e interesantes en nuestro conjunto de respuestas. Sin embargo, ninguno de los métodos de aprendizaje de conjunto que intentamos podría superar a nuestro modelo de interés humano. La razón es que ambos sistemas están recogiendo oraciones muy diferentes como respuestas de definición. En esencia, nuestros dos expertos están en desacuerdo sobre qué oraciones son definitivas. En las 10 oraciones principales de ambos sistemas, solo el 4.4% de estas oraciones aparecieron en ambos conjuntos de respuestas. Las respuestas restantes fueron completamente diferentes. Incluso cuando examinamos las 500 oraciones principales generadas por ambos sistemas, la tasa de acuerdo seguía siendo un 5.3%extremadamente baja. Sin embargo, a pesar de la baja tasa de acuerdo entre ambos sistemas, cada sistema individual aún puede alcanzar una puntuación F3 relativamente alta. Existe la clara posibilidad de que cada sistema pueda seleccionar diferentes oraciones con diferentes estructuras sintácticas, pero en realidad tiene el mismo contenido semántico o similar. Esto podría dar como resultado que ambos sistemas tengan las mismas pepitas marcadas como correctas a pesar de que las oraciones de respuesta de origen son estructuralmente diferentes. Desafortunadamente, no podemos verificar esto automáticamente, ya que el software de evaluación que estamos utilizando no informa correctamente las pepitas de respuesta identificadas correctamente. Para verificar si ambos sistemas seleccionan las mismas pepitas de respuesta, seleccionamos aleatoriamente un subconjunto de 10 temas del conjunto de preguntas TREC 2005 e identificamos manualmente las pepitas de respuesta correctas (según lo definido por los accesorios TREC) de ambos sistemas. Cuando comparamos las pepitas de respuesta encontradas por ambos sistemas para este subconjunto de temas, encontramos que la tasa de acuerdo de pepita entre ambos sistemas era del 16,6%. Si bien la tasa de acuerdo de Nugget es más alta que la tasa de acuerdo de oración, ambos sistemas generalmente siguen recogiendo diferentes pepitas de respuesta. Vemos esto como una indicación adicional de que las definiciones están formadas por una mezcla de pepitas informativas e interesantes. También es una indicación de que, en general, las pepitas interesantes e informativas son bastante diferentes en la naturaleza. Por lo tanto, existen razones racionales y motivación práctica para unificar respuestas de los enfoques basados en patrones y basados en el corpus. Sin embargo, las diferencias entre los dos sistemas también causan problemas cuando intentamos combinar ambos conjuntos de respuestas. Actualmente, el mejor enfoque que encontramos para combinar ambos conjuntos de respuestas es fusionar y volver a clasificar ambos conjuntos de respuestas con acuerdos de aumento. Primero normalizamos las 1,000 oraciones clasificadas principales de cada sistema, para obtener el puntaje del modelo de interés humano normalizado, él (s) y el puntaje del modelo BigRam de patrón suave normalizado, SP (s), para cada oración única, s.Para cada oración, los dos puntajes separados para luego se unifican en una sola puntuación utilizando la Ecuación 5. Cuando solo un sistema cree que la oración es definitiva, simplemente conservamos que los sistemas normalizaron la puntuación como puntaje unificado. Cuando ambos sistemas están de acuerdo en que la oración es definitiva, el puntaje de oraciones se ve impulsado por el grado de acuerdo entre ambos sistemas. Puntaje (s) = max (shim, ssp) 1-min (shim, ssp) (5) Para mantener un conjunto diverso de respuestas, así como para garantizar que las oraciones similares no tengan una clasificación similar, volvemos a clasificar aún másNuestra lista combinada de respuestas utilizando relevancia marginal máxima o MMR [2]. Usando el enfoque descrito aquí, logramos una puntuación F3 de 0.3081. Esta puntuación es equivalente a la puntuación inicial del modelo de interés humano de 0.3031, pero no supera al modelo de modelo de interés humano optimizado.7. Conclusión Este documento ha presentado una perspectiva novedosa para responder preguntas de definición a través de la identificación de pepitas interesantes. Las pepitas interesantes son información poco común sobre el tema que puede evocar una curiosidad de los lectores humanos. La noción de un lector humano promedio es una consideración importante en nuestro enfoque. Esto es muy diferente del enfoque de patrón léxico-sintáctico donde el contexto de un lector humano ni siquiera se considera al encontrar respuestas para la respuesta de las preguntas de definición. Usando esta perspectiva, hemos demostrado que utilizando una combinación de un corpus externo cuidadosamente seleccionado, que coincide con múltiples centroides y teniendo en cuenta términos raros pero altamente específicos de temas, podemos construir un módulo de respuesta de pregunta definitivo que se centre más en identificar pepitas que sonde interés para los seres humanos. Los resultados experimentales han demostrado que este enfoque puede superar significativamente a los sistemas de contestadores de preguntas definitionales de última generación. Además, demostramos que se requieren al menos dos tipos diferentes de respuestas que se requieren nuggets para formar un conjunto más exhaustivo de respuestas de definición. Lo que parece ser un buen conjunto de respuestas de definición es una información general que proporciona una visión general informativa rápida mezclada con algunos aspectos novedosos o interesantes sobre el tema. Por lo tanto, creemos que un buen sistema de respuesta de preguntas de definición necesitaría recoger tipos de pepitas informativos e interesantes para proporcionar una cobertura de definición completa sobre todos los aspectos importantes del tema. Si bien hemos intentado construir dicho sistema combinando nuestro modelo de interés humano propuesto con el modelo BigRam de patrón blando de Cui et al., Las diferencias inherentes entre ambos tipos de pepitas aparentemente causadas por las bajas tasas de acuerdo entre ambos modelos han hecho que esto sea difíciltarea. De hecho, esto es natural ya que los dos modelos han sido diseñados para identificar dos tipos muy diferentes de respuestas de definición utilizando tipos de características muy diferentes. Como resultado, actualmente solo podemos lograr un sistema híbrido que tenga el mismo nivel de rendimiento que nuestro modelo de interés humano propuesto. Abordamos el problema de la respuesta de las preguntas definitivas desde una perspectiva novedosa, con la noción de que el factor de interés juega un papel en la identificación de respuestas definitivas. Aunque los métodos que utilizamos son simples, se ha demostrado que experimentalmente son efectivos. Nuestro enfoque también puede proporcionar una idea de algunas anomalías en los ensayos de respuesta de preguntas de definición pasadas. Por ejemplo, el sistema de definición superior en la reciente evaluación de TREC 2006 pudo superar significativamente a todos los demás sistemas utilizando probabilidades unigram relativamente simples extraídas de los fragmentos de Google. Sospechamos que el principal contribuyente a la organización de temas de tipo de entidad de rendimiento de sistemas de la Universidad DePauw, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefónica de España, Lions Club International, Amway, McDonalds Corporation, Harley-Davidson, EE. UU. Academia Naval, OPEP, OTAN, Oficina Internacional de la Unión Postal Universal (UPU), Organización de la Conferencia Islámica (OIC), PBGC Persona Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il Thing F16, Bollywood, Viagra, Howdy Dooddod Show, Museum, Museo, Meteoritas, Virginia Wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, Kudzu, Medalla de Honor de EE. UU., Tsunami, Genoma, Acuerdo de Alimentos para el Propiedad, Shiite, Kinmen Island Event Submarine Kursk Fregads, Miss Universo 2000Coronada, Port Arthur Massacre, Francia gana la Copa Mundial en fútbol, clips de avión cables de cable en el complejo italiano, tiroteo en la escuela Kip Kinkel, accidente del vuelo 990 de Egyptairio, Preakness 1998, primer debate presidencial de 2000 Bush-Gore, 1998 acusación y juicio de Susan McDougal, Regreso de Hong Kong a la soberanía china, 1998 Juegos Olímpicos de Nagano, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens Eruption, 1998 Baseball World Series, Hindenburg Desastre, Huracán Mitch Tabla 3: Temas de TREC 2005 agrupados porEl tipo de entidad es el algoritmo de PageRank Googles, que consideró principalmente el número de vínculos, tiene un efecto indirecto de clasificar los documentos web por grado de interés humano. En nuestro trabajo futuro, buscamos mejorar aún más el sistema combinado incorporando más evidencia en apoyo de las respuestas de definición correctas o para filtrar las respuestas obviamente incorrectas.8. Referencias [1] S. Blair-Goldensohn, K. R. McKeown y A. H. Schlaikjer. Un enfoque híbrido para las preguntas de definición de la pista de control de calidad. En TREC 03: Actas de la 12ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell y J. Goldstein. El uso de MMR, Reranking basado en la diversidad para reordenar documentos y producir resúmenes. En investigación y desarrollo en recuperación de información, páginas 335-336, 1998. [3] Y. Chen, M. Zhou y S. Wang. Respuestas para volver a ser un control de definición utilizando el modelado de idiomas. En Actas de la 21a Conferencia Internacional sobre Lingüística Computacional y la 44ª Reunión Anual de la Asociación de Lingüística Computacional, páginas 1081-1088, Sydney, Australia, julio de 2006. Asociación de Lingüística Computacional.[4] H. Cui, M.-Y. Kan y T.-S.Chua. Modelos genéricos de patrones suaves para la respuesta de preguntas de definición. En Sigir 05: Actas de la 28ª Conferencia Internacional ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 384-391, Nueva York, NY, EE. UU., 2005. ACM Press.[5] T. G. Dietterich. Métodos de conjunto en el aprendizaje automático. Notas de conferencia en informática, 1857: 1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl y P. Wang. Empleando dos sistemas de respuesta de preguntas en TREC 2005. En TREC 05: Actas de la 14ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible y B. Webber. Experimentos en la Universidad de Edimburgo para la pista de control de calidad TREC 2006. En Trec 06 Notebook: Actas de la 14ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2006. Instituto Nacional de Normas y Tecnología.[8] J. Lin. Medidas de divergencia basadas en la entropía de Shannon. IEEE Transactions on Information Theory, 37 (1): 145 - 151, enero de 1991. [9] J. Lin, E. Abels, D. Demner -Fushman, D. W. Oard, P. Wu e Y. Wu. Una colección de pistas en Maryland: Hard, Enterprise, QA y Genomics, ¡Oh, Dios mío! En TREC 05: Actas de la 14a Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. [10] J. Lin y D. Demner-Fushman. Evaluar automáticamente las respuestas a las preguntas de definición. En Actas de la Conferencia de Tecnología del Lenguaje Humano y la Conferencia sobre Métodos Empíricos en Procesamiento del Lenguaje Natural, páginas 931-938, Vancouver, Columbia Británica, Canadá, octubre de 2005. Asociación de Lingüística Computacional.[11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S.Chua y M.-Y. Kan. Uso del análisis de relaciones sintácticas y semánticas en la respuesta de cuestión. En TREC 05: Actas de la 14a Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees. Descripción general de la pista de respuesta a la pregunta TREC 2003. En la Conferencia de Recuperación de Textos 2003, Gaithersburg, Maryland, 2003. Instituto Nacional de Normas y Tecnología.[13] E. M. Voorhees. Descripción general de la pista de respuesta a la pregunta TREC 2005. En TREC 05: Actas de la 14ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2005. Instituto Nacional de Normas y Tecnología.[14] J. Xu, A. Licuanan y R. Weischedel. TREC 2003 QA en BBN: Respondiendo preguntas de definición. En TREC 03: Actas de la 12ª Conferencia de Recuperación de Textos, Gaithersburg, Maryland, 2003. [15] D. Zhang y W. S. Lee. Un enfoque de modelado de idiomas para la respuesta de las preguntas del pasaje. En TREC 03: Actas de la 12ª Conferencia de recuperación de texto, Gaithersburg, Maryland, 2003.",
    "original_sentences": [
        "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
        "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
        "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
        "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
        "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
        "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
        "The Definition questions, also called Other questions in recent years, are defined as follows.",
        "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
        "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
        "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
        "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
        "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
        "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
        "Rather, these topic nuggets have a trivia-like quality associated with them.",
        "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
        "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
        "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
        "We now have two very different perspective as to what constitutes an answer to Definition questions.",
        "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
        "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
        "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
        "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
        "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
        "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
        "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
        "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
        "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
        "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
        "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
        "RELATED WORK There are currently two general methods for Definitional Question Answering.",
        "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
        "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
        "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
        "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
        "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
        "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
        "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
        "This process requires a lot of manual labor, expertise and is not scalable.",
        "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
        "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
        "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
        "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
        "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
        "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
        "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
        "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
        "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
        "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
        "A similar approach has also been used as a baseline system for TREC 2003 [14].",
        "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
        "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
        "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
        "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
        "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
        "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
        "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
        "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
        "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
        "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
        "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
        "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
        "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
        "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
        "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
        "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
        "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
        "This may explain why relevance-based method can perform competitively in definitional question answering.",
        "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
        "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
        "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
        "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
        "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
        "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
        "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
        "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
        "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
        "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
        "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
        "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
        "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
        "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
        "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
        "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
        "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
        "Such articles are useful as they contain concise information about the topic.",
        "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
        "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
        "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
        "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
        "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
        "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
        "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
        "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
        "From the search results, we extracted the top 100 snippets.",
        "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
        "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
        "We also extracted documents from other resources.",
        "However, as these resources are more specific in nature, we do not always get any single relevant document.",
        "These resources are listed below.",
        "Biography.com is the website for the Biography television cable channel.",
        "The channels website contains searchable biographies on over 25,000 notable people.",
        "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
        "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
        "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
        "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
        "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
        "Figure 1: Human Interest Model Architecture.",
        "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
        "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
        "We add this short definition, if there is one, into our Interest Corpus.",
        "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
        "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
        "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
        "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
        "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
        "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
        "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
        "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
        "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
        "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
        "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
        "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
        "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
        "The module first builds the unigram language model, I, from the collected web documents.",
        "This language model will be used to weight the importance of terms within sentences.",
        "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
        "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
        "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
        "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
        "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
        "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
        "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
        "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
        "In this manner, every candidate sentence is ranked by interestingness.",
        "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
        "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
        "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
        "This is done in order to explore how interestingness plays a factor in definitional answers.",
        "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
        "In order to ensure comparable results, both systems are provided identical input data.",
        "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
        "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
        "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
        "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
        "TREC provides a list of vital and okay nuggets for each question topic.",
        "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
        "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
        "The evaluation is automatically conducted using Pourpre v1.0c [10].",
        "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
        "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
        "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
        "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
        "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
        "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
        "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
        "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
        "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
        "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
        "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
        "In general, it is harder to locate a single web article that describes an event or a general object.",
        "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
        "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
        "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
        "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
        "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
        "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
        "A standard unigram language model would not capture these low-frequency terms as important terms.",
        "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
        "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
        "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
        "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
        "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
        "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
        "General English words are likely to have similar distributions in both language models I and A.",
        "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
        "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
        "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
        "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
        "These high frequency topic specific words occur very much more frequently in I than in A.",
        "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
        "For this reason, we explored another divergence measure as a possible term weighting scheme.",
        "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
        "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
        "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
        "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
        "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
        "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
        "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
        "TFIDF performed the worst as we had anticipated.",
        "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
        "This causes the IDF component to be the main factor in scoring sentences.",
        "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
        "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
        "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
        "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
        "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
        "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
        "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
        "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
        "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
        "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
        "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
        "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
        "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
        "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
        "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
        "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
        "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
        "A consistent trend can be observed for each entity class.",
        "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
        "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
        "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
        "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
        "Human readers are likely to be interested in news events that spotlight these personalities.",
        "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
        "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
        "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
        "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
        "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
        "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
        "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
        "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
        "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
        "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
        "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
        "The reason is that both systems are picking up very different sentences as definitional answers.",
        "In essence, our two experts are disagreeing on which sentences are definitional.",
        "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
        "The remaining answers were completely different.",
        "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
        "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
        "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
        "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
        "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
        "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
        "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
        "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
        "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
        "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
        "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
        "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
        "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
        "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
        "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
        "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
        "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
        "Using the approach described here, we achieve a F3 score of 0.3081.",
        "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
        "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
        "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
        "The notion of an average human reader is an important consideration in our approach.",
        "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
        "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
        "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
        "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
        "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
        "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
        "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
        "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
        "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
        "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
        "Although the methods we used are simple, they have been shown experimentally to be effective.",
        "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
        "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
        "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
        "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
        "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
        "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
        "A hybrid approach for qa track definitional questions.",
        "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
        "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
        "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
        "Reranking answers for definitional qa using language modeling.",
        "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
        "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
        "Kan, and T.-S. Chua.",
        "Generic soft pattern models for definitional question answering.",
        "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
        "ACM Press. [5] T. G. Dietterich.",
        "Ensemble methods in machine learning.",
        "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
        "Employing two question answering systems at trec 2005.",
        "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
        "Experiments at the university of edinburgh for the trec 2006 qa track.",
        "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
        "National Institute of Standards and Technology. [8] J. Lin.",
        "Divergence measures based on the shannon entropy.",
        "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
        "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
        "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
        "Automatically evaluating answers to definition questions.",
        "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
        "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
        "Kan.",
        "Using syntactic and semantic relation analysis in question answering.",
        "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
        "Overview of the trec 2003 question answering track.",
        "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
        "National Institute of Standards and Technology. [13] E. M. Voorhees.",
        "Overview of the trec 2005 question answering track.",
        "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
        "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
        "TREC 2003 QA at BBN: Answering definitional questions.",
        "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
        "A language modeling approach to passage question answering.",
        "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
    ],
    "error_count": 0,
    "keys": {
        "use of linguistic": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the <br>use of linguistic</br> or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nuggets interesantes y su impacto en las preguntas de definición respondiendo a Kian-Wei Kor Departamento de Informática Escuela de Computación Universidad Nacional de Singapur dkor@comp.nus.edu.sg Tat-Seng Chua Departamento de Ciencias de la Computación Escuela de Computación de la Universidad Nacional de Singapur@comp.nus.edu.sg Enfoques actuales abstractos para identificar oraciones de definición en el contexto de la respuesta de preguntas implica principalmente el \"uso de patrones lingüísticos\" o sintácticos para identificar pepitas informativas.Uso de lingüístico"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "linguistic use": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "external knowledge": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from <br>external knowledge</br>.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este documento propone abordar la deficiencia construyendo un modelo de interés humano a partir de \"conocimiento externo\".conocimiento externo"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "computation of human interest": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the <br>computation of human interest</br> in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Se espera que dicho modelo permita el \"cálculo del interés humano\" en la oración con respecto al tema.Cálculo del interés humano"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "human interest computation": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "news corpus": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a <br>news corpus</br> and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El sistema de control de calidad definitivo es buscar a través de un \"corpus de noticias\" y devolver un conjunto de respuestas que mejor describe el tema de la pregunta.cuerpo de noticias"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "question topic": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a <br>question topic</br> X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the <br>question topic</br>.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the <br>question topic</br>. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each <br>question topic</br>.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Dado un \"tema de la pregunta\" x, la tarea de un sistema de control de calidad definitivo es similar a responder la pregunta ¿Qué es X?¿O quién es x?.tema de preguntas",
                "El sistema de control de calidad definitivo es buscar a través de un corpus de noticias y devolver un conjunto de respuestas que mejor describe el \"tema de la pregunta\".tema de preguntas",
                "Cada respuesta debe ser una pepita única específica del tema que constituya una faceta en la definición del \"tema de la pregunta\".1.1 Los dos aspectos de las pepitas de tema oficialmente, las pepitas de respuesta específicas del tema o simplemente las pepitas de temas se describen como pepitas informativas.tema de preguntas",
                "TREC proporciona una lista de pepitas vitales y bien para cada \"tema de la pregunta\".tema de preguntas"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "informative nugget": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each <br>informative nugget</br> is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cada \"pepita informativa\" es un fragmento de oración que describe cierta información objetiva sobre el tema.pepita informativa"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "sentence fragment": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a <br>sentence fragment</br> that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Cada pepita informativa es un \"fragmento de oración\" que describe cierta información objetiva sobre el tema.fragmento de oración"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "human reader": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average <br>human reader</br> would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a <br>human reader</br> would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a <br>human reader</br>, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average <br>human reader</br> is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a <br>human reader</br> is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como boxeador profesional de peso pesado profesional afroamericano, un \"lector humano\" promedio encontraría las últimas tres pepitas sobre George Foreman interesante porque los boxeadores generalmente no prestan sus nombres a los productos de preparación de alimentos, ni los boxeadores se retiran durante 10 años antes de regresar al anilloy convertirse en el campeón de boxeo más antiguo del mundo.lector humano",
                "Modelo de interés humano Obtener un sistema informático para identificar oraciones que un \"lector humano\" encontraría interesante es una tarea difícil.lector humano",
                "Sin embargo, desde la perspectiva de un \"lector humano\", las pepitas informativas e interesantes son útiles y definitionales.lector humano",
                "La noción de un \"lector humano\" promedio es una consideración importante en nuestro enfoque.lector humano",
                "Esto es muy diferente del enfoque de patrón léxico-sintáctico donde el contexto de un \"lector humano\" ni siquiera se considera al encontrar respuestas para la respuesta de las preguntas de definición.lector humano"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "interest": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human <br>interest</br> Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human <br>interest</br> in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers <br>interest</br>.",
                "For this reason, we decided to define answer nuggets that can evoke human <br>interest</br> as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human <br>interest</br> Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human <br>interest</br> Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN <br>interest</br> MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our <br>interest</br> Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the <br>interest</br> Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the <br>interest</br> Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our <br>interest</br> Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our <br>interest</br> Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the <br>interest</br> Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the <br>interest</br> Corpus.",
                "Figure 1: Human <br>interest</br> Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our <br>interest</br> Corpus.",
                "We have two major uses for this topic specific <br>interest</br> Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the <br>interest</br> Corpus of articles we collected from the internet are likely to contain nuggets that are of <br>interest</br> to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the <br>interest</br> Corpus essentially raises a different aspect of the topic for consideration as a sentence of <br>interest</br> to human readers.",
                "By performing a pairwise sentence comparison between sentences in the <br>interest</br> Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the <br>interest</br> Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics <br>interest</br> Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the <br>interest</br> Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human <br>interest</br>-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the <br>interest</br> Corpus.",
                "The HIM Ranker, or Human <br>interest</br> Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the <br>interest</br> Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the <br>interest</br> Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human <br>interest</br> Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human <br>interest</br>-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human <br>interest</br> Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human <br>interest</br> Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human <br>interest</br> Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human <br>interest</br> Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human <br>interest</br> Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human <br>interest</br> Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human <br>interest</br> Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of <br>interest</br> to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the <br>interest</br> Corpus having the term and N is the total number of sentences in the <br>interest</br> Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the <br>interest</br> Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the <br>interest</br> Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human <br>interest</br> Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human <br>interest</br> Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the <br>interest</br> Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the <br>interest</br> Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human <br>interest</br> Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the <br>interest</br> Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human <br>interest</br> Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human <br>interest</br> Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human <br>interest</br> Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human <br>interest</br> Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human <br>interest</br> Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human <br>interest</br> Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human <br>interest</br> Model score of 0.3031 but fails to outperform the optimized Human <br>interest</br> Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of <br>interest</br> to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human <br>interest</br> Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human <br>interest</br> Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that <br>interest</br> factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human <br>interest</br>.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este documento propone abordar la deficiencia construyendo un modelo de \"interés\" humano a partir del conocimiento externo.interés",
                "Se espera que dicho modelo permita el cálculo del \"interés\" humano en la oración con respecto al tema.interés",
                "Por lo general, estos están fuera de la información ordinaria sobre un tema que puede despertar a los lectores humanos \"interés\".interés",
                "Por esta razón, decidimos definir las pepitas de respuesta que pueden evocar el \"interés\" humano como pepitas interesantes.interés",
                "Un sistema de respuesta de preguntas de definición de modelo de \"interés\" humano se desarrolla con énfasis en la identificación de pepitas interesantes para evaluar el impacto de las pepitas interesantes en el desempeño de un sistema de respuesta de preguntas de definición.interés",
                "Además, experimentamos con la combinación del modelo de \"interés\" humano con un sistema de respuesta de preguntas de definición basado en patrones léxicos para capturar pepitas informativas e interesantes.2. Interés",
                "El modelo de \"interés\" humano que obtiene un sistema informático para identificar oraciones que un lector humano encontraría interesante es una tarea difícil.interés",
                "Aprovechamos este conocimiento mundial mediante la recopilación de artículos para cada tema de los siguientes recursos externos para desarrollar nuestro corpus de \"interés\" para cada tema.interés",
                "Utilizamos una instantánea de Wikipedia tomada en marzo de 2006 e incluimos el artículo más relevante en el corpus de \"interés\".interés",
                "Para cada tema, descargamos los 50 artículos más relevantes e incluimos el título y el primer párrafo de cada artículo en el corpus \"Interest\".interés",
                "Debido a su cobertura integral de una amplia variedad de temas, los recursos anteriores forman la mayor parte de nuestro corpus de \"interés\".interés",
                "Si el tema es una persona y podemos encontrar una biografía relevante en la persona, la incluimos en nuestro corpus de \"interés\".interés",
                "Al igual que Biography.com, incluimos la biografía más relevante que podemos encontrar en el corpus de \"interés\".interés",
                "Utilizamos esta función y extraemos cualquier definición que el motor de búsqueda de Google haya encontrado para cada tema en el corpus de \"interés\".interés",
                "Figura 1: Arquitectura del modelo de \"interés\" humano.interés",
                "Agregamos esta breve definición, si hay una, en nuestro corpus de \"interés\".interés",
                "Tenemos dos usos principales para este Corpus de \"interés\" específico del tema, como fuente de oraciones que contienen pepitas interesantes y como un modelo de lenguaje unigram de términos temáticos, I. 3.2 múltiples centroides interesantes que hemos visto que las pepitas interesantes son altamente específicas para un tema.interés",
                "Dado que las oraciones en el corpus de \"interés\" de los artículos que recopilamos de Internet es probable que contengan pepitas que son de \"interés\" para los lectores humanos, esencialmente podemos usar cada oración como pseudocentroides.interés",
                "Cada oración en el corpus de \"interés\" esencialmente plantea un aspecto diferente del tema para su consideración como una oración de \"interés\" para los lectores humanos.interés",
                "Al realizar una comparación de oraciones por pares entre oraciones en el corpus de \"interés\" y oraciones candidatas recuperadas del Corpus Aquaint, aumentamos el número de comparaciones de oraciones de O (N) a O (NM).interés",
                "Aquí, N es el número de posibles oraciones candidatas y M es el número de oraciones en el corpus de \"interés\".interés",
                "A cambio, obtenemos una lista diversa de respuestas clasificadas que son individualmente similares a varias oraciones que se encuentran en el corpus de \"interés\" de los temas.interés",
                "Una respuesta solo puede estar altamente clasificada si es muy similar a una oración en el corpus de \"interés\", y también es muy relevante para el tema.3.3 Implementación La Figura 1 muestra la arquitectura del sistema para el sistema de control de calidad de definición basado en \"interés\" humano propuesto.interés",
                "El módulo de recuperación web, por otro lado, busca los recursos en línea descritos en la Sección 3.1 para obtener documentos interesantes para llenar el corpus de \"interés\".interés",
                "El Him Ranker, o el módulo de clasificación de modelos de \"interés\" humano, es la implementación de lo que se describe en este documento.interés",
                "Clasificamos oraciones por interesante utilizando oraciones tanto del cuerpo de \"interés\" de documentos externos como del modelo de idioma unigram que construimos anteriormente que usamos para peso.interés",
                "Los pesos de términos se utilizan para ajustar la importancia relativa de cada término único que se encuentra en el corpus de \"interés\".interés",
                "Elegimos el puntaje de similitud más alto para una sentencia candidata como el puntaje del modelo de \"interés\" humano para la sentencia candidata.interés",
                "Experimentos iniciales El sistema basado en \"interés\" humano descrito en la sección anterior está diseñado para identificar solo pepitas interesantes y no pepitas informativas.interés",
                "Fscore = β2 ∗ np ∗ nr (β2 + 1) np + nr (1) sistema f3-score mejor trec 2005 sistema 0.2480 soft patern (sp) 0.2872 modelo de \"interés\" humano (HIT) 0.3031 Tabla 1: rendimiento en trec 2005Conjunto de preguntas Figura 2: rendimiento por tipos de entidades.4.1 Informatividad versus Interesante Nuestro primer experimento compara el rendimiento de identificar únicamente pepitas interesantes contra la identificación únicamente de pepitas informativas.interés",
                "Comparamos los resultados alcanzados por el modelo de \"interés\" humano que solo identifican las pepitas interesantes con los resultados del modelo de búsqueda de patrón sintáctico, así como el resultado del sistema de definición de mayor rendimiento en TREC 2005 [13].interés",
                "El modelo de \"interés\" humano supera claramente tanto el patrón suave como el mejor sistema TREC 2005 con una puntuación F3 de 0.303.interés",
                "Para obtener una mejor perspectiva de qué tan bien se desempeña el modelo de \"interés humano para diferentes tipos de temas, dividimos manualmente los temas de TREC 2005 en cuatro amplias categorías de persona, organización, cosa y evento como se enumeran en la Tabla 3. Interés.",
                "El desempeño del modelo de \"interés\" humano y el modelo BigRam de patrón suave para cada tipo de entidad se pueden ver en la Figura 2. Interés",
                "Sin embargo, dado el mismo conjunto de información basada en la web, el modelo de \"interés humano supera constantemente el modelo de patrón suave para los cuatro tipos de entidades.interés",
                "Esto sugiere que el modelo de \"interés\" humano está mejor capaz de aprovechar la información que se encuentra en los recursos web para identificar respuestas de definición.5. Interés",
                "Como hemos señalado, las pepitas interesantes a menudo tienen una cualidad trivial que los hace de \"interés\" para los seres humanos.interés",
                "Para nuestros experimentos, calculamos el peso de cada término como tf × log (n nt), donde tf es la frecuencia de término, nt es el número de oraciones en el corpus de \"interés\" que tiene el término y n es el número total de oracionesEn el corpus de \"interés\".interés",
                "Aquí, tratamos al Corpus de Aquaint como un modelo de idioma unigram de inglés general [15], a, y el corpus de \"interés\" como un modelo de idioma unigram que consiste en términos específicos del tema y términos generales de inglés, I. Interés",
                "Por lo tanto, el uso de la divergencia de KL como un esquema de ponderación de término hará que se otorgan pesos fuertes a los términos específicos de los temas porque su distribución en el corpus de \"interés\" ocurren significativamente más a menudo o menos a menudo que en el inglés general.interés",
                "Al igual que con la divergencia KL, también usamos la divergencia JS para medir la diferencia entre nuestros dos modelos de lenguaje, I y A. DJS (i a) = 1 2 ¢ dkl i i+a 2 ¡+dkl a i+a 2 ¡£ (3 (3) Figura 3: rendimiento por diversos esquemas de ponderación a término en el modelo de \"interés\" humano.interés",
                "La Figura 3 muestra el resultado de aplicar los cinco esquemas de ponderación a término en el modelo de \"interés\" humano.interés",
                "A medida que calculamos la frecuencia de documentos inversos para los términos en el corpus de \"interés\" recopilado de los recursos web, las FDI en gran medida los términos temáticos de alta frecuencia y los términos relevantes.interés",
                "Esto da como resultado que TFIDF favorezca todos los términos de baja frecuencia en términos de alta frecuencia en el corpus de \"interés\".interés",
                "Sin embargo, creemos que a pesar de esto, la divergencia de Jensen-Shannon proporciona un aumento pequeño pero medible en el rendimiento de nuestro modelo de \"interés\" humano.1 JS Divergencia también tiene la propiedad de estar limitado, lo que permite que los resultados se traten como una probabilidad si es necesario.interés",
                "Sin embargo, la propiedad limitada no se requiere aquí, ya que solo estamos tratando la divergencia calculada por la divergencia JS como Pesos de término 5.2 Selección de recursos web en uno de nuestros experimentos iniciales, observamos que la calidad de los recursos web incluidos en el Corpus de \"interés\" puedeTener un impacto directo en los resultados que obtenemos.interés",
                "Queríamos determinar qué impacto tiene la elección de los recursos web en el desempeño de nuestro modelo de \"interés\" humano.interés",
                "Todas las ejecuciones se realizaron en el modelo de \"interés\" humano utilizando la divergencia JS como esquema de ponderación del término.interés",
                "Uniendo la información con interesante, hasta ahora hemos estado comparando el modelo de \"interés\" humano con el modelo de patrón suave para comprender las diferencias entre pepitas interesantes e informativas.interés",
                "El modelo de \"interés\" humano que hemos descrito en este documento, por otro lado, es un experto en encontrar pepitas interesantes.interés",
                "Sin embargo, ninguno de los métodos de aprendizaje de conjunto que intentamos podría superar a nuestro modelo de \"interés\" humano.interés",
                "Primero normalizamos las 1,000 oraciones clasificadas principales de cada sistema, para obtener el puntaje de modelo de \"interés\" humano normalizado, él (s) y el puntaje del modelo BigRam de patrón suave normalizado, SP (s), para cada oración única, s.Para cada oración, los dos puntajes separados para luego se unifican en un solo puntaje utilizando la ecuación 5. Interés",
                "Esta puntuación es equivalente a la puntuación inicial del modelo de \"interés\" humano de 0.3031, pero no supera al modelo de modelo de \"interés\" optimizado.7. Interés",
                "Usando esta perspectiva, hemos demostrado que utilizando una combinación de un corpus externo cuidadosamente seleccionado, que coincide con múltiples centroides y teniendo en cuenta términos raros pero altamente específicos de temas, podemos construir un módulo de respuesta de pregunta definitivo que se centre más en identificar pepitas que sonde \"interés\" a los seres humanos.interés",
                "Si bien hemos intentado construir dicho sistema combinando nuestro modelo de \"interés humano\" propuesto con el modelo BigRam de patrón suave de Cui et al., Las diferencias inherentes entre ambos tipos de pepitas aparentemente causadas por las bajas tasas de acuerdo entre ambos modelos han hecho esto.una tarea difícil.interés",
                "Como resultado, actualmente solo podemos lograr un sistema híbrido que tenga el mismo nivel de rendimiento que nuestro modelo de \"interés\" humano propuesto.interés",
                "Abordamos el problema de la respuesta de las preguntas definitivas desde una perspectiva novedosa, con la noción de que el factor de \"interés\" juega un papel en la identificación de respuestas de definición.interés",
                "Academia Naval, OPEP, OTAN, Oficina Internacional de la Unión Postal Universal (UPU), Organización de la Conferencia Islámica (OIC), PBGC Persona Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il Thing F16, Bollywood, Viagra, Howdy Dooddod Show, Museum, Museo, Meteoritas, Virginia Wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, Kudzu, Medalla de Honor de EE. UU., Tsunami, Genoma, Acuerdo de Alimentos para el Propiedad, Shiite, Kinmen Island Event Submarine Kursk Fregads, Miss Universo 2000Coronada, Port Arthur Massacre, Francia gana la Copa Mundial en fútbol, clips de avión cables de cable en el complejo italiano, tiroteo en la escuela Kip Kinkel, accidente del vuelo 990 de Egyptairio, Preakness 1998, primer debate presidencial de 2000 Bush-Gore, 1998 acusación y juicio de Susan McDougal, Regreso de Hong Kong a la soberanía china, 1998 Juegos Olímpicos de Nagano, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens Eruption, 1998 Baseball World Series, Hindenburg Desastre, Huracán Mitch Tabla 3: Temas de TREC 2005 agrupados porEl tipo de entidad es el algoritmo de PageRank Googles, que consideró principalmente el número de vínculos, tiene un efecto indirecto de clasificar los documentos web por el grado de \"interés\" humano.interés"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "interesting nugget": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and <br>interesting nugget</br> types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Por lo tanto, creemos que un buen sistema de respuesta de preguntas de definición necesitaría recoger tipos informativos e \"interesantes de pepita\" para proporcionar una cobertura de definición completa sobre todos los aspectos importantes del tema.Nugget interesante"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "unique quality": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or <br>unique quality</br> that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como se ve aquí, Nuggets interesantes tiene algún factor sorpresa o \"calidad única\" que los hace interesantes para los lectores humanos.1.2 Identificación de pepitas interesantes Dado que la descripción oficial original de las definiciones comprende identificar pepitas informativas, la mayoría de las investigaciones se han centrado completamente en identificar pepitas informativas.calidad única"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "surprise factor": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some <br>surprise factor</br> or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como se ve aquí, Nuggets interesante tiene un \"factor sorpresa\" o una calidad única que los hace interesantes para los lectores humanos.1.2 Identificación de pepitas interesantes Dado que la descripción oficial original de las definiciones comprende identificar pepitas informativas, la mayoría de las investigaciones se han centrado completamente en identificar pepitas informativas.factor sorpresa"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "lexical pattern": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a <br>lexical pattern</br> based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, experimentamos con la combinación del modelo de interés humano con un sistema de respuesta de preguntas de definición basada en el \"patrón léxico\" para capturar pepitas informativas e interesantes.2. Patrón léxico"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "manual labor": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of <br>manual labor</br>, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este proceso requiere mucho \"trabajo manual\", experiencia y no es escalable.labor manual"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "baseline system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a <br>baseline system</br> for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The <br>baseline system</br> in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 <br>baseline system</br> [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un enfoque similar también se ha utilizado como un \"sistema de referencia\" para TREC 2003 [14].sistema de referencia",
                "El \"sistema de referencia\" en TREC 2003 simplemente usa las palabras temáticas como su corpus de definición.sistema de referencia",
                "Sin embargo, el \"sistema de referencia\" de TREC 2003 [14] superó a todos menos a otro sistema.sistema de referencia"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "definitional question answer": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a Human Interest Model from external knowledge.",
                "It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic.",
                "We compare and contrast our model with current <br>definitional question answer</br>ing models to show that interestingness plays an important factor in <br>definitional question answer</br>ing.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for <br>definitional question answer</br>ing from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke human interest as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A Human Interest Model <br>definitional question answer</br>ing system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a <br>definitional question answer</br>ing system.",
                "We further experimented with combining the Human Interest Model with a lexical pattern based <br>definitional question answer</br>ing system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 <br>definitional question answer</br>ing system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a <br>definitional question answer</br>ing system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous <br>definitional question answer</br>ing evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in <br>definitional question answer</br>ing.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in <br>definitional question answer</br>ing.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "HUMAN INTEREST MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at <br>definitional question answer</br>ing. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: Human Interest Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed Human Interest-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or Human Interest Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the Human Interest Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The Human Interest-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in <br>definitional question answer</br>ing.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 Human Interest Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the Human Interest Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The Human Interest Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the Human Interest Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of Human Interest Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the Human Interest Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the Human Interest Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the Human Interest Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the Human Interest Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our Human Interest Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our Human Interest Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on Human Interest Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the Human Interest Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good <br>definitional question answer</br>ing system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The Human Interest Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate <br>definitional question answer</br>ing systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our Human Interest Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized Human Interest Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial Human Interest Model score of 0.3031 but fails to outperform the optimized Human Interest Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for <br>definitional question answer</br>ing.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a <br>definitional question answer</br>ing module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art <br>definitional question answer</br>ing systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good <br>definitional question answer</br>ing system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed Human Interest Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed Human Interest Model.",
                "We approached the problem of <br>definitional question answer</br>ing from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past <br>definitional question answer</br>ings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of human interest.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for <br>definitional question answer</br>ing.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Comparamos y contrastamos nuestro modelo con los modelos actuales de \"respuesta de pregunta de definición\" para mostrar que la interesante juega un factor importante en la \"respuesta de la pregunta de definición\".Respuesta de la pregunta de definición",
                "Desde la observación de la respuesta establecida para la \"respuesta de la pregunta de definición\" de TREC 2003 a 2005, parece que un número significativo de temas que las pepitas no pueden describirse simplemente como pepitas informativas.Respuesta de la pregunta de definición",
                "Un modelo de interés de interés humano \"respuesta de pregunta de definición\" se desarrolla con énfasis en la identificación de pepitas interesantes para evaluar el impacto de las pepitas interesantes en el desempeño de un sistema de \"respuesta de la pregunta de definición\".Respuesta de la pregunta de definición",
                "Además, experimentamos con la combinación del modelo de interés humano con un sistema de \"respuesta de pregunta de definición\" basado en patrones léxicos para capturar pepitas informativas e interesantes.2. Respuesta de la pregunta de definición",
                "Por ejemplo, Xu et al.Usó 40 patrones estructurados definidos manualmente en su sistema de \"respuesta de pregunta de definición\" de 2003.Respuesta de la pregunta de definición",
                "Un sistema reciente de Harabagiu et al.[6] creó un sistema de \"respuesta de definición\" que combina el uso de 150 patrones positivos y negativos definidos manualmente, relaciones con la entidad nombradas y plantillas de extracción de información especialmente diseñadas para 33 dominios objetivo.Respuesta de la pregunta de definición",
                "En lugar de codificar patrones manualmente, las respuestas a las evaluaciones anteriores de la \"respuesta de la pregunta de definición\" se convirtieron en patrones genéricos y un modelo probabilístico está capacitado para identificar tales patrones en las oraciones.Respuesta de la pregunta de definición",
                "Esto lleva a la exploración del segundo enfoque basado en relevancia que se ha utilizado en la \"respuesta de la pregunta de definición\".Respuesta de la pregunta de definición",
                "Esto puede explicar por qué el método basado en la relevancia puede funcionar de manera competitiva en la \"respuesta de la pregunta de definición\".Respuesta de la pregunta de definición",
                "A través de una serie de experimentos, mostraremos que incluso un enfoque tan simple puede ser muy efectivo en la \"respuesta de la pregunta de definición\".3.1 Recursos web Existe en los artículos de Internet sobre cualquier tema que un humano pueda pensar.Respuesta de la pregunta de definición",
                "Por lo tanto, se puede describir como un sistema discapacitado que solo trata con la mitad del problema en la \"respuesta de la pregunta de definición\".Respuesta de la pregunta de definición",
                "Creemos que un buen sistema de \"respuesta de definición\" debería proporcionar al lector una mezcla combinada de ambos tipos de pepitas como un conjunto de respuestas de definición.Respuesta de la pregunta de definición",
                "Inicialmente esperábamos unificar los dos sistemas separados de \"respuesta de definición\" aplicando un método de aprendizaje de conjunto [5], como votar o aumentar para lograr una buena mezcla de pepitas informativas e interesantes en nuestro conjunto de respuestas.Respuesta de la pregunta de definición",
                "Esto es muy diferente del enfoque de patrón léxico-sintáctico donde el contexto de un lector humano ni siquiera se considera al encontrar respuestas para la \"respuesta de la pregunta de definición\".Respuesta de la pregunta de definición",
                "Usando esta perspectiva, hemos demostrado que utilizando una combinación de un corpus externo cuidadosamente seleccionado, que coincide con múltiples centroides y teniendo en cuenta los términos raros pero altamente específicos del tema, podemos construir un módulo de \"respuesta de la pregunta de definición\" que se centre más en identificarpepitas que son de interés para los seres humanos.Respuesta de la pregunta de definición",
                "Los resultados experimentales han demostrado que este enfoque puede superar significativamente a los sistemas de respuesta de la \"pregunta de definición\" de última generación.Respuesta de la pregunta de definición",
                "Por lo tanto, creemos que un buen sistema de \"respuesta de definición\" necesitaría recoger tipos de pepitas informativos e interesantes para proporcionar una cobertura de definición completa sobre todos los aspectos importantes del tema.Respuesta de la pregunta de definición",
                "Abordamos el problema de la \"respuesta de la pregunta de definición\" desde una perspectiva novedosa, con la noción de que el factor de interés juega un papel en la identificación de respuestas de definición.Respuesta de la pregunta de definición",
                "Nuestro enfoque también puede proporcionar una idea de algunas anomalías en ensayos pasados de \"respuesta de pregunta de definición\".Respuesta de la pregunta de definición",
                "Modelos genéricos de patrones suaves para \"respuesta de pregunta de definición\" ing.Respuesta de la pregunta de definición"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "human interest": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Interesting Nuggets and Their Impact on Definitional Question Answering Kian-Wei Kor Department of Computer Science School of Computing National University of Singapore dkor@comp.nus.edu.sg Tat-Seng Chua Department of Computer Science School of Computing National University of Singapore chuats@comp.nus.edu.sg ABSTRACT Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets.",
                "This is insufficient as they do not address the novelty factor that a definitional nugget must also possess.",
                "This paper proposes to address the deficiency by building a <br>human interest</br> Model from external knowledge.",
                "It is hoped that such a model will allow the computation of <br>human interest</br> in the sentence with respect to the topic.",
                "We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models; H.1.2 [User/Machine Systems]: Human Factors General Terms Algorithms, Human Factors, Experimentation 1.",
                "DEFINITIONAL QUESTION ANSWERING Definitional Question Answering was first introduced to the TExt Retrieval Conference Question Answering Track main task in 2003.",
                "The Definition questions, also called Other questions in recent years, are defined as follows.",
                "Given a question topic X, the task of a definitional QA system is akin to answering the question What is X? or Who is X?.",
                "The definitional QA system is to search through a news corpus and return return a set of answers that best describes the question topic.",
                "Each answer should be a unique topic-specific nugget that makes up one facet in the definition of the question topic. 1.1 The Two Aspects of Topic Nuggets Officially, topic-specific answer nuggets or simply topic nuggets are described as informative nuggets.",
                "Each informative nugget is a sentence fragment that describe some factual information about the topic.",
                "Depending on the topic type and domain, this can include topic properties, relationships the topic has with some closely related entity, or events that happened to the topic.",
                "From observation of the answer set for definitional question answering from TREC 2003 to 2005, it seems that a significant number of topic nuggets cannot simply be described as informative nuggets.",
                "Rather, these topic nuggets have a trivia-like quality associated with them.",
                "Typically, these are out of the ordinary pieces of information about a topic that can pique a human readers interest.",
                "For this reason, we decided to define answer nuggets that can evoke <br>human interest</br> as interesting nuggets.",
                "In essence, interesting nuggets answer the questions What is X famous for?, What defines X? or What is extraordinary about X?.",
                "We now have two very different perspective as to what constitutes an answer to Definition questions.",
                "An answer can be some important factual information about the topic or some novel and interesting aspect about the topic.",
                "This duality of informativeness and interestingness can be clearly observed in the five vital answer nuggets for a TREC 2005 topic of George Foreman.",
                "Certain answer nuggets are more informative while other nuggets are more interesting in nature.",
                "Informative Nuggets - Was graduate of Job Corps. - Became oldest world champion in boxing history.",
                "Interesting Nuggets - Has lent his name to line of food preparation products. - Waved American flag after winning 1968 Olympics championship. - Returned to boxing after 10 yr hiatus.",
                "As an African-American professional heavyweight boxer, an average human reader would find the last three nuggets about George Foreman interesting because boxers do not usually lend their names to food preparation products, nor do boxers retire for 10 years before returning to the ring and become the worlds oldest boxing champion.",
                "Foremans waving of the American flag at the Olympics is interesting because the innocent action caused some AfricanAmericans to accuse Foreman of being an Uncle Tom.",
                "As seen here, interesting nuggets has some surprise factor or unique quality that makes them interesting to human readers. 1.2 Identifying Interesting Nuggets Since the original official description for definitions comprise of identifying informative nuggets, most research has focused entirely on identifying informative nuggets.",
                "In this paper, we focus on exploring the properties of interesting nuggets and develop ways of identify such interesting nuggets.",
                "A <br>human interest</br> Model definitional question answering system is developed with emphasis on identifying interesting nuggets in order to evaluate the impact of interesting nuggets on the performance of a definitional question answering system.",
                "We further experimented with combining the <br>human interest</br> Model with a lexical pattern based definitional question answering system in order to capture both informative and interesting nuggets. 2.",
                "RELATED WORK There are currently two general methods for Definitional Question Answering.",
                "The more common method uses a lexical patternbased approach was first proposed by Blair-Goldensohn et al. [1] and Xu et al. [14].",
                "Both groups predominantly used patterns such as copulas and appositives, as well as manually crafted lexicosyntactic patterns to identify sentences that contain informative nuggets.",
                "For example, Xu et al. used 40 manually defined structured patterns in their 2003 definitional question answering system.",
                "Since then, in an attempt to capture a wider class of informational nuggets, many such systems of increasing complexity has been created.",
                "A recent system by Harabagiu et al. [6] created a definitional question answering system that combines the use of 150 manually defined positive and negative patterns, named entity relations and specially crafted information extraction templates for 33 target domains.",
                "Here, a musician template may contain lexical patterns that identify information such as the musicians musical style, songs sung by the musician and the band, if any, that the musician belongs to.",
                "As one can imagine, this is a knowledge intensive approach that requires an expert linguist to manually define all possible lexical or syntactic patterns required to identify specific types of information.",
                "This process requires a lot of manual labor, expertise and is not scalable.",
                "This lead to the development of the soft-pattern approach by Cui et al. [4, 11].",
                "Instead of manually encoding patterns, answers to previous definitional question answering evaluations were converted into generic patterns and a probabilistic model is trained to identify such patterns in sentences.",
                "Given a potential answer sentence, the probabilistic model outputs a probability that indicates how likely the sentence matches one or more patterns that the model has seen in training.",
                "Such lexicalosyntactic patterns approach have been shown to be adept at identifying factual informative nuggets such as a persons birthdate, or the name of a companys CEO.",
                "However, these patterns are either globally applicable to all topics or to a specific set of entities such as musicians or organizations.",
                "This is in direct contrast to interesting nuggets that are highly specific to individual topics and not to a set of entities.",
                "For example, the interesting nuggets for George Foreman are specific only George Foreman and no other boxer or human being.",
                "Topic specificity or topic relevance is thus an important criteria that helps identify interesting nuggets.",
                "This leads to the exploration of the second relevance-based approach that has been used in definitional question answering.",
                "Predominantly, this approach has been used as a backup method for identifying definitional sentences when the primary method of lexicalosyntactic patterns failed to find a sufficient number of informative nuggets [1].",
                "A similar approach has also been used as a baseline system for TREC 2003 [14].",
                "More recently, Chen et al. [3] adapted a bi-gram or bi-term language model for definitional Question Answering.",
                "Generally, the relevance-based approach requires a definitional corpus that contain documents highly relevant to the topic.",
                "The baseline system in TREC 2003 simply uses the topic words as its definitional corpus.",
                "Blair-Goldensohn et al. [1] uses a machine learner to include in the definitonal corpus sentences that are likely to be definitional.",
                "Chen et al. [3] collect snippets from Google to build its definitional corpus.",
                "From the definitional corpus, a definitional centroid vector is built or a set of centroid words are selected.",
                "This centroid vector or set of centroid words is taken to be highly indicative of the topic.",
                "Systems can then use this centroid to identify definitional answers by using a variety of distance metrics to compare against sentences found in the set of retrieved documents for the topic.",
                "BlairGoldensohn et al. [1] uses Cosine similarity to rank sentences by centrality.",
                "Chen et al. [3] builds a bigram language model using the 350 most frequently occurring google snippet terms, described in their paper as an ordered centroid, to estimate the probability that a sentence is similar to the ordered centroid.",
                "As described here, the relevance-based approach is highly specific to individual topics due to its dependence on a topic specific definitional corpus.",
                "However if individual sentences are viewed as a document, then relevance-based approaches essentially use the collected topic specific centroid words as a form of document retrieval with automated query expansion to identify strongly relevant sentences.",
                "Thus such methods identify relevant sentences and not sentences containing definitional nuggets.",
                "Yet, the TREC 2003 baseline system [14] outperformed all but one other system.",
                "The bi-term language model [3] is able to report results that are highly competitive to state-of-the-art results using this retrieval-based approach.",
                "At TREC 2006, a simple weighted sum of all terms model with terms weighted using solely Google snippets outperformed all other systems by a significant margin [7].",
                "We believe that interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly cooccur with direct mention of topic keywords.",
                "This may explain why relevance-based method can perform competitively in definitional question answering.",
                "However, simply comparing against a single centroid vector or set of centroid words may have over emphasized topic relevance and has only identified interesting definitional nuggets in an indirect manner.",
                "Still, relevance based retrieval methods can be used as a starting point in identifying interesting nuggets.",
                "We will describe how we expand upon such methods to identify interesting nuggets in the next section. 3.",
                "<br>human interest</br> MODEL Getting a computer system to identify sentences that a human reader would find interesting is a tall order.",
                "However, there are many documents on the world wide web that are contain concise, human written summaries on just about any topic.",
                "Whats more, these documents are written explicitly for human beings and will contain information about the topic that most human readers would be interested in.",
                "Assuming we can identify such relevant documents on the web, we can leverage them to assist in identifying definitional answers to such topics.",
                "We can take the assumption that most sentences found within these web documents will contain interesting facets about the topic at hand.",
                "This greatly simplifies the problem to that of finding within the AQUAINT corpus sentences similar to those found in web documents.",
                "This approach has been successfully used in several factoid and list Question Answering systems [11] and we feel the use of such an approach for definitional or Other question answering is justified.",
                "Identifying interesting nuggets requires computing machinery to understand world knowledge and human insight.",
                "This is still a very challenging task and the use of human written documents dramatically simplifies the complexity of the task.",
                "In this paper, we report on such an approach by experimenting with a simple word-level edit distance based weighted term comparison algorithm.",
                "We use the edit distance algorithm to score the similarity of a pair of sentences, with one sentence coming from web resources and the other sentence selected from the AQUAINT corpus.",
                "Through a series of experiments, we will show that even such a simple approach can be very effective at definitional question answering. 3.1 Web Resources There exists on the internet articles on just about any topic a human can think of.",
                "Whats more, many such articles are centrally located on several prominent websites, making them an easily accessible source of world knowledge.",
                "For our work on identifying interesting nuggets, we focused on finding short one or two page articles on the internet that are highly relevant to our desired topic.",
                "Such articles are useful as they contain concise information about the topic.",
                "More importantly, the articles are written by humans, for human readers and thus contain the critical human world knowledge that a computer system currently is unable to capture.",
                "We leverage this world knowledge by collecting articles for each topic from the following external resources to build our Interest Corpus for each topic.",
                "Wikipedia is a Web-based, free-content encyclopedia written collaboratively by volunteers.",
                "This resource has been used by many Question Answering system as a source of knowledge about each topic.",
                "We use a snapshot of Wikipedia taken in March 2006 and include the most relevant article in the Interest Corpus.",
                "NewsLibrary is a searchable archive of news articles from over 100 different newspaper agencies.",
                "For each topic, we download the 50 most relevant articles and include the title and first paragraph of each article in the Interest Corpus.",
                "Google Snippets are retrieved by issuing the topic as a query to the Google search engine.",
                "From the search results, we extracted the top 100 snippets.",
                "While Google snippets are not articles, we find that they provide a wide coverage of authorative information about most topics.",
                "Due to their comprehensive coverage of a wide variety of topics, the above resources form the bulk of our Interest Corpus.",
                "We also extracted documents from other resources.",
                "However, as these resources are more specific in nature, we do not always get any single relevant document.",
                "These resources are listed below.",
                "Biography.com is the website for the Biography television cable channel.",
                "The channels website contains searchable biographies on over 25,000 notable people.",
                "If the topic is a person and we can find a relevant biography on the person, we include it it in our Interest Corpus.",
                "Bartleby.com contains a searchable copy of several resources including the Columbia Encyclopedia, the World Factbook, and several English dictionaries. s9.com is a biography dictionary on over 33,000 notable people.",
                "Like Biography.com, we include the most relevant biography we can find in the Interest Corpus.",
                "Google Definitions Google search engine offers a feature called Definitions that provides the definition for a query, if it has one.",
                "We use this feature and extract whatever definitions the Google search engine has found for each topic into the Interest Corpus.",
                "Figure 1: <br>human interest</br> Model Architecture.",
                "WordNet WordNet is an well-known electronic semantic lexicon for the English language.",
                "Besides grouping English words into sets of synonyms called synsets, it also provide a short definition on the meaning of words found in each synset.",
                "We add this short definition, if there is one, into our Interest Corpus.",
                "We have two major uses for this topic specific Interest Corpus, as a source of sentences containing interesting nuggets and as a unigram language model of topic terms, I. 3.2 Multiple Interesting Centroids We have seen that interesting nuggets are highly specific to a topic.",
                "Relevance-based approaches such as the bigram language model used by Chen et al. [3] are focused on identifying highly relevant sentences and pick up definitional answer nuggets as an indirect consequence.",
                "We believe that the use of only a single collection of centroid words has over-emphasized topic relevance and choose instead to use multiple centroids.",
                "Since sentences in the Interest Corpus of articles we collected from the internet are likely to contain nuggets that are of interest to human readers, we can essentially use each sentence as pseudocentroids.",
                "Each sentence in the Interest Corpus essentially raises a different aspect of the topic for consideration as a sentence of interest to human readers.",
                "By performing a pairwise sentence comparison between sentences in the Interest Corpus and candidate sentences retrieved from the AQUAINT corpus, we increase the number of sentence comparisons from O(n) to O(nm).",
                "Here, n is the number of potential candidate sentences and m is the number of sentences in the Interest Corpus.",
                "In return, we obtain a diverse ranked list of answers that are individually similar to various sentences found in the topics Interest Corpus.",
                "An answer can only be highly ranked if it is strongly similar to a sentence in the Interest Corpus, and is also strongly relevant to the topic. 3.3 Implementation Figure 1 shows the system architecture for the proposed <br>human interest</br>-based definitional QA system.",
                "The AQUAINT Retrieval module shown in Figure 1 reuses a document retrieval module of a current Factoid and List Question Answering system we have implemented.",
                "Given a set of words describing the topic, the AQUAINT Retrieval module does query expansion using Google and searches an index of AQUAINT documents to retrieve the 800 most relevant documents for consideration.",
                "The Web Retrieval module on the other hand, searches the online resources described in Section 3.1 for interesting documents in order to populate the Interest Corpus.",
                "The HIM Ranker, or <br>human interest</br> Model Ranking module, is the implementation of what is described in this paper.",
                "The module first builds the unigram language model, I, from the collected web documents.",
                "This language model will be used to weight the importance of terms within sentences.",
                "Next, a sentence chunker is used to segment all 800 retrieved documents into individual sentences.",
                "Each of these sentences can be a potential answer sentence that will be independently ranked by interestingness.",
                "We rank sentences by interestingness using sentences from both the Interest Corpus of external documents as well as the unigram language model we built earlier which we use to weight terms.",
                "A candidate sentence in our top 800 relevant AQUAINT documents is considered interesting if it is highly similar in content to a sentence found in our collection of external web-documents.",
                "To achieve this, we perform a pairwise similarity comparison between a candidate sentence and sentences in our external documents using a weighted-term edit distance algorithm.",
                "Term weights are used to adjust the relative importance of each unique term found in the Interest Corpus.",
                "When both sentences share the same term, the similarity score is incremented by the two times the terms weight and every dissimilar term decrements the similarity score by the dissimilar terms weight.",
                "We choose the highest achieved similarity score for a candidate sentence as the <br>human interest</br> Model score for the candidate sentence.",
                "In this manner, every candidate sentence is ranked by interestingness.",
                "Finally, to obtain the answer set, we select the top 12 highest ranked and non redundant sentences as definitional answers for the topic. 4.",
                "INITIAL EXPERIMENTS The <br>human interest</br>-based system described in the previous section is designed to identify only interesting nuggets and not informative nuggets.",
                "Thus, it can be described as a handicapped system that only deals with half the problem in definitional question answering.",
                "This is done in order to explore how interestingness plays a factor in definitional answers.",
                "In order to compare and contrast the differences between informative and interesting nuggets, we also implemented the soft-pattern bigram model proposed by Cui et al. [4, 11].",
                "In order to ensure comparable results, both systems are provided identical input data.",
                "Since both system require the use of external resources, they are both provided the same web articles retrieved by our Web Retrieval module.",
                "Both systems also rank the same same set of candidate sentences in the form of 800 most relevant documents as retrieved by our AQUAINT Retrieval module.",
                "For the experiments, we used the TREC 2004 question set to tune any system parameters and use the TREC 2005 question sets to test the both systems.",
                "Both systems are evaluated the results using the standard scoring methodology for TREC definitions.",
                "TREC provides a list of vital and okay nuggets for each question topic.",
                "Every question is scored on nugget recall (NR) and nugget precision (NP) and a single final score is computed using F-Measure (see equation 1) with β = 3 to emphasize nugget recall.",
                "Here, NR is the number of vital nuggets returned divided by total number of vital nuggets while NP is computed using a minimum allowed character length function defined in [12].",
                "The evaluation is automatically conducted using Pourpre v1.0c [10].",
                "FScore = β2 ∗ NP ∗ NR (β2 + 1)NP + NR (1) System F3-Score Best TREC 2005 System 0.2480 Soft-Pattern (SP) 0.2872 <br>human interest</br> Model (HIM) 0.3031 Table 1: Performance on TREC 2005 Question Set Figure 2: Performance by entity types. 4.1 Informativeness vs Interestingness Our first experiment compares the performance of solely identifying interesting nuggets against solely identifying informative nuggets.",
                "We compare the results attained by the <br>human interest</br> Model that only identify interesting nuggets with the results of the syntactic pattern finding Soft-Pattern model as well as the result of the top performing definitional system in TREC 2005 [13].",
                "Table 1 shows the F3 score the three systems for the TREC 2005 question set.",
                "The <br>human interest</br> Model clearly outperform both soft pattern and the best TREC 2005 system with a F3 score of 0.303.",
                "The result is also comparable with the result of a human manual run, which attained a F3 score of 0.299 on the same question set [9].",
                "This result is confirmation that interesting nuggets does indeed play a significant role in picking up definitional answers, and may be more vital than using information finding lexical patterns.",
                "In order to get a better perspective of how well the <br>human interest</br> Model performs for different types of topics, we manually divided the TREC 2005 topics into four broad categories of PERSON, ORGANIZATION, THING and EVENT as listed in Table 3.",
                "These categories conform to TRECs general division of question topics into 4 main entity types [13].",
                "The performance of <br>human interest</br> Model and Soft Pattern Bigram Model for each entity type can be seen in Figure 2.",
                "Both systems exhibit consistent behavior across entity types, with the best performance coming from PERSON and ORGANIZATION topics and the worst performance from THING and EVENT topics.",
                "This can mainly be attributed to our selection of web-based resources for the definitional corpus used by both system.",
                "In general, it is harder to locate a single web article that describes an event or a general object.",
                "However given the same set of web-based information, the <br>human interest</br> Model consistently outperforms the soft-pattern model for all four entity types.",
                "This suggests that the <br>human interest</br> Model is better able to leverage the information found in web resources to identify definitional answers. 5.",
                "REFINEMENTS Encouraged by the initial experimental results, we explored two further optimization of the basic algorithm. 5.1 Weighting Interesting Terms The word trivia refer to tidbits of unimportant or uncommon information.",
                "As we have noted, interesting nuggets often has a trivialike quality that makes them of interest to human beings.",
                "From this description of interesting nuggets and trivia, we hypothesize that interesting nuggets are likely to occur rarely in a text corpora.",
                "There is a possibility that some low-frequency terms may actually be important in identifying interesting nuggets.",
                "A standard unigram language model would not capture these low-frequency terms as important terms.",
                "To explore this possibility, we experimented with three different term weighting schemes that can provide more weight to certain low-frequency terms.",
                "The weighting schemes we considered include commonly used TFIDF, as well as information theoretic Kullback-Leiber divergence and Jensen-Shannon divergence [8].",
                "TFIDF, or Term Frequency × Inverse Document Frequency, is a standard Information Retrieval weighting scheme that balances the importance of a term in a document and in a corpus.",
                "For our experiments, we compute the weight of each term as tf × log( N nt ), where tf is the term frequency, nt is the number of sentences in the Interest Corpus having the term and N is the total number of sentences in the Interest Corpus.",
                "Kullback-Leibler Divergence (Equation 2) is also called KL Divergence or relative entropy, can be viewed as measuring the dissimilarity between two probability distributions.",
                "Here, we treat the AQUAINT corpus as a unigram language model of general English [15], A, and the Interest Corpus as a unigram language model consisting of topic specific terms and general English terms, I.",
                "General English words are likely to have similar distributions in both language models I and A.",
                "Thus using KL Divergence as a term weighting scheme will cause strong weights to be given to topicspecific terms because their distribution in the Interest Corpus they occur significantly more often or less often than in general English.",
                "In this way, high frequency centroid terms as well as low frequency rare but topic-specific terms are both identified and highly weighted using KL Divergence.",
                "DKL(I A) = t I(t)log I(t) A(t) (2) Due to the power law distribution of terms in natural language, there are only a small number of very frequent terms and a large number of rare terms in both I and A.",
                "While the common terms in English consist of stop words, the common terms in the topic specific corpus, I, consist of both stop words and relevant topic words.",
                "These high frequency topic specific words occur very much more frequently in I than in A.",
                "As a result, we found that KL Divergence has a bias towards highly frequent topic terms as we are measuring direct dissimilarity against a model of general English where such topic terms are very rare.",
                "For this reason, we explored another divergence measure as a possible term weighting scheme.",
                "Jensen-Shannon Divergence or JS Divergence extends upon KL Divergence as seen in Equation 3.",
                "As with KL Divergence, we also use JS divergence to measure the dissimilarity between our two language models, I and A. DJS(I A) = 1 2 ¢DKL  I I+A 2 ¡+ DKL  A I+A 2 ¡£ (3) Figure 3: Performance by various term weighting schemes on the <br>human interest</br> Model.",
                "However, JS Divergence has additional properties1 of being symmetric and non-negative as seen in Equation 4.",
                "The symmetric property gives a more balanced measure of dissimilarity and avoids the bias that KL divergence has.",
                "DJS(I A) = DJS(A I) = 0 I = A > 0 I <> A (4) We conducted another experiment, substituting the unigram languge model weighting scheme we used in the initial experiments with the three term weighting schemes described above.",
                "As lower bound reference, we included a term weighting scheme consisting of a constant 1 for all terms.",
                "Figure 3 show the result of applying the five different term weighting schemes on the <br>human interest</br> Model.",
                "TFIDF performed the worst as we had anticipated.",
                "The reason is that most terms only appear once within each sentence, resulting in a term frequency of 1 for most terms.",
                "This causes the IDF component to be the main factor in scoring sentences.",
                "As we are computing the Inverse Document Frequency for terms in the Interest Corpus collected from web resources, IDF heavily down-weights highly frequency topic terms and relevant terms.",
                "This results in TFIDF favoring all low frequency terms over high frequency terms in the Interest Corpus.",
                "Despite this, the TFIDF weighting scheme only scored a slight 0.0085 lower than our lower bound reference of constant weights.",
                "We view this as a positive indication that low frequency terms can indeed be useful in finding interesting nuggets.",
                "Both KL and JS divergence performed marginally better than the uniform language model probabilistic scheme that we used in our initial experiments.",
                "From inspection of the weighted list of terms, we observed that while low frequency relevant terms were boosted in strength, high frequency relevant terms still dominate the top of the weighted term list.",
                "Only a handful of low frequency terms were weighted as strongly as topic keywords and combined with their low frequency, may have limited the impact of re-weighting such terms.",
                "However we feel that despite this, Jensen-Shannon divergence does provide a small but measurable increase in the performance of our <br>human interest</br> Model. 1 JS divergence also has the property of being bounded, allowing the results to be treated as a probability if required.",
                "However, the bounded property is not required here as we are only treating the divergence computed by JS divergence as term weights 5.2 Selecting Web Resources In one of our initial experiments, we observed that the quality of web resources included in the Interest Corpus may have a direct impact on the results we obtain.",
                "We wanted to determine what impact the choice of web resources have on the performance of our <br>human interest</br> Model.",
                "For this reason, we split our collection of web resources into four major groups listed here: N - News: Title and first paragraph of the top 50 most relevant articles found in NewsLibrary.",
                "W - Wikipedia: Text from the most relevant article found in Wikipedia.",
                "S - Snippets: Snippets extracted from the top 100 most relevant links after querying Google.",
                "M - Miscellaneous sources: Combination of content (when available) from secondary sources including biography.com, s9.com, bartleby.com articles, Google definitions and WordNet definitions.",
                "We conducted a gamut of runs on the TREC 2005 question set using all possible combinations of the above four groups of web resources to identify the best possible combination.",
                "All runs were conducted on <br>human interest</br> Model using JS divergence as term weighting scheme.",
                "The runs were sorted in descending F3-Score and the top 3 best performing runs for each entity class are listed in Table 2 together with earlier reported F3-scores from Figure 2 as a baseline reference.",
                "A consistent trend can be observed for each entity class.",
                "For PERSON and EVENT topics, NewsLibrary articles are the main source of interesting nuggets with Google snippets and miscellaneous articles offering additional supporting evidence.",
                "This seem intuitive for events as newspapers predominantly focus on reporting breaking newsworthy events and are thus excellent sources of interesting nuggets.",
                "We had expected Wikipedia rather than news articles to be a better source of interesting facts about people and were surprised to discover that news articles outperformed Wikipedia.",
                "We believe that the reason is because the people selected as topics thus far have been celebrities or well known public figures.",
                "Human readers are likely to be interested in news events that spotlight these personalities.",
                "Conversely for ORGANIZATION and THING topics, the best source of interesting nuggets come from Wikipedias most relevant article on the topic with Google snippets again providing additional information for organizations.",
                "With an oracle that can classify topics by entity class with 100% accuracy and by using the best web resources for each entity class as shown in Table 2, we can attain a F3-Score of 0.3158. 6.",
                "UNIFYING INFORMATIVENESS WITH INTERESTINGNESS We have thus far been comparing the <br>human interest</br> Model against the Soft-Pattern model in order to understand the differences between interesting and informative nuggets.",
                "However from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.",
                "Informative nuggets present a general overview of the topic while interesting nuggets give readers added depth and insight by providing novel and unique aspects about the topic.",
                "We believe that a good definitional question answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.",
                "Rank PERSON ORG THING EVENT Baseline Unigram Weighting Scheme, N+W+S+M 0.3279 0.3630 0.2551 0.2644 1 N+S+M W+S W+M N+M 0.3584 0.3709 0.2688 0.2905 2 N+S N+W+S W+S+M N+S+M 0.3469 0.3702 0.2665 0.2745 3 N+M N+W+S+M W+S N+S 0.3431 0.3680 0.2616 0.2690 Table 2: Top 3 runs using different web resources for each entity class We now have two very different experts at identifying definitions.",
                "The Soft Pattern Bigram Model proposed by Cui et al. is an expert in identifying informative nuggets.",
                "The <br>human interest</br> Model we have described in this paper on the other hand is an expert in finding interesting nuggets.",
                "We had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.",
                "However, none of the ensemble learning methods we attempted could outperform our <br>human interest</br> Model.",
                "The reason is that both systems are picking up very different sentences as definitional answers.",
                "In essence, our two experts are disagreeing on which sentences are definitional.",
                "In the top 10 sentences from both systems, only 4.4% of these sentences appeared in both answer sets.",
                "The remaining answers were completely different.",
                "Even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.",
                "Yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high F3 score.",
                "There is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.",
                "This could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.",
                "Unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.",
                "To verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the TREC 2005 question set and manually identified correct answer nuggets (as defined by TREC accessors) from both systems.",
                "When we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.",
                "While the nugget agreement rate is higher than the sentence agreement rate, both systems are generally still picking up different answer nuggets.",
                "We view this as further indication that definitions are indeed made up of a mixture of informative and interesting nuggets.",
                "It is also indication that in general, interesting and informative nuggets are quite different in nature.",
                "There are thus rational reasons and practical motivation in unifying answers from both the pattern based and corpus based approaches.",
                "However, the differences between the two systems also cause issues when we attempt to combine both answer sets.",
                "Currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.",
                "We first normalize the top 1,000 ranked sentences from each system, to obtain the Normalized <br>human interest</br> Model score, him(s), and the Normalized Soft Pattern Bigram Model score, sp(s), for every unique sentence, s. For each sentence, the two separate scores for are then unified into a single score using Equation 5.",
                "When only one system believes that the sentence is definitional, we simply retain that systems normalized score as the unified score.",
                "When both systems agree agree that the sentence is definitional, the sentences score is boosted by the degree of agreement between between both systems.",
                "Score(s) = max(shim, ssp)1−min(shim,ssp) (5) In order to maintain a diverse set of answers as well as to ensure that similar sentences are not given similar ranking, we further re-rank our combined list of answers using Maximal Marginal Relevance or MMR [2].",
                "Using the approach described here, we achieve a F3 score of 0.3081.",
                "This score is equivalent to the initial <br>human interest</br> Model score of 0.3031 but fails to outperform the optimized <br>human interest</br> Model model. 7.",
                "CONCLUSION This paper has presented a novel perspective for answering definitional questions through the identification of interesting nuggets.",
                "Interesting nuggets are uncommon pieces of information about the topic that can evoke a human readers curiosity.",
                "The notion of an average human reader is an important consideration in our approach.",
                "This is very different from the lexico-syntactic pattern approach where the context of a human reader is not even considered when finding answers for definitional question answering.",
                "Using this perspective, we have shown that using a combination of a carefully selected external corpus, matching against multiple centroids and taking into consideration rare but highly topic specific terms, we can build a definitional question answering module that is more focused on identifying nuggets that are of interest to human beings.",
                "Experimental results has shown this approach can significantly outperform state-of-the-art definitional question answering systems.",
                "We further showed that at least two different types of answer nuggets are required to form a more thorough set of definitional answers.",
                "What seems to be a good set of definition answers is some general information that provides a quick informative overview mixed together with some novel or interesting aspects about the topic.",
                "Thus we feel that a good definitional question answering system would need to pick up both informative and interesting nugget types in order to provide a complete definitional coverage on all important aspects of the topic.",
                "While we have attempted to build such a system by combining our proposed <br>human interest</br> Model with Cui et al.s Soft Pattern Bigram Model, the inherent differences between both types of nuggets seemingly caused by the low agreement rates between both models have made this a difficult task.",
                "Indeed, this is natural as the two models have been designed to identify two very different types of definition answers using very different types of features.",
                "As a result, we are currently only able to achieve a hybrid system that has the same level of performance as our proposed <br>human interest</br> Model.",
                "We approached the problem of definitional question answering from a novel perspective, with the notion that interest factor plays a role in identifying definitional answers.",
                "Although the methods we used are simple, they have been shown experimentally to be effective.",
                "Our approach may also provide some insight into a few anomalies in past definitional question answerings trials.",
                "For instance, the top definitional system at the recent TREC 2006 evaluation was able to significantly outperform all other systems using relatively simple unigram probabilities extracted from Google snippets.",
                "We suspect the main contributor to the systems performance Entity Type Topics ORGANIZATION DePauw University, Merck & Co., Norwegian Cruise Lines (NCL), United Parcel Service (UPS), Little League Baseball, Cliffs Notes, American Legion, Sony Pictures Entertainment (SPE), Telefonica of Spain, Lions Club International, AMWAY, McDonalds Corporation, Harley-Davidson, U.S.",
                "Naval Academy, OPEC, NATO, International Bureau of Universal Postal Union (UPU), Organization of Islamic Conference (OIC), PBGC PERSON Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il THING F16, Bollywood, Viagra, Howdy Doody Show, Louvre Museum, meteorites, Virginia wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, kudzu, U.S. Medal of Honor, tsunami, genome, Food-for-Oil Agreement, Shiite, Kinmen Island EVENT Russian submarine Kursk sinks, Miss Universe 2000 crowned, Port Arthur Massacre, France wins World Cup in soccer, Plane clips cable wires in Italian resort, Kip Kinkel school shooting, Crash of EgyptAir Flight 990, Preakness 1998, first 2000 Bush-Gore presidential debate , 1998 indictment and trial of Susan McDougal, return of Hong Kong to Chinese sovereignty, 1998 Nagano Olympic Games, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens eruption, 1998 Baseball World Series, Hindenburg disaster, Hurricane Mitch Table 3: TREC 2005 Topics Grouped by Entity Type is Googles PageRank algorithm, which mainly consider the number of linkages, has an indirect effect of ranking web documents by the degree of <br>human interest</br>.",
                "In our future work, we seek to further improve on the combined system by incorporating more evidence in support of correct definitional answers or to filter away obviously wrong answers. 8.",
                "REFERENCES [1] S. Blair-Goldensohn, K. R. McKeown, and A. H. Schlaikjer.",
                "A hybrid approach for qa track definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [2] J. G. Carbonell and J. Goldstein.",
                "The use of MMR, diversity-based reranking for reordering documents and producing summaries.",
                "In Research and Development in Information Retrieval, pages 335-336, 1998. [3] Y. Chen, M. Zhou, and S. Wang.",
                "Reranking answers for definitional qa using language modeling.",
                "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1081-1088, Sydney, Australia, July 2006.",
                "Association for Computational Linguistics. [4] H. Cui, M.-Y.",
                "Kan, and T.-S. Chua.",
                "Generic soft pattern models for definitional question answering.",
                "In SIGIR 05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 384-391, New York, NY, USA, 2005.",
                "ACM Press. [5] T. G. Dietterich.",
                "Ensemble methods in machine learning.",
                "Lecture Notes in Computer Science, 1857:1-15, 2000. [6] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang.",
                "Employing two question answering systems at trec 2005.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [7] M. Kaisser, S. Scheible, and B. Webber.",
                "Experiments at the university of edinburgh for the trec 2006 qa track.",
                "In TREC 06 Notebook: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2006.",
                "National Institute of Standards and Technology. [8] J. Lin.",
                "Divergence measures based on the shannon entropy.",
                "IEEE Transactions on Information Theory, 37(1):145 - 151, Jan 1991. [9] J. Lin, E. Abels, D. Demner-Fushman, D. W. Oard, P. Wu, and Y. Wu.",
                "A menagerie of tracks at maryland: Hard, enterprise, qa, and genomics, oh my!",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [10] J. Lin and D. Demner-Fushman.",
                "Automatically evaluating answers to definition questions.",
                "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 931-938, Vancouver, British Columbia, Canada, October 2005.",
                "Association for Computational Linguistics. [11] R. Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.",
                "Kan.",
                "Using syntactic and semantic relation analysis in question answering.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005. [12] E. M. Voorhees.",
                "Overview of the trec 2003 question answering track.",
                "In Text REtrieval Conference 2003, Gaithersburg, Maryland, 2003.",
                "National Institute of Standards and Technology. [13] E. M. Voorhees.",
                "Overview of the trec 2005 question answering track.",
                "In TREC 05: Proceedings of the 14th Text REtrieval Conference, Gaithersburg, Maryland, 2005.",
                "National Institute of Standards and Technology. [14] J. Xu, A. Licuanan, and R. Weischedel.",
                "TREC 2003 QA at BBN: Answering definitional questions.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003. [15] D. Zhang and W. S. Lee.",
                "A language modeling approach to passage question answering.",
                "In TREC 03: Proceedings of the 12th Text REtrieval Conference, Gaithersburg, Maryland, 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Este documento propone abordar la deficiencia construyendo un modelo de \"interés humano\" a partir del conocimiento externo.interés humano",
                "Se espera que dicho modelo permita el cálculo del \"interés humano\" en la oración con respecto al tema.interés humano",
                "Por esta razón, decidimos definir las pepitas de respuesta que pueden evocar el \"interés humano\" como pepitas interesantes.interés humano",
                "Se desarrolla un sistema de respuesta de preguntas de definición de modelo de \"interés humano\" con énfasis en la identificación de pepitas interesantes para evaluar el impacto de las pepitas interesantes en el rendimiento de un sistema de respuesta de preguntas de definición.interés humano",
                "Además, experimentamos con la combinación del modelo de \"interés humano\" con un sistema de respuesta de preguntas de definición basado en patrones léxicos para capturar pepitas informativas e interesantes.2. Interés humano",
                "Modelo de \"interés humano\" Obtener un sistema informático para identificar oraciones que un lector humano encontraría interesante es una tarea difícil.interés humano",
                "Figura 1: Arquitectura del modelo de \"interés humano\".interés humano",
                "Una respuesta solo puede estar altamente clasificada si es muy similar a una oración en el Corpus de Interest, y también es muy relevante para el tema.3.3 Implementación La Figura 1 muestra la arquitectura del sistema para el sistema de control de calidad definitivo basado en \"interés humano\" propuesto.interés humano",
                "El Him Ranker, o el módulo de clasificación de modelos de \"interés humano\", es la implementación de lo que se describe en este documento.interés humano",
                "Elegimos el puntaje de similitud más alto para una sentencia candidata como el puntaje del modelo de \"interés humano\" para la sentencia candidata.interés humano",
                "Experimentos iniciales El sistema basado en el \"interés humano\" descrito en la sección anterior está diseñado para identificar solo pepitas interesantes y no pepitas informativas.interés humano",
                "Fscore = β2 ∗ np ∗ nr (β2 + 1) np + nr (1) sistema f3-score mejor trec 2005 sistema 0.2480 soft patern (sp) 0.2872 modelo de interés humano (él) 0.3031 Tabla 1: rendimiento en trec 2005Conjunto de preguntas Figura 2: rendimiento por tipos de entidades.4.1 Informatividad versus Interesante Nuestro primer experimento compara el rendimiento de identificar únicamente pepitas interesantes contra la identificación únicamente de pepitas informativas.interés humano",
                "Comparamos los resultados alcanzados por el modelo de \"interés humano\" que solo identifican pepitas interesantes con los resultados del modelo de búsqueda de patrones sintácticos que encuentran el modelo de patrón suave, así como el resultado del sistema de definición de mayor rendimiento en TREC 2005 [13].interés humano",
                "El modelo de \"interés humano\" supera claramente tanto el patrón suave como el mejor sistema TREC 2005 con una puntuación F3 de 0.303.interés humano",
                "Para obtener una mejor perspectiva de qué tan bien se desempeña el modelo de \"interés humano\" para diferentes tipos de temas, dividimos manualmente los temas de TREC 2005 en cuatro amplias categorías de persona, organización, cosa y evento como se enumeran en la Tabla 3. Interés humano",
                "El rendimiento del modelo de \"interés humano\" y el modelo BigRam de patrón suave para cada tipo de entidad se pueden ver en la Figura 2. Interés humano",
                "Sin embargo, dado el mismo conjunto de información basada en la web, el modelo de \"interés humano\" supera constantemente el modelo de patrón suave para los cuatro tipos de entidades.interés humano",
                "Esto sugiere que el modelo de \"interés humano\" está mejor capaz de aprovechar la información que se encuentra en los recursos web para identificar respuestas de definición.5. Interés humano",
                "Al igual que con la divergencia KL, también usamos la divergencia JS para medir la diferencia entre nuestros dos modelos de lenguaje, I y A. DJS (i a) = 1 2 ¢ dkl i i+a 2 ¡+dkl a i+a 2 ¡£ (3 (3) Figura 3: rendimiento por varios esquemas de ponderación a término en el modelo de \"interés humano\".interés humano",
                "La Figura 3 muestra el resultado de aplicar los cinco esquemas de ponderación a término en el modelo de \"interés humano\".interés humano",
                "Sin embargo, creemos que a pesar de esto, la divergencia de Jensen-Shannon proporciona un aumento pequeño pero medible en el rendimiento de nuestro modelo de \"interés humano\".1 JS Divergencia también tiene la propiedad de estar limitado, lo que permite que los resultados se traten como una probabilidad si es necesario.interés humano",
                "Queríamos determinar qué impacto tiene la elección de los recursos web en el desempeño de nuestro modelo de \"interés humano\".interés humano",
                "Todas las ejecuciones se realizaron en el modelo de \"interés humano\" utilizando la divergencia JS como esquema de ponderación del término.interés humano",
                "Uniendo la información con interesante, hasta ahora hemos estado comparando el modelo de \"interés humano\" con el modelo de patrón suave para comprender las diferencias entre pepitas interesantes e informativas.interés humano",
                "El modelo de \"interés humano\" que hemos descrito en este documento, por otro lado, es un experto en encontrar pepitas interesantes.interés humano",
                "Sin embargo, ninguno de los métodos de aprendizaje de conjunto que intentamos podría superar a nuestro modelo de \"interés humano\".interés humano",
                "Primero normalizamos las 1,000 oraciones clasificadas principales de cada sistema, para obtener el puntaje de modelo de \"interés humano\" normalizado, él (s) y el puntaje del modelo BigRam de patrón suave normalizado, SP (s), para cada oración única, s.Para cada oración, los dos puntajes separados para luego se unifican en un solo puntaje utilizando la ecuación 5. Interés humano",
                "Esta puntuación es equivalente a la puntuación inicial del modelo de \"interés humano\" de 0.3031, pero no supera al modelo de modelo de \"interés humano\" optimizado.7. Interés humano",
                "Si bien hemos intentado construir dicho sistema combinando nuestro modelo propuesto de \"interés humano\" con el modelo BigRam de patrón suave de Cui et al.una tarea difícil.interés humano",
                "Como resultado, actualmente solo podemos lograr un sistema híbrido que tenga el mismo nivel de rendimiento que nuestro modelo de \"interés humano\" propuesto.interés humano",
                "Academia Naval, OPEP, OTAN, Oficina Internacional de la Unión Postal Universal (UPU), Organización de la Conferencia Islámica (OIC), PBGC Persona Bing Crosby, George Foreman, Akira Kurosawa, Sani Abacha, Enrico Fermi, Arnold Palmer, Woody Guthrie, Sammy Sosa, Michael Weiss, Paul Newman, Jesse Ventura, Rose Crumb, Rachel Carson, Paul Revere, Vicente Fox, Rocky Marciano, Enrico Caruso, Pope Pius XII, Kim Jong Il Thing F16, Bollywood, Viagra, Howdy Dooddod Show, Museum, Museo, Meteoritas, Virginia Wine, Counting Crows, Boston Big Dig, Chunnel, Longwood Gardens, Camp David, Kudzu, Medalla de Honor de EE. UU., Tsunami, Genoma, Acuerdo de Alimentos para el Propiedad, Shiite, Kinmen Island Event Submarine Kursk Fregads, Miss Universo 2000Coronada, Port Arthur Massacre, Francia gana la Copa Mundial en fútbol, clips de avión cables de cable en el complejo italiano, tiroteo en la escuela Kip Kinkel, accidente del vuelo 990 de Egyptairio, Preakness 1998, primer debate presidencial de 2000 Bush-Gore, 1998 acusación y juicio de Susan McDougal, Regreso de Hong Kong a la soberanía china, 1998 Juegos Olímpicos de Nagano, Super Bowl XXXIV, 1999 North American International Auto Show, 1980 Mount St. Helens Eruption, 1998 Baseball World Series, Hindenburg Desastre, Huracán Mitch Tabla 3: Temas de TREC 2005 agrupados porEl tipo de entidad es el algoritmo de PageRank Googles, que consideró principalmente el número de enlaces, tiene un efecto indirecto de clasificar los documentos web por grado de \"interés humano\".interés humano"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}