{
    "id": "H-92",
    "original_text": "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting. We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features. We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine. We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance. Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services. General Terms Algorithms, Measurement, Experimentation 1. INTRODUCTION Millions of users interact with search engines daily. They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions. These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments. Implicit relevance feedback for ranking and personalization has become an active area of research. Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process. Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval. How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank? While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies. Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned. To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine. The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6). We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2. BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval. Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24]. However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings. Implicit relevance measures have been studied by several research groups. An overview of implicit measures is compiled in Kelly and Teevan [14]. This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings. Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions. Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions. This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior. However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions. Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking. More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence. By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting. Unfortunately, the extent to which previous research applies to real-world web search is unclear. At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines. We build on existing research to develop robust user behavior interpretation techniques for the real web search setting. Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3. INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm. We describe the two general ranking approaches next. The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions. Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone. While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders. The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores. We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets). We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers. For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result. We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback. The query results are ordered in by decreasing values of SM to produce the final ranking. One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline. Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work. The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features. We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain). In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values. Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm. During training or tuning, the ranker can be tuned as before but with additional features. At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair. This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available. We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]). In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results. Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments. RankNet is one such algorithm. It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences. While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods. An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results. We use a 2-layer implementation of RankNet in order to model non-linear relationships between features. Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques. Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4. IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine. Interpreting implicit feedback in real web search setting is not an easy task. We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities. The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results. We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query). We design our features to take advantage of aggregated user behavior. The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values. The features used to represent user interactions with web search results are summarized in Table 4.1. This information was obtained via opt-in client-side instrumentation from users of a major web search engine. We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position. We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time. Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust. For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary. To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary. Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL. Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries. We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries. Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments. These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model. This approach is particularly attractive as it does not require heuristics beyond feature engineering. The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5. EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results. Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges. In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions. We describe this dataset in detail next. Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs. The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution. On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad. Overall, there were over 83,000 results with explicit relevance judgments. In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant. Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query. The user interactions were collected over a period of 8 weeks using voluntary opt-in information. In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine. The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted. These actions were aggregated across users and search sessions and converted to features in Table 4.1. To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries. The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP). Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant. In our setting, we require a relevant document to be labeled Good or higher. The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10]. For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j. Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions. NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved. The final MAP value is defined as the mean of average precisions of all queries in the test set. This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search. One dimension is to compare the utility of implicit feedback with other information available to a web search engine. Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27]. BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline. The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth). The scoring function and field-specific tuning is described in detail in [23]. Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result. This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries. A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above. Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance. This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks). The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results. This method serves as our baseline implicit feedback reranking method. BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4). This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2). At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced. The merged ranking is computed as described in Section 3.1. Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2). We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced). The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN). As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6. EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways. We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features). We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2. The results were averaged over three random splits of the overall dataset. Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint. We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings). We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial. We first experimented with different methods of re-ranking the output of the BM25F search results. Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1). Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone. The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63. Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1. While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR). If users considered only the relevance of a result to their query, they would click on the topmost relevant results. Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically. Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant. Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure. For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5. As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone. We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine. RN incorporates BM25F, link-based features, and hundreds of other features. Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings. In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods. This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1. We report the Mean Average Precision (MAP) score for each system. While not intuitive to interpret, MAP allows quantitative comparison on a single metric. The gains marked with * are significant at p=0.01 level using two tailed t-test. MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies. So far we reported results averaged across all queries in the test set. Unfortunately, less than half had sufficient interactions to attempt reranking. Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user). This is not surprising: web search is heavy-tailed, and there are many unique queries. We now consider the performance on the queries for which user interactions were available. Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features. The gains at top 1 are dramatic. The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users. When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2. MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced. Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful. Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6. Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1). Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score. One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines. Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest. Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking. The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7. CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking. We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance. We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function. Our experiments showed significant improvement over methods that do not consider implicit feedback. The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features. Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1). One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries. As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users. ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments. We also thank Robert Ragno for his valuable suggestions and many discussions. 8. REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda. Inferring user interest. IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White. Evaluating implicit measures to improve the search experience. In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick. Learning users interests by unobtrusively observing their normal behavior. In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data. In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical. Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography. In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl. GroupLens: Applying collaborative filtering to usenet news. In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval. Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim. Implicit feedback for recommender systems. In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim. Modeling information content using observable behavior. In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin. The SST method: a tool for analyzing web information search processes. In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web. In Working with Technology in Mind: Brunswikian. Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill. Introduction to modern information retrieval. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. Ma, W.S. Xi, and W.G. Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson. Microsoft Cambridge at TREC 13: Web and Hard Tracks. In Proceedings of TREC 2004",
    "original_translation": "Mejora de la clasificación de búsqueda web mediante la incorporación de información de comportamiento del usuario Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research Brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Resumen que la incorporación de datos de comportamiento del usuario puede mejorar significativamente la orden de la orden de arribaResultados en una configuración de búsqueda web real. Examinamos alternativas para incorporar comentarios en el proceso de clasificación y explorar las contribuciones de los comentarios de los usuarios en comparación con otras características de búsqueda web comunes. Reportamos los resultados de una evaluación a gran escala más de 3.000 consultas y 12 millones de interacciones de usuario con un popular motor de búsqueda web. Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de un algoritmos competitivos de clasificación de búsqueda web hasta un 31% en relación con el rendimiento original. Categorías y descriptores de sujetos H.3.3 Búsqueda y recuperación de información - Comentarios de relevancia, proceso de búsqueda;H.3.5 Servicios de información en línea - Servicios basados en la web. Algoritmos de términos generales, medición, experimentación 1. Introducción millones de usuarios interactúan diariamente con los motores de búsqueda. Emiten consultas, siguen algunos de los enlaces en los resultados, hacen clic en anuncios, pasan tiempo en páginas, reforman sus consultas y realizan otras acciones. Estas interacciones pueden servir como una fuente valiosa de información para ajustar y mejorar la clasificación de los resultados de la búsqueda web y pueden complementar juicios explícitos más costosos. La retroalimentación de relevancia implícita para la clasificación y la personalización se ha convertido en un área activa de investigación. El trabajo reciente de Joachims y otros que exploran la retroalimentación implícita en entornos controlados han demostrado el valor de incorporar la retroalimentación implícita en el proceso de clasificación. Nuestra motivación para este trabajo es comprender cómo se puede utilizar la retroalimentación implícita en un entorno operativo a gran escala para mejorar la recuperación. ¿Cómo se compara y complementa la evidencia del contenido de la página, el texto de anclaje o las características basadas en enlaces como Inlinks o PageRank? Si bien es intuitivo que las interacciones del usuario con el motor de búsqueda web revelen al menos alguna información que podría usarse para la clasificación, estimar las preferencias de los usuarios en la configuración de búsqueda web real es un problema desafiante, ya que las interacciones reales del usuario tienden a ser más ruidosas de lo que comúnmente se supone.En los entornos controlados de estudios anteriores. Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación de los usuarios puede ser ruidosa (o adversa) y un motor de búsqueda web ya usa cientos de características y está muy sintonizado. Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el comportamiento real del usuario obtenido como parte de las interacciones normales con el motor de búsqueda web. Las contribuciones específicas de este documento incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3).• Una aplicación de un modelo de retroalimentación implícita robusto derivado de la extracción de millones de interacciones del usuario con un importante motor de búsqueda web (Sección 4).• Una evaluación a gran escala sobre consultas reales de usuario y resultados de búsqueda, que muestra mejoras significativas derivadas de la incorporación de comentarios de los usuarios (Sección 6). Resumimos nuestros hallazgos y discutimos extensiones al trabajo actual en la Sección 7, lo que concluye el documento.2. Antecedentes y resultados de búsqueda de clasificación de trabajo relacionados es un problema fundamental en la recuperación de la información. Los enfoques más comunes se centran principalmente en la similitud de la consulta y una página, así como en la calidad general de la página [3,4,24]. Sin embargo, con la creciente popularidad de los motores de búsqueda, los comentarios implícitos (es decir, las acciones que toman los usuarios al interactuar con el motor de búsqueda) se pueden usar para mejorar las clasificaciones. Las medidas de relevancia implícita han sido estudiadas por varios grupos de investigación. Una descripción general de las medidas implícitas se compila en Kelly y Teevan [14]. Esta investigación, mientras desarrollaba información valiosa sobre medidas de relevancia implícitas, no se aplicó para mejorar la clasificación de los resultados de búsqueda web en entornos realistas. Estrechamente relacionado con nuestro trabajo, Joachims [11] recopilaron medidas implícitas en lugar de medidas explícitas, introduciendo una técnica basada completamente en datos de clic para aprender funciones de clasificación. Fox et al.[8] exploró la relación entre las medidas implícitas y explícitas en la búsqueda web, y desarrolló modelos bayesianos para correlacionar las medidas implícitas y los juicios de relevancia explícita para consultas individuales y sesiones de búsqueda. Este trabajo consideró una amplia gama de comportamientos del usuario (por ejemplo, tiempo de permanencia, tiempo de desplazamiento, patrones de reformulación) además del popular comportamiento de clics. Sin embargo, el esfuerzo de modelado tenía como objetivo predecir juicios de relevancia explícita de las acciones implícitas del usuario y no específicamente en las funciones de clasificación de aprendizaje. Otros estudios del comportamiento del usuario en la búsqueda web incluyen Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación. Más recientemente, Joachims et al.[12] presentó una evaluación empírica de la interpretación de la evidencia de clics. Al realizar estudios de seguimiento ocular y correlacionar predicciones de sus estrategias con calificaciones explícitas, los autores mostraron que es posible interpretar con precisión los clics en un entorno de laboratorio controlado. Desafortunadamente, la medida en que la investigación anterior se aplica a la búsqueda web del mundo real no está claro. Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de información de clics para mejorar la clasificación de búsqueda web es prometedor, solo captura un aspecto de las interacciones del usuario con los motores de búsqueda web. Construimos una investigación existente para desarrollar técnicas sólidas de interpretación del comportamiento del usuario para la configuración de búsqueda web real. En lugar de tratar a cada usuario como un experto confiable, agregamos información de trazas de sesión de búsqueda de usuarios múltiples, poco confiables, como describimos en las siguientes dos secciones.3. Incorporando la retroalimentación implícita, consideramos dos enfoques complementarios para la clasificación con la retroalimentación implícita: (1) Tratar la retroalimentación implícita como evidencia independiente para los resultados de clasificación y (2) integrar las características de retroalimentación implícita directamente en el algoritmo de clasificación. Describimos los dos enfoques de clasificación general a continuación. Las características de retroalimentación implícita específicas se describen en la Sección 4, y los algoritmos para interpretar e incorporar la retroalimentación implícita se describen en la Sección 5. 3.1 Comentarios implícitos como evidencia independiente El enfoque general es volver a clasificar los resultados obtenidos por un motor de búsqueda web de acuerdo conClickthrough observado y otras interacciones de usuario para la consulta en sesiones de búsqueda anteriores. A cada resultado se le asigna una puntuación de acuerdo con la relevancia esperada/satisfacción del usuario basada en interacciones anteriores, lo que resulta en un orden de preferencia basado solo en las interacciones del usuario. Si bien ha habido un trabajo significativo para fusionar múltiples clasificaciones, adaptamos un enfoque simple y robusto de ignorar los puntajes originales de Rankers, y simplemente fusionar las órdenes de rango. La razón principal para ignorar los puntajes originales es que, dado que los espacios de características y los algoritmos de aprendizaje son diferentes, las puntuaciones no son directamente comparables y la re-normalización tiende a eliminar el beneficio de incorporar las puntuaciones del clasificador. Experimentamos con una variedad de funciones de fusión en el conjunto de consultas de desarrollo (y utilizando un conjunto de interacciones desde un período de tiempo diferente a partir de conjuntos de evaluación final). Descubrimos que un rango simple que fusiona la combinación heurística funciona bien, y es robusto a las variaciones en los valores de puntaje de los rankers originales. Para una consulta dada Q, el puntaje implícito ISD se calcula para cada resultado D de las características de interacción del usuario disponibles, lo que resulta en la ID de rango implícita para cada resultado. Calculamos una puntuación fusionada SM (D) para D combinando los rangos obtenidos de la retroalimentación implícita, ID con el rango original de D, OD: ¡¢ £ + + + + = de lo contrario o DforexistsFeedBackimpliCitif oi W Woids D DD I IDDM 1 1 1 11 1 1 1) ,,, (donde el peso WI es un factor de escala heurísticamente sintonizado que representa la importancia relativa de la retroalimentación implícita. Los resultados de la consulta se ordenan disminuyendo los valores de SM para producir la clasificación final. Un caso especial de este modelo surge al establecer WI en un valor muy grande, forzando efectivamente los resultados de los clics para clasificarse más altos que los resultados no haciendo clic, una heurística intuitiva y efectiva que usaremos como línea de base. La aplicación de algoritmos de combinación de clasificadores y ranker más sofisticados puede dar lugar a mejoras adicionales, y es una dirección prometedora para el trabajo futuro. El enfoque anterior supone que no hay interacciones entre las características subyacentes que producen la clasificación de búsqueda web original y las características de retroalimentación implícitas. Ahora relajamos esta suposición integrando las características de retroalimentación implícita directamente en el proceso de clasificación.3.2 Clasificación con características de retroalimentación implícita Los motores de búsqueda web modernos Los resultados de clasificación en función de una gran cantidad de características, incluidas las características basadas en el contenido (es decir, cuán de cerca una consulta coincide con el texto o el título o el texto de anclaje del documento), y la página independiente de la consultaCaracterísticas de calidad (por ejemplo, PageRank del documento o el dominio). En la mayoría de los casos, se desarrollan métodos automáticos (o semiautomáticos) para ajustar la función de clasificación específica que combina estos valores de características. Por lo tanto, un enfoque natural es incorporar características de retroalimentación implícita directamente como características para el algoritmo de clasificación. Durante el entrenamiento o el ajuste, el ranker se puede ajustar como antes pero con características adicionales. En tiempo de ejecución, el motor de búsqueda obtendría las características de retroalimentación implícita asociadas con cada par de URL de resistencia de consulta. Este modelo requiere que un algoritmo de clasificación sea robusto para los valores faltantes: más del 50% de las consultas a los motores de búsqueda web son únicos, sin comentarios implícitos previos disponibles. Ahora describimos un ranker de este tipo que solíamos aprender sobre los conjuntos de características combinadas, incluidos los comentarios implícitos.3.3 Aprender a clasificar los resultados de la búsqueda web Un aspecto clave de nuestro enfoque es explotar los avances recientes en el aprendizaje automático, a saber, los algoritmos de clasificación capacitables para la búsqueda en la web y la recuperación de información (por ejemplo, [5, 11] y resultados clásicos revisados en [3]). En nuestro entorno, los juicios explícitos de relevancia humana (etiquetas) están disponibles para un conjunto de consultas y resultados de búsqueda web. Por lo tanto, una opción atractiva para usar es una técnica de aprendizaje automático supervisado para aprender una función de clasificación que mejor predice juicios de relevancia. RankNet es uno de esos algoritmo. Es un algoritmo de ajuste de red neuronal que optimiza los pesos de las características para que mejor coincida explícitamente las preferencias de usuario proporcionadas por pares. Si bien los algoritmos de entrenamiento específicos utilizados por RankNet están más allá del alcance de este documento, se describe en detalle en [5] e incluye una evaluación extensa y comparación con otros métodos de clasificación. Una característica atractiva de RankNet es la eficiencia del tiempo de ejecución y el tiempo de ejecución: la clasificación de tiempo de ejecución se puede calcular rápidamente y puede escalar a la web, y la capacitación se puede realizar durante miles de consultas y resultados juzgados asociados. Utilizamos una implementación de 2 capas de RankNet para modelar relaciones no lineales entre características. Además, RankNet puede aprender con muchas funciones de costos (diferenciables) y, por lo tanto, puede aprender automáticamente una función de clasificación de etiquetas proporcionadas por humanos, una alternativa atractiva a las técnicas de combinación de características heurísticas. Por lo tanto, también usaremos RankNet como un ranker genérico para explorar la contribución de la retroalimentación implícita para diferentes alternativas de clasificación.4. Modelo de retroalimentación de usuarios implícito Nuestro objetivo es interpretar con precisión la retroalimentación de los usuarios ruidosas obtenidas como rastrear las interacciones del usuario con el motor de búsqueda. La interpretación de comentarios implícitos en la configuración de búsqueda web real no es una tarea fácil. Caracterizamos este problema en detalle en [1], donde motivamos y evaluamos una amplia variedad de modelos de actividades de usuario implícitas. El enfoque general es representar las acciones del usuario para cada resultado de la búsqueda como un vector de características, y luego capacitar a un ranker en estas características para descubrir valores de características indicativos de resultados de búsqueda relevantes (y no relevantes). Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar información suficiente para replicar nuestros métodos de clasificación y los experimentos posteriores.4.1 Representación de las acciones del usuario como características modelamos los comportamientos de búsqueda web observados como una combinación de un componente de fondo `` (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos los sesgos posicionales con interacciones de resultados) y un componente de relevancia `` ((es decir, comportamiento específico de consulta indicativo de relevancia de un resultado a una consulta). Diseñamos nuestras características para aprovechar el comportamiento agregado del usuario. El conjunto de características se compone de características observadas directamente (calculadas directamente a partir de observaciones para cada consulta), así como características derivadas de consulta, calculadas como la desviación de la distribución general independiente de la consulta de los valores para los valores de características observados directamente correspondientes. Las características utilizadas para representar las interacciones del usuario con los resultados de búsqueda web se resumen en la Tabla 4.1. Esta información se obtuvo a través de la instrumentación del lado del cliente de los usuarios de un importante motor de búsqueda web. Incluimos las características tradicionales de retroalimentación implícita, como los recuentos de clics para los resultados, así como nuestras nuevas características derivadas, como la desviación del número de clics observado para un par de consulta dada del número esperado de clics en un resultado en el resultado dado.posición. También modelamos el comportamiento de navegación después de que se hizo clic en un resultado, por ejemplo, el tiempo promedio de permanencia de la página para un par de consulta -URL dada, así como su desviación del tiempo de permanencia esperado (promedio). Además, el conjunto de características fue diseñado para proporcionar información esencial sobre la experiencia del usuario para hacer que la interpretación de comentarios sea robusta. Por ejemplo, los usuarios de búsqueda web a menudo pueden determinar si un resultado es relevante al observar el título de los resultados, la URL y el resumen; en muchos casos, no es necesario mirar el documento original. Para modelar este aspecto de la experiencia del usuario, incluimos características como superposición en palabras en título y palabras en consulta (titeoverlap) y la fracción de palabras compartidas por la consulta y el resumen de resultados. Haga clic en las características de la posición Posición de la URL en la clasificación actual El número de clics de la frecuencia de consecuencia de los clics para esta consulta, el par de la URL Click -Probability Probabilidad de un clic para esta consulta y la desviación de la URL ClickDeviation Desviándose de la probabilidad de clic esperadaEn la posición anterior, 0 de lo contrario IsClickABove 1 Si hay un clic arriba, 0 de lo contrario ISCLICKBELOW 1 Si hay clic a continuación, 0 de lo contrario las características de la página Tiempo de permanencia de la página de tiempo CumulativetimeonPage Tiempo acumulativo para todasTiempo acumulativo en el prefijo de URL, no hay parámetros ISFollowDlink 1 Si se sigue el enlace al resultado, 0 de lo contrario isExacturlMatch 0 Si se usa la normalización agresiva, 1 de lo contrario ISredirected 1 si la URL inicial es igual a la URL final, 0 de otra manera ISPATHFROMSearch 1 si solo siguió los enlaces después de la consulta, 0 de otra manera de otra manera de otra maneraClicksFromSearch Número de lúpulos para llegar a la página desde consulta el tiempo promedio de promedio de tiempo en la página para esta consulta desviación de la evitación de la evitación del tiempo promedio de permanencia en la página Desviación de evaluación acumulada por la adviación de la desviación promedio del tiempo de permanencia acumulativa del tiempo de dominio del tiempo de permanencia promedio en el dominio.Título Resumen Palabras de SummaryOverLap compartidas entre la consulta y el fragmento Palabras de consulta de consultas compartidas entre consultas y url Consulta Palabras de consulta compartidas entre la consulta y el dominio URL El número de consultas de la consulta de tokens en Query QueryExtoverlap fracción de palabras compartidas con la siguiente consulta Tabla 4.1: Algunas características utilizadas para representar la historia posterior a la búsqueda de la historia de navegación Historia de navegaciónpara una consulta dada y URL de resultado de búsqueda. Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de comportamiento del usuario.4.2 Derando un modelo de retroalimentación del usuario para aprender a interpretar el comportamiento del usuario observado, correlacionamos las acciones del usuario (es decir, las características de la Tabla 4.1 que representan las acciones) con los juicios de usuario explícitos para un conjunto de consultas de capacitación. Encontramos todas las instancias en nuestros registros de sesión donde estas consultas se enviaron al motor de búsqueda y agregamos las características de comportamiento del usuario para todas las sesiones de búsqueda que involucran estas consultas. Cada par de consultas observado está representado por las características en la Tabla 4.1, con valores promediados en todas las sesiones de búsqueda, y se asigna una de las seis etiquetas de relevancia posibles, que van desde perfectas a malas, según lo asignado por juicios de relevancia explícita. Estos vectores de características etiquetadas se utilizan como entrada al algoritmo de entrenamiento RankNet (Sección 3.3) que produce un modelo de comportamiento de usuario entrenado. Este enfoque es particularmente atractivo, ya que no requiere heurística más allá de la ingeniería de características. El modelo de comportamiento del usuario resultante se utiliza para ayudar a clasificar la búsqueda de la búsqueda en la web directamente o en combinación con otras características, como se describe a continuación.5. Configuración experimental El objetivo final de incorporar la retroalimentación implícita en la clasificación es mejorar la relevancia de los resultados de búsqueda web devueltos. Por lo tanto, comparamos los métodos de clasificación con un gran conjunto de consultas juzgadas con etiquetas de relevancia explícita proporcionadas por jueces humanos. Para que la evaluación sea realista, obtuvimos una muestra aleatoria de consultas de los registros de búsqueda web de un motor de búsqueda importante, con resultados asociados y trazas para las acciones del usuario. Describimos este conjunto de datos en detalle a continuación. Nuestras métricas se describen en la Sección 5.2 que utilizamos para evaluar las alternativas de clasificación, enumeradas en la Sección 5.3 en los experimentos de la Sección 6. 5.1 conjuntos de datos comparamos nuestros métodos de clasificación con una muestra aleatoria de 3.000 consultas de los registros de consultas de motores de búsqueda. Las consultas se extrajeron de los registros de manera uniforme al azar por token sin reemplazo, lo que resultó en una muestra de consulta representativa de la distribución general de la consulta. En promedio, 30 resultados fueron etiquetados explícitamente por jueces humanos utilizando una escala de seis puntos que varía desde perfecta hasta mal. En general, hubo más de 83,000 resultados con juicios de relevancia explícita. Para calcular varias estadísticas, los documentos con la etiqueta bueno o mejor se considerarán relevantes, y con las etiquetas más bajas para no ser relevantes. Tenga en cuenta que los experimentos se realizaron sobre los resultados ya altamente clasificados por un motor de búsqueda web, que corresponde a una experiencia de usuario típica que se limita al pequeño número de resultados altamente clasificados para una consulta de búsqueda web típica. Las interacciones del usuario se recopilaron durante un período de 8 semanas utilizando información voluntaria de opción. En total, se instrumentaron más de 1.2 millones de consultas únicas, lo que resultó en más de 12 millones de interacciones individuales con el motor de búsqueda. Los datos consistieron en interacciones del usuario con el motor de búsqueda web (por ejemplo, hacer clic en un enlace de resultados, volver a los resultados de búsqueda, etc.) realizados después de que se envió una consulta. Estas acciones se agregaron entre los usuarios y las sesiones de búsqueda y se convirtieron en características en la Tabla 4.1. Para crear los conjuntos de capacitación, validación y consultas de prueba, creamos tres divisiones aleatorias diferentes de 1,500 capacitación, 500 validación y 1000 consultas de prueba. Las divisiones se hicieron al azar mediante consulta, de modo que no hubo superposición en el entrenamiento, la validación y las consultas de prueba.5.2 Métricas de evaluación Evaluamos los algoritmos de clasificación en un rango de métricas de recuperación de información aceptadas, a saber, precisión en K (p (k)), ganancia acumulativa con descuento normalizada (NDCG) y precisión promedio media (MAP). Cada métrica se centra en un aspecto deferente del rendimiento del sistema, como describimos a continuación.• Precisión en K: Como la métrica más intuitiva, P (k) informa la fracción de documentos clasificados en los resultados de K superiores que se etiquetan como relevantes. En nuestro entorno, requerimos un documento relevante para ser etiquetado como bueno o más alto. La posición de los documentos relevantes dentro de la K superior es irrelevante y, por lo tanto, esta métrica medida general de satisfacción del usuario con los resultados principales de K.• NDCG en K: NDCG es una medida de recuperación ideada específicamente para la evaluación de búsqueda web [10]. Para una consulta dada Q, los resultados clasificados se examinan desde la parte superior clasificada, y el NDCG calculado como: = + - = k j jr qq jmn 1) () 1log (/) 12 (donde MQ es una constante de normalización calculada SOque un pedido perfecto obtendría NDCG de 1; y cada R (j) es una etiqueta de relevancia entera (0 = mala y 5 = perfecta) de resultado devuelto en la posición j. Tenga en cuenta que los documentos no etiquetados y malos no contribuyen a la suma, pero reducirán NDCG para la consulta que empuja los documentos etiquetados relevantes, reduciendo sus contribuciones. NDCG es muy adecuado para la evaluación de la búsqueda web, ya que recompensa los documentos relevantes en los resultados mejor clasificados más en gran medida que los que se clasifican más bajos.• Mapa: la precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de que se recuperó cada documento relevante. El valor final del mapa se define como la media de las precisiones promedio de todas las consultas en el conjunto de pruebas. Esta métrica es el resumen de valor único más utilizado de una ejecución sobre un conjunto de consultas.5.3 Métodos de clasificación comparados Recuerde que nuestro objetivo es cuantificar la efectividad del comportamiento implícito para la búsqueda web real. Una dimensión es comparar la utilidad de la retroalimentación implícita con otra información disponible para un motor de búsqueda web. Específicamente, comparamos la efectividad de los comportamientos de usuario implícitos con características de calidad de página estática basadas en contenido y combinaciones de todas las características.• BM25F: como una línea de base de búsqueda web fuerte, utilizamos la puntuación BM25F, que se utilizó en uno de los sistemas de mejor rendimiento en la pista web de TREC 2004 [23,27]. BM25F y sus variantes se han descrito y evaluado ampliamente en la literatura IR y, por lo tanto, sirven como una línea de base fuerte y reproducible. La variante BM25F que utilizamos para nuestros experimentos calcula puntajes de coincidencia separados para cada campo para un documento de resultado (por ejemplo, texto del cuerpo, título y texto de anclaje), e incorpora información basada en enlaces independientes de la consulta (por ejemplo, PageRank, ClickDistance y URL Profundation). La función de puntuación y el ajuste específico de campo se describen en detalle en [23]. Tenga en cuenta que BM25F no considera directamente la retroalimentación explícita o implícita para el ajuste.• RN: la clasificación producida por un Ranker de red neuronal (RankNet, descrita en la Sección 3.3) que aprende a clasificar los resultados de búsqueda web incorporando BM25F y una gran cantidad de características estáticas y dinámicas adicionales que describen cada resultado de la búsqueda. Este sistema aprende automáticamente pesos para todas las características (incluida la puntuación BM25F para un documento) basado en etiquetas humanas explícitas para un gran conjunto de consultas. Un sistema que incorpora una implementación de RankNet está actualmente en uso por un importante motor de búsqueda y puede considerarse representativo del estado de la técnica en la búsqueda web.• BM25F-RerankCT: la clasificación producida al incorporar estadísticas de clic para reordenar los resultados de búsqueda web clasificados por BM25F anterior. Clickthrough es un caso especial particularmente importante de retroalimentación implícita, y se ha demostrado que se correlaciona con la relevancia de los resultados. Este es un caso especial del método de clasificación en la Sección 3.1, con el peso WI establecido en 1000 y la ID de clasificación es simplemente el número de clics en el resultado correspondiente a d.En efecto, esta clasificación lleva a la parte superior todos los resultados de búsqueda web devueltos con al menos un clic (y los ordena en orden decreciente por número de clics). La clasificación relativa del resto de los resultados no cambia y se insertan debajo de todos los resultados haciendo clic. Este método sirve como nuestro método de referencia de retroalimentación implícita de línea de base. BM25F-Rerankall La clasificación producida reordenando los resultados BM25F utilizando todas las características de comportamiento del usuario (Sección 4). Este método aprende un modelo de preferencias del usuario correlacionando los valores de características con etiquetas de relevancia explícita utilizando el algoritmo de red neuronal RankNet (Sección 4.2). En tiempo de ejecución, para una consulta dada, la puntuación implícita IR se calcula para cada resultado R con las características de interacción del usuario disponibles, y se produce la clasificación implícita. La clasificación fusionada se calcula como se describe en la Sección 3.1. Según los experimentos sobre el conjunto de desarrollo, fijamos el valor de WI a 3 (el efecto del parámetro WI para este ranker resultó ser insignificante).• BM25F+ALL: Ranking Derivado mediante la capacitación del alumno RankNet (Sección 3.3) sobre el conjunto de características del puntaje BM25F, así como todas las características de retroalimentación implícita (Sección 3.2). Utilizamos la implementación de 2 capas de RankNet [5] capacitada en las consultas y etiquetas en los conjuntos de capacitación y validación.• RN+ALL: Ranking derivado de la capacitación del algoritmo de clasificación RankNet de 2 capas (Sección 3.3) sobre la unión de todas las características de retroalimentación de contenido, dinámica e implícita (es decir, todas las características descritas anteriormente, así como todas las nuevas implícitas.Características de retroalimentación que presentamos). Los métodos de clasificación sobre el rango de la información utilizada para la clasificación, desde no usar la retroalimentación implícita o explícita (es decir, BM25F) hasta un motor de búsqueda web moderno que utiliza cientos de características y sintonizados en juicios explícitos (RN). Como mostraremos a continuación, la incorporación del comportamiento del usuario en estos sistemas de clasificación mejora dramáticamente la relevancia de los documentos devueltos.6. Resultados experimentales La retroalimentación implícita para la clasificación de búsqueda web se puede explotar de varias maneras. Comparamos métodos alternativos para explotar la retroalimentación implícita, tanto al volver a clasificar los resultados principales (es decir, los métodos BM25F-RerankCT y BM25F-Rerankall que reordenan los resultados de BM25F), así como integrando las características implícitas directamente en el proceso de clasificación (es decir, es decir,, el RN+All y BM25F+todos los métodos que aprenden a clasificar los resultados sobre la retroalimentación implícita y otras características). Comparamos nuestros métodos sobre líneas de base fuertes (BM25F y RN) sobre el NDCG, la precisión en K y las medidas del mapa definidas en la Sección 5.2. Los resultados se promediaron en tres divisiones aleatorias del conjunto de datos general. Cada división contenía 1500 capacitación, 500 validación y 1000 consultas de prueba, todas las consultas establecen disjuntos. Primero presentamos los resultados sobre las consultas de prueba 1000 (es decir, incluidas las consultas para las cuales no hay medidas implícitas, por lo que usamos las clasificaciones web originales). Luego profundizamos para examinar los efectos sobre la reperancia para los intentos de consultas con más detalle, analizando cuando la retroalimentación implícita resultó más beneficiosa. Primero experimentamos con diferentes métodos para volver a clasificar la salida de los resultados de búsqueda BM25F. Las Figuras 6.1 y 6.2 informan NDCG y precisión para BM25F, así como para las estrategias que vuelven a ser los resultados con los comentarios de los usuarios (Sección 3.1). La incorporación de todos los comentarios de los usuarios (ya sea en el marco de reestructura o como las características del alumno directamente) da como resultado mejoras significativas (utilizando la prueba t de dos colas con p = 0.01) tanto en la clasificación BM25F original como en exceso de rerantería con clic solo. La mejora es consistente en los 10 resultados principales y más grande para el resultado superior: NDCG en 1 para BM25F+todo es 0.622 en comparación con 0.518 de los resultados originales, y la precisión a 1 aumenta de manera similar de 0.5 a 0.63. En base a estos resultados, utilizaremos la combinación de características directas (es decir, BM25F+All) Ranker para comparaciones posteriores que implican comentarios implícitos.0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+Toda la Figura 6.1: NDCG en K para BM25F, BM25F-Rerankct, BM25F--------------------------------Rerank-All, y BM25F+All para variar K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 10 K Precisión BM25 BM25-Rerank-CT BM25-Rerank-All BM25+Toda Figura 6.2: Precisión en K para BM25F, BM25F Rerankct-Rerankct-Rerankct-Rerankct, BM25F-Rerank-All, y BM25F+All para variar K curiosamente, usando solo hacer clic, al tiempo que brindan un beneficio significativo sobre la clasificación BM25F original, no es tan efectivo como considerando el conjunto completo de características en la Tabla 4.1. Si bien analizamos el comportamiento del usuario (y las características de componentes más efectivas) en un artículo separado [1], vale la pena dar un ejemplo concreto del tipo de ruido inherente a la retroalimentación real de los usuarios en la configuración de búsqueda web.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Posición de resultado RelativeclickFrequency PTR = 2 PTR = 3 PTR = 5 Figura 6.3: Frecuencia de clic relativo para consultas con una posición variable de resultado relevante superior (PTR). Si los usuarios consideraron solo la relevancia de un resultado a su consulta, harían clic en los resultados más relevantes. Desafortunadamente, como han demostrado Joachims y otros, la presentación también influye en lo que los usuarios hacen que los usuarios hagan clic bastante dramáticamente. Los usuarios a menudo hacen clic en los resultados anteriores al relevante presumiblemente porque los resúmenes breves no proporcionan suficiente información para hacer evaluaciones de relevancia precisas y han aprendido que, en promedio, los elementos superiores son relevantes. La Figura 6.3 muestra frecuencias relativas de clic para consultas con elementos relevantes conocidos en posiciones distintas de la primera posición;La posición del resultado más relevante (PTR) varía de 2-10 en la figura. Por ejemplo, para consultas con el primer resultado relevante en la posición 5 (PTR = 5), hay más clics en los resultados no relevantes en posiciones de mayor rango que en el primer resultado relevante en la posición 5. Como veremos, aprender sobre un conjunto de características de comportamiento más ricas, resulta en una mejora de precisión sustancial solo sobre el clic. Ahora consideramos incorporar el comportamiento del usuario en un conjunto de características mucho más rico, RN (Sección 5.3) utilizado por un importante motor de búsqueda web. RN incorpora BM25F, características basadas en enlaces y cientos de otras características. La Figura 6.4 informa que NDCG en K y la Figura 6.5 informa precisión en K. Curiosamente, mientras que las clasificaciones RN originales son significativamente más precisas que BM25F solo, incorporando características de retroalimentación implícita (BM25F+ALL) da como resultado que la clasificación supera significativamente las clasificaciones RN originales. En otras palabras, la retroalimentación implícita incorpora información suficiente para reemplazar las cientos de otras características disponibles para el alumno RankNet capacitado en el conjunto de características RN.0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+Todos BM25 BM25+Todos Figura 6.4: NDCG en K para BM25F, BM25F+All, RN, y RN+All For For For For For For For For For For For For For For For For For For For For For para BM25F.Variando K además, enriquecer las características de RN con un conjunto de retroalimentación implícita exhibe una ganancia significativa en todas las medidas, lo que permite que RN+todos superen a todos los demás métodos. Esto demuestra la naturaleza complementaria de la retroalimentación implícita con otras características disponibles para un motor de búsqueda web de última generación.0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precisión RN RN+Todos BM25 BM25+Todas la Figura 6.5: Precisión en K para BM25F, BM25F+ALL, RN y RN+All para variando K Resumimos el rendimiento de la diferente ranuraMétodos en la Tabla 6.1. Reportamos la puntuación media de precisión promedio (MAP) para cada sistema. Si bien no es intuitivo para interpretar, MAP permite una comparación cuantitativa en una sola métrica. Las ganancias marcadas con * son significativas en el nivel p = 0.01 usando dos pruebas t de cola. Ganancia del mapa P (1) Gane BM25F 0.184-0.503bm25f-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankimpliCit 0.218 0.003 0.605 0.028* BM25F+implícito 0.222 0.004 0.620 0.015* RN 0.215- 29 0.032* Tabla 6.1: Precisión promedio media (MAP) para todas las estrategias. Hasta ahora informamos que los resultados promediaron en todas las consultas en el conjunto de pruebas. Desafortunadamente, menos de la mitad tuvo interacciones suficientes para intentar el rescate. De las 1000 consultas en la prueba, entre 46% y 49%, dependiendo de la división de la prueba de tren, tenía suficiente información de interacción para hacer predicciones (es decir, hubo al menos 1 sesión de búsqueda en la que se hizo al menos 1 resultado URL de resultadospor el usuario). Esto no es sorprendente: la búsqueda web es de cola pesada, y hay muchas consultas únicas. Ahora consideramos el rendimiento en las consultas para las que estaban disponibles las interacciones del usuario. La Figura 6.6 informa NDCG para el subconjunto de las consultas de prueba con las características de retroalimentación implícita. Las ganancias en el Top 1 son dramáticas. El NDCG a 1 de BM25F+aumenta de 0.6 a 0.75 (una ganancia relativa del 31%), lo que alcanza el rendimiento comparable a RN+que operan en un conjunto de características mucho más rico.0.6 0.65 0.7 0.7 0.7 0.8 1 3 5 10k NDCG RN RN+Todos BM25 BM25+Todos Figura 6.6: NDCG en K para BM25F, BM25F+ALL, RN y RN+All en las consultas de prueba con interacciones de usuario de manera similar, se refiere a la precisión en Top en TOP1 son sustanciales (Figura 6.7), y es probable que sean evidentes para los usuarios de búsqueda web. Cuando hay una retroalimentación implícita disponible, el BM25F+All System devuelve el documento relevante en el top 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando el sistema BM25F no considera la retroalimentación implícita.0.45 0.5 0.55 0.6 0.65 0.7 1 3 3 5 10k Precisión RN RN+Todos BM25 BM25+Todos Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+ALL, RN y RN+ALL en Probar las interacciones de los usuarios Resumimos elResultados en la medida del mapa para consultas intentadas en la Tabla 6.2. Las mejoras de los mapas son sustanciales y significativas, con mejoras sobre el ranker BM25F más pronunciado. Método Mapa ganancia P (1) Ganancia RN 0.269 0.632 Rn+All 0.321 0.051 (19%) 0.693 0.061 (10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Tabla 6.2: Precisión media media precisión media Precisión media media(Mapa) En las consultas intentadas para los métodos de mejor rendimiento, ahora analizamos los casos en que la retroalimentación implícita se mostró más útil. La Figura 6.8 informa las mejoras del mapa sobre la línea de base BM25F para cada consulta con MAP bajo 0.6. Tenga en cuenta que la mayor parte de la mejora es para consultas de bajo rendimiento (es decir, MAP <0.1). Curiosamente, la incorporación de la información de comportamiento del usuario degrada la precisión para consultas con una puntuación de mapa original alta. Una posible explicación es que estas consultas fáciles tienden a ser navegacionales (es decir, tener una sola respuesta más apropiada altamente clasificada), y las interacciones de los usuarios con resultados de menor clasificación pueden indicar las necesidades de información divergente que son mejor atendidas por los resultados menos populares ((con calificaciones de relevancia generales correspondientemente pobres).0 50 100 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 ganancia promedio de frecuencia Figura 6.8: ganancia de BM25F+en todo el rango de BM25F originalPara resumir nuestros resultados experimentales, la incorporación de comentarios implícitos en la configuración de búsqueda web real dio como resultado mejoras significativas sobre las clasificaciones originales, utilizando las líneas de base BM25F y RN. Nuestro rico conjunto de características implícitas, como el tiempo en la página y las desviaciones del comportamiento promedio, proporciona ventajas sobre el uso de clic solo como un indicador de interés. Además, la incorporación de características de retroalimentación implícita directamente en la función de clasificación aprendida es más efectiva que usar comentarios implícitos para el reranking. Las mejoras observadas sobre grandes conjuntos de consultas de prueba (1,000 en total, entre 466 y 495 con retroalimentación implícita disponible) son sustanciales y estadísticamente significativas.7. Conclusiones y trabajo futuro En este documento exploramos la utilidad de incorporar retroalimentación implícita ruidosas obtenidas en una configuración de búsqueda web real para mejorar la clasificación de búsqueda web. Realizamos una evaluación a gran escala de más de 3.000 consultas y más de 12 millones de interacciones de los usuarios con un importante motor de búsqueda, estableciendo la utilidad de incorporar retroalimentación implícita ruidosas para mejorar la relevancia de la búsqueda en la web. Comparamos dos alternativas para incorporar la retroalimentación implícita en el proceso de búsqueda, a saber, replicar la retroalimentación implícita e incorporar características de retroalimentación implícita directamente en la función de clasificación capacitada. Nuestros experimentos mostraron una mejora significativa sobre los métodos que no consideran la retroalimentación implícita. Las ganancias son particularmente dramáticas para el resultado de K = 1 superior en la clasificación final, con mejoras de precisión de hasta el 31%, y las ganancias son sustanciales para todos los valores de K. Nuestros experimentos mostraron que la retroalimentación implícita de los usuarios puede mejorar aún más el rendimiento de la búsqueda web., cuando se incorpora directamente con características populares basadas en contenido y enlaces. Curiosamente, la retroalimentación implícita es particularmente valiosa para consultas con una clasificación original de los resultados (por ejemplo, mapa inferior a 0.1). Una dirección prometedora para el trabajo futuro es aplicar investigaciones recientes sobre la predicción de la dificultad de consultas automáticamente, y solo intenta incorporar comentarios implícitos para las consultas difíciles. Como otra dirección de investigación, estamos explorando métodos para extender nuestras predicciones a las consultas previamente invisibles (por ejemplo, agrupación de consultas), lo que debería mejorar aún más la experiencia de búsqueda en la web de los usuarios. Agradecimientos Agradecemos a Chris Burges y Matt Richardson por una implementación de RankNet para nuestros experimentos. También agradecemos a Robert Ragno por sus valiosas sugerencias y muchas discusiones.8. Referencias [1] E. Agichtein, E. Brill, S. Dumais y R.Ragno, Aprender modelos de interacción de usuario para predecir las preferencias de los resultados de la búsqueda web. En Actas de la Conferencia de la ACM sobre Investigación y Desarrollo sobre Recuperación de Información (SIGIR), 2006 [2] J. Allan, Hard Track Descripción general en TREC 2003, Recuperación de alta precisión de los documentos, 2003 [3] R. Baeza-Yates y B.Ribeiro-Neto, Recuperación de información moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala, en los procedimientos de WWW, 1997 [5] C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Aprendiendo a clasificarse utilizando descenso de gradiente, en Actas de la Conferencia Internacional de Aprendizaje Machine, 2005 [6] D.M. Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee y M. Waseda. Inferir el interés del usuario. IEEE Internet Computing.2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais y T. White. Evaluar las medidas implícitas para mejorar la experiencia de búsqueda. En Transacciones ACM en Sistemas de Información, 2005 [9] J. Goecks y J. Shavlick. Aprender los intereses de los usuarios observando discretamente su comportamiento normal. En Actas del Taller IJCAI sobre aprendizaje automático para el filtrado de información.1999. [10] K Jarvelin y J. Kekalainen. IR Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la Conferencia ACM sobre Investigación y Desarrollo sobre Recuperación de Información (SIGIR), 2000 [11] T. Joachims, optimizando los motores de búsqueda utilizando datos de clics. En Actas de la Conferencia de ACM sobre Discovery y Datamining (Sigkdd), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, interpretando con precisión los datos de clics como comentarios implícitos, procedimientos deLa Conferencia de ACM sobre investigación y desarrollo sobre recuperación de información (SIGIR), 2005 [13] T. Joachims, haciendo práctico el aprendizaje SVM a gran escala. Avances en los métodos del núcleo, en el aprendizaje de vectores de soporte, MIT Press, 1999 [14] D. Kelly y J. Teevan, Comentarios implícitos para inferir la preferencia del usuario: una bibliografía. En Sigir Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon y J. Riedl. Grouplens: aplicando filtrado colaborativo a Usenet News. En Communications of ACM, 1997. [16] M. morita e Y. shinoda, filtrado de información basado en el análisis de comportamiento del usuario y la mejor recuperación de texto de coincidencia. Actas de la Conferencia ACM sobre investigación y desarrollo sobre recuperación de información (SIGIR), 1994 [17] D. Oard y J. Kim. Comentarios implícitos para los sistemas de recomendación. En Actas del Taller AAAI sobre Sistemas de Recomendación.1998 [18] D. Oard y J. Kim. Modelado de contenido de información utilizando comportamiento observable. En Actas de la 64ª Reunión Anual de la Sociedad Americana de Ciencias y Tecnología de la Información.2001 [19] N. Pharo, N. y K. Järvelin. El método SST: una herramienta para analizar los procesos de búsqueda de información web. En Information Processing & Management, 2004 [20] P. Pirolli, El uso del aroma de información proximal para buscar contenido distal en la red mundial. Al trabajar con la tecnología en mente: Brunswikian. Recursos para la ciencia e ingeniería cognitiva, Oxford University Press, 2004 [21] F. Radlinski y T. Joachims, Cadenas de consulta: aprendiendo a clasificarse a partir de comentarios implícitos. En Actas de la Conferencia de ACM sobre descubrimiento de conocimiento y minería de datos (SIGKDD), 2005. [22] F. Radlinski y T. Joachims, evaluando la solidez del aprendizaje de los comentarios implícitos, en los procedimientos del taller ICML sobre el aprendizaje en la búsqueda en la web,2005 [23] S. E. Robertson, H. Zaragoza y M. Taylor, Extensión simple de BM25 a múltiples campos ponderados, en Actas de la Conferencia sobre Gestión de Información y Conocimiento (CIKM), 2004 [24] G. Salton y M. McGill. Introducción a la recuperación de información moderna. McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Descripción general de Trec, 2001 [26] G.R. Xue, H.J. Zeng, Z. Chen, Y. Yu, W.Y. MA, W.S. XI y W.G. Fan, Optimización de la búsqueda web utilizando datos de clic en la web, en Actas de la Conferencia sobre Información y Gestión del Conocimiento (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria y S. Robertson. Microsoft Cambridge en TREC 13: pistas web y duras. En Actas de Trec 2004",
    "original_sentences": [
        "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
        "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
        "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
        "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
        "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
        "General Terms Algorithms, Measurement, Experimentation 1.",
        "INTRODUCTION Millions of users interact with search engines daily.",
        "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
        "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
        "Implicit relevance feedback for ranking and personalization has become an active area of research.",
        "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
        "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
        "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
        "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
        "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
        "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
        "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
        "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
        "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
        "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
        "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
        "Implicit relevance measures have been studied by several research groups.",
        "An overview of implicit measures is compiled in Kelly and Teevan [14].",
        "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
        "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
        "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
        "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
        "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
        "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
        "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
        "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
        "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
        "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
        "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
        "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
        "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
        "We describe the two general ranking approaches next.",
        "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
        "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
        "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
        "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
        "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
        "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
        "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
        "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
        "The query results are ordered in by decreasing values of SM to produce the final ranking.",
        "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
        "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
        "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
        "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
        "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
        "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
        "During training or tuning, the ranker can be tuned as before but with additional features.",
        "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
        "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
        "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
        "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
        "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
        "RankNet is one such algorithm.",
        "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
        "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
        "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
        "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
        "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
        "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
        "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
        "Interpreting implicit feedback in real web search setting is not an easy task.",
        "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
        "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
        "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
        "We design our features to take advantage of aggregated user behavior.",
        "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
        "The features used to represent user interactions with web search results are summarized in Table 4.1.",
        "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
        "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
        "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
        "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
        "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
        "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
        "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
        "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
        "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
        "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
        "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
        "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
        "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
        "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
        "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
        "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
        "We describe this dataset in detail next.",
        "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
        "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
        "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
        "Overall, there were over 83,000 results with explicit relevance judgments.",
        "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
        "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
        "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
        "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
        "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
        "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
        "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
        "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
        "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
        "In our setting, we require a relevant document to be labeled Good or higher.",
        "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
        "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
        "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
        "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
        "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
        "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
        "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
        "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
        "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
        "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
        "The scoring function and field-specific tuning is described in detail in [23].",
        "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
        "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
        "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
        "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
        "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
        "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
        "This method serves as our baseline implicit feedback reranking method.",
        "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
        "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
        "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
        "The merged ranking is computed as described in Section 3.1.",
        "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
        "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
        "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
        "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
        "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
        "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
        "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
        "The results were averaged over three random splits of the overall dataset.",
        "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
        "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
        "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
        "We first experimented with different methods of re-ranking the output of the BM25F search results.",
        "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
        "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
        "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
        "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
        "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
        "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
        "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
        "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
        "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
        "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
        "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
        "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
        "RN incorporates BM25F, link-based features, and hundreds of other features.",
        "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
        "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
        "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
        "We report the Mean Average Precision (MAP) score for each system.",
        "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
        "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
        "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
        "So far we reported results averaged across all queries in the test set.",
        "Unfortunately, less than half had sufficient interactions to attempt reranking.",
        "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
        "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
        "We now consider the performance on the queries for which user interactions were available.",
        "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
        "The gains at top 1 are dramatic.",
        "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
        "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
        "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
        "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
        "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
        "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
        "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
        "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
        "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
        "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
        "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
        "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
        "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
        "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
        "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
        "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
        "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
        "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
        "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
        "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
        "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
        "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
        "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
        "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
        "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
        "Inferring user interest.",
        "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
        "Evaluating implicit measures to improve the search experience.",
        "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
        "Learning users interests by unobtrusively observing their normal behavior.",
        "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
        "IR evaluation methods for retrieving highly relevant documents.",
        "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
        "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
        "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
        "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
        "GroupLens: Applying collaborative filtering to usenet news.",
        "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
        "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
        "Implicit feedback for recommender systems.",
        "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
        "Modeling information content using observable behavior.",
        "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
        "The SST method: a tool for analyzing web information search processes.",
        "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
        "In Working with Technology in Mind: Brunswikian.",
        "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
        "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
        "Introduction to modern information retrieval.",
        "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
        "Xue, H.J.",
        "Zeng, Z. Chen, Y. Yu, W.Y.",
        "Ma, W.S.",
        "Xi, and W.G.",
        "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
        "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
        "In Proceedings of TREC 2004"
    ],
    "error_count": 0,
    "keys": {
        "web search": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving <br>web search</br> Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real <br>web search</br> setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common <br>web search</br> features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular <br>web search</br> engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive <br>web search</br> ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving <br>web search</br> result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the <br>web search</br> engine should reveal at least some information that could be used for ranking, estimating user preferences in real <br>web search</br> settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a <br>web search</br> engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking <br>web search</br> results using real user behavior obtained as part of normal interactions with the <br>web search</br> engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into <br>web search</br> ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major <br>web search</br> engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of <br>web search</br> results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in <br>web search</br>, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in <br>web search</br> include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world <br>web search</br> is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving <br>web search</br> ranking is promising, it captures only one aspect of the user interactions with <br>web search</br> engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real <br>web search</br> setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a <br>web search</br> engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original <br>web search</br> ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern <br>web search</br> engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to <br>web search</br> engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank <br>web search</br> Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for <br>web search</br> and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of <br>web search</br> queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real <br>web search</br> setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed <br>web search</br> behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with <br>web search</br> results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major <br>web search</br> engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, <br>web search</br> users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank <br>web search</br> resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned <br>web search</br> results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from <br>web search</br> logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a <br>web search</br> engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical <br>web search</br> query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the <br>web search</br> engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for <br>web search</br> evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to <br>web search</br> evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real <br>web search</br>.",
                "One dimension is to compare the utility of implicit feedback with other information available to a <br>web search</br> engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong <br>web search</br> baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank <br>web search</br> results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in <br>web search</br>. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder <br>web search</br> results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned <br>web search</br> results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern <br>web search</br> engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for <br>web search</br> ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in <br>web search</br> setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major <br>web search</br> engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art <br>web search</br> engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: <br>web search</br> is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to <br>web search</br> users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real <br>web search</br> setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real <br>web search</br> setting to improve <br>web search</br> ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve <br>web search</br> relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve <br>web search</br> performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the <br>web search</br> experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting <br>web search</br> Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual <br>web search</br> Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in <br>web search</br>, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing <br>web search</br> using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mejora de la clasificación de \"búsqueda web\" mediante la incorporación de información de comportamiento del usuario Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research Brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Resumen Resumen que incorporar los datos de comportamiento del usuario puede mejorar significativamente la ordende los mejores resultados en la configuración real de \"búsqueda web\".búsqueda Web",
                "Examinamos alternativas para incorporar comentarios en el proceso de clasificación y explorar las contribuciones de los comentarios de los usuarios en comparación con otras características comunes de \"búsqueda web\".búsqueda Web",
                "Reportamos los resultados de una evaluación a gran escala más de 3.000 consultas y 12 millones de interacciones de usuario con un popular motor de \"búsqueda web\".búsqueda Web",
                "Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de un algoritmos competitivos de clasificación de \"búsqueda web\" hasta un 31% en relación con el rendimiento original.búsqueda Web",
                "Estas interacciones pueden servir como una fuente valiosa de información para ajustar y mejorar la clasificación de resultados de \"búsqueda web\" y pueden complementar juicios explícitos más costosos.búsqueda Web",
                "Si bien es intuitivo que las interacciones del usuario con el motor de \"búsqueda web\" deben revelar al menos alguna información que podría usarse para la clasificación, estimar las preferencias de los usuarios en la configuración real de \"búsqueda web\" es un problema desafiante, ya que las interacciones reales del usuario tienden a ser másruidoso que comúnmente asumido en los entornos controlados de estudios anteriores.búsqueda Web",
                "Nuestro artículo explora si la retroalimentación implícita puede ser útil en entornos realistas, donde la retroalimentación de los usuarios puede ser ruidosa (o adversaria) y un motor de \"búsqueda web\" ya usa cientos de características y está muy sintonizado.búsqueda Web",
                "Con este fin, exploramos diferentes enfoques para clasificar los resultados de \"búsqueda web\" utilizando el comportamiento real del usuario obtenido como parte de las interacciones normales con el motor de \"búsqueda web\".búsqueda Web",
                "Las contribuciones específicas de este documento incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de \"búsqueda web\" (Sección 3).• Una aplicación de un modelo de retroalimentación implícita robusto derivado de la extracción de millones de interacciones del usuario con un importante motor de \"búsqueda web\" (Sección 4).• Una evaluación a gran escala sobre consultas reales de usuario y resultados de búsqueda, que muestra mejoras significativas derivadas de la incorporación de comentarios de los usuarios (Sección 6).búsqueda Web",
                "Esta investigación, mientras desarrollaba información valiosa sobre medidas de relevancia implícitas, no se aplicó para mejorar la clasificación de los resultados de la \"búsqueda en la web\" en entornos realistas.búsqueda Web"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "ranking": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search <br>ranking</br> by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the <br>ranking</br> process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search <br>ranking</br> algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result <br>ranking</br> and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for <br>ranking</br> and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the <br>ranking</br> process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for <br>ranking</br>, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for <br>ranking</br> web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search <br>ranking</br> (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK <br>ranking</br> search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the <br>ranking</br> of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn <br>ranking</br> functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning <br>ranking</br> functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve <br>ranking</br>.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search <br>ranking</br> is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to <br>ranking</br> with implicit feedback: (1) treating implicit feedback as independent evidence for <br>ranking</br> results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general <br>ranking</br> approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final <br>ranking</br>.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search <br>ranking</br> and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the <br>ranking</br> process. 3.2 <br>ranking</br> with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific <br>ranking</br> function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the <br>ranking</br> algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a <br>ranking</br> algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable <br>ranking</br> algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a <br>ranking</br> function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other <br>ranking</br> methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime <br>ranking</br> can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a <br>ranking</br> function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different <br>ranking</br> alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our <br>ranking</br> methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current <br>ranking</br> ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into <br>ranking</br> is to improve the relevance of the returned web search results.",
                "Hence, we compare the <br>ranking</br> methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the <br>ranking</br> alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our <br>ranking</br> methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the <br>ranking</br> algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 <br>ranking</br> Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The <br>ranking</br> produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The <br>ranking</br> produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the <br>ranking</br> method in Section 3.1, with the weight wI set to 1000 and the <br>ranking</br> Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative <br>ranking</br> of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The <br>ranking</br> produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit <br>ranking</br> is produced.",
                "The merged <br>ranking</br> is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: <br>ranking</br> derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: <br>ranking</br> derived by training the 2-layer RankNet <br>ranking</br> algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The <br>ranking</br> methods above span the range of the information used for <br>ranking</br>, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these <br>ranking</br> systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search <br>ranking</br> can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-<br>ranking</br> the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the <br>ranking</br> process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-<br>ranking</br> the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F <br>ranking</br> as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F <br>ranking</br>, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in <br>ranking</br> that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different <br>ranking</br> methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F <br>ranking</br> To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned <br>ranking</br> function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search <br>ranking</br>.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained <br>ranking</br> function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final <br>ranking</br>, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original <br>ranking</br> of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mejora de la búsqueda web \"clasificación\" incorporando información de comportamiento del usuario Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research Brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Resumen Resumen que la incorporación de los datos de comportamiento del usuario puede mejorar significativamente la ordende los mejores resultados en una configuración de búsqueda web real.clasificación",
                "Examinamos alternativas para incorporar comentarios en el proceso de \"clasificación\" y explorar las contribuciones de los comentarios de los usuarios en comparación con otras características de búsqueda web comunes.clasificación",
                "Mostramos que la incorporación de retroalimentación implícita puede aumentar otras características, mejorando la precisión de una búsqueda web competitiva de algoritmos de \"clasificación\" de hasta un 31% en relación con el rendimiento original.clasificación",
                "Estas interacciones pueden servir como una fuente valiosa de información para ajustar y mejorar la \"clasificación\" de los resultados de la búsqueda web y pueden complementar juicios explícitos más costosos.clasificación",
                "La retroalimentación de relevancia implícita para la \"clasificación\" y la personalización se ha convertido en un área activa de investigación.clasificación",
                "El trabajo reciente de Joachims y otros que exploran la retroalimentación implícita en entornos controlados han demostrado el valor de incorporar la retroalimentación implícita en el proceso de \"clasificación\".clasificación",
                "Si bien es intuitivo que las interacciones del usuario con el motor de búsqueda web revelen al menos alguna información que podría usarse para \"clasificar\", estimar las preferencias de los usuarios en la configuración de búsqueda web real es un problema desafiante, ya que las interacciones reales del usuario tienden a ser más ruidosas quecomúnmente asumido en los entornos controlados de estudios anteriores.clasificación",
                "Con este fin, exploramos diferentes enfoques para \"clasificar\" los resultados de búsqueda web utilizando el comportamiento real del usuario obtenido como parte de las interacciones normales con el motor de búsqueda web.clasificación",
                "Las contribuciones específicas de este documento incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la \"clasificación\" de búsqueda web (Sección 3).• Una aplicación de un modelo de retroalimentación implícita robusto derivado de la extracción de millones de interacciones del usuario con un importante motor de búsqueda web (Sección 4).• Una evaluación a gran escala sobre consultas reales de usuario y resultados de búsqueda, que muestra mejoras significativas derivadas de la incorporación de comentarios de los usuarios (Sección 6).clasificación",
                "Antecedentes y trabajos relacionados Los resultados de búsqueda de \"clasificación\" son un problema fundamental en la recuperación de la información.clasificación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "information retrieval": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information retrieval</br>.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and <br>information retrieval</br> (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted <br>information retrieval</br> metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern <br>information retrieval</br>, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on <br>information retrieval</br> (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern <br>information retrieval</br>.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Antecedentes y resultados de búsqueda de clasificación de trabajo relacionados es un problema fundamental en la \"recuperación de la información\".recuperación de información",
                "Ahora describimos un ranker de este tipo que solíamos aprender sobre los conjuntos de características combinadas, incluidos los comentarios implícitos.3.3 Aprender a clasificar los resultados de la búsqueda web Un aspecto clave de nuestro enfoque es explotar los avances recientes en el aprendizaje automático, a saber, los algoritmos de clasificación capacitables para la búsqueda web y la \"recuperación de información\" (por ejemplo, [5, 11] y resultados clásicos revisados en [3]).recuperación de información",
                "Las divisiones se hicieron al azar mediante consulta, de modo que no hubo superposición en el entrenamiento, la validación y las consultas de prueba.5.2 Métricas de evaluación Evaluamos los algoritmos de clasificación en un rango de métricas aceptadas de \"recuperación de información\", a saber, precisión en K (p (k)), ganancia acumulativa con descuento normalizada (NDCG) y precisión promedio media (MAP).recuperación de información",
                "En Actas de la Conferencia de la ACM sobre investigación y desarrollo sobre \"Recuperación de información\" (Sigir), 2006 [2] J. Allan, Hard Track Descripción general en TREC 2003, Recuperación de alta precisión de los documentos, 2003 [3] R. Baeza-Yates yB. Ribeiro-Neto, \"Recuperación de información\" moderna, Addison-Wesley, 1999. [4] S. Brin y L. Page, La anatomía de un motor de búsqueda web hipertextual a gran escala, en los procedimientos de WWW, 1997 [5]C.J.C.recuperación de información",
                "En Actas de la Conferencia de ACM sobre investigación y desarrollo sobre \"recuperación de información\" (SIGIR), 2000 [11] T. Joachims, optimizando los motores de búsqueda utilizando datos de clic.recuperación de información",
                "En Actas de la Conferencia de ACM sobre Discovery y Datamining (Sigkdd), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke y G. Gay, interpretando con precisión los datos de clics como comentarios implícitos, procedimientos deLa Conferencia ACM sobre investigación y desarrollo sobre \"recuperación de información\" (Sigir), 2005 [13] T. Joachims, haciendo práctico el aprendizaje SVM a gran escala.recuperación de información",
                "Actas de la Conferencia de ACM sobre investigación y desarrollo sobre \"recuperación de información\" (SIGIR), 1994 [17] D. Oard y J. Kim.recuperación de información",
                "Introducción a la \"recuperación de información\" moderna.recuperación de información"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "user behavior": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating <br>user behavior</br> Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating <br>user behavior</br> data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real <br>user behavior</br> obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating <br>user behavior</br> into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of <br>user behavior</br> in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust <br>user behavior</br> interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in <br>user behavior</br>, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated <br>user behavior</br>.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a <br>user behavior</br> model. 4.2 Deriving a User Feedback Model To learn to interpret the observed <br>user behavior</br>, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the <br>user behavior</br> features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained <br>user behavior</br> model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting <br>user behavior</br> model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all <br>user behavior</br> features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating <br>user behavior</br> into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze <br>user behavior</br> (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating <br>user behavior</br> into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating <br>user behavior</br> information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on <br>user behavior</br> analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mejora de la clasificación de búsqueda web incorporando información de \"comportamiento del usuario\" Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research Brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Resumen Resumen que incorporar que los datos de \"comportamiento del usuario\" pueden ser significativamente puedenMejorar el pedido de los mejores resultados en la configuración de búsqueda web real.comportamiento del usuario",
                "Con este fin, exploramos diferentes enfoques para clasificar los resultados de búsqueda web utilizando el \"comportamiento del usuario\" real obtenido como parte de las interacciones normales con el motor de búsqueda web.comportamiento del usuario",
                "Las contribuciones específicas de este documento incluyen: • Análisis de alternativas para incorporar el \"comportamiento del usuario\" en la clasificación de búsqueda web (Sección 3).• Una aplicación de un modelo de retroalimentación implícita robusto derivado de la extracción de millones de interacciones del usuario con un importante motor de búsqueda web (Sección 4).• Una evaluación a gran escala sobre consultas reales de usuario y resultados de búsqueda, que muestra mejoras significativas derivadas de la incorporación de comentarios de los usuarios (Sección 6).comportamiento del usuario",
                "Otros estudios sobre el \"comportamiento del usuario\" en la búsqueda web incluyen Pharo y Järvelin [19], pero no se aplicaron directamente para mejorar la clasificación.comportamiento del usuario",
                "Construimos una investigación existente para desarrollar técnicas de interpretación sólidas de \"comportamiento del usuario\" para la configuración de búsqueda web real.comportamiento del usuario",
                "Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar información suficiente para replicar nuestros métodos de clasificación y los experimentos posteriores.4.1 Representando las acciones del usuario como características, modelamos los comportamientos de búsqueda web observados como una combinación de un `` componente de fondo (es decir, ruido independiente de la consulta y relevancia en el \"comportamiento del usuario\", incluidos los sesgos posicionales con interacciones de resultados), y una `` relevancia ``componente (es decir, comportamiento específico de consulta indicativo de relevancia de un resultado a una consulta).comportamiento del usuario",
                "Diseñamos nuestras características para aprovechar el \"comportamiento del usuario\" agregado.comportamiento del usuario",
                "Habiendo descrito nuestro conjunto de características, revisamos brevemente nuestro método general para derivar un modelo de \"comportamiento del usuario\".4.2 Derando un modelo de retroalimentación del usuario para aprender a interpretar el \"comportamiento del usuario\" observado, correlacionamos las acciones del usuario (es decir, las características en la Tabla 4.1 que representan las acciones) con los juicios de usuario explícitos para un conjunto de consultas de capacitación.comportamiento del usuario",
                "Encontramos todas las instancias en nuestros registros de sesión donde estas consultas se enviaron al motor de búsqueda y agregamos las características de \"comportamiento del usuario\" para todas las sesiones de búsqueda que involucran estas consultas.comportamiento del usuario",
                "Estos vectores de características etiquetadas se utilizan como entrada al algoritmo de entrenamiento RankNet (Sección 3.3) que produce un modelo de \"comportamiento del usuario\" capacitado.comportamiento del usuario"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating <br>feedback</br> into the ranking process and explore the contributions of user <br>feedback</br> compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit <br>feedback</br> can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance <br>feedback</br>, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance <br>feedback</br> for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit <br>feedback</br> in controlled environments have shown the value of incorporating implicit <br>feedback</br> into the ranking process.",
                "Our motivation for this work is to understand how implicit <br>feedback</br> can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit <br>feedback</br> can be helpful in realistic environments, where user <br>feedback</br> can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit <br>feedback</br> model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user <br>feedback</br> (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit <br>feedback</br> (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT <br>feedback</br> We consider two complementary approaches to ranking with implicit <br>feedback</br>: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit <br>feedback</br> features are described in Section 4, and the algorithms for interpreting and incorporating implicit <br>feedback</br> are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit <br>feedback</br>, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit <br>feedback</br>.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit <br>feedback</br> features.",
                "We now relax this assumption by integrating implicit <br>feedback</br> features directly into the ranking process. 3.2 Ranking with Implicit <br>feedback</br> Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit <br>feedback</br> features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit <br>feedback</br> features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit <br>feedback</br> available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit <br>feedback</br>. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit <br>feedback</br> for different ranking alternatives. 4.",
                "IMPLICIT USER <br>feedback</br> MODEL Our goal is to accurately interpret noisy user <br>feedback</br> obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit <br>feedback</br> in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit <br>feedback</br> features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make <br>feedback</br> interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User <br>feedback</br> Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit <br>feedback</br> into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit <br>feedback</br> with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit <br>feedback</br> for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit <br>feedback</br>, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit <br>feedback</br> reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit <br>feedback</br> features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit <br>feedback</br> features (i.e., all of the features described above as well as all of the new implicit <br>feedback</br> features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit <br>feedback</br> at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit <br>feedback</br> for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit <br>feedback</br>, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit <br>feedback</br> and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit <br>feedback</br> proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user <br>feedback</br> (Section 3.1).",
                "Incorporating all user <br>feedback</br> (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit <br>feedback</br>. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user <br>feedback</br> in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit <br>feedback</br> features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit <br>feedback</br> incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit <br>feedback</br> set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit <br>feedback</br> with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit <br>feedback</br> features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit <br>feedback</br> is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit <br>feedback</br> is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit <br>feedback</br> was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit <br>feedback</br> in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit <br>feedback</br> features directly into the learned ranking function is more effective than using implicit <br>feedback</br> for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit <br>feedback</br> available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit <br>feedback</br> obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit <br>feedback</br> to improve web search relevance.",
                "We compared two alternatives of incorporating implicit <br>feedback</br> into the search process, namely reranking with implicit <br>feedback</br> and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit <br>feedback</br>.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user <br>feedback</br> can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit <br>feedback</br> is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit <br>feedback</br> for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit <br>feedback</br>, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit <br>feedback</br> for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit <br>feedback</br> for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit <br>feedback</br>.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit <br>feedback</br>, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Examinamos alternativas para incorporar \"comentarios\" en el proceso de clasificación y explorar las contribuciones de los \"comentarios\" del usuario en comparación con otras características de búsqueda web comunes.comentario",
                "Mostramos que la incorporación de \"retroalimentación\" implícita puede aumentar otras características, mejorando la precisión de un algoritmos competitivos de clasificación de búsqueda web hasta un 31% en relación con el rendimiento original.comentario",
                "Categorías y descriptores de sujetos H.3.3 Búsqueda y recuperación de información - Relevancia \"Comentarios\", proceso de búsqueda;H.3.5 Servicios de información en línea - Servicios basados en la web.comentario",
                "La \"retroalimentación\" implícita para la clasificación y la personalización se ha convertido en un área activa de investigación.comentario",
                "El trabajo reciente de Joachims y otros que exploran \"comentarios\" implícitos en entornos controlados han demostrado el valor de incorporar \"comentarios\" implícitos en el proceso de clasificación.comentario",
                "Nuestra motivación para este trabajo es comprender cómo se puede utilizar la \"retroalimentación\" implícita en un entorno operativo a gran escala para mejorar la recuperación.comentario",
                "Nuestro artículo explora si la \"retroalimentación\" implícita puede ser útil en entornos realistas, donde los \"comentarios\" del usuario pueden ser ruidosos (o adversos) y un motor de búsqueda web ya usa cientos de características y está muy sintonizado.comentario",
                "Las contribuciones específicas de este documento incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en la clasificación de búsqueda web (Sección 3).• Una aplicación de un modelo de \"retroalimentación\" implícito robusto derivado de la extracción de millones de interacciones de los usuarios con un importante motor de búsqueda web (Sección 4).• Una evaluación a gran escala sobre consultas reales de usuario y resultados de búsqueda, que muestra mejoras significativas derivadas de la incorporación de \"retroalimentación\" del usuario (Sección 6).comentario",
                "Sin embargo, con la creciente popularidad de los motores de búsqueda, la \"retroalimentación\" implícita (es decir, las acciones que los usuarios toman al interactuar con el motor de búsqueda) pueden usarse para mejorar las clasificaciones.comentario",
                "Incorporando la \"retroalimentación\" implícita, consideramos dos enfoques complementarios para clasificar con \"retroalimentación\" implícita: (1) tratar la retroalimentación implícita como evidencia independiente de resultados de clasificación e (2) integrar las características de retroalimentación implícita directamente en el algoritmo de clasificación.comentario"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "result": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search <br>result</br> ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each <br>result</br> is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each <br>result</br> d from available user interaction features, resulting in the implicit rank Id for each <br>result</br>.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may <br>result</br> in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-<br>result</br> URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search <br>result</br> as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with <br>result</br> interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a <br>result</br> to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a <br>result</br> in the given position.",
                "We also model the browsing behavior after a <br>result</br> was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a <br>result</br> is relevant by looking at the <br>result</br> title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the <br>result</br> summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to <br>result</br>, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search <br>result</br> URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a <br>result</br> link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of <br>result</br> returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a <br>result</br> document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search <br>result</br>.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with <br>result</br> relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the <br>result</br> corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each <br>result</br> r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top <br>result</br>: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 <br>result</br> position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant <br>result</br> (PTR).",
                "If users considered only the relevance of a <br>result</br> to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant <br>result</br> (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant <br>result</br> at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant <br>result</br> at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 <br>result</br> URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 <br>result</br> in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search <br>result</br> Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Estas interacciones pueden servir como una valiosa fuente de información para ajustar y mejorar la clasificación de \"resultados\" de búsqueda web y pueden complementar juicios explícitos más costosos.resultado",
                "A cada \"resultado\" se le asigna una puntuación de acuerdo con la relevancia esperada/satisfacción del usuario basada en interacciones anteriores, lo que resulta en algún orden de preferencia basado solo en las interacciones del usuario.resultado",
                "Para una consulta dada Q, el puntaje implícito ISD se calcula para cada \"resultado\" d de las características de interacción del usuario disponibles, lo que resulta en la ID de rango implícita para cada \"resultado\".resultado",
                "La aplicación de algoritmos de combinación de clasificadores y ranker más sofisticados puede \"dar como resultado mejoras adicionales, y es una dirección prometedora para el trabajo futuro.resultado",
                "En tiempo de ejecución, el motor de búsqueda obtendría las características de retroalimentación implícita asociadas con cada par de la URL de \"resultado\".resultado",
                "El enfoque general es representar las acciones del usuario para cada \"resultado\" de búsqueda como un vector de características, y luego capacitar a un ranker en estas características para descubrir valores de características indicativos de resultados de búsqueda relevantes (y no relevantes).resultado",
                "Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar información suficiente para replicar nuestros métodos de clasificación y los experimentos posteriores.4.1 Representando las acciones del usuario como características, modelamos los comportamientos de búsqueda web observados como una combinación de un componente de fondo `` (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos los sesgos posicionales con interacciones de \"resultado\") y una `` relevancia ``componente (es decir, comportamiento específico de consulta indicativo de relevancia de un \"resultado\" para una consulta).resultado",
                "Incluimos las características de retroalimentación implícita tradicionales, como los recuentos de clics para los resultados, así como nuestras nuevas características derivadas, como la desviación del número de clic observado para un par de consulta dada del número esperado de clics en un \"resultado\".la posición dada.resultado",
                "También modelamos el comportamiento de navegación después de que se hizo clic en un \"resultado\", por ejemplo, el tiempo promedio de permanencia de la página para un par de consultas dada, así como su desviación del tiempo de permanencia esperado (promedio).resultado",
                "Por ejemplo, los usuarios de búsqueda web a menudo pueden determinar si un \"resultado\" es relevante al observar el título, URL y resumen de \"resultado\", en muchos casos, no es necesario mirar el documento original.resultado"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "relevance feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - <br>relevance feedback</br>, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit <br>relevance feedback</br> for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Categorías y descriptores de sujetos H.3.3 Búsqueda y recuperación de información: \"Comentarios de relevancia\", proceso de búsqueda;H.3.5 Servicios de información en línea - Servicios basados en la web.Comentarios de relevancia",
                "La \"retroalimentación de relevancia\" implícita para la clasificación y la personalización se ha convertido en un área activa de investigación.Comentarios de relevancia"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "user interaction": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available <br>user interaction</br> features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available <br>user interaction</br> features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning <br>user interaction</br> Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para una consulta Q, la puntuación implícita ISD se calcula para cada resultado d de las características disponibles de \"interacción del usuario\", lo que resulta en la ID de rango implícita para cada resultado.la interacción del usuario",
                "En tiempo de ejecución, para una consulta dada, la puntuación implícita IR se calcula para cada resultado R con las características disponibles de \"interacción del usuario\", y se produce la clasificación implícita.la interacción del usuario",
                "Referencias [1] E. Agichtein, E. Brill, S. Dumais y R.RAGNO, Modelos de \"interacción de usuario\" de aprendizaje para predecir las preferencias de los resultados de la búsqueda web.la interacción del usuario"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "information": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior <br>information</br> Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 <br>information</br> Search and Retrieval - Relevance feedback, search process; H.3.5 Online <br>information</br> Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of <br>information</br> for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some <br>information</br> that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in <br>information</br> retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough <br>information</br> for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate <br>information</br> from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and <br>information</br> retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient <br>information</br> to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This <br>information</br> was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential <br>information</br> about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in <br>information</br>.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted <br>information</br> retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other <br>information</br> available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased <br>information</br> (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the <br>information</br> used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough <br>information</br> to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient <br>information</br> to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction <br>information</br> to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior <br>information</br> degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent <br>information</br> needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern <br>information</br> Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on <br>information</br> Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for <br>information</br> Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, <br>information</br> filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on <br>information</br> Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling <br>information</br> content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for <br>information</br> Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web <br>information</br> search processes.",
                "In <br>information</br> Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal <br>information</br> Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on <br>information</br> and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern <br>information</br> retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on <br>information</br> and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mejora de la clasificación de búsqueda web incorporando el comportamiento del usuario \"información\" Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com Resumen Resumen que la incorporación de datos de comportamiento del usuario puede mejorar significativamente la ordende los mejores resultados en una configuración de búsqueda web real.información",
                "Categorías y descriptores de asignaturas H.3.3 \"Información\" Búsqueda y recuperación de la relevancia: Comentarios de relevancia, proceso de búsqueda;H.3.5 Servicios de \"información\" en línea - Servicios basados en la web.información",
                "Estas interacciones pueden servir como una fuente valiosa de \"información\" para ajustar y mejorar la clasificación de los resultados de la búsqueda web y pueden complementar juicios explícitos más costosos.información",
                "Si bien es intuitivo que las interacciones del usuario con el motor de búsqueda web revelen al menos cierta \"información\" que podría usarse para la clasificación, estimar las preferencias de los usuarios en la configuración de búsqueda web real es un problema desafiante, ya que las interacciones reales del usuario tienden a ser más ruidosas quecomúnmente asumido en los entornos controlados de estudios anteriores.información",
                "Antecedentes y resultados de búsqueda de clasificación de trabajo relacionados es un problema fundamental en la recuperación de \"información\".información",
                "Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de \"información\" de clics para mejorar la clasificación de búsqueda web es prometedor, captura solo un aspecto de las interacciones del usuario con los motores de búsqueda web.información",
                "En lugar de tratar a cada usuario como un experto confiable, agregamos \"información\" de múltiples trazas de búsqueda de usuarios múltiples, como describimos en las siguientes dos secciones.3. Información",
                "Ahora describimos un ranker de este tipo que solíamos aprender sobre los conjuntos de características combinadas, incluidos los comentarios implícitos.3.3 Aprender a clasificar los resultados de la búsqueda web Un aspecto clave de nuestro enfoque es explotar los avances recientes en el aprendizaje automático, a saber, los algoritmos de clasificación capacitables para la búsqueda web y la recuperación de \"información\" (por ejemplo, [5, 11] y resultados clásicos revisados en [3]).información",
                "Primero resumimos brevemente nuestras características y modelo, y el enfoque de aprendizaje (Sección 4.2) para proporcionar suficiente \"información\" para replicar nuestros métodos de clasificación y los experimentos posteriores.4.1 Representación de las acciones del usuario como características modelamos los comportamientos de búsqueda web observados como una combinación de un componente de fondo `` (es decir, ruido independiente de la consulta y relevancia en el comportamiento del usuario, incluidos los sesgos posicionales con interacciones de resultados) y un componente de relevancia `` ((es decir, comportamiento específico de consulta indicativo de relevancia de un resultado a una consulta).información",
                "Esta \"información\" se obtuvo a través de una instrumentación del lado del cliente optada de los usuarios de un importante motor de búsqueda web.información"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "score": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a <br>score</br> according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in <br>score</br> values from original rankers.",
                "For a given query q, the implicit <br>score</br> ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged <br>score</br> SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F <br>score</br> for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit <br>score</br> Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F <br>score</br> as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) <br>score</br> for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP <br>score</br>.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "A cada resultado se le asigna un \"puntaje\" de acuerdo con la relevancia esperada/satisfacción del usuario basada en interacciones anteriores, lo que resulta en algún orden de preferencia basado solo en las interacciones del usuario.puntaje",
                "Descubrimos que un rango simple que fusiona la combinación heurística funciona bien, y es robusto a las variaciones en los valores de \"puntaje\" de los rankers originales.puntaje",
                "Para una consulta dada Q, el ISD implícito de \"puntaje\" se calcula para cada resultado D de las características de interacción del usuario disponibles, lo que resulta en la ID de rango implícita para cada resultado.puntaje",
                "Calculamos un \"puntaje\" fusionado SM (D) para D combinando los rangos obtenidos de la retroalimentación implícita, ID con el rango original de D, OD: ¡¢ £ + + + + = de lo contrario o DforexistsfeedBackimpliCitif oi w woids d dd i iddm1 1 1 1 1 1) ,,, (donde el peso WI es un factor de escala heurísticamente sintonizado que representa la importancia relativa de la retroalimentación implícita.",
                "Este sistema aprende automáticamente pesos para todas las características (incluido el \"puntaje\" BM25F para un documento) basado en etiquetas humanas explícitas para un gran conjunto de consultas.puntaje",
                "En tiempo de ejecución, para una consulta dada, el \"puntaje\" IR implícito se calcula para cada resultado R con las características de interacción del usuario disponibles, y se produce la clasificación implícita.puntaje",
                "Según los experimentos sobre el conjunto de desarrollo, fijamos el valor de WI a 3 (el efecto del parámetro WI para este ranker resultó ser insignificante).• BM25F+ALL: Ranking Derivado mediante el entrenamiento del alumno RankNet (Sección 3.3) sobre el conjunto de características del \"puntaje\" BM25F, así como todas las características de retroalimentación implícita (Sección 3.2).puntaje",
                "Reportamos la \"puntuación\" de precisión promedio (MAP) media para cada sistema.puntaje",
                "Curiosamente, la incorporación de la información de comportamiento del usuario degrada la precisión para consultas con \"puntaje\" de alto mapa original.puntaje"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "document": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the <br>document</br>), and query-independent page quality features (e.g., PageRank of the <br>document</br> or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original <br>document</br> is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant <br>document</br> to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant <br>document</br> was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result <br>document</br> (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a <br>document</br>) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant <br>document</br> at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Ahora relajamos esta suposición integrando las características de retroalimentación implícita directamente en el proceso de clasificación.3.2 La clasificación con las características de retroalimentación implícitas Los motores de búsqueda de web modernos tienen resultados de clasificación basados en una gran cantidad de características, incluidas las características basadas en el contenido (es decir, cuán de cerca una consulta coincide con el texto o el título o el texto de anclaje del \"documento\"), y la consulta-Características de calidad de página independientes (por ejemplo, PageRank del \"documento\" o el dominio).documento",
                "Por ejemplo, los usuarios de búsqueda web a menudo pueden determinar si un resultado es relevante al observar el título del resultado, la URL y el resumen; en muchos casos, no es necesario mirar el \"documento\" original.documento",
                "En nuestro entorno, requerimos que un \"documento\" relevante sea etiquetado como bueno o superior.documento",
                "NDCG es muy adecuado para la evaluación de la búsqueda web, ya que recompensa los documentos relevantes en los resultados mejor clasificados más en gran medida que los que se clasifican más bajos.• Mapa: la precisión promedio para cada consulta se define como la media de la precisión en los valores de K calculados después de cada \"documento\" relevante.documento",
                "La variante BM25F que utilizamos para nuestros experimentos calcula puntajes de coincidencia separados para cada campo para un resultado \"documento\" (por ejemplo, texto del cuerpo, título y texto de anclaprofundidad).documento",
                "Este sistema aprende automáticamente pesos para todas las características (incluido el puntaje BM25F para un \"documento\") basado en etiquetas humanas explícitas para un gran conjunto de consultas.documento",
                "Cuando hay comentarios implícitos disponibles, el BM25F+All System devuelve el \"documento\" relevante en el top 1 casi el 70% del tiempo, en comparación con el 53% del tiempo cuando el sistema BM25F original no considera la retroalimentación implícita.0.45 0.5 0.55 0.6 0.65 0.7 1 3 3 5 10k Precisión RN RN+Todos BM25 BM25+Todos Figura 6.7: Precisión en K NDCG en K para BM25F, BM25F+ALL, RN y RN+ALL en Probar las interacciones de los usuarios Resumimos elResultados en la medida del mapa para consultas intentadas en la Tabla 6.2.documento"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "implicit relevance feedback": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive web search ranking algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "<br>implicit relevance feedback</br> for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into web search ranking (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving web search ranking is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for web search ranking can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve web search ranking.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "La \"retroalimentación de relevancia implícita\" para la clasificación y la personalización se ha convertido en un área activa de investigación.retroalimentación de relevancia implícita"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "web search rank": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "Improving Web Search Ranking by Incorporating User Behavior Information Eugene Agichtein Microsoft Research eugeneag@microsoft.com Eric Brill Microsoft Research brill@microsoft.com Susan Dumais Microsoft Research sdumais@microsoft.com ABSTRACT We show that incorporating user behavior data can significantly improve ordering of top results in real web search setting.",
                "We examine alternatives for incorporating feedback into the ranking process and explore the contributions of user feedback compared to other common web search features.",
                "We report results of a large scale evaluation over 3,000 queries and 12 million user interactions with a popular web search engine.",
                "We show that incorporating implicit feedback can augment other features, improving the accuracy of a competitive <br>web search rank</br>ing algorithms by as much as 31% relative to the original performance.",
                "Categories and Subject Descriptors H.3.3 Information Search and Retrieval - Relevance feedback, search process; H.3.5 Online Information Services - Web-based services.",
                "General Terms Algorithms, Measurement, Experimentation 1.",
                "INTRODUCTION Millions of users interact with search engines daily.",
                "They issue queries, follow some of the links in the results, click on ads, spend time on pages, reformulate their queries, and perform other actions.",
                "These interactions can serve as a valuable source of information for tuning and improving web search result ranking and can compliment more costly explicit judgments.",
                "Implicit relevance feedback for ranking and personalization has become an active area of research.",
                "Recent work by Joachims and others exploring implicit feedback in controlled environments have shown the value of incorporating implicit feedback into the ranking process.",
                "Our motivation for this work is to understand how implicit feedback can be used in a large-scale operational environment to improve retrieval.",
                "How does it compare to and compliment evidence from page content, anchor text, or link-based features such as inlinks or PageRank?",
                "While it is intuitive that user interactions with the web search engine should reveal at least some information that could be used for ranking, estimating user preferences in real web search settings is a challenging problem, since real user interactions tend to be more noisy than commonly assumed in the controlled settings of previous studies.",
                "Our paper explores whether implicit feedback can be helpful in realistic environments, where user feedback can be noisy (or adversarial) and a web search engine already uses hundreds of features and is heavily tuned.",
                "To this end, we explore different approaches for ranking web search results using real user behavior obtained as part of normal interactions with the web search engine.",
                "The specific contributions of this paper include: • Analysis of alternatives for incorporating user behavior into <br>web search rank</br>ing (Section 3). • An application of a robust implicit feedback model derived from mining millions of user interactions with a major web search engine (Section 4). • A large scale evaluation over real user queries and search results, showing significant improvements derived from incorporating user feedback (Section 6).",
                "We summarize our findings and discuss extensions to the current work in Section 7, which concludes the paper. 2.",
                "BACKGROUND AND RELATED WORK Ranking search results is a fundamental problem in information retrieval.",
                "Most common approaches primarily focus on similarity of query and a page, as well as the overall page quality [3,4,24].",
                "However, with increasing popularity of search engines, implicit feedback (i.e., the actions users take when interacting with the search engine) can be used to improve the rankings.",
                "Implicit relevance measures have been studied by several research groups.",
                "An overview of implicit measures is compiled in Kelly and Teevan [14].",
                "This research, while developing valuable insights into implicit relevance measures, was not applied to improve the ranking of web search results in realistic settings.",
                "Closely related to our work, Joachims [11] collected implicit measures in place of explicit measures, introducing a technique based entirely on clickthrough data to learn ranking functions.",
                "Fox et al. [8] explored the relationship between implicit and explicit measures in Web search, and developed Bayesian models to correlate implicit measures and explicit relevance judgments for both individual queries and search sessions.",
                "This work considered a wide range of user behaviors (e.g., dwell time, scroll time, reformulation patterns) in addition to the popular clickthrough behavior.",
                "However, the modeling effort was aimed at predicting explicit relevance judgments from implicit user actions and not specifically at learning ranking functions.",
                "Other studies of user behavior in web search include Pharo and Järvelin [19], but were not directly applied to improve ranking.",
                "More recently, Joachims et al. [12] presented an empirical evaluation of interpreting clickthrough evidence.",
                "By performing eye tracking studies and correlating predictions of their strategies with explicit ratings, the authors showed that it is possible to accurately interpret clickthroughs in a controlled, laboratory setting.",
                "Unfortunately, the extent to which previous research applies to real-world web search is unclear.",
                "At the same time, while recent work (e.g., [26]) on using clickthrough information for improving <br>web search rank</br>ing is promising, it captures only one aspect of the user interactions with web search engines.",
                "We build on existing research to develop robust user behavior interpretation techniques for the real web search setting.",
                "Instead of treating each user as a reliable expert, we aggregate information from multiple, unreliable, user search session traces, as we describe in the next two sections. 3.",
                "INCORPORATING IMPLICIT FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.",
                "We describe the two general ranking approaches next.",
                "The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5. 3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.",
                "Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.",
                "While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers scores, and instead simply merge the rank orders.",
                "The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.",
                "We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).",
                "We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.",
                "For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.",
                "We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.",
                "The query results are ordered in by decreasing values of SM to produce the final ranking.",
                "One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.",
                "Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.",
                "The approach above assumes that there are no interactions between the underlying features producing the original <br>web search rank</br>ing and the implicit feedback features.",
                "We now relax this assumption by integrating implicit feedback features directly into the ranking process. 3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).",
                "In most cases, automatic (or semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.",
                "Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.",
                "During training or tuning, the ranker can be tuned as before but with additional features.",
                "At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.",
                "This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.",
                "We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback. 3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).",
                "In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.",
                "Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.",
                "RankNet is one such algorithm.",
                "It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.",
                "While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.",
                "An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.",
                "We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.",
                "Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.",
                "Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 4.",
                "IMPLICIT USER FEEDBACK MODEL Our goal is to accurately interpret noisy user feedback obtained as by tracing user interactions with the search engine.",
                "Interpreting implicit feedback in real web search setting is not an easy task.",
                "We characterize this problem in detail in [1], where we motivate and evaluate a wide variety of models of implicit user activities.",
                "The general approach is to represent user actions for each search result as a vector of features, and then train a ranker on these features to discover feature values indicative of relevant (and nonrelevant) search results.",
                "We first briefly summarize our features and model, and the learning approach (Section 4.2) in order to provide sufficient information to replicate our ranking methods and the subsequent experiments. 4.1 Representing User Actions as Features We model observed web search behaviors as a combination of a ``background component (i.e., query- and relevance-independent noise in user behavior, including positional biases with result interactions), and a ``relevance component (i.e., query-specific behavior indicative of relevance of a result to a query).",
                "We design our features to take advantage of aggregated user behavior.",
                "The feature set is comprised of directly observed features (computed directly from observations for each query), as well as queryspecific derived features, computed as the deviation from the overall query-independent distribution of values for the corresponding directly observed feature values.",
                "The features used to represent user interactions with web search results are summarized in Table 4.1.",
                "This information was obtained via opt-in client-side instrumentation from users of a major web search engine.",
                "We include the traditional implicit feedback features such as clickthrough counts for the results, as well as our novel derived features such as the deviation of the observed clickthrough number for a given query-URL pair from the expected number of clicks on a result in the given position.",
                "We also model the browsing behavior after a result was clicked - e.g., the average page dwell time for a given query-URL pair, as well as its deviation from the expected (average) dwell time.",
                "Furthermore, the feature set was designed to provide essential information about the user experience to make feedback interpretation robust.",
                "For example, web search users can often determine whether a result is relevant by looking at the result title, URL, and summary - in many cases, looking at the original document is not necessary.",
                "To model this aspect of user experience we include features such as overlap in words in title and words in query (TitleOverlap) and the fraction of words shared by the query and the result summary.",
                "Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickProbability Probability of a click for this query and URL ClickDeviation Deviation from expected click probability IsNextClicked 1 if clicked on next position, 0 otherwise IsPreviousClicked 1 if clicked on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, no parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from average dwell time on page CumulativeDeviation Deviation from average cumulative dwell time DomainDeviation Deviation from average dwell time on domain Query-text features TitleOverlap Words shared between query and title SummaryOverlap Words shared between query and snippet QueryURLOverlap Words shared between query and URL QueryDomainOverlap Words shared between query and URL domain QueryLength Number of tokens in query QueryNextOverlap Fraction of words shared with next query Table 4.1: Some features used to represent post-search navigation history for a given query and search result URL.",
                "Having described our feature set, we briefly review our general method for deriving a user behavior model. 4.2 Deriving a User Feedback Model To learn to interpret the observed user behavior, we correlate user actions (i.e., the features in Table 4.1 representing the actions) with the explicit user judgments for a set of training queries.",
                "We find all the instances in our session logs where these queries were submitted to the search engine, and aggregate the user behavior features for all search sessions involving these queries.",
                "Each observed query-URL pair is represented by the features in Table 4.1, with values averaged over all search sessions, and assigned one of six possible relevance labels, ranging from Perfect to Bad, as assigned by explicit relevance judgments.",
                "These labeled feature vectors are used as input to the RankNet training algorithm (Section 3.3) which produces a trained user behavior model.",
                "This approach is particularly attractive as it does not require heuristics beyond feature engineering.",
                "The resulting user behavior model is used to help rank web search resultseither directly or in combination with other features, as described below. 5.",
                "EXPERIMENTAL SETUP The ultimate goal of incorporating implicit feedback into ranking is to improve the relevance of the returned web search results.",
                "Hence, we compare the ranking methods over a large set of judged queries with explicit relevance labels provided by human judges.",
                "In order for the evaluation to be realistic we obtained a random sample of queries from web search logs of a major search engine, with associated results and traces for user actions.",
                "We describe this dataset in detail next.",
                "Our metrics are described in Section 5.2 that we use to evaluate the ranking alternatives, listed in Section 5.3 in the experiments of Section 6. 5.1 Datasets We compared our ranking methods over a random sample of 3,000 queries from the search engine query logs.",
                "The queries were drawn from the logs uniformly at random by token without replacement, resulting in a query sample representative of the overall query distribution.",
                "On average, 30 results were explicitly labeled by human judges using a six point scale ranging from Perfect down to Bad.",
                "Overall, there were over 83,000 results with explicit relevance judgments.",
                "In order to compute various statistics, documents with label Good or better will be considered relevant, and with lower labels to be non-relevant.",
                "Note that the experiments were performed over the results already highly ranked by a web search engine, which corresponds to a typical user experience which is limited to the small number of the highly ranked results for a typical web search query.",
                "The user interactions were collected over a period of 8 weeks using voluntary opt-in information.",
                "In total, over 1.2 million unique queries were instrumented, resulting in over 12 million individual interactions with the search engine.",
                "The data consisted of user interactions with the web search engine (e.g., clicking on a result link, going back to search results, etc.) performed after a query was submitted.",
                "These actions were aggregated across users and search sessions and converted to features in Table 4.1.",
                "To create the training, validation, and test query sets, we created three different random splits of 1,500 training, 500 validation, and 1000 test queries.",
                "The splits were done randomly by query, so that there was no overlap in training, validation, and test queries. 5.2 Evaluation Metrics We evaluate the ranking algorithms over a range of accepted information retrieval metrics, namely Precision at K (P(K)), Normalized Discounted Cumulative Gain (NDCG), and Mean Average Precision (MAP).",
                "Each metric focuses on a deferent aspect of system performance, as we describe below. • Precision at K: As the most intuitive metric, P(K) reports the fraction of documents ranked in the top K results that are labeled as relevant.",
                "In our setting, we require a relevant document to be labeled Good or higher.",
                "The position of relevant documents within the top K is irrelevant, and hence this metric measure overall user satisfaction with the top K results. • NDCG at K: NDCG is a retrieval measure devised specifically for web search evaluation [10].",
                "For a given query q, the ranked results are examined from the top ranked down, and the NDCG computed as:   = +−= K j jr qq jMN 1 )( )1log(/)12( Where Mq is a normalization constant calculated so that a perfect ordering would obtain NDCG of 1; and each r(j) is an integer relevance label (0=Bad and 5=Perfect) of result returned at position j.",
                "Note that unlabeled and Bad documents do not contribute to the sum, but will reduce NDCG for the query pushing down the relevant labeled documents, reducing their contributions.",
                "NDCG is well suited to web search evaluation, as it rewards relevant documents in the top ranked results more heavily than those ranked lower. • MAP: Average precision for each query is defined as the mean of the precision at K values computed after each relevant document was retrieved.",
                "The final MAP value is defined as the mean of average precisions of all queries in the test set.",
                "This metric is the most commonly used single-value summary of a run over a set of queries. 5.3 Ranking Methods Compared Recall that our goal is to quantify the effectiveness of implicit behavior for real web search.",
                "One dimension is to compare the utility of implicit feedback with other information available to a web search engine.",
                "Specifically, we compare effectiveness of implicit user behaviors with content-based matching, static page quality features, and combinations of all features. • BM25F: As a strong web search baseline we used the BM25F scoring, which was used in one of the best performing systems in the TREC 2004 Web track [23,27].",
                "BM25F and its variants have been extensively described and evaluated in IR literature, and hence serve as a strong, reproducible baseline.",
                "The BM25F variant we used for our experiments computes separate match scores for each field for a result document (e.g., body text, title, and anchor text), and incorporates query-independent linkbased information (e.g., PageRank, ClickDistance, and URL depth).",
                "The scoring function and field-specific tuning is described in detail in [23].",
                "Note that BM25F does not directly consider explicit or implicit feedback for tuning. • RN: The ranking produced by a neural net ranker (RankNet, described in Section 3.3) that learns to rank web search results by incorporating BM25F and a large number of additional static and dynamic features describing each search result.",
                "This system automatically learns weights for all features (including the BM25F score for a document) based on explicit human labels for a large set of queries.",
                "A system incorporating an implementation of RankNet is currently in use by a major search engine and can be considered representative of the state of the art in web search. • BM25F-RerankCT: The ranking produced by incorporating clickthrough statistics to reorder web search results ranked by BM25F above.",
                "Clickthrough is a particularly important special case of implicit feedback, and has been shown to correlate with result relevance.",
                "This is a special case of the ranking method in Section 3.1, with the weight wI set to 1000 and the ranking Id is simply the number of clicks on the result corresponding to d. In effect, this ranking brings to the top all returned web search results with at least one click (and orders them in decreasing order by number of clicks).",
                "The relative ranking of the remainder of results is unchanged and they are inserted below all clicked results.",
                "This method serves as our baseline implicit feedback reranking method.",
                "BM25F-RerankAll The ranking produced by reordering the BM25F results using all user behavior features (Section 4).",
                "This method learns a model of user preferences by correlating feature values with explicit relevance labels using the RankNet neural net algorithm (Section 4.2).",
                "At runtime, for a given query the implicit score Ir is computed for each result r with available user interaction features, and the implicit ranking is produced.",
                "The merged ranking is computed as described in Section 3.1.",
                "Based on the experiments over the development set we fix the value of wI to 3 (the effect of the wI parameter for this ranker turned out to be negligible). • BM25F+All: Ranking derived by training the RankNet (Section 3.3) learner over the features set of the BM25F score as well as all implicit feedback features (Section 3.2).",
                "We used the 2-layer implementation of RankNet [5] trained on the queries and labels in the training and validation sets. • RN+All: Ranking derived by training the 2-layer RankNet ranking algorithm (Section 3.3) over the union of all content, dynamic, and implicit feedback features (i.e., all of the features described above as well as all of the new implicit feedback features we introduced).",
                "The ranking methods above span the range of the information used for ranking, from not using the implicit or explicit feedback at all (i.e., BM25F) to a modern web search engine using hundreds of features and tuned on explicit judgments (RN).",
                "As we will show next, incorporating user behavior into these ranking systems dramatically improves the relevance of the returned documents. 6.",
                "EXPERIMENTAL RESULTS Implicit feedback for <br>web search rank</br>ing can be exploited in a number of ways.",
                "We compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the BM25F-RerankCT and BM25F-RerankAll methods that reorder BM25F results), as well as by integrating the implicit features directly into the ranking process (i.e., the RN+ALL and BM25F+All methods which learn to rank results over the implicit feedback and other features).",
                "We compare our methods over strong baselines (BM25F and RN) over the NDCG, Precision at K, and MAP measures defined in Section 5.2.",
                "The results were averaged over three random splits of the overall dataset.",
                "Each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.",
                "We first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).",
                "We then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.",
                "We first experimented with different methods of re-ranking the output of the BM25F search results.",
                "Figures 6.1 and 6.2 report NDCG and Precision for BM25F, as well as for the strategies reranking results with user feedback (Section 3.1).",
                "Incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original BM25F ranking as well as over reranking with clickthrough alone.",
                "The improvement is consistent across the top 10 results and largest for the top result: NDCG at 1 for BM25F+All is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63.",
                "Based on these results we will use the direct feature combination (i.e., BM25F+All) ranker for subsequent comparisons involving implicit feedback. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10K NDCG BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.1: NDCG at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision BM25 BM25-Rerank-CT BM25-Rerank-All BM25+All Figure 6.2: Precision at K for BM25F, BM25F-RerankCT, BM25F-Rerank-All, and BM25F+All for varying K Interestingly, using clickthrough alone, while giving significant benefit over the original BM25F ranking, is not as effective as considering the full set of features in Table 4.1.",
                "While we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 Result position Relativeclickfrequency PTR=2 PTR=3 PTR=5 Figure 6.3: Relative clickthrough frequency for queries with varying Position of Top Relevant result (PTR).",
                "If users considered only the relevance of a result to their query, they would click on the topmost relevant results.",
                "Unfortunately, as Joachims and others have shown, presentation also influences which results users click on quite dramatically.",
                "Users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average topranked items are relevant.",
                "Figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (PTR) ranges from 2-10 in the figure.",
                "For example, for queries with first relevant result at position 5 (PTR=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5.",
                "As we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.",
                "We now consider incorporating user behavior into a much richer feature set, RN (Section 5.3) used by a major web search engine.",
                "RN incorporates BM25F, link-based features, and hundreds of other features.",
                "Figure 6.4 reports NDCG at K and Figure 6.5 reports Precision at K. Interestingly, while the original RN rankings are significantly more accurate than BM25F alone, incorporating implicit feedback features (BM25F+All) results in ranking that significantly outperforms the original RN rankings.",
                "In other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the RankNet learner trained on the RN feature set. 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10K NDCG RN RN+All BM25 BM25+All Figure 6.4: NDCG at K for BM25F, BM25F+All, RN, and RN+All for varying K Furthermore, enriching the RN features with implicit feedback set exhibits significant gain on all measures, allowing RN+All to outperform all other methods.",
                "This demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine. 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 K Precision RN RN+All BM25 BM25+All Figure 6.5: Precision at K for BM25F, BM25F+All, RN, and RN+All for varying K We summarize the performance of the different ranking methods in Table 6.1.",
                "We report the Mean Average Precision (MAP) score for each system.",
                "While not intuitive to interpret, MAP allows quantitative comparison on a single metric.",
                "The gains marked with * are significant at p=0.01 level using two tailed t-test.",
                "MAP Gain P(1) Gain BM25F 0.184 - 0.503BM25F-Rerank-CT 0.215 0.031* 0.577 0.073* BM25F-RerankImplicit 0.218 0.003 0.605 0.028* BM25F+Implicit 0.222 0.004 0.620 0.015* RN 0.215 - 0.597RN+All 0.248 0.033* 0.629 0.032* Table 6.1: Mean Average Precision (MAP) for all strategies.",
                "So far we reported results averaged across all queries in the test set.",
                "Unfortunately, less than half had sufficient interactions to attempt reranking.",
                "Out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result URL was clicked on by the user).",
                "This is not surprising: web search is heavy-tailed, and there are many unique queries.",
                "We now consider the performance on the queries for which user interactions were available.",
                "Figure 6.6 reports NDCG for the subset of the test queries with the implicit feedback features.",
                "The gains at top 1 are dramatic.",
                "The NDCG at 1 of BM25F+All increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to RN+All operating over a much richer feature set. 0.6 0.65 0.7 0.75 0.8 1 3 5 10K NDCG RN RN+All BM25 BM25+All Figure 6.6: NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions Similarly, gains on precision at top 1 are substantial (Figure 6.7), and are likely to be apparent to web search users.",
                "When implicit feedback is available, the BM25F+All system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original BM25F system. 0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10K Precision RN RN+All BM25 BM25+All Figure 6.7: Precision at K NDCG at K for BM25F, BM25F+All, RN, and RN+All on test queries with user interactions We summarize the results on the MAP measure for attempted queries in Table 6.2.",
                "MAP improvements are both substantial and significant, with improvements over the BM25F ranker most pronounced.",
                "Method MAP Gain P(1) Gain RN 0.269 0.632 RN+All 0.321 0.051 (19%) 0.693 0.061(10%) BM25F 0.236 0.525 BM25F+All 0.292 0.056 (24%) 0.687 0.162 (31%) Table 6.2: Mean Average Precision (MAP) on attempted queries for best performing methods We now analyze the cases where implicit feedback was shown most helpful.",
                "Figure 6.8 reports the MAP improvements over the baseline BM25F run for each query with MAP under 0.6.",
                "Note that most of the improvement is for poorly performing queries (i.e., MAP < 0.1).",
                "Interestingly, incorporating user behavior information degrades accuracy for queries with high original MAP score.",
                "One possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings). 0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 Frequency Average Gain Figure 6.8: Gain of BM25F+All over original BM25F ranking To summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both BM25F and RN baselines.",
                "Our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.",
                "Furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.",
                "The improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 7.",
                "CONCLUSIONS AND FUTURE WORK In this paper we explored the utility of incorporating noisy implicit feedback obtained in a real web search setting to improve <br>web search rank</br>ing.",
                "We performed a large-scale evaluation over 3,000 queries and more than 12 million user interactions with a major search engine, establishing the utility of incorporating noisy implicit feedback to improve web search relevance.",
                "We compared two alternatives of incorporating implicit feedback into the search process, namely reranking with implicit feedback and incorporating implicit feedback features directly into the trained ranking function.",
                "Our experiments showed significant improvement over methods that do not consider implicit feedback.",
                "The gains are particularly dramatic for the top K=1 result in the final ranking, with precision improvements as high as 31%, and the gains are substantial for all values of K. Our experiments showed that implicit user feedback can further improve web search performance, when incorporated directly with popular content- and link-based features.",
                "Interestingly, implicit feedback is particularly valuable for queries with poor original ranking of results (e.g., MAP lower than 0.1).",
                "One promising direction for future work is to apply recent research on automatically predicting query difficulty, and only attempt to incorporate implicit feedback for the difficult queries.",
                "As another research direction we are exploring methods for extending our predictions to the previously unseen queries (e.g., query clustering), which should further improve the web search experience of users.",
                "ACKNOWLEDGMENTS We thank Chris Burges and Matt Richardson for an implementation of RankNet for our experiments.",
                "We also thank Robert Ragno for his valuable suggestions and many discussions. 8.",
                "REFERENCES [1] E. Agichtein, E. Brill, S. Dumais, and R.Ragno, Learning User Interaction Models for Predicting Web Search Result Preferences.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2006 [2] J. Allan, HARD Track Overview in TREC 2003, High Accuracy Retrieval from Documents, 2003 [3] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval, Addison-Wesley, 1999. [4] S. Brin and L. Page, The Anatomy of a Large-scale Hypertextual Web Search Engine, in Proceedings of WWW, 1997 [5] C.J.C.",
                "Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, G. Hullender, Learning to Rank using Gradient Descent, in Proceedings of the International Conference on Machine Learning, 2005 [6] D.M.",
                "Chickering, The WinMine Toolkit, Microsoft Technical Report MSR-TR-2002-103, 2002 [7] M. Claypool, D. Brown, P. Lee and M. Waseda.",
                "Inferring user interest.",
                "IEEE Internet Computing. 2001 [8] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White.",
                "Evaluating implicit measures to improve the search experience.",
                "In ACM Transactions on Information Systems, 2005 [9] J. Goecks and J. Shavlick.",
                "Learning users interests by unobtrusively observing their normal behavior.",
                "In Proceedings of the IJCAI Workshop on Machine Learning for Information Filtering. 1999. [10] K Jarvelin and J. Kekalainen.",
                "IR evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2000 [11] T. Joachims, Optimizing Search Engines Using Clickthrough Data.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Datamining (SIGKDD), 2002 [12] T. Joachims, L. Granka, B. Pang, H. Hembrooke, and G. Gay, Accurately Interpreting Clickthrough Data as Implicit Feedback, Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 2005 [13] T. Joachims, Making Large-Scale SVM Learning Practical.",
                "Advances in Kernel Methods, in Support Vector Learning, MIT Press, 1999 [14] D. Kelly and J. Teevan, Implicit feedback for inferring user preference: A bibliography.",
                "In SIGIR Forum, 2003 [15] J. Konstan, B. Miller, D. Maltz, J. Herlocker, L. Gordon, and J. Riedl.",
                "GroupLens: Applying collaborative filtering to usenet news.",
                "In Communications of ACM, 1997. [16] M. Morita, and Y. Shinoda, Information filtering based on user behavior analysis and best match text retrieval.",
                "Proceedings of the ACM Conference on Research and Development on Information Retrieval (SIGIR), 1994 [17] D. Oard and J. Kim.",
                "Implicit feedback for recommender systems.",
                "In Proceedings of the AAAI Workshop on Recommender Systems. 1998 [18] D. Oard and J. Kim.",
                "Modeling information content using observable behavior.",
                "In Proceedings of the 64th Annual Meeting of the American Society for Information Science and Technology. 2001 [19] N. Pharo, N. and K. Järvelin.",
                "The SST method: a tool for analyzing web information search processes.",
                "In Information Processing & Management, 2004 [20] P. Pirolli, The Use of Proximal Information Scent to Forage for Distal Content on the World Wide Web.",
                "In Working with Technology in Mind: Brunswikian.",
                "Resources for Cognitive Science and Engineering, Oxford University Press, 2004 [21] F. Radlinski and T. Joachims, Query Chains: Learning to Rank from Implicit Feedback.",
                "In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005. [22] F. Radlinski and T. Joachims, Evaluating the Robustness of Learning from Implicit Feedback, in Proceedings of the ICML Workshop on Learning in Web Search, 2005 [23] S. E. Robertson, H. Zaragoza, and M. Taylor, Simple BM25 extension to multiple weighted fields, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [24] G. Salton & M. McGill.",
                "Introduction to modern information retrieval.",
                "McGraw-Hill, 1983 [25] E.M. Voorhees, D. Harman, Overview of TREC, 2001 [26] G.R.",
                "Xue, H.J.",
                "Zeng, Z. Chen, Y. Yu, W.Y.",
                "Ma, W.S.",
                "Xi, and W.G.",
                "Fan, Optimizing web search using web clickthrough data, in Proceedings of the Conference on Information and Knowledge Management (CIKM), 2004 [27] H. Zaragoza, N. Craswell, M. Taylor, S. Saria, and S. Robertson.",
                "Microsoft Cambridge at TREC 13: Web and Hard Tracks.",
                "In Proceedings of TREC 2004"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Mostramos que la incorporación de comentarios implícitos puede aumentar otras características, mejorando la precisión de un algoritmos competitivos de \"rango de búsqueda web\" en hasta un 31% en relación con el rendimiento original.rango de búsqueda web",
                "Las contribuciones específicas de este documento incluyen: • Análisis de alternativas para incorporar el comportamiento del usuario en el \"rango de búsqueda web\" ING (Sección 3).• Una aplicación de un modelo de retroalimentación implícita robusto derivado de la extracción de millones de interacciones del usuario con un importante motor de búsqueda web (Sección 4).• Una evaluación a gran escala sobre consultas reales de usuario y resultados de búsqueda, que muestra mejoras significativas derivadas de la incorporación de comentarios de los usuarios (Sección 6).rango de búsqueda web",
                "Al mismo tiempo, si bien el trabajo reciente (por ejemplo, [26]) sobre el uso de información de clics para mejorar el \"rango de búsqueda web\" es prometedor, solo captura un aspecto de las interacciones del usuario con los motores de búsqueda web.rango de búsqueda web",
                "El enfoque anterior supone que no hay interacciones entre las características subyacentes que producen el \"rango de búsqueda web\" original y las características de retroalimentación implícitas.rango de búsqueda web",
                "Resultados experimentales La retroalimentación implícita para el \"rango de búsqueda en la web\" se puede explotar de varias maneras.rango de búsqueda web",
                "Conclusiones y trabajo futuro En este documento exploramos la utilidad de incorporar retroalimentación implícita ruidosa obtenida en una configuración de búsqueda web real para mejorar el \"rango de búsqueda web\".rango de búsqueda web"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}