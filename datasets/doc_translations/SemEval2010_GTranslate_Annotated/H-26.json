{
    "id": "H-26",
    "original_text": "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores. Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1. INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions. However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP). Instead, current algorithms tend to take one of two general approaches. The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]). If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance. However, achieving high MAP only requires finding a good ordering of the documents. As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance. The second common approach is to learn a function that maximizes a surrogate measure. Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3]. Learning a model to optimize for such measures might result in suboptimal MAP performance. In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7]. In this paper, we present a general approach for learning ranking functions that maximize MAP performance. Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP. This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics. The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea. In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution. Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features. We now describe the algorithm in detail and provide proof of correctness. Following this, we provide an analysis of running time. We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus. We have also developed a software package implementing our algorithm that is available for public use1 . 2. THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus). In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y. The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP. We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y). The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized. Of course, P(x, y) is unknown. But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)). In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}. We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0). We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0. We further assume that all predicted rankings are complete rankings (no ties). Let p = rank(y) and ˆp = rank(ˆy). The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy. MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea. While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP. ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair. In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking. Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1. These two hypotheses predict a ranking for query x over a corpus of eight documents. Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2. Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP. Models which optimize for accuracy are not directly concerned with the ranking. Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant. Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses. Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q). The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds. For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64. Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73. A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3. OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea. Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section. Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant). Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively. We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)). Our approach is to learn a discriminant function F : X × Y → over input-output pairs. Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 . We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| . For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank. We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity). Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings. Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0). Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y). As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order. We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings. Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training. Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training. We now present a method based on structural SVMs [19] to address this problem. We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN . Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi. As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document. Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks. For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem. Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)). During prediction, our model chooses the ranking which maximizes the discriminant (1). If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied. Therefore, the sum of slacks, P ξi, upper bounds the MAP loss. This is stated formally in Proposition 1. Proposition 1. Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set. Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1. Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19]. The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint. If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision . Theorem 1. Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y. Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term. Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13]. Solving argmax H for ∆map is more difficult. This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair. MAP, on the other hand, does not decompose in the same way as ROCArea. The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map. One useful property of ∆map is that it is invariant to swapping two documents with equal relevance. For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map. By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves. However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y). This leads us to Observation 1. Observation 1. Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant). Every ranking which satisfies the same set of constraints will have the same ∆map. If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings. Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise. By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists. For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d). For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x . We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 . For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di). The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1. The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . . Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1). The bottom ranking differs from the top only where d¯x j slides up one rank. The difference in the value of H for these two rankings is exactly δj(i, i + 1). For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) . Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document. Due to Observation 1, this encoding uniquely identifies a complete ranking. We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking. Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking. We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8). Conceptually, Algorithm 2 starts with a perfect ranking. Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents. In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9). In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8). Therefore, it suffices to prove that Algorithm 2 satisfies (10). We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1. For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2). Proof. Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms. We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1. Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)). We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1). In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j . Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof. The result of Lemma 1 leads directly to our main correctness result: Theorem 2. In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal. Proof. We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10). Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0. Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0. Suppose for contradiction that optj+1 < optj. Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13). Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts. The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |. The second part computes each optj, which requires O(|Cx | · |C¯x |) time. Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n). For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n). Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2. Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour). Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d). We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process. To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4. EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea. We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus. For each query, TREC provides the relevance judgments of the documents. We generated our features using the scores of existing retrieval functions on these queries. While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features. As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP. We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods. Comparing with the best base functions tests our methods ability to learn a useful combination. Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice. The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments. For the first set, we generated three indices over the WT10g corpus using Indri5 . The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords. For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior. All parameters were kept as their defaults. We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total. For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function. For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions. We used only the non-manual, non-short submissions from both years. For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively. A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided. For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values. From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins. Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions. Figure 2 shows an example of our feature mapping method. In this example we have a single feature F = {f}. Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc . For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) . This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative. We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10. For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments. Table 5 contains statistics of the generated datasets. There are many ways to generate features, and we are not advocating our method over others. This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5. EXPERIMENTS For each dataset in Table 5, we performed 50 trials. For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set. Models were trained using a wide range of C values. The model which performed best on the validation set was selected and tested on the remaining 35 queries. All queries were selected to be in the training, validation and test sets the same number of times. Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20]. All SVM methods used a linear kernel. We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions? Table 6 presents the comparison of SVM∆ map with the best Indri base functions. Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function. The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score. Significance tests were performed using the two-tailed Wilcoxon signed rank test. Two stars indicate a significance level of 0.95. All tables displaying our experimental results are structured identically. Here, we find that SVM∆ map significantly outperforms the best base functions. Table 7 shows the comparison when trained on TREC submissions. While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant. Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions. As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission. Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed. Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training). Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods? Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively. Table 11 contains the corresponding results when trained on the TREC submissions without the best submission. To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically. As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments. The vast majority of the documents are not relevant. SVMacc2 addresses this problem by assigning more penalty to false negative errors. For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset. Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map. Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant. It may be that different queries require different values of b. Having the learning method trying to find a good b value (when one does not exist) may be detrimental. We took two approaches to address this issue. The first method, SVMacc3, converts the retrieval function scores into percentiles. For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9. Each Kf contains 50 evenly spaced values between 0 and 1. Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map. The second method, SVMacc4, normalizes the scores given by f for each query. For example, assume for query q that f outputs scores in the range 0.2 to 0.7. Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8. Each Kf contains 50 evenly spaced values between 0 and 1. Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments. When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map. However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions. Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed. The performance of most models degraded by a small amount, with SVM∆ map still having the best performance. TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6. CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP. It provides a principled approach and avoids difficult to control heuristics. We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time. We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods. Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea. The computational cost for training is very reasonable in practice. Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance. The learning framework used by our method is fairly general. A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7. ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo! Research. The third author was also partly supported by a Microsoft Research Fellowship. 8. REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q. Le. Learning to rank with non-smooth cost functions. In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. Adapting ranking SVM to document retrieval. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova. Learning a ranking from pairwise preferences. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes. Ensemble selection from libraries of models. In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich. The relationship between precision-recall and ROC curves. In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking. Overview of the TREC-9 web track. In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell. Overview of the TREC-2001 web track. In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression. Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti. Optimising area under the ROC curve using gradient descent. In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen. Ir evaluation methods for retrieving highly relevant documents. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims. A support vector method for multivariate performance measures. In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005. ACM Press. [14] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classification in nonstandard situations. Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims. Combining statistical learning with a knowledge-based approach. In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson. The probability ranking principle in ir. journal of documentation. Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik. Statistical Learning Theory. Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz. Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic. In Proceedings of the International Conference on Machine Learning (ICML), 2003.",
    "original_translation": "Un método de vector de soporte para optimizar la precisión promedio de la Universidad de Yisong Yue Cornell Ithaca, NY, EE. UU. Yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, EE. UU.@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, EE. UU. TJ@cs.cornell.edu Abstract Machine Learning se usa comúnmente para mejorar los sistemas de recuperación clasificados. Debido a las dificultades computacionales, se han desarrollado pocas técnicas de aprendizaje para optimizar directamente la precisión promedio media (MAP), a pesar de su uso generalizado en la evaluación de dichos sistemas. Los enfoques existentes de optimización del mapa no encuentran una solución globalmente óptima o son computacionalmente costosos. En contraste, presentamos un algoritmo general de aprendizaje SVM que encuentra eficientemente una solución globalmente óptima para una relajación directa del mapa. Evaluamos nuestro enfoque utilizando los corpus de seguimiento web TREC 9 y TREC 10 (WT10G), comparando contra SVM optimizados para la precisión y Rocarea. En la mayoría de los casos, mostramos nuestro método para producir mejoras estadísticamente significativas en los puntajes de los mapas. Categorías y descriptores de sujetos H.3.3 [Búsqueda y recuperación de información]: Modelos de recuperación Algoritmo de términos generales, teoría, experimentación 1. Introducción Los sistemas de recuperación de información de última generación utilizan comúnmente técnicas de aprendizaje automático para aprender funciones de clasificación. Sin embargo, la mayoría de los enfoques actuales no optimizan para la medida de evaluación utilizada con mayor frecuencia, a saber, la precisión promedio media (MAP). En cambio, los algoritmos actuales tienden a adoptar uno de los dos enfoques generales. El primer enfoque es aprender un modelo que estima la probabilidad de que un documento sea relevante dada una consulta (por ejemplo, [18, 14]). Si se resuelve de manera efectiva, la clasificación con el mejor rendimiento del mapa puede derivarse fácilmente de las probabilidades de relevancia. Sin embargo, lograr un mapa alto solo requiere encontrar un buen pedido de los documentos. Como resultado, encontrar buenas probabilidades requiere resolver un problema más difícil de lo necesario, lo que probablemente requiere más datos de capacitación para lograr el mismo rendimiento del mapa. El segundo enfoque común es aprender una función que maximice una medida sustituta. Las medidas de rendimiento optimizadas incluyen precisión [17, 15], Rocarea [1, 5, 10, 11, 13, 21] o modificaciones de Rocarea [4] y NDCG [2, 3]. Aprender un modelo para optimizar para tales medidas podría dar lugar al rendimiento del mapa subóptimo. De hecho, aunque algunos sistemas anteriores han obtenido un buen rendimiento del mapa, se sabe que ni lograr una precisión óptima ni Rocarea puede garantizar un rendimiento óptimo del mapa [7]. En este artículo, presentamos un enfoque general para las funciones de clasificación de aprendizaje que maximizan el rendimiento del mapa. Específicamente, presentamos un algoritmo SVM que a nivel mundial optimiza una relajación de mapa de la pérdida de bisagra. Este enfoque simplifica el proceso de obtener funciones de clasificación con un alto rendimiento del mapa evitando pasos intermedios y heurísticas adicionales. El nuevo algoritmo también hace que conceptualmente sea tan fácil optimizar los SVM para el mapa como era posible solo para la precisión y ROCAREA. En contraste con el trabajo reciente, optimización directamente para el rendimiento del mapa de Metzler & Croft [16] y Caruana et al.[6], nuestra técnica es computacionalmente eficiente al tiempo que encuentra una solución globalmente óptima. Al igual que [6, 16], nuestro método aprende un modelo lineal, pero es mucho más eficiente en la práctica y, a diferencia de [16], puede manejar muchos miles de características. Ahora describimos el algoritmo en detalle y proporcionamos pruebas de corrección. Después de esto, proporcionamos un análisis del tiempo de ejecución. Terminamos con los resultados empíricos de los experimentos en el Corpus de pista web TREC 9 y TREC 10. También hemos desarrollado un paquete de software que implementa nuestro algoritmo que está disponible para uso público1.2. El problema de aprendizaje que sigue a la configuración de aprendizaje automático estándar, nuestro objetivo es aprender una función h: x → y entre un espacio de entrada x (todas las consultas posibles) y el espacio de salida Y (clasificaciones sobre un corpus). Para cuantificar la calidad de una predicción, ˆy = h (x), consideraremos una función de pérdida ∆: y × y →.∆ (y, ˆy) cuantifica la penalización por hacer una predicción ˆy si la salida correcta es y. La función de pérdida nos permite incorporar medidas de rendimiento específicas, que explotaremos 1 http://svmrank.yisongyue.com para optimizar el mapa. Nos restringimos al escenario de aprendizaje supervisado, donde los pares de entrada/salida (x, y) están disponibles para el entrenamiento y se supone que provienen de alguna distribución fija P (x, y). El objetivo es encontrar una función h de tal manera que el riesgo (es decir, pérdida esperada), r∆ p (h) = z x × y ∆ (y, h (x)) dp (x, y) se minimice. Por supuesto, P (x, y) es desconocido. Pero dado un conjunto finito de pares de entrenamiento, s = {(xi, yi) ∈ X × Y: i = 1 ,..., n}, el rendimiento de H en S puede medirse por el riesgo empírico, r∆ s (h) = 1 n nx i = 1 ∆ (yi, h (xi)). En el caso de aprender una función de recuperación clasificada, X denota un espacio de consultas, y y el espacio de clasificaciones (posiblemente débiles) sobre algún corpus de documentos c = {d1 ,..., D | C |}. Podemos definir la pérdida promedio de precisión como ∆map (y, ˆy) = 1 - mapa (rango (y), rango (ˆy)), donde el rango (y) es un vector de los valores de rango de cada documento en C. por ejemplo, para un corpus de dos documentos, {d1, d2}, con d1 que tiene rango más alto que d2, rango (y) = (1, 0). Suponemos que las clasificaciones verdaderas tienen dos valores de rango, donde los documentos relevantes tienen valor 1 de rango 1 y documentos no relevantes Valor de rango 0. Además, asumimos que todas las clasificaciones predichas son clasificaciones completas (sin lazos). Sea p = rango (y) y ˆp = rango (ˆy). El puntaje de precisión promedio se define como map (p, ˆp) = 1 rel x j: pj = 1 prec@j, donde rel = | {i: pi = 1} |es el número de documentos relevantes, y prec@j es el porcentaje de documentos relevantes en los documentos J superiores en la clasificación predicha ˆy. El mapa es la media de los puntajes de precisión promedio de un grupo de consultas.2.1 MAP vs ROCAREA La mayoría de los algoritmos de aprendizaje se optimizan para la precisión o ROCAREA. Si bien la optimización de estas medidas puede lograr un buen rendimiento del mapa, usamos dos ejemplos simples para mostrar que también puede ser subóptimo en términos de mapa. Rocarea asigna la misma penalización a cada pedido erróneo de un par relevante/no relevante. Por el contrario, MAP asigna mayores sanciones a los errores más altos en la clasificación prevista. Usando nuestra notación, Rocarea se puede definir como roc (p, ˆp) = 1 rel · (| c | - rel) x i: pi = 1 x j: pj = 0 1 [ˆpi> ˆpj], donde p es el verdadero(débil) Ranking, ˆp es la clasificación predicha, y 1 [b] es la función indicadora condicionada en b.ID de DOC 1 2 3 4 5 6 7 8 P 1 0 0 0 0 0 1 1 0 Rango (H1 (x)) 8 7 6 5 4 3 2 1 Rango (H2 (x)) 1 2 3 4 5 6 7 8 Tabla1: Ejemplo y modelos de juguete Suponga que tenemos un espacio de hipótesis con solo dos funciones de hipótesis, H1 y H2, como se muestra en la Tabla 1. Estas dos hipótesis predicen una clasificación para la consulta X sobre un corpus de ocho documentos. Mapa de hipótesis Rocarea H1 (X) 0.59 0.47 H2 (X) 0.51 0.53 Tabla 2: Performance de los modelos de juguete La Tabla 2 muestra el mapa y las puntuaciones de Rocarea de H1 y H2. Aquí, un método de aprendizaje que optimiza para Rocarea elegiría H2, ya que eso da como resultado una puntuación ROCAREA más alta, pero esto produce una puntuación de MAP subóptima.2.2 Mapa vs precisión utilizando un ejemplo muy similar, ahora demostramos cómo la optimización para la precisión podría dar como resultado un mapa subóptimo. Los modelos que optimizan para la precisión no se preocupan directamente por la clasificación. En cambio, aprenden un umbral de tal manera que los documentos que obtienen puntaje más alto que el umbral pueden clasificarse como relevantes y los documentos que tienen una puntuación más baja como no relevante. Doc ID 1 2 3 4 5 6 7 8 9 10 11 P 1 0 0 0 0 0 1 1 1 1 0 0 Rango (H1 (x)) 11 10 9 8 7 6 5 4 3 2 1 Rango (H2 (x))1 2 3 4 5 6 7 8 9 10 11 Tabla 3: Ejemplo de juguete y modelos Consideramos nuevamente un espacio de hipótesis con dos hipótesis. La Tabla 3 muestra las predicciones de las dos hipótesis en una sola consulta x.Mapa de hipótesis mejor acc.H1 (Q) 0.70 0.64 H2 (Q) 0.64 0.73 Tabla 4: Rendimiento de los modelos de juguete La Tabla 4 muestra el mapa y las mejores puntuaciones de precisión de H1 (Q) y H2 (Q). La mejor precisión se refiere a la mayor precisión alcanzable en esa clasificación al considerar todos los umbrales posibles. Por ejemplo, con H1 (Q), un umbral entre los documentos 1 y 2 da 4 errores (documentos 6-9 clasificados incorrectamente como no relevantes), lo que produce una precisión de 0.64. Del mismo modo, con H2 (Q), un umbral entre los documentos 5 y 6 da 3 errores (documentos 10-11 clasificados incorrectamente como relevantes, y el documento 1 como no relevante), lo que produce una precisión de 0.73. Un método de aprendizaje que optimiza para la precisión elegiría H2, ya que eso da como resultado una puntuación de precisión más alta, pero esto produce una puntuación de mapa subóptima.3. Optimización de la precisión promedio, construimos sobre el enfoque utilizado por [13] para optimizar Rocarea. Sin embargo, a diferencia de Rocarea, MAP no se descompone linealmente en los ejemplos y requiere un algoritmo sustancialmente extendido, que describimos en esta sección. Recuerde que la clasificación verdadera es una clasificación débil con dos valores de rango (relevantes y no relevantes). Deje que CX y C¯x denoten el conjunto de documentos relevantes y no relevantes de C para la consulta X, respectivamente. Nos centramos en las funciones que se parametrizan por un vector de peso W, y por lo tanto deseamos encontrar W para minimizar el riesgo empírico, r∆ s (w) ≡ r∆ s (h (·; w)). Nuestro enfoque es aprender una función discriminante F: x × y → sobre pares de entrada-salida. Dada la consulta x, podemos derivar una predicción encontrando la clasificación y que maximiza la función discriminante: h (x; w) = argmax y∈Y f (x, y; w).(1) Suponemos que F es lineal en alguna representación de características combinadas de entradas y salidas ψ (x, y) ∈ Rn, es decir, F (x, y; w) = wt ψ (x, y).(2) La función de característica combinada que usamos es ψ (x, y) = 1 | cx |· | C¯x |X i: di∈Cx x j: dj ∈C¯x [yij (φ (x, di) - φ (x, dj))], donde φ: x × c → n es una función de mapeo de características de una consulta/Documentar el par a un punto en el espacio n dimensional2. Representamos las clasificaciones como una matriz de pedidos por pares, y ⊂ {−1, 0, +1} | c | × | c |. Para cualquier y ∈ Y, yij = +1 si Di se clasifica por delante de DJ, y yij = −1 si DJ se clasifica por delante de DI, y yij = 0 si DI y DJ tienen igual rango. Consideramos solo matrices que corresponden a clasificaciones válidas (es decir, obedecer la antisimetría y la transitividad). Intuitivamente, ψ es una suma sobre las diferencias vectoriales de todos los emparejamientos de documentos relevantes/no relevantes. Como asumimos que las clasificaciones predichas son clasificaciones completas, YIJ es +1 o −1 (nunca 0). Dado un vector de peso aprendido w, predecir una clasificación (es decir, la ecuación de resolución (1)) dada la consulta x se reduce a elegir cada YIJ para maximizar WT ψ (x, y). Como también se discute en [13], esto se logra clasificando los documentos por WT φ (x, d) en orden descendente. Más adelante discutiremos las opciones de φ que utilizamos para nuestros experimentos.3.1 SVM estructurales La formulación anterior es muy similar a aprender un modelo lineal directo mientras se capacita en la diferencia por pares de emparejamientos de documentos relevantes/no relevantes. Muchos enfoques basados en SVM se optimizan sobre estas diferencias por pares (por ejemplo, [5, 10, 13, 4]), aunque estos métodos no optimizan para MAP durante el entrenamiento. Anteriormente, no estaba claro cómo incorporar funciones de pérdida multivariadas no lineales, como la pérdida de mapas directamente en problemas de optimización global, como la capacitación SVM. Ahora presentamos un método basado en SVM estructurales [19] para abordar este problema. Usamos la formulación SVM estructural, presentada en el problema de optimización 1, para aprender un w ∈ Rn. Problema de optimización 1. (SVM estructural) Min W, ξ≥0 1 2 W 2 + C N Nx I = 1 ξi (3) S.T.∀i, ∀y ∈ Y \\ yi: wt ψ (xi, yi) ≥ wt ψ (xi, y) + ∆ (yi, y) - ξi (4) La función objetivo que se minimiza (3) es una compensación entreComplejidad del modelo, W 2, y una relajación de pérdida de bisagra de la pérdida de mapas, p ξi. Como es habitual en el entrenamiento SVM, C es un 2 Por ejemplo, una dimensión podría ser la cantidad de veces que las palabras de consulta aparecen en el documento. Algoritmo 1 Algoritmo de plano de corte para resolver OP 1 dentro de la tolerancia.1: Entrada: (x1, y1) ,..., (xn, yn), c, 2: wi ← ∅ para todos i = 1 ,..., N 3: Repita 4: para i = 1 ,..., n do 5: h (y; w) ≡ ∆ (yi, y) + wt ψ (xi, y) - wt ψ (xi, yi) 6: Compute ˆy = argmaxy∈Y H (y; W) 7: Computξi = max {0, maxy∈Wi h (y; w)} 8: si h (ˆy; w)> ξi + entonces 9: wi ← wi ∪ {ˆy} 10: w ← optimizar (3) sobre w = sI WI 11: Fin si 12: finalice para 13: hasta que no haya cambiado WI durante el parámetro de iteración que controla esta compensación y puede ajustarse para lograr un buen rendimiento en diferentes tareas de capacitación. Para cada (xi, yi) en el conjunto de entrenamiento, se agrega un conjunto de restricciones de la forma en la ecuación (4) al problema de optimización. Tenga en cuenta que wt ψ (x, y) es exactamente nuestra función discriminante f (x, y; w) (ver ecuación (2)). Durante la predicción, nuestro modelo elige la clasificación que maximiza el discriminante (1). Si el valor discriminante para una clasificación incorrecta y es mayor que para la verdadera clasificación yi (por ejemplo, F (xi, y; w)> f (xi, yi; w)), entonces la variable de holgura correspondiente, ξi, debe ser al menos∆ (yi, y) para que esa restricción esté satisfecha. Por lo tanto, la suma de las holguras, p ξi, los límites superiores la pérdida del mapa. Esto se indica formalmente en la Propuesta 1. Proposición 1. Sea ξ ∗ (w) la solución óptima de las variables flojas para OP 1 para un vector de peso dado w.Entonces 1 n pn i = 1 ξi es un límite superior en el riesgo empírico r∆ s (w).(Ver [19] para la prueba) La propuesta 1 muestra que OP 1 aprende una función de clasificación que optimiza un límite superior en el error del mapa en el conjunto de entrenamiento. Desafortunadamente, hay un problema: se requiere una restricción para cada salida incorrecta posible y, y el número de posibles salidas incorrectas es exponencial en el tamaño de C. Afortunadamente, podemos emplear el Algoritmo 1 para resolver OP 1. El algoritmo 1 es un algoritmo de plano de corte, introduciendo iterativamente restricciones hasta que hayamos resuelto el problema original dentro de una tolerancia deseada [19]. El algoritmo comienza sin restricciones, y se encuentra iterativamente para cada ejemplo (xi, yi) la salida ˆy asociada con la restricción más violada. Si la restricción correspondiente es violada por más de lo que introducimos en el conjunto de trabajo WI de las restricciones activas, por ejemplo, I, y volver a resolver (3) utilizando el W. Actualizado se puede demostrar que el algoritmo 1S externo se garantiza que se detenga dentro deUn número polinomial de iteraciones para cualquier precisión deseada. Teorema 1. Sea ¯r = maxi maxy ψ (xi, yi) - ψ (xi, y), ¯∆ = maxi maxy ∆ (yi, y), y para cualquier> 0, el algoritmo 1 termina después de agregar como máximo max  ¯ ¯ ¯ ¯ ¯, 8c ¯∆ ¯r2 2 FF restricciones al conjunto de trabajo W. (ver [19] para la prueba) Sin embargo, dentro del bucle interno de este algoritmo tenemos que calcular argmaxy∈Y H (y; w), donde h (y;w) = ∆ (yi, y) + wt ψ (xi, y) - wt ψ (xi, yi), o de manera equivalente, argmax y∈Y ∆ (yi, y) + wt ψ (xi, y), ya que wtΨ (xi, yi) es constante con respecto a y. Aunque estrechamente relacionado con el procedimiento de clasificación, esto tiene la complicación sustancial de que debemos lidiar con el término adicional ∆ (yi, y). Sin la capacidad de encontrar eficientemente la restricción más violada (es decir, resolver argmaxy∈Y h (y, w)), el procedimiento de generación de restricciones no es manejable.3.2 Encontrar la restricción más violada usando OP 1 y optimizar a la pérdida de rocarea (∆ROC), el problema de encontrar la restricción más violada o resolver argmaxy∈ H (y, w) (en adelante argmax h), se aborda en [13]. Resolver Argmax H para ∆map es más difícil. Esto se debe principalmente a que Rocarea se descompone muy bien en una suma de puntajes calculados de forma independiente en cada orden relativo de un par de documentos relevante/no relevante. El mapa, por otro lado, no se descompone de la misma manera que Rocarea. La principal contribución algorítmica de este documento es un método eficiente para resolver Argmax H para ∆MAP. Una propiedad útil de ∆map es que es invariante al intercambiar dos documentos con igual relevancia. Por ejemplo, si los documentos DA y DB son relevantes, entonces intercambiar las posiciones de DA y DB en cualquier clasificación no afecta a ∆map. Por extensión, ∆map es invariante a cualquier permutación arbitraria de los documentos relevantes entre ellos y de los documentos no relevantes entre ellos. Sin embargo, esta reshu afectará la puntuación discriminante, wt ψ (x, y). Esto nos lleva a la observación 1. Observación 1. Considere las clasificaciones que están limitadas al fijar la relevancia en cada posición en la clasificación (por ejemplo, el tercer documento en la clasificación debe ser relevante). Cada clasificación que satisface el mismo conjunto de restricciones tendrá el mismo ∆map. Si los documentos relevantes son ordenados por WT φ (x, d) en orden descendente, y los documentos no relevantes también están ordenados por WT φ (x, d), entonces el intercalación de las dos listas ordenadas que satisface las restricciones maximizaránH para ese conjunto restringido de clasificaciones. La observación 1 implica que en la clasificación que maximiza H, los documentos relevantes serán ordenados por WT φ (x, d), y los documentos no relevantes también se clasificarán de la misma manera. Al clasificar primero los documentos relevantes y no relevantes, el problema se simplifica para encontrar el intercalación óptima de dos listas ordenadas. Para el resto de nuestra discusión, suponemos que los documentos relevantes y los documentos no relevantes se clasifican descendiendo WT φ (x, d). Por conveniencia, también nos referimos a documentos relevantes como {dx 1 ,...dx | cx |} = cx, y documentos no relevantes como {d¯x 1 ,...d¯x | c¯x |} = c¯x. Definimos ΔJ (i1, i2), con i1 <i2, como el cambio en H desde cuando el documento relevante mejor clasificado clasificado después de d¯x j es dx i1 a cuando es dx i2. Para i2 = i1 + 1, tenemos ΔJ (i, i + 1) = 1 | cx |„J J + I - J - 1 J + I - 1« - 2 · (SX I - S¯x J), (5) donde Si = Wt φ (x, di). El primer término en (5) es el cambio en ∆map cuando el documento relevante con IPI tiene documentos J no relevantes clasificados ante él, en lugar de J −1. El segundo término es el cambio en la puntuación discriminante, wt ψ (x, y), cuando YIJ cambia de +1 a −1...., DX I, D¯x J, DX I+1 ,......, D¯x J, DX I, DX I+1 ,... Figura 1: Ejemplo para ΔJ (i, i + 1) La Figura 1 da un ejemplo conceptual para ΔJ (i, i + 1). La clasificación inferior difiere de la parte superior solo donde D¯x J se desliza hacia arriba de un rango. La diferencia en el valor de H para estas dos clasificaciones es exactamente ΔJ (i, i + 1). Para cualquier i1 <i2, podemos definir ΔJ (i1, i2) como ΔJ (i1, i2) = i2−1 x k = i1 ΔJ (k, k + 1), (6) o de manera equivalente, ΔJ (i1,i2) = i2−1 x k = i1 »1 | cx |„J J + K - J - 1 J + K - 1« - 2 · (SX K - S¯x J). Sea O1 ,..., o | c¯x |Codifique las posiciones de los documentos no relevantes, donde DX OJ es el documento relevante mejor clasificado clasificado después del documento JTH no relevante. Debido a la observación 1, esta codificación identifica de manera única una clasificación completa. Podemos recuperar la clasificación como yij = 8 >>> <>>>: 0 si i = j signo (si - sj) Si di, dj signo de igual relevancia (oj - i - 0.5) si di = dx i, dj =d¯x j signo (j - oi + 0.5) si di = d¯x i, dj = dx j.(7) Ahora podemos reformular h en una nueva función objetivo, H (O1, ..., o | c¯x || w) = h (¯y | w) + | c¯x |X k = 1 ΔK (OK, | CX | + 1), donde ¯y es la clasificación verdadera (débil). Conceptualmente H comienza con una clasificación perfecta y agrega el cambio en H cuando cada documento no relevante sucesivo desliza por la clasificación. Entonces podemos reformular el problema de Argmax H como argmax h = argmax o1, ..., o | c¯x || C¯x |X k = 1 ΔK (OK, | CX | + 1) (8) S.T.O1 ≤...≤ o | c¯x |.(9) El algoritmo 2 describe el algoritmo utilizado para resolver la ecuación (8). Conceptualmente, el algoritmo 2 comienza con una clasificación perfecta. Luego, para cada documento no relevante sucesivo, el algoritmo modifica la solución deslizando que documenta la clasificación para maximizar localmente H mientras mantiene constante las posiciones de los otros documentos no relevantes.3.2.1 El algoritmo de prueba de corrección 2 es codicioso en el sentido de que encuentra la mejor posición de cada documento no relevante independientemente de los otros documentos no relevantes. En otras palabras, el algoritmo maximiza H para cada documento no relevante, D¯x J, Algoritmo 2 Encontrar la restricción más violada (Argmax H) para el algoritmo 1 con ∆Map 1: Entrada: W, CX, C¯x 2: OrdenarCx y c¯x en orden descendente de wt φ (x, d) 3: sx i ← wt φ (x, dx i), i = 1 ,..., | Cx |4: S¯x I ← Wt φ (x, d¯x i), i = 1 ,..., | C¯x |5: para j = 1 ,..., | C¯x |Do 6: Optj ← ArgMaxk ΔJ (k, | cx | + 1) 7: Fin para 8: codifica ˆy de acuerdo con (7) 9: return ˆy sin considerar las posiciones de los otros documentos no relevantes, y así ignora las restriccionesde (9). Para que la solución sea factible, el documento JTH no relevante debe clasificarse después de los primeros documentos no relevantes J-1, satisfaciendo así Opt1 ≤ Opt2 ≤...≤ opt | c¯x |.(10) Si la solución es factible, se resuelve claramente (8). Por lo tanto, es suficiente demostrar que el algoritmo 2 satisface (10). Primero demostramos que ΔJ (·, ·) está disminuyendo monotónicamente en j.Lema 1. Para cualquier 1 ≤ i1 <i2 ≤ | cx |+ 1 y 1 ≤ j <| c¯x |, debe ser el caso que ΔJ+ 1 (i1, i2) ≤ ΔJ (i1, i2). Prueba. Recuerde de (6) que tanto ΔJ (i1, i2) como ΔJ+1 (i1, i2) son sumaciones de términos I2 - I1. Mostraremos que cada término en la suma de ΔJ+1 (i1, i2) no es mayor que el término correspondiente en ΔJ (i1, i2) o ΔJ+1 (k, k+1) ≤ ΔJ (k, k+ 1) para k = i1 ,..., i2 - 1. Cada término en ΔJ (k, k +1) y ΔJ +1 (k, k +1) puede descomponerse aún más en dos partes (ver (5)). Mostraremos que cada parte de ΔJ + 1 (k, k + 1) no es mayor que la parte correspondiente en ΔJ (k, k + 1). En otras palabras, mostraremos que tanto J + 1 J + K + 1 - J J + K ≤ J J + K - J - 1 J + K - 1 (11) como −2 · (SX K - S¯x J +1) ≤ −2 · (sx k - s¯x j) (12) son verdaderos para los valores mencionados de j y k.Es fácil ver que (11) es cierto al observar que para dos enteros positivos 1 ≤ a <b, a + 1 b + 1 - a b ≤ a b - a - 1 b - 1, y eligiendo a = j y b= j + k.La segunda desigualdad (12) se mantiene porque el algoritmo 2 primero tipos d¯x en orden descendente de s¯x, lo que implica s¯x j+1 ≤ s¯x j. Por lo tanto, vemos que cada término en ΔJ+1 no es mayor que el término correspondiente en ΔJ, lo que completa la prueba. El resultado de Lemma 1 conduce directamente a nuestro principal resultado de corrección: Teorema 2. En el algoritmo 2, los valores calculados de OPTJ satisfacen (10), lo que implica que la solución devuelta por el algoritmo 2 es factible y, por lo tanto, óptima. Prueba. Probaremos que OPTJ ≤ OptJ+1 se mantiene para cualquier 1 ≤ J <| C¯x |, lo que implica (10). Dado que el algoritmo 2 calcula optj como optj = argmax k Δj (k, | cx | + 1), (13) entonces, por definición de ΔJ (6), para cualquier 1 ≤ i <optj, ΔJ (i, optj) = ΔJ ((i, | cx | + 1) - ΔJ (optj, | cx | + 1) <0. Usando Lemma 1, sabemos que ΔJ+1 (i, Optj) ≤ ΔJ (i, Optj) <0, lo que implica que para cualquier 1 ≤ i <optJ, ΔJ+1 (i, | cx |+1) - ΔJ+1 (optj, | cx | + 1) <0. Supongamos por contradicción que optj+1 <optj. Entonces ΔJ+1 (optj+1, | cx |+1) <ΔJ+1 (optj, | cx |+1), que contradice (13). Por lo tanto, debe ser el caso que optj ≤ optj+1, que completa la prueba.3.2.2 Tiempo de ejecución El tiempo de ejecución del Algoritmo 2 se puede dividir en dos partes. La primera parte es el tipo de wt φ (x, d), que requiere o (n log n) tiempo, donde n = | cx |+ | C¯x |. La segunda parte calcula cada optj, que requiere tiempo o (| cx | · | c¯x |). Aunque en el peor de los casos, este es O (N2), el número de documentos relevantes, | cx |, a menudo es muy pequeño (por ejemplo, constante con respecto a n), en cuyo caso el tiempo de ejecución de la segunda parte es simplemente o (norte). Para la mayoría de los conjuntos de datos del mundo real, el algoritmo 2 está dominado por el tipo y tiene complejidad o (n log n). Se garantiza que el algoritmo 1 se detendrá en un número polinomial de iteraciones [19], y cada iteración ejecuta el Algoritmo 2. Prácticamente todos los modelos bien realizados fueron entrenados en una cantidad razonable de tiempo (generalmente menos de una hora). Una vez que se completa el entrenamiento, hacer predicciones sobre la consulta x utilizando la hipótesis resultante h (x | w) requiere solo clasificación por wt φ (x, d). Desarrollamos nuestro software utilizando una interfaz de Python3 para svmstruct, ya que el lenguaje de Python simplificó enormemente el proceso de codificación. Para mejorar el rendimiento, es aconsejable utilizar la implementación estándar C4 de SVMstruct.4. Configuración del experimento El objetivo principal de nuestros experimentos es evaluar si la optimización directa del MAP conduce a un mejor rendimiento del mapa en comparación con los métodos SVM convencionales que optimizan una pérdida sustitutiva, como la precisión o la rocarea. Evaluamos empíricamente nuestro método utilizando dos conjuntos de consultas de pista web TREC, una de TREC 9 y TREC 10 (Temas 451-500 y 501-550), los cuales utilizaron el Corpus WT10G. Para cada consulta, TREC proporciona los juicios de relevancia de los documentos. Generamos nuestras características utilizando los puntajes de las funciones de recuperación existentes en estas consultas. Si bien nuestro método es agnóstico para el significado de las características, elegimos utilizar las funciones de recuperación existentes como una forma simple pero efectiva de adquirir características útiles. Como tal, nuestro 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html DataSet base base características Trec 9 Indri 15 750 Trec 10 Indri 15 750TREC 9 SUMISIONES 53 2650 TREC 10 SUMISIONES 18 900 Tabla 5: Experimentos de estadísticas del conjunto de datos esencialmente prueban nuestros métodos la capacidad de volver a clasificar los documentos altamente clasificados (por ejemplo, volver a combinar las puntuaciones de las funciones de recuperación) para mejorar el mapa. Comparamos nuestro método con las mejores funciones de recuperación entrenadas (en adelante funciones base), así como contra los métodos SVM propuestos previamente. La comparación con las mejores funciones base prueba nuestra capacidad de métodos para aprender una combinación útil. La comparación con los métodos SVM anteriores nos permite probar si la optimización directamente para MAP (en lugar de precisión o ROCAREA) logra una puntuación de MAP más alta en la práctica. El resto de esta sección describe las funciones base y el método de generación de características en detalle.4.1 Elegir funciones de recuperación, elegimos dos conjuntos de funciones base para nuestros experimentos. Para el primer set, generamos tres índices sobre el Corpus WT10G usando Indri5. El primer índice se generó utilizando la configuración predeterminada, el segundo portero usado y el último de portador usado y las palabras de parada predeterminadas de Indris. Tanto para TREC 9 como TREC 10, utilizamos la parte de descripción de cada consulta y calificamos los documentos utilizando cinco de los métodos de recuperación de Indris incorporados, que son similitudes de coseno, TFIDF, OKAPI, modelo de lenguaje con Dirichlet Prior y modelo de idioma con Jelinek-Mercer Prior. Todos los parámetros se mantuvieron como sus valores predeterminados. Calculamos los puntajes de estos cinco métodos de recuperación sobre los tres índices, dando 15 funciones base en total. Para cada consulta, consideramos los puntajes de los documentos encontrados en la unión de los 1000 principales documentos de cada función base. Para nuestro segundo conjunto de funciones base, utilizamos puntajes de las presentaciones de pista web TREC 9 [8] y TREC 10 [9]. Utilizamos solo las presentaciones no manuales y sin escortes de ambos años. Para TREC 9 y TREC 10, hubo 53 y 18 de este tipo, respectivamente. Una presentación típica contenía puntajes de sus 1000 documentos principales.B CA WT φ (X, D) F (D | X) Figura 2: Ejemplo de características Binning 4.2 Generación de características Para generar ejemplos de entrada para nuestro método, se debe proporcionar una instanciación concreta de φ. Para cada DOC5 http://www.lemurproject.org trec 9 trec 10 mapa de modelo w/l w/l svm∆ mapa 0.242 - 0.236best func.0.204 39/11 ** 0.181 37/13 ** 2nd mejor 0.199 38/12 ** 0.174 43/7 ** 3rd mejor 0.188 34/16 ** 0.174 38/12 ** Tabla 6: Comparación con las funciones de Indri Umento D anotadoPor un conjunto de funciones de recuperación f en la consulta x, generamos las características como un vector φ (x, d) = 1 [f (d | x)> k]: ∀f ∈ F, ∀k ∈ Kf, donde f ((D | X) denota la puntuación que la función de recuperación F asigna al documento D para la consulta X, y cada KF es un conjunto de valores reales. Desde un alto nivel, estamos expresando la puntuación de cada función de recuperación usando | KF |+ 1 contenedores. Dado que estamos utilizando núcleos lineales, uno puede pensar en el problema de aprendizaje como encontrar una buena combinación por partes constantes de las puntuaciones de las funciones de recuperación. La Figura 2 muestra un ejemplo de nuestro método de mapeo de características. En este ejemplo tenemos una sola característica f = {f}. Aquí, kf = {a, b, c}, y el vector de peso es w = wa, wb, wc. Para cualquier documento d y consulta x, tenemos wt φ (x, d) = 8 >> <>>: 0 si f (d | x) <a wa si a ≤ f (d | x) <b wa + wbSi b ≤ f (d | x) <c wa + wb + wc si c ≤ f (d | x). Esto se expresa cualitativamente en la Figura 2, donde WA y WB son positivos, y WC es negativo. Ejecutamos nuestros principales experimentos utilizando cuatro opciones de F: el conjunto de funciones de recuperación de Indri mencionadas anteriormente para TREC 9 y TREC 10, y las presentaciones de la pista web para TREC 9 y TREC 10. Para cada F y cada función f ∈ F, elegimos 50 valores para KF que están razonablemente espaciados y capturan la región sensible de f.Usando las cuatro opciones de F, generamos cuatro conjuntos de datos para nuestros experimentos principales. La Tabla 5 contiene estadísticas de los conjuntos de datos generados. Hay muchas formas de generar características, y no estamos abogando por nuestro método sobre los demás. Este era simplemente un medio eficiente para normalizar las salidas de diferentes funciones y permitir un modelo más expresivo.5. Experimentos para cada conjunto de datos en la Tabla 5, realizamos 50 ensayos. Para cada ensayo, entrenamos en 10 consultas seleccionadas al azar y seleccionamos otras 5 consultas al azar para un conjunto de validación. Los modelos fueron entrenados utilizando una amplia gama de valores de C. El modelo que funcionó mejor en el conjunto de validación se seleccionó y probó en las 35 consultas restantes. Todas las consultas fueron seleccionadas para estar en la capacitación, validación y prueba establece el mismo número de veces. Usando esta configuración, realizamos los mismos experimentos mientras usamos nuestro método (mapa SVM∆), una optimización SVM para ROCAREA (SVM∆ ROC) [13] y una clasificación convencional SVM (SVMACC) [20]. Todos los métodos SVM utilizaron un núcleo lineal. Informamos el rendimiento promedio de todos los modelos en los 50 ensayos.5.1 Comparación con las funciones base en el análisis de nuestros resultados, la primera pregunta que responde es, ¿puede svm∆ mapa aprender un modelo que supera el mejor mapa de modelos de trec 9 trec 10 w/l w/l svm∆ mapa 0.290 - 0.287 mejor funce.0.280 28/22 0.283 29/21 2nd mejor 0.269 30/20 0.251 36/14 ** 3rd mejor 0.266 30/20 0.233 36/14 ** Tabla 7: Comparación con los envíos TREC TREC 9 TREC 10 Mapa de modelo W/L W W/L W W/L MAP W/L/L svm∆ mapa 0.284 - 0.288 mejor func.0.280 27/23 0.283 31/19 2nd mejor 0.269 30/20 0.251 36/14 ** 3er mejor 0.266 30/20 0.233 35/15 ** Tabla 8: Comparación con TREC Subt.(sin las mejores) funciones? La Tabla 6 presenta la comparación del mapa SVM∆ con las mejores funciones base de Indri. Cada grupo de columna contiene el rendimiento del mapa de macro promediado del mapa SVM∆ o una función base. Las columnas W/L muestran el número de consultas donde el mapa SVM∆ logró una puntuación de mapa más alta. Las pruebas de significancia se realizaron utilizando la prueba de rango firmada de Wilcoxon de dos colas. Dos estrellas indican un nivel de significancia de 0.95. Todas las tablas que muestran nuestros resultados experimentales están estructuradas de manera idéntica. Aquí, encontramos que el mapa SVM∆ supera significativamente las mejores funciones base. La Tabla 7 muestra la comparación cuando se entrena en las presentaciones de TREC. Si bien logran una puntuación de mapa más alta que las mejores funciones base, la diferencia de rendimiento entre el mapa SVM∆ las funciones base no es significativa. Dado que muchas de estas presentaciones utilizan funciones de puntuación que están cuidadosamente elaboradas para lograr un mapa alto, es posible que las presentaciones de mejor rendimiento usen técnicas que subsumen las técnicas de las otras presentaciones. Como resultado, el mapa SVM∆ no podría aprender una hipótesis que puede superar significativamente la mejor presentación. Por lo tanto, ejecutamos los mismos experimentos utilizando un conjunto de datos modificado donde se eliminaron las características calculadas utilizando el mejor envío. La Tabla 8 muestra los resultados (tenga en cuenta que todavía estamos comparando con la mejor presentación, aunque no la estamos utilizando para capacitar). Observe que si bien el rendimiento del mapa SVM∆ se degradó ligeramente, el rendimiento aún era comparable con el de la mejor presentación.5.2 Comparación con métodos SVM anteriores La siguiente pregunta para responder es: ¿El mapa SVM∆ produce puntajes de mapas más altos que los métodos SVM anteriores? Las tablas 9 y 10 presentan los resultados del mapa SVM∆, SVM∆ ROC y SVMACC cuando se entrenan en las funciones de recuperación de Indri y las presentaciones de TREC, respectivamente. La Tabla 11 contiene los resultados correspondientes cuando se entrenan en las presentaciones de TREC sin la mejor presentación. Para comenzar, nuestros resultados indican que SVMACC no fue competitivo con SVM∆ MAP y SVM∆ ROC, y a veces tuvo un rendimiento inferior dramáticamente. Como tal, probamos varios enfoques para mejorar el rendimiento de SVMACC.5.2.1 Métodos alternativos de SVMACC Un problema que puede hacer que SVMACC tenga un rendimiento inferior es el desequilibrio severo entre el Doctrec -Rocio relevante y no relevante 9 TREC 10 MAP W/L MAP W/L SVM∆ MAP 0.242 - 0.236SVM∆ ROC 0.237 29/210.234 24/26 SVMACC 0.147 47/3 ** 0.155 47/3 ** SVMACC2 0.219 39/11 ** 0.207 43/7 ** SVMACC3 0.113 49/1 ** 0.153 45/5 ** SVMACC4 0.155 48/2 **0.155 48/2 ** Tabla 9: Entrenado en funciones de Indri TREC 9 TREC 10 MAPE MOMENTO W/L MAP W/L SVM∆ MAP 0.290 - 0.287SVM∆ ROC 0.282 29/21 0.278 35/15 ** SVMACC 0.213 49/1** 0.222 49/1 ** SVMACC2 0.270 34/16 ** 0.261 42/8 ** SVMACC3 0.133 50/0 ** 0.182 46/4 ** SVMACC4 0.233 47/3 ** 0.238 46/4 ** Tabla 10:Entrenado en TREC presentaciones Uments. La gran mayoría de los documentos no son relevantes. SVMACC2 aborda este problema asignando más penalización a errores falsos negativos. Para cada conjunto de datos, la relación de las penalizaciones falsas a falsas positivas es igual a la relación del número de documentos no relevantes y relevantes en ese conjunto de datos. Las tablas 9, 10 y 11 indican que SVMACC2 todavía funciona significativamente peor que el mapa SVM∆. Otro posible problema es que SVMACC intenta encontrar un solo umbral bisural B que sea invariante de la consulta. Puede ser que diferentes consultas requieran diferentes valores de b. Tener el método de aprendizaje tratando de encontrar un buen valor B (cuando uno no existe) puede ser perjudicial. Tomamos dos enfoques para abordar este problema. El primer método, SVMACC3, convierte los puntajes de la función de recuperación en percentiles. Por ejemplo, para el documento D, la consulta Q y la función de recuperación F, si la puntuación F (d | Q) está en el 90% superior de los puntajes F (· | Q) para la consulta Q, entonces el puntaje convertido es F (D| q) = 0.9. Cada KF contiene 50 valores espaciados uniformemente entre 0 y 1. Las Tablas 9, 10 y 11 muestran que el rendimiento de SVMACC3 tampoco fue competitivo con el mapa SVM∆. El segundo método, SVMACC4, normaliza los puntajes dados por F para cada consulta. Por ejemplo, suponga la consulta Q que F emite puntajes en el rango 0.2 a 0.7. Luego, para el documento D, si F (D | Q) = 0.6, la puntuación convertida sería F (D | Q) = (0.6 - 0.2)/(0.7 - 0.2) = 0.8. Cada KF contiene 50 valores espaciados uniformemente entre 0 y 1. Nuevamente, las Tablas 9, 10 y 11 muestran que SVMACC4 no fue competitivo con SVM∆ MAP 5.2.2 MAP vs ROCAREA SVM∆ ROC funcionó mucho mejor que SVMACC en nuestros experimentos. Cuando se entrenan en funciones de recuperación de Indri (ver Tabla 9), el rendimiento de SVM∆ ROC fue leve, aunque no significativamente, peor que las actuaciones del mapa SVM∆. Sin embargo, la Tabla 10 muestra que el mapa SVM∆ superó significativamente a SVM∆ ROC cuando fue entrenado en las presentaciones de TREC. La Tabla 11 muestra el rendimiento de los modelos cuando se entrenan en las presentaciones de TREC con la mejor presentación eliminada. El rendimiento de la mayoría de los modelos degradado por una pequeña cantidad, con el mapa SVM∆ todavía tiene el mejor rendimiento. Trec 9 trec 10 mapa modelo mapa w/l w/l svm∆ mapa 0.284 - 0.288svm∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMACC 0.215 49/1 ** 0.211 50/0 ** SVMACC2 0.267 35/15 ** 0.258 44/6 ** SVMACC3 0.133 50/0 ** 0.174 46/4 ** SVMACC4 0.228 46/4 ** 0.234 45/5 ** Tabla 11: Entrenado en Trec Subt.(sin el mejor) 6. Conclusiones y trabajos futuros hemos presentado un método SVM que optimiza directamente el MAP. Proporciona un enfoque de principios y evita que sean difíciles de controlar las heurísticas. Formulamos el problema de optimización y presentamos un algoritmo que probablemente encuentra la solución en el tiempo polinomial. Hemos demostrado empíricamente que nuestro método es generalmente superior o competitivo con los métodos SVMS convencionales. Nuestro nuevo método hace que conceptualmente sea tan fácil de optimizar los SVM para el MAP como era posible solo para la precisión y Rocarea. El costo computacional para la capacitación es muy razonable en la práctica. Dado que otros métodos generalmente requieren ajustar múltiples heurísticas, también esperamos entrenar menos modelos antes de encontrar uno que logre un buen rendimiento. El marco de aprendizaje utilizado por nuestro método es bastante general. Una extensión natural de este marco sería desarrollar métodos para optimizar otras medidas IR importantes, como la ganancia acumulativa con descuento normalizada [2, 3, 4, 12] y el rango recíproco medio.7. Agradecimientos Este trabajo fue financiado bajo el premio NSF IIS-0412894, NSF Carrerae Award 02373381 y un regalo de Yahoo! Investigación. El tercer autor también fue apoyado en parte por una beca de investigación de Microsoft.8. Referencias [1] B. T. Bartell, G. W. Cottrell y R. K. Belew. Combinación automática de sistemas de recuperación de rango múltiple. En Actas de la Conferencia de ACM sobre Investigación y Desarrollo en Recuperación de Información (Sigir), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton y G. Hullender. Aprendiendo a clasificarse usando descenso de gradiente. En Actas de la Conferencia Internacional sobre Aprendizaje Autor (ICML), 2005. [3] C. J. C. Burges, R. Ragno y Q. Le. Aprender a clasificarse con funciones de costo no suaves. En Actas de la Conferencia Internacional sobre Avances en Sistemas de Procesamiento de Información Neural (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang y H.-W.Excmo Adaptación de ranking SVM para la recuperación de documentos. En Actas de la Conferencia de ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2006. [5] B. Carterette y D. Petkova. Aprender una clasificación de preferencias por pares. En Actas de la Conferencia de ACM sobre Investigación y Desarrollo en Recuperación de Información (Sigir), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew y A. Ksikes. Selección de conjunto de bibliotecas de modelos. En Actas de la Conferencia Internacional sobre Aprendizaje Autor (ICML), 2004. [7] J. Davis y M. Goadrich. La relación entre las curvas de recepción de precisión y ROC. En Actas de la Conferencia Internacional sobre Aprendizaje Autor (ICML), 2006. [8] D. Hawking. Descripción general de la pista web TREC-9. En Actas de TREC-2000, 2000. [9] D. Hawking y N. Craswell. Descripción general de la pista web TREC-2001. En Actas de TREC-2001, noviembre de 2001. [10] R. Herbrich, T. Graepel y K. Obermayer. Límites de rango de margen grande para la regresión ordinal. Avances en clasificadores de margen grande, 2000. [11] A. Herschtal y B. Raskutti. Optimización del área bajo la curva ROC utilizando descenso de gradiente. En Actas de la Conferencia Internacional sobre Aprendizaje Autor (ICML), 2004. [12] K. Jarvelin y J. Kekalainen. IR Métodos de evaluación para recuperar documentos altamente relevantes. En Actas de la Conferencia ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), 2000. [13] T. Joachims. Un método de vector de soporte para medidas de rendimiento multivariadas. En Actas de la Conferencia Internacional sobre Aprendizaje Autor (ICML), páginas 377-384, Nueva York, NY, EE. UU., 2005. ACM Press.[14] J. Lafferty y C. Zhai. Documentar modelos de lenguaje, modelos de consulta y minimización de riesgos para la recuperación de información. En Actas de la Conferencia de ACM sobre Investigación y Desarrollo en Recuperación de Información (SIGIR), Páginas 111-119, 2001. [15] Y. Lin, Y. Lee y G. Wahba. Soporte de máquinas vectoriales para la clasificación en situaciones no estándar. Machine Learning, 46: 191-202, 2002. [16] D. Metzler y W. B. Croft. Un modelo de campo aleatorio de Markov para dependencias de términos. En Actas de la 28ª Conferencia Anual de ACM Sigir sobre investigación y desarrollo en recuperación de información, páginas 472-479, 2005. [17] K. Morik, P. Brockhausen y T. Joachims. Combinando el aprendizaje estadístico con un enfoque basado en el conocimiento. En Actas de la Conferencia Internacional sobre Aprendizaje Machine, 1999. [18] S. Robertson. El principio de clasificación de probabilidad en IR.Revista de documentación. Journal of Documentation, 33 (4): 294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims e Y. Altun. Métodos de margen grande para variables de salida estructuradas e interdependientes. Journal of Machine Learning Research (JMLR), 6 (sep): 1453-1484, 2005. [20] V. Vapnik. Teoría del aprendizaje estadístico. Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer y R. Wolniewicz. Optimización del rendimiento del clasificador a través de la aproximación a la estadística de Wilcoxon-Mann-Whitney. En Actas de la Conferencia Internacional sobre Aprendizaje Autor (ICML), 2003.",
    "original_sentences": [
        "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
        "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
        "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
        "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
        "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
        "In most cases we show our method to produce statistically significant improvements in MAP scores.",
        "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
        "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
        "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
        "Instead, current algorithms tend to take one of two general approaches.",
        "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
        "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
        "However, achieving high MAP only requires finding a good ordering of the documents.",
        "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
        "The second common approach is to learn a function that maximizes a surrogate measure.",
        "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
        "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
        "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
        "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
        "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
        "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
        "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
        "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
        "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
        "We now describe the algorithm in detail and provide proof of correctness.",
        "Following this, we provide an analysis of running time.",
        "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
        "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
        "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
        "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
        "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
        "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
        "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
        "Of course, P(x, y) is unknown.",
        "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
        "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
        "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
        "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
        "We further assume that all predicted rankings are complete rankings (no ties).",
        "Let p = rank(y) and ˆp = rank(ˆy).",
        "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
        "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
        "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
        "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
        "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
        "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
        "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
        "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
        "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
        "Models which optimize for accuracy are not directly concerned with the ranking.",
        "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
        "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
        "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
        "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
        "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
        "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
        "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
        "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
        "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
        "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
        "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
        "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
        "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
        "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
        "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
        "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
        "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
        "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
        "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
        "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
        "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
        "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
        "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
        "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
        "We now present a method based on structural SVMs [19] to address this problem.",
        "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
        "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
        "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
        "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
        "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
        "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
        "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
        "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
        "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
        "This is stated formally in Proposition 1.",
        "Proposition 1.",
        "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
        "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
        "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
        "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
        "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
        "Theorem 1.",
        "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
        "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
        "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
        "Solving argmax H for ∆map is more difficult.",
        "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
        "MAP, on the other hand, does not decompose in the same way as ROCArea.",
        "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
        "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
        "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
        "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
        "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
        "This leads us to Observation 1.",
        "Observation 1.",
        "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
        "Every ranking which satisfies the same set of constraints will have the same ∆map.",
        "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
        "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
        "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
        "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
        "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
        "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
        "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
        "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
        "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
        "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
        "The bottom ranking differs from the top only where d¯x j slides up one rank.",
        "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
        "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
        "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
        "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
        "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
        "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
        "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
        "Conceptually, Algorithm 2 starts with a perfect ranking.",
        "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
        "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
        "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
        "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
        "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
        "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
        "Proof.",
        "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
        "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
        "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
        "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
        "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
        "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
        "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
        "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
        "Proof.",
        "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
        "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
        "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
        "Suppose for contradiction that optj+1 < optj.",
        "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
        "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
        "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
        "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
        "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
        "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
        "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
        "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
        "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
        "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
        "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
        "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
        "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
        "For each query, TREC provides the relevance judgments of the documents.",
        "We generated our features using the scores of existing retrieval functions on these queries.",
        "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
        "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
        "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
        "Comparing with the best base functions tests our methods ability to learn a useful combination.",
        "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
        "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
        "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
        "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
        "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
        "All parameters were kept as their defaults.",
        "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
        "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
        "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
        "We used only the non-manual, non-short submissions from both years.",
        "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
        "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
        "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
        "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
        "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
        "Figure 2 shows an example of our feature mapping method.",
        "In this example we have a single feature F = {f}.",
        "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
        "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
        "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
        "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
        "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
        "Table 5 contains statistics of the generated datasets.",
        "There are many ways to generate features, and we are not advocating our method over others.",
        "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
        "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
        "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
        "Models were trained using a wide range of C values.",
        "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
        "All queries were selected to be in the training, validation and test sets the same number of times.",
        "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
        "All SVM methods used a linear kernel.",
        "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
        "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
        "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
        "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
        "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
        "Two stars indicate a significance level of 0.95.",
        "All tables displaying our experimental results are structured identically.",
        "Here, we find that SVM∆ map significantly outperforms the best base functions.",
        "Table 7 shows the comparison when trained on TREC submissions.",
        "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
        "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
        "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
        "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
        "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
        "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
        "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
        "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
        "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
        "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
        "The vast majority of the documents are not relevant.",
        "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
        "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
        "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
        "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
        "It may be that different queries require different values of b.",
        "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
        "We took two approaches to address this issue.",
        "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
        "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
        "Each Kf contains 50 evenly spaced values between 0 and 1.",
        "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
        "The second method, SVMacc4, normalizes the scores given by f for each query.",
        "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
        "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
        "Each Kf contains 50 evenly spaced values between 0 and 1.",
        "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
        "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
        "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
        "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
        "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
        "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
        "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
        "It provides a principled approach and avoids difficult to control heuristics.",
        "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
        "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
        "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
        "The computational cost for training is very reasonable in practice.",
        "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
        "The learning framework used by our method is fairly general.",
        "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
        "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
        "Research.",
        "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
        "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
        "Automatic combination of multiple ranked retrieval systems.",
        "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
        "Learning to rank using gradient descent.",
        "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
        "Le.",
        "Learning to rank with non-smooth cost functions.",
        "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
        "Liu, H. Li, Y. Huang, and H.-W. Hon.",
        "Adapting ranking SVM to document retrieval.",
        "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
        "Learning a ranking from pairwise preferences.",
        "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
        "Ensemble selection from libraries of models.",
        "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
        "The relationship between precision-recall and ROC curves.",
        "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
        "Overview of the TREC-9 web track.",
        "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
        "Overview of the TREC-2001 web track.",
        "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
        "Large margin rank boundaries for ordinal regression.",
        "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
        "Optimising area under the ROC curve using gradient descent.",
        "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
        "Ir evaluation methods for retrieving highly relevant documents.",
        "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
        "A support vector method for multivariate performance measures.",
        "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
        "ACM Press. [14] J. Lafferty and C. Zhai.",
        "Document language models, query models, and risk minimization for information retrieval.",
        "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
        "Support vector machines for classification in nonstandard situations.",
        "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
        "A markov random field model for term dependencies.",
        "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
        "Combining statistical learning with a knowledge-based approach.",
        "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
        "The probability ranking principle in ir. journal of documentation.",
        "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
        "Large margin methods for structured and interdependent output variables.",
        "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
        "Statistical Learning Theory.",
        "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
        "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
        "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
    ],
    "error_count": 0,
    "keys": {
        "machine learning": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT <br>machine learning</br> is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use <br>machine learning</br> techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard <br>machine learning</br> setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on <br>machine learning</br> (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on <br>machine learning</br> (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on <br>machine learning</br> (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on <br>machine learning</br> (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on <br>machine learning</br> (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "<br>machine learning</br>, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on <br>machine learning</br>, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of <br>machine learning</br> Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on <br>machine learning</br> (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un método de vector de soporte para optimizar la precisión promedio de la Universidad de Yisong Yue Cornell Ithaca, NY, EE. UU. Yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, EE. UU.@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, EE. UU. TJ@cs.cornell.edu Resumen \"Aprendizaje automático\" se usa comúnmente para mejorar los sistemas de recuperación de clasificación.aprendizaje automático",
                "Introducción Los sistemas de recuperación de información de última generación utilizan comúnmente técnicas de \"aprendizaje automático\" para aprender funciones de clasificación.aprendizaje automático",
                "El problema de aprendizaje que sigue a la configuración estándar de \"aprendizaje automático\", nuestro objetivo es aprender una función h: x → y entre un espacio de entrada x (todas las consultas posibles) y el espacio de salida Y (clasificaciones sobre un corpus).aprendizaje automático",
                "En Actas de la Conferencia Internacional sobre \"Aprendizaje automático\" (ICML), 2005. [3] C. J. C. Burges, R. Ragno y Q. Aprendizaje automático",
                "En Actas de la Conferencia Internacional sobre \"Aprendizaje automático\" (ICML), 2004. [7] J. Davis y M. Goadrich.aprendizaje automático",
                "En Actas de la Conferencia Internacional sobre \"Aprendizaje automático\" (ICML), 2006. [8] D. Hawking.aprendizaje automático",
                "En Actas de la Conferencia Internacional sobre \"Aprendizaje automático\" (ICML), 2004. [12] K. Jarvelin y J. Kekalainen.aprendizaje automático",
                "En Actas de la Conferencia Internacional sobre \"Aprendizaje automático\" (ICML), páginas 377-384, Nueva York, NY, EE. UU., 2005. Aprendizaje automático",
                "\"Aprendizaje automático\", 46: 191-202, 2002. [16] D. Metzler y W. B. Croft.aprendizaje automático",
                "En Actas de la Conferencia Internacional sobre \"Aprendizaje automático\", 1999. [18] S. Robertson.aprendizaje automático"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "ranked retrieval system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve <br>ranked retrieval system</br>s.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple <br>ranked retrieval system</br>s.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un método de vector de soporte para optimizar la precisión promedio de la Universidad de Yisong Yue Cornell Ithaca, NY, EE. UU. Yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, EE. UU.@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, EE. UU. TJ@cs.cornell.edu Abstract Machine Learning se usa comúnmente para mejorar el \"sistema de recuperación clasificado\".Sistema de recuperación de clasificación",
                "Combinación automática de múltiples \"Sistema de recuperación de clasificación\" s.Sistema de recuperación de clasificación"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "learning technique": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few <br>learning technique</br>s have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine <br>learning technique</br>s to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Debido a las dificultades computacionales, se han desarrollado pocas \"técnicas de aprendizaje\" para optimizar directamente la precisión promedio media (MAP), a pesar de su uso generalizado en la evaluación de dichos sistemas.técnica de aprendizaje",
                "Introducción Los sistemas de recuperación de información de vanguardia comúnmente utilizan la máquina \"técnica de aprendizaje\" para aprender funciones de clasificación.técnica de aprendizaje"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "mean average precision": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for <br>mean average precision</br> (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely <br>mean average precision</br> (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Debido a las dificultades computacionales, se han desarrollado pocas técnicas de aprendizaje para optimizar directamente la \"precisión promedio media\" (MAP), a pesar de su uso generalizado en la evaluación de dichos sistemas.Precisión promedio media",
                "Sin embargo, la mayoría de los enfoques actuales no optimizan para la medida de evaluación con mayor frecuencia utilizada, es decir, \"precisión promedio media\" (MAP).Precisión promedio media"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "optimal solution": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally <br>optimal solution</br>, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally <br>optimal solution</br> to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally <br>optimal solution</br>.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the <br>optimal solution</br> of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Los enfoques existentes de optimización del mapa no encuentran una \"solución óptima\" globalmente o son computacionalmente costosas.solucion optima",
                "Por el contrario, presentamos un algoritmo general de aprendizaje SVM que encuentra eficientemente una \"solución óptima\" global a una relajación directa del mapa.solucion optima",
                "En contraste con el trabajo reciente, optimización directamente para el rendimiento del mapa de Metzler & Croft [16] y Caruana et al.[6], nuestra técnica es computacionalmente eficiente al tiempo que encuentra una \"solución óptima\" global.solucion optima",
                "Sea ξ ∗ (w) la \"solución óptima\" de las variables flojas para OP 1 para un vector de peso dado w.Entonces 1 n pn i = 1 ξi es un límite superior en el riesgo empírico r∆ s (w).(Ver [19] para la prueba) La propuesta 1 muestra que OP 1 aprende una función de clasificación que optimiza un límite superior en el error del mapa en el conjunto de entrenamiento.solucion optima"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "relaxation of map": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward <br>relaxation of map</br>.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss <br>relaxation of map</br>.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss <br>relaxation of map</br> loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En contraste, presentamos un algoritmo general de aprendizaje SVM que encuentra eficientemente una solución globalmente óptima para una \"relajación del mapa\" directa.Relajación del mapa",
                "Específicamente, presentamos un algoritmo SVM que a nivel mundial optimiza una \"relajación del mapa\" a nivel mundial.Relajación del mapa",
                "Problema de optimización 1. (SVM estructural) Min W, ξ≥0 1 2 W 2 + C N Nx I = 1 ξi (3) S.T.∀i, ∀y ∈ Y \\ yi: wt ψ (xi, yi) ≥ wt ψ (xi, y) + ∆ (yi, y) - ξi (4) La función objetivo que se minimiza (3) es una compensación entreComplejidad del modelo, W 2, y una pérdida de \"Relajación del mapa\" de una pérdida de bisagra, p ξi.Relajación del mapa"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "map relaxation": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "information retrieval system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art <br>information retrieval system</br>s commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Introducción de estado del arte \"Sistema de recuperación de información\" S utiliza comúnmente técnicas de aprendizaje automático para aprender funciones de clasificación.sistema de recuperación de información"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "probability": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the <br>probability</br> of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The <br>probability</br> ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El primer enfoque es aprender un modelo que estima que la \"probabilidad\" de un documento es relevante dada una consulta (por ejemplo, [18, 14]).probabilidad",
                "El principio de clasificación de \"probabilidad\" en IR.Revista de documentación.probabilidad"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "surrogate measure": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a <br>surrogate measure</br>.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "El segundo enfoque común es aprender una función que maximice una \"medida sustituta\".medida sustituta"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "loss function": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a <br>loss function</br> ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The <br>loss function</br> allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Para cuantificar la calidad de una predicción, ˆy = h (x), consideraremos una \"función de pérdida\" ∆: y × y →.∆ (y, ˆy) cuantifica la penalización por hacer una predicción ˆy si la salida correcta es y.función de pérdida",
                "La \"función de pérdida\" nos permite incorporar medidas de rendimiento específicas, que explotaremos 1 http://svmrank.yisongyue.com para optimizar el mapa.función de pérdida"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "supervised learning": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the <br>supervised learning</br> scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Nos restringimos al escenario de \"aprendizaje supervisado\", donde los pares de entrada/salida (x, y) están disponibles para el entrenamiento y se supone que provienen de alguna distribución fija P (x, y).aprendizaje supervisado"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "machine learn for information retrieval": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "support vector machine": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = rank(y) and ˆp = rank(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one rank.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-rank the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed rank test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal Rank. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to rank using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to rank with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin rank boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "rank": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Support Vector Method for Optimizing Average Precision Yisong Yue Cornell University Ithaca, NY, USA yyue@cs.cornell.edu Thomas Finley Cornell University Ithaca, NY, USA tomf@cs.cornell.edu Filip Radlinski Cornell University Ithaca, NY, USA filip@cs.cornell.edu Thorsten Joachims Cornell University Ithaca, NY, USA tj@cs.cornell.edu ABSTRACT Machine learning is commonly used to improve ranked retrieval systems.",
                "Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems.",
                "Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive.",
                "In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP.",
                "We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea.",
                "In most cases we show our method to produce statistically significant improvements in MAP scores.",
                "Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Retrieval Models General Terms Algorithm, Theory, Experimentation 1.",
                "INTRODUCTION State of the art information retrieval systems commonly use machine learning techniques to learn ranking functions.",
                "However, most current approaches do not optimize for the evaluation measure most often used, namely Mean Average Precision (MAP).",
                "Instead, current algorithms tend to take one of two general approaches.",
                "The first approach is to learn a model that estimates the probability of a document being relevant given a query (e.g., [18, 14]).",
                "If solved effectively, the ranking with best MAP performance can easily be derived from the probabilities of relevance.",
                "However, achieving high MAP only requires finding a good ordering of the documents.",
                "As a result, finding good probabilities requires solving a more difficult problem than necessary, likely requiring more training data to achieve the same MAP performance.",
                "The second common approach is to learn a function that maximizes a surrogate measure.",
                "Performance measures optimized include accuracy [17, 15], ROCArea [1, 5, 10, 11, 13, 21] or modifications of ROCArea [4], and NDCG [2, 3].",
                "Learning a model to optimize for such measures might result in suboptimal MAP performance.",
                "In fact, although some previous systems have obtained good MAP performance, it is known that neither achieving optimal accuracy nor ROCArea can guarantee optimal MAP performance[7].",
                "In this paper, we present a general approach for learning ranking functions that maximize MAP performance.",
                "Specifically, we present an SVM algorithm that globally optimizes a hinge-loss relaxation of MAP.",
                "This approach simplifies the process of obtaining ranking functions with high MAP performance by avoiding additional intermediate steps and heuristics.",
                "The new algorithm also makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for accuracy and ROCArea.",
                "In contrast to recent work directly optimizing for MAP performance by Metzler & Croft [16] and Caruana et al. [6], our technique is computationally efficient while finding a globally optimal solution.",
                "Like [6, 16], our method learns a linear model, but is much more efficient in practice and, unlike [16], can handle many thousands of features.",
                "We now describe the algorithm in detail and provide proof of correctness.",
                "Following this, we provide an analysis of running time.",
                "We finish with empirical results from experiments on the TREC 9 and TREC 10 Web Track corpus.",
                "We have also developed a software package implementing our algorithm that is available for public use1 . 2.",
                "THE LEARNING PROBLEM Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).",
                "In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → . ∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.",
                "The loss function allows us to incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.",
                "We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are available for training and are assumed to come from some fixed distribution P(x, y).",
                "The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.",
                "Of course, P(x, y) is unknown.",
                "But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, . . . , n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).",
                "In the case of learning a ranked retrieval function, X denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, . . . ,d|C|}.",
                "We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(<br>rank</br>(y), <br>rank</br>(ˆy)), where rank(y) is a vector of the rank values of each document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).",
                "We assume true rankings have two <br>rank</br> values, where relevant documents have <br>rank</br> value 1 and non-relevant documents rank value 0.",
                "We further assume that all predicted rankings are complete rankings (no ties).",
                "Let p = <br>rank</br>(y) and ˆp = <br>rank</br>(ˆy).",
                "The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.",
                "MAP is the mean of the average precision scores of a group of queries. 2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or ROCArea.",
                "While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.",
                "ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.",
                "In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.",
                "Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 <br>rank</br>(h1(x)) 8 7 6 5 4 3 2 1 <br>rank</br>(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two hypothesis functions, h1 and h2, as shown in Table 1.",
                "These two hypotheses predict a ranking for query x over a corpus of eight documents.",
                "Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.",
                "Here, a learning method which optimizes for ROCArea would choose h2 since that results in a higher ROCArea score, but this yields a suboptimal MAP score. 2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.",
                "Models which optimize for accuracy are not directly concerned with the ranking.",
                "Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as nonrelevant.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 <br>rank</br>(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 <br>rank</br>(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two hypotheses.",
                "Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).",
                "The best accuracy refers to the highest achievable accuracy on that ranking when considering all possible thresholds.",
                "For instance, with h1(q), a threshold between documents 1 and 2 gives 4 errors (documents 6-9 incorrectly classified as non-relevant), yielding an accuracy of 0.64.",
                "Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.",
                "A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 3.",
                "OPTIMIZING AVERAGE PRECISION We build upon the approach used by [13] for optimizing ROCArea.",
                "Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a substantially extended algorithm, which we describe in this section.",
                "Recall that the true ranking is a weak ranking with two <br>rank</br> values (relevant and non-relevant).",
                "Let Cx and C¯x denote the set of relevant and non-relevant documents of C for query x, respectively.",
                "We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).",
                "Our approach is to learn a discriminant function F : X × Y → over input-output pairs.",
                "Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w). (1) We assume F to be linear in some combined feature representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y). (2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .",
                "We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .",
                "For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal <br>rank</br>.",
                "We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).",
                "Intuitively, Ψ is a summation over the vector differences of all relevant/non-relevant document pairings.",
                "Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).",
                "Given a learned weight vector w, predicting a ranking (i.e. solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).",
                "As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.",
                "We will discuss later the choices of φ we used for our experiments. 3.1 Structural SVMs The above formulation is very similar to learning a straightforward linear model while training on the pairwise difference of relevant/non-relevant document pairings.",
                "Many SVM-based approaches optimize over these pairwise differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.",
                "Previously, it was not clear how to incorporate non-linear multivariate loss functions such as MAP loss directly into global optimization problems such as SVM training.",
                "We now present a method based on structural SVMs [19] to address this problem.",
                "We use the structural SVM formulation, presented in Optimization Problem 1, to learn a w ∈ RN .",
                "Optimization Problem 1. (Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t. ∀i, ∀y ∈ Y \\ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.",
                "As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.",
                "Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance . 1: Input: (x1, y1), . . . , (xn, yn), C, 2: Wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.",
                "For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.",
                "Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).",
                "During prediction, our model chooses the ranking which maximizes the discriminant (1).",
                "If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.",
                "Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.",
                "This is stated formally in Proposition 1.",
                "Proposition 1.",
                "Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w). (see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the training set.",
                "Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.",
                "Algorithm 1 is a cutting plane algorithm, iteratively introducing constraints until we have solved the original problem within a desired tolerance [19].",
                "The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.",
                "If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .",
                "Theorem 1.",
                "Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.",
                "Though closely related to the classification procedure, this has the substantial complication that we must contend with the additional ∆(yi, y) term.",
                "Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable. 3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].",
                "Solving argmax H for ∆map is more difficult.",
                "This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative ordering of a relevant/non-relevant document pair.",
                "MAP, on the other hand, does not decompose in the same way as ROCArea.",
                "The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.",
                "One useful property of ∆map is that it is invariant to swapping two documents with equal relevance.",
                "For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.",
                "By extension, ∆map is invariant to any arbitrary permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.",
                "However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).",
                "This leads us to Observation 1.",
                "Observation 1.",
                "Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).",
                "Every ranking which satisfies the same set of constraints will have the same ∆map.",
                "If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant documents are likewise sorted by wT φ(x, d), then the interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.",
                "Observation 1 implies that in the ranking which maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.",
                "By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.",
                "For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).",
                "For convenience, we also refer to relevant documents as {dx 1 , . . . dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , . . . d¯x |C¯x|} = C¯x .",
                "We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .",
                "For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).",
                "The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.",
                "The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . .",
                "Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).",
                "The bottom ranking differs from the top only where d¯x j slides up one <br>rank</br>.",
                "The difference in the value of H for these two rankings is exactly δj(i, i + 1).",
                "For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j ) .",
                "Let o1, . . . , o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant document ranked after the jth non-relevant document.",
                "Due to Observation 1, this encoding uniquely identifies a complete ranking.",
                "We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) We can now reformulate H into a new objective function, H (o1, . . . , o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.",
                "Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.",
                "We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|C¯x|. (9) Algorithm 2 describes the algorithm used to solve equation (8).",
                "Conceptually, Algorithm 2 starts with a perfect ranking.",
                "Then for each successive non-relevant document, the algorithm modifies the solution by sliding that document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant. 3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.",
                "In other words, the algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, . . . , |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, . . . , |C¯x | 5: for j = 1, . . . , |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).",
                "In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ . . . ≤ opt|C¯x|. (10) If the solution is feasible, the it clearly solves (8).",
                "Therefore, it suffices to prove that Algorithm 2 satisfies (10).",
                "We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.",
                "For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).",
                "Proof.",
                "Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.",
                "We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1.",
                "Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).",
                "We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).",
                "In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .",
                "Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.",
                "The result of Lemma 1 leads directly to our main correctness result: Theorem 2.",
                "In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.",
                "Proof.",
                "We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).",
                "Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.",
                "Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.",
                "Suppose for contradiction that optj+1 < optj.",
                "Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).",
                "Therefore, it must be the case that optj ≤ optj+1, which completes the proof. 3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.",
                "The first part is the sort by wT φ(x, d), which requires O(n log n) time, where n = |Cx | + |C¯x |.",
                "The second part computes each optj, which requires O(|Cx | · |C¯x |) time.",
                "Though in the worst case this is O(n2 ), the number of relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).",
                "For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).",
                "Algorithm 1 is guaranteed to halt in a polynomial number of iterations [19], and each iteration runs Algorithm 2.",
                "Virtually all well-performing models were trained in a reasonable amount of time (usually less than one hour).",
                "Once training is complete, making predictions on query x using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).",
                "We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.",
                "To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 4.",
                "EXPERIMENT SETUP The main goal of our experiments is to evaluate whether directly optimizing MAP leads to improved MAP performance compared to conventional SVM methods that optimize a substitute loss such as accuracy or ROCArea.",
                "We empirically evaluate our method using two sets of TREC Web Track queries, one each from TREC 9 and TREC 10 (topics 451-500 and 501-550), both of which used the WT10g corpus.",
                "For each query, TREC provides the relevance judgments of the documents.",
                "We generated our features using the scores of existing retrieval functions on these queries.",
                "While our method is agnostic to the meaning of the features, we chose to use existing retrieval functions as a simple yet effective way of acquiring useful features.",
                "As such, our 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html Dataset Base Funcs Features TREC 9 Indri 15 750 TREC 10 Indri 15 750 TREC 9 Submissions 53 2650 TREC 10 Submissions 18 900 Table 5: Dataset Statistics experiments essentially test our methods ability to re-<br>rank</br> the highly ranked documents (e.g., re-combine the scores of the retrieval functions) to improve MAP.",
                "We compare our method against the best retrieval functions trained on (henceforth base functions), as well as against previously proposed SVM methods.",
                "Comparing with the best base functions tests our methods ability to learn a useful combination.",
                "Comparing with previous SVM methods allows us to test whether optimizing directly for MAP (as opposed to accuracy or ROCArea) achieves a higher MAP score in practice.",
                "The rest of this section describes the base functions and the feature generation method in detail. 4.1 Choosing Retrieval Functions We chose two sets of base functions for our experiments.",
                "For the first set, we generated three indices over the WT10g corpus using Indri5 .",
                "The first index was generated using default settings, the second used Porter-stemming, and the last used Porter-stemming and Indris default stopwords.",
                "For both TREC 9 and TREC 10, we used the description portion of each query and scored the documents using five of Indris built-in retrieval methods, which are Cosine Similarity, TFIDF, Okapi, Language Model with Dirichlet Prior, and Language Model with Jelinek-Mercer Prior.",
                "All parameters were kept as their defaults.",
                "We computed the scores of these five retrieval methods over the three indices, giving 15 base functions in total.",
                "For each query, we considered the scores of documents found in the union of the top 1000 documents of each base function.",
                "For our second set of base functions, we used scores from the TREC 9 [8] and TREC 10 [9] Web Track submissions.",
                "We used only the non-manual, non-short submissions from both years.",
                "For TREC 9 and TREC 10, there were 53 and 18 such submissions, respectively.",
                "A typical submission contained scores of its top 1000 documents. b ca wT φ(x,d) f(d|x) Figure 2: Example Feature Binning 4.2 Generating Features In order to generate input examples for our method, a concrete instantiation of φ must be provided.",
                "For each doc5 http://www.lemurproject.org TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236Best Func. 0.204 39/11 ** 0.181 37/13 ** 2nd Best 0.199 38/12 ** 0.174 43/7 ** 3rd Best 0.188 34/16 ** 0.174 38/12 ** Table 6: Comparison with Indri Functions ument d scored by a set of retrieval functions F on query x, we generate the features as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ F, ∀k ∈ Kf , where f(d|x) denotes the score that retrieval function f assigns to document d for query x, and each Kf is a set of real values.",
                "From a high level, we are expressing the score of each retrieval function using |Kf | + 1 bins.",
                "Since we are using linear kernels, one can think of the learning problem as finding a good piecewise-constant combination of the scores of the retrieval functions.",
                "Figure 2 shows an example of our feature mapping method.",
                "In this example we have a single feature F = {f}.",
                "Here, Kf = {a, b, c}, and the weight vector is w = wa, wb, wc .",
                "For any document d and query x, we have wT φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) .",
                "This is expressed qualitatively in Figure 2, where wa and wb are positive, and wc is negative.",
                "We ran our main experiments using four choices of F: the set of aforementioned Indri retrieval functions for TREC 9 and TREC 10, and the Web Track submissions for TREC 9 and TREC 10.",
                "For each F and each function f ∈ F, we chose 50 values for Kf which are reasonably spaced and capture the sensitive region of f. Using the four choices of F, we generated four datasets for our main experiments.",
                "Table 5 contains statistics of the generated datasets.",
                "There are many ways to generate features, and we are not advocating our method over others.",
                "This was simply an efficient means to normalize the outputs of different functions and allow for a more expressive model. 5.",
                "EXPERIMENTS For each dataset in Table 5, we performed 50 trials.",
                "For each trial, we train on 10 randomly selected queries, and select another 5 queries at random for a validation set.",
                "Models were trained using a wide range of C values.",
                "The model which performed best on the validation set was selected and tested on the remaining 35 queries.",
                "All queries were selected to be in the training, validation and test sets the same number of times.",
                "Using this setup, we performed the same experiments while using our method (SVM∆ map), an SVM optimizing for ROCArea (SVM∆ roc) [13], and a conventional classification SVM (SVMacc) [20].",
                "All SVM methods used a linear kernel.",
                "We reported the average performance of all models over the 50 trials. 5.1 Comparison with Base Functions In analyzing our results, the first question to answer is, can SVM∆ map learn a model which outperforms the best base TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287Best Func. 0.280 28/22 0.283 29/21 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 36/14 ** Table 7: Comparison with TREC Submissions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288Best Func. 0.280 27/23 0.283 31/19 2nd Best 0.269 30/20 0.251 36/14 ** 3rd Best 0.266 30/20 0.233 35/15 ** Table 8: Comparison with TREC Subm. (w/o best) functions?",
                "Table 6 presents the comparison of SVM∆ map with the best Indri base functions.",
                "Each column group contains the macro-averaged MAP performance of SVM∆ map or a base function.",
                "The W/L columns show the number of queries where SVM∆ map achieved a higher MAP score.",
                "Significance tests were performed using the two-tailed Wilcoxon signed <br>rank</br> test.",
                "Two stars indicate a significance level of 0.95.",
                "All tables displaying our experimental results are structured identically.",
                "Here, we find that SVM∆ map significantly outperforms the best base functions.",
                "Table 7 shows the comparison when trained on TREC submissions.",
                "While achieving a higher MAP score than the best base functions, the performance difference between SVM∆ map the base functions is not significant.",
                "Given that many of these submissions use scoring functions which are carefully crafted to achieve high MAP, it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions.",
                "As a result, SVM∆ map would not be able to learn a hypothesis which can significantly out-perform the best submission.",
                "Hence, we ran the same experiments using a modified dataset where the features computed using the best submission were removed.",
                "Table 8 shows the results (note that we are still comparing against the best submission though we are not using it for training).",
                "Notice that while the performance of SVM∆ map degraded slightly, the performance was still comparable with that of the best submission. 5.2 Comparison w/ Previous SVM Methods The next question to answer is, does SVM∆ map produce higher MAP scores than previous SVM methods?",
                "Tables 9 and 10 present the results of SVM∆ map, SVM∆ roc, and SVMacc when trained on the Indri retrieval functions and TREC submissions, respectively.",
                "Table 11 contains the corresponding results when trained on the TREC submissions without the best submission.",
                "To start with, our results indicate that SVMacc was not competitive with SVM∆ map and SVM∆ roc, and at times underperformed dramatically.",
                "As such, we tried several approaches to improve the performance of SVMacc. 5.2.1 Alternate SVMacc Methods One issue which may cause SVMacc to underperform is the severe imbalance between relevant and non-relevant docTREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.242 - 0.236SVM∆ roc 0.237 29/21 0.234 24/26 SVMacc 0.147 47/3 ** 0.155 47/3 ** SVMacc2 0.219 39/11 ** 0.207 43/7 ** SVMacc3 0.113 49/1 ** 0.153 45/5 ** SVMacc4 0.155 48/2 ** 0.155 48/2 ** Table 9: Trained on Indri Functions TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.290 - 0.287SVM∆ roc 0.282 29/21 0.278 35/15 ** SVMacc 0.213 49/1 ** 0.222 49/1 ** SVMacc2 0.270 34/16 ** 0.261 42/8 ** SVMacc3 0.133 50/0 ** 0.182 46/4 ** SVMacc4 0.233 47/3 ** 0.238 46/4 ** Table 10: Trained on TREC Submissions uments.",
                "The vast majority of the documents are not relevant.",
                "SVMacc2 addresses this problem by assigning more penalty to false negative errors.",
                "For each dataset, the ratio of the false negative to false positive penalties is equal to the ratio of the number non-relevant and relevant documents in that dataset.",
                "Tables 9, 10 and 11 indicate that SVMacc2 still performs significantly worse than SVM∆ map.",
                "Another possible issue is that SVMacc attempts to find just one discriminating threshold b that is query-invariant.",
                "It may be that different queries require different values of b.",
                "Having the learning method trying to find a good b value (when one does not exist) may be detrimental.",
                "We took two approaches to address this issue.",
                "The first method, SVMacc3, converts the retrieval function scores into percentiles.",
                "For example, for document d, query q and retrieval function f, if the score f(d|q) is in the top 90% of the scores f(·|q) for query q, then the converted score is f (d|q) = 0.9.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Tables 9, 10 and 11 show that the performance of SVMacc3 was also not competitive with SVM∆ map.",
                "The second method, SVMacc4, normalizes the scores given by f for each query.",
                "For example, assume for query q that f outputs scores in the range 0.2 to 0.7.",
                "Then for document d, if f(d|q) = 0.6, the converted score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8.",
                "Each Kf contains 50 evenly spaced values between 0 and 1.",
                "Again, Tables 9, 10 and 11 show that SVMacc4 was not competitive with SVM∆ map 5.2.2 MAP vs ROCArea SVM∆ roc performed much better than SVMacc in our experiments.",
                "When trained on Indri retrieval functions (see Table 9), the performance of SVM∆ roc was slight, though not significantly, worse than the performances of SVM∆ map.",
                "However, Table 10 shows that SVM∆ map did significantly outperform SVM∆ roc when trained on the TREC submissions.",
                "Table 11 shows the performance of the models when trained on the TREC submissions with the best submission removed.",
                "The performance of most models degraded by a small amount, with SVM∆ map still having the best performance.",
                "TREC 9 TREC 10 Model MAP W/L MAP W/L SVM∆ map 0.284 - 0.288SVM∆ roc 0.274 31/19 ** 0.272 38/12 ** SVMacc 0.215 49/1 ** 0.211 50/0 ** SVMacc2 0.267 35/15 ** 0.258 44/6 ** SVMacc3 0.133 50/0 ** 0.174 46/4 ** SVMacc4 0.228 46/4 ** 0.234 45/5 ** Table 11: Trained on TREC Subm. (w/o Best) 6.",
                "CONCLUSIONS AND FUTURE WORK We have presented an SVM method that directly optimizes MAP.",
                "It provides a principled approach and avoids difficult to control heuristics.",
                "We formulated the optimization problem and presented an algorithm which provably finds the solution in polynomial time.",
                "We have shown empirically that our method is generally superior to or competitive with conventional SVMs methods.",
                "Our new method makes it conceptually just as easy to optimize SVMs for MAP as was previously possible only for Accuracy and ROCArea.",
                "The computational cost for training is very reasonable in practice.",
                "Since other methods typically require tuning multiple heuristics, we also expect to train fewer models before finding one which achieves good performance.",
                "The learning framework used by our method is fairly general.",
                "A natural extension of this framework would be to develop methods to optimize for other important IR measures, such as Normalized Discounted Cumulative Gain [2, 3, 4, 12] and Mean Reciprocal <br>rank</br>. 7.",
                "ACKNOWLEDGMENTS This work was funded under NSF Award IIS-0412894, NSF CAREER Award 0237381, and a gift from Yahoo!",
                "Research.",
                "The third author was also partly supported by a Microsoft Research Fellowship. 8.",
                "REFERENCES [1] B. T. Bartell, G. W. Cottrell, and R. K. Belew.",
                "Automatic combination of multiple ranked retrieval systems.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 1994. [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.",
                "Learning to <br>rank</br> using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2005. [3] C. J. C. Burges, R. Ragno, and Q.",
                "Le.",
                "Learning to <br>rank</br> with non-smooth cost functions.",
                "In Proceedings of the International Conference on Advances in Neural Information Processing Systems (NIPS), 2006. [4] Y. Cao, J. Xu, T.-Y.",
                "Liu, H. Li, Y. Huang, and H.-W. Hon.",
                "Adapting ranking SVM to document retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [5] B. Carterette and D. Petkova.",
                "Learning a ranking from pairwise preferences.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2006. [6] R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes.",
                "Ensemble selection from libraries of models.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [7] J. Davis and M. Goadrich.",
                "The relationship between precision-recall and ROC curves.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2006. [8] D. Hawking.",
                "Overview of the TREC-9 web track.",
                "In Proceedings of TREC-2000, 2000. [9] D. Hawking and N. Craswell.",
                "Overview of the TREC-2001 web track.",
                "In Proceedings of TREC-2001, Nov. 2001. [10] R. Herbrich, T. Graepel, and K. Obermayer.",
                "Large margin <br>rank</br> boundaries for ordinal regression.",
                "Advances in large margin classifiers, 2000. [11] A. Herschtal and B. Raskutti.",
                "Optimising area under the ROC curve using gradient descent.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2004. [12] K. Jarvelin and J. Kekalainen.",
                "Ir evaluation methods for retrieving highly relevant documents.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), 2000. [13] T. Joachims.",
                "A support vector method for multivariate performance measures.",
                "In Proceedings of the International Conference on Machine Learning (ICML), pages 377-384, New York, NY, USA, 2005.",
                "ACM Press. [14] J. Lafferty and C. Zhai.",
                "Document language models, query models, and risk minimization for information retrieval.",
                "In Proceedings of the ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 111-119, 2001. [15] Y. Lin, Y. Lee, and G. Wahba.",
                "Support vector machines for classification in nonstandard situations.",
                "Machine Learning, 46:191-202, 2002. [16] D. Metzler and W. B. Croft.",
                "A markov random field model for term dependencies.",
                "In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 472-479, 2005. [17] K. Morik, P. Brockhausen, and T. Joachims.",
                "Combining statistical learning with a knowledge-based approach.",
                "In Proceedings of the International Conference on Machine Learning, 1999. [18] S. Robertson.",
                "The probability ranking principle in ir. journal of documentation.",
                "Journal of Documentation, 33(4):294-304, 1977. [19] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.",
                "Large margin methods for structured and interdependent output variables.",
                "Journal of Machine Learning Research (JMLR), 6(Sep):1453-1484, 2005. [20] V. Vapnik.",
                "Statistical Learning Theory.",
                "Wiley and Sons Inc., 1998. [21] L. Yan, R. Dodier, M. Mozer, and R. Wolniewicz.",
                "Optimizing classifier performance via approximation to the Wilcoxon-Mann-Witney statistic.",
                "In Proceedings of the International Conference on Machine Learning (ICML), 2003."
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Podemos definir la pérdida de precisión promedio como ∆map (y, ˆy) = 1 - mapa (\"rango\" (y), \"rango\" (ˆy)), donde el rango (y) es un vector de los valores de rango de cada documento enC. Por ejemplo, para un corpus de dos documentos, {d1, d2}, con d1 tener un rango más alto que d2, rango (y) = (1, 0).rango",
                "Suponemos que las clasificaciones verdaderas tienen dos valores de \"rango\", donde los documentos relevantes tienen un valor de \"rango\" y los documentos no relevantes valoran 0. rango",
                "Sea p = \"rango\" (y) y ˆp = \"rango\" (ˆy).rango",
                "Usando nuestra notación, Rocarea se puede definir como roc (p, ˆp) = 1 rel · (| c | - rel) x i: pi = 1 x j: pj = 0 1 [ˆpi> ˆpj], donde p es el verdadero(débil) Ranking, ˆp es la clasificación predicha, y 1 [b] es la función indicadora condicionada en b.Doc ID 1 2 3 4 5 6 7 8 P 1 0 0 0 0 1 1 0 \"Rango\" (H1 (x)) 8 7 6 5 4 3 2 1 \"Rango\" (H2 (x)) 1 2 3 4 56 7 8 Tabla 1: Ejemplo y modelos de juguete Suponga que tenemos un espacio de hipótesis con solo dos funciones de hipótesis, H1 y H2, como se muestra en la Tabla 1.",
                "Doc ID 1 2 3 4 5 6 7 8 9 10 11 P 1 0 0 0 0 0 1 1 1 0 0 \"Rango\" (H1 (x)) 11 10 9 8 7 6 5 4 3 2 1 \"Rango\" (H2(x)) 1 2 3 4 5 6 7 8 9 10 11 Tabla 3: Ejemplo de juguete y modelos Consideramos nuevamente un espacio de hipótesis con dos hipótesis.rango",
                "Recuerde que la clasificación verdadera es una clasificación débil con dos valores de \"rango\" (relevantes y no relevantes).rango",
                "Para cualquier y ∈ Y, yij = +1 si DI se clasifica por delante de DJ, y yij = −1 si DJ se clasifica por delante de DI, y yij = 0 si DI y DJ tienen igual \"rango\".rango",
                "La clasificación inferior difiere desde la parte superior solo donde D¯x J se desliza hacia arriba un \"rango\".rango",
                "Como tal, nuestro 3 http://www.cs.cornell.edu/~tomf/svmpython/ 4 http://svmlight.joachims.org/svm_struct.html DataSet base base características Trec 9 Indri 15 750 Trec 10 Indri 15 750TREC 9 SUMISIONES 53 2650 TREC 10 SUMENTOS 18 900 Tabla 5: Experimentos de estadísticas del conjunto de datos esencialmente prueba nuestros métodos la capacidad de volver a \"clasificar\" los documentos altamente clasificados (por ejemplo, volver a combinar las puntuaciones de las funciones de recuperación) para mejorar el mapa.rango",
                "Las pruebas de significancia se realizaron utilizando la prueba \"Rank\" firmada de Wilcoxon de dos colas.rango"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}