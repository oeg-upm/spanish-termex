{
    "id": "I-71",
    "original_text": "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems. Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources. In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in. It hence makes the situation in which the alignment occurs explicit in the model. We resort to Channel Theory to carry out the formalisation. Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems. General Terms Theory 1. INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain. It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers. As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web. But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3]. Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate. This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems. Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time. In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information). Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8]. Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances. We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism. Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain. Imagine a situation in which two agents are facing each other in front of a checker board. Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right. Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board. Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment. The same agents situated differently may produce a different alignment. This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12]. This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment. In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation. In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1]. This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information. Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in. We shall therefore consider a scenario with two or more agents situated in an environment. Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state. Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies). We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment. The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment. In Section 2 we describe our formal model for Situated Semantic Alignment (SSA). First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment. Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic. These approximations gradually become more reliable as the method is applied. In Section 3 we report on an application of our method. Conclusions and further work are analyzed in Section 4. Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper. We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2. A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward). We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states. We further assume that each agent is able to observe the environment and has its own perception of it. This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different. According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A). So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system. For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi. Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai. Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities. Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε. The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment. However, the set of types of the power classification includes all possible token configurations potentially described by types. Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε. The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory. The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them. The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}. Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types. We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α. We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types. Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A). The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2. Notice however that these constraints are obvious. As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below. As important as the notion of channel is the concept of distributed logic (see appendix A). Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel. In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion. On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2). On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens). All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic). In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2). Log(E) is the logic of SSA. Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions. Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B). If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B). That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints. This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA. Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely. In this section, we present a method by which agents obtain approximations to Th(Log(E)). We also prove these approximations gradually become more reliable as the method is applied. Agents can obtain approximations to Th(Log(E)) through communication. A1 and A2 communicate by exchanging information about their perceptions of environment states. This information is expressed in terms of their own classification relations. Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not. This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information. Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e . The significant point is that C is a refinement of C (see appendix A). Theorem 2.1 below ensures that the refined channel involves more reliable information. The communication supposedly ends when agents have observed all the environment states. Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}. Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)). Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable. Theorem 2.1. Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels. If C is a refinement of C then: 1. Th(Log(C )) ⊆ Th(Log(C)) 2. NLog(C ) ⊇ NLog(C) Proof. Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi . Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1. Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ]. We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ]. We proceed by reductio ad absurdum. Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] . Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ. Let us choose an arbitrary γ ∈ Γ. We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}. Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)). Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ. Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ). A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] . Hence Γ Log(C) Δ as we wanted to prove. 2. Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C). Therefore, there exists c token in C such that a1, a2 = ˇf(c). Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}. Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ). Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof. Remark 2.1. Theorem 2.1 asserts that the more refined channel gives more reliable information. Even though its theory has less constraints, it has more normal tokens to which they apply. In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2. Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form. We also choose an infinite numerable set of symbols {cn | n ∈ N}1 . We omit informorphisms superscripts when no confusion arises. Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a). Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E). A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ). We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1. So can A2 do. Since both typ(A1) and typ(A2) are finite, this process eventually finishes. After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2). C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication. It is easy to prove that Th(Log(C1 )) = Th(C1 ). The significant point is that both agents know C1 as the result of the communication. Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 . Now, let us assume that E turns to a new state e2 . Agents can proceed as before, exchanging this time information about their perceptions of e2 . Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up. We define C2 so as to take also into account the information provided by the previous stage of communication. On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents. Note this set is chosen with the same cardinality of S. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages. Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 . A1 and A2 knows C2 so they can use these constraints. The key point is that channel C2 is a refinement of C1 . It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3). By Theorem 2.1, C2 constraints are more reliable than C1 constraints. In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment. Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3). Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en . Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable. It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2. At that point, the family of channels {Cn }n∈N would inform of all the communication stages. It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks. But the study of possible termination criteria is outside the scope of this paper and left for future work. From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states. On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper. It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA. Theorem 2.2. The following statements hold: 1. For all n ∈ N, C∗ is a refinement of Cn . 2. Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )). Proof. 1. It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2. The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ????????????????? Cn 1282 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3. AN EXAMPLE In the previous section we have described in great detail our formal model for SSA. However, we have not tackled the practical aspect of the model yet. In this section, we give a brushstroke of the pragmatic view of our approach. We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication. Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4). Robots can carry only one package at a time and they can not move through a package. Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have. Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5). We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively. Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary. We assume the reader has all those descriptions in mind. All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other. In order to communicate, agents send messages using some ontology. In our scenario, there coexist two ontologies, the UDRL and LCR ontologies. Both of them are very simple and are just confined to describe what robots observe. Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance. Let us suppose two URDL robots are in a situation like the one depicted in Figure 6. Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request. This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square. Now, problems arise when a LCR robot and a URDL robot try to interoperate. See Figure 7. Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 . Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}. From these, only the following constraints are plausible according to C1 : The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place. C2 also considers the previous information and hence refines C1 . The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic. Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic. Figure 8: Refinement 4. CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states. Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow. Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process. Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities. In the future, our effort will focus on the practical side of the situated semantic alignment problem. We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact. The formal model exposed in this paper will constitute a solid base of future practical results. Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253. Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5. REFERENCES [1] J. Barwise and J. Seligman. Information Flow: The Logic of Distributed Systems. Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia. Local models semantics, or contextual reasoning = locality + compatibility. Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko. Semantic matching. The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer. IF-Map: An ontology-mapping method based on information-flow theory. In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer. Ontology mapping: The sate of the art. The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent. Semantic integration in the Information Flow Framework. In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat. CyC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta. PowerMap: Mapping the real Semantic Web on the fly. Proceedings of the ISWC06, 2006. [9] F. McNeill. Dynamic Ontology Refinement. PhD 1284 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou. Progressive ontology alignment for meaning coordination: An information-theoretic foundation. In 4th Int. Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat. A survey of schema-based matching approaches. In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels. The Origins of Ontologies and Communication Conventions in Multi-Agent Systems. In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al. ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int. Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A. CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A). If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A). Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) . Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ. A binary relation between subsets of Σ is called a consequence relation on Σ. A theory is a pair T = Σ, where is a consequence relation on Σ. A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1. Identity: α α 2. Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel. A channel can be defined with an arbitrary index set. 3. Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification. A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ. The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ . Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3. NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L). A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L). L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L). Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A). Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }. Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2. Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2. A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr). Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B. CHANNEL THEORY THEOREMS Theorem B.1. The logic generated by a classification is sound and complete. Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A). Theorem B.2. Let L be a logic on a classification B and f : A → B an infomorphism. 1. If L is complete then f−1 [L] is complete. 2. If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular. The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285",
    "original_translation": "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technologyPara lograr la alineación semántica de las entidades ontológicas utilizadas por las aplicaciones basadas en el conocimiento y, por lo tanto, para permitir su interoperabilidad en entornos distribuidos como sistemas multiagentes. Sin embargo, la mayoría de los mecanismos de coincidencia de ontología suponen una integración previa de coincidencia y dependen de la semántica que se ha codificado a priori en jerarquías de conceptos o fuentes externas. En este artículo, presentamos un modelo formal para un procedimiento de alineación semántica que alinea incrementalmente conceptualizaciones de dos o más agentes en relación con su percepción respectiva del entorno o dominio en el que están actuando. Por lo tanto, hace que la situación en la que la alineación ocurre explícita en el modelo. Recurrimos a la teoría del canal para llevar a cabo la formalización. Categorías y descriptores de sujetos I.2.11 [Inteligencia artificial]: coherencia y coordinación de inteligencia artificial distribuida, sistemas multiagentes;D.2.12 [Ingeniería de software]: mapeo de datos de interoperabilidad;I.2.4 [Inteligencia artificial]: Formalismos de representación del conocimiento y redes semánticas de métodos, sistemas de relaciones. Términos generales Teoría 1. Introducción Una ontología se define comúnmente como una especificación de la conceptualización de un dominio particular. Correge el vocabulario utilizado por los ingenieros de conocimiento para denotar conceptos y sus relaciones, y limita la interpretación de este vocabulario al significado originalmente previsto por los ingenieros de conocimiento. Como tal, las ontologías se han adoptado ampliamente como una tecnología clave que puede favorecer el intercambio de conocimientos en entornos distribuidos, como sistemas de agentes múltiples, bases de datos federadas o la web semántica. Pero la proliferación de muchas ontologías diversas causadas por diferentes conceptualizaciones de incluso el mismo dominio, y su especificación posterior utilizando una terminología variable, ha resaltado la necesidad de técnicas de coincidencia ontológica que son capaces de calcular relaciones semánticas entre entidades de ontologías de ingeniería por separado.[5, 11] Hasta hace poco, la mayoría de los mecanismos de coincidencia de ontología desarrollados hasta ahora han adoptado un enfoque funcional clásico para el problema de heterogeneidad semántica, en el que la coincidencia de ontología se ve como un proceso que toma dos o más ontologías como entrada y produce una alineación semántica de ontológica deEntidades como salida [3]. Además, la coincidencia a menudo se ha llevado a cabo en el tiempo de diseño, antes de integrar sistemas basados en el conocimiento o hacerlos interoperar. Esto podría haber sido exitoso para dominios claramente delimitados y estables y para sistemas distribuidos cerrados, pero es insostenible e incluso indeseable para el tipo de aplicaciones que actualmente se implementan en sistemas abiertos. La comunicación de múltiples agentes, el intercambio de información entre pares y la composición del servicio web son una naturaleza descentralizada, dinámica y abierta, y requieren que la coincidencia de ontología se realice localmente durante el tiempo de ejecución. Además, en muchas situaciones, las ontologías pares ni siquiera están abiertas para la inspección (por ejemplo, cuando se basan en información comercialmente confidencial). Ciertamente, existen esfuerzos para igualar eficientemente entidades ontológicas en tiempo de ejecución, tomando solo aquellos fragmentos de ontología que son necesarios para la tarea en cuestión [10, 13, 9, 8]. Sin embargo, las técnicas utilizadas por estos sistemas para establecer las relaciones semánticas entre las entidades ontológicas, aunque se aplican en tiempo de ejecución, todavía explotan las taxonomías conceptuales definidas a priori, ya que están representadas en las estructuras basadas en gráficos de las ontologías para que se coincidan, se usen, se usan, se usan.Fuentes externas previamente existentes, como los tesauros (por ejemplo, WordNet) y las ontologías de nivel superior (por ejemplo, CYC o SUMO), o recurren a repositorios de conocimiento o instancias compartidas adicionales. Afirmamos que la alineación semántica de la terminología ontológica es, en última instancia, en relación con la situación particular en la que se lleva a cabo la alineación, y que esta situación debe ser explícita y llevada al mecanismo de alineación. Incluso dos agentes con capacidades de conceptualización idénticas, y el uso exactamente del mismo vocabulario para especificar sus respectivas conceptualizaciones pueden no hacer interoperar 1278 978-81-904262-7-5 (RPS) c 2007 Ifaamas en una situación concreta debido a su diferente perceptación de losdominio. Imagine una situación en la que dos agentes se enfrentan entre sí frente a una placa de verificación. El agente A1 puede conceptualizar una figura en el tablero tal como está situada en el margen izquierdo del tablero, mientras que el agente A2 puede conceptualizar la misma figura que se encuentra a la derecha. Aunque la conceptualización de la izquierda y la derecha se realiza exactamente de la misma manera por parte de ambos agentes, e incluso si ambos usan los términos izquierdo y derecho en su comunicación, aún necesitarán alinear sus respectivos vocabularios si quieren comunicarse con éxito entre sí.Acciones que cambian la posición de las cifras en la placa del verificador. Sin embargo, su alineación semántica solo será válida en el alcance de su interacción dentro de esta situación o entorno particular. Los mismos agentes situados de manera diferente pueden producir una alineación diferente. Este escenario recuerda a aquellos en los que un grupo de agentes distribuidos se adapta a formar una ontología y un léxico compartido de una manera emergente, de abajo hacia arriba, con solo interacciones locales y sin autoridad de control central [12]. Este tipo de aparición autoorganizada del significado compartido se basa en la interacción física de los agentes con el medio ambiente. En este artículo, sin embargo, abordamos el caso en el que los agentes ya están dotados de una ontología de ingeniería de arriba hacia abajo (incluso puede ser el mismo), que no se adaptan o refinan, sino para la cual quieren encontrar la semántica.Relaciones con ontologías separadas de otros agentes sobre la base de su comunicación dentro de una situación específica. En particular, proporcionamos un modelo formal que formaliza la alineación semántica situada como una secuencia de refinamientos de canales de información en el sentido de la teoría del flujo de información de Barrawis y Seligmans [1]. Esta teoría es particularmente útil para nuestro esfuerzo porque modela el flujo de información que ocurre en los sistemas distribuidos debido a las situaciones particulares, o tokens, que tienen información. Análogamente, la alineación semántica que permitirá que la información fluya en última instancia será llevada a cabo por la situación particular en la que actúan los agentes. Por lo tanto, consideraremos un escenario con dos o más agentes situados en un entorno. Cada agente tendrá su propio punto de vista del entorno para que, si el entorno está en un estado concreto, ambos agentes pueden tener diferentes percepciones de este estado. Debido a estas diferencias, puede haber un desajuste en el significado de las entidades sintácticas por las cuales los agentes describen sus percepciones (y que constituyen las ontologías respectivas de los agentes). Afirmamos que estas entidades sintácticas pueden estar relacionadas de acuerdo con la semántica intrínseca proporcionada por la relación existente entre el punto de vista de los agentes del entorno. La existencia de esta relación está justificada con precisión por el hecho de que los agentes están situados y observan el mismo entorno. En la Sección 2 describimos nuestro modelo formal para la alineación semántica situada (SSA). Primero, en la Sección 2.1 asociamos un canal al escenario en consideración y mostramos cómo la lógica distribuida generada por este canal proporciona las relaciones lógicas entre los puntos de vista de los agentes del entorno. En segundo lugar, en la Sección 2.2 presentamos un método por el cual los agentes obtienen aproximaciones de esta lógica distribuida. Estas aproximaciones se vuelven gradualmente más confiables a medida que se aplica el método. En la Sección 3 informamos sobre una aplicación de nuestro método. Las conclusiones y el trabajo adicional se analizan en la Sección 4. Finalmente, un apéndice resume los términos y teoremas de la teoría del canal utilizados a lo largo del documento. No asumimos ningún conocimiento de la teoría del canal;Reafiremos las definiciones y teoremas básicos en el apéndice, pero cualquier exposición detallada de la teoría está fuera del alcance de este documento.2. Un modelo formal para SSA 2.1 La lógica de SSA considera un escenario con dos agentes A1 y A2 situados en un entorno e (la generalización a cualquier conjunto numerable de agentes es sencillo). Asociamos un conjunto numerable de estados a E y, en cualquier instante dado, suponemos que E en uno de estos estados. Además, suponemos que cada agente puede observar el medio ambiente y tiene su propia percepción del mismo. Esta habilidad es capturada fielmente por una función surjetiva Seei: S → Pi, donde i ∈ {1, 2}, y típicamente see1 y See2 son diferentes. Según la teoría del canal, la información solo es viable donde existe una forma sistemática de clasificar una gama de cosas como de esta manera o aquello, en otras palabras, donde hay una clasificación (ver Apéndice A). Entonces, para estar dentro del marco de la teoría del canal, debemos asociar las clasificaciones a los componentes de nuestro sistema. Para cada i ∈ {1, 2}, consideramos una clasificación AI que modela el punto de vista AIS de E. Primero, Tok (Ai) está compuesto por percepciones de AIS de los estados E, es decir, Tok (Ai) = pi. En segundo lugar, Typ (AI) contiene las entidades sintácticas por las cuales la IA describe sus percepciones, las que constituyen la ontología de la IA. Finalmente, | = ai sintetiza cómo AI relaciona sus percepciones con estas entidades sintácticas. Ahora, con el objetivo de asociar el entorno E con una clasificación E, elegimos la clasificación de potencia de S como E, que es la clasificación cuyo conjunto de tipos es igual a 2S, cuyos tokens son los elementos de S y para los cuales un token Ees de tipo ε si e ∈ ε. La razón para tomar la clasificación de energía es porque no hay entidades sintácticas que puedan desempeñar el papel de los tipos para E ya que, en general, no existe una conceptualización global del entorno. Sin embargo, el conjunto de tipos de la clasificación de potencia incluye todas las configuraciones de tokens posibles que se describen potencialmente por los tipos. Así tok (e) = s, typ (e) = 2s y e | = e ε si y solo si e ∈ ε. La noción de canal (ver Apéndice A) es fundamental en la teoría de Barrawise y Seligmans. El flujo de información entre los componentes de un sistema distribuido se modela en términos de un canal y las relaciones entre estos componentes se expresan a través de infomorfismos (ver Apéndice A) que proporciona una forma de mover información entre ellos. El flujo de información del escenario bajo consideración se describe con precisión por el canal e = {fi: ai → e} i∈ {1,2} definido como sigue: • ˆfi (α) = {e ∈ Tok (e) |Seei (e) | = ai α} para cada α ∈ Tip (ai) • ˇfi (e) = Seei (e) para cada e ∈ Tok (e) donde i ∈ {1, 2}. La definición de ˇFi parece natural, mientras que ˆFi se define de tal manera que se cumple la propiedad fundamental de los infomorfismos: ˇfi (e) | = ai α iff see (e) | = ai α (por definición de ˇfi) iff e ∈ ˆfi(α) (por definición de ˆfi) iff e | = e ˆfi (α) (por definición de | = e) el sexto intl. Conf.En los agentes autónomos y los sistemas de múltiples agentes (AAMAS 07) 1279 En consecuencia, E es el núcleo del canal E y un estado E ∈ Tok (E) conecta las percepciones de los agentes ˇf1 (E) y ˇf2 (E) (ver Figura 1).typ (e) typ (a1) ˆf1 99tttttttttt typ (a2) ˆf2 eEJJJJJJJJJjjJ (E) | = E ˇf1yttttttttt ˇf2 %% jjjjjjjjjjj(A2) | = A2 Figura 1: El canal E E explica el flujo de información de nuestro escenario en virtud de que los agentes A1 y A2 se encuentran y perciben el mismo entorno E. Queremos obtener relaciones significativas entre los agentes sintácticosEntidades, es decir, tipos de agentes. Afirmamos que el significado debe estar de acuerdo con E. La operación de suma (ver el Apéndice A) nos da una forma de unir las clasificaciones de los dos agentes del Canal E en una sola clasificación, a saber, A1 +A2, y también los dos infomorfismos juntosUn solo infomorfismo, F1 + F2: A1 + A2 → E. A1 + A2 ensambla clasificaciones de agentes de una manera muy gruesa.Tok (A1 + A2) es el producto cartesiano de Tok (A1) y Tok (A2), es decir, Tok (A1 + A2) = {P1, P2 |Pi ∈ Pi}, por lo que un token de A1 + A2 es un par de percepciones de agentes sin restricciones.Typ (A1 + A2) es la unión disjunta de TYP (A1) y TYP (A2), y P1, P2 es de tipo I, α si Pi es de tipo α. Deleguamos la importancia para tomar la unión disjunta porque A1 y A2 podrían usar tipos idénticos con el propósito de describir sus respectivas percepciones de E. La clasificación A1 + A2 parece ser el lugar natural para buscar relaciones entre los tipos de agentes. Ahora, la teoría del canal proporciona una manera de hacer que todas estas relaciones sean explícitas de manera lógica por medio de teorías y lógicas locales (ver Apéndice A). La teoría generada por la clasificación de suma, TH (A1 + A2), y por lo tanto, su lógica generada, log (A1 + A2), implica todas esas restricciones entre los tipos de agentes válidos según A1 + A2. Sin embargo, tenga en cuenta que estas restricciones son obvias. Como dijimos anteriormente, el significado debe estar de acuerdo con el canal E. Las clasificaciones A1 + A2 y E están conectadas a través de la suma Infomorfismo, F = F1 + F2, donde: • ˆf (i, α) = ˆfi (α) = {E∈ Tok (e) |Veri (e) | = ai α} para cada i, α ∈ Tip (a1 + a2) • ˇf (e) = ˇf1 (e), ˇf2 (e) = ver1 (e), ver2 (e) para cada e ∈Tok (e) Las restricciones significativas entre los tipos de agentes están de acuerdo con el canal E porque se calculan haciendo uso de F como exponemos a continuación. Tan importante como la noción de canal es el concepto de lógica distribuida (ver Apéndice A). Dado un canal C y una lógica L en su núcleo, DLOGC (L) representa el razonamiento sobre las relaciones entre los componentes de C justificados por L. Si L = log (c), la lógica distribuida, denotamos por log (c),Captura de manera lógica el flujo de información inherente al canal. En nuestro caso, log (e) explica la relación entre los puntos de vista de los agentes del medio ambiente de manera lógica. Por un lado, las restricciones de th (log (e)) se definen mediante: γ log (e) δ si ˆf [γ] log (e) ˆf [δ] (1) donde γ, δ ⊆ típ (a1 + a2). Por otro lado, el conjunto de tokens normales, nlog (e), es igual al rango de función ˇf: nlog (e) = ˇf [tok (e)] = {ver1 (e), ver2 (e) |e ∈ Tok (e)} Por lo tanto, un token normal es un par de percepciones de agentes que están restringidas al provenir del mismo estado de entorno (a diferencia de los tokens A1 + A2). Todas las restricciones del (log (e)) son satisfechas por todos los tokens normales (debido a una lógica). En este caso particular, esta condición también es suficiente (la prueba es sencilla);Como alternativa a (1) tenemos: γ log (e) δ IFF para todo e ∈ Tok (e), if (∀ i, γ ∈ γ) [véase (e) | = ai γ] entonces (∃ j, δ∈ δ) [SeeJ (e) | = aj δ] (2) donde γ, δ ⊆ típ (a1 + a2). Log (e) es la lógica de SSA. TH (log (e)) comprende las limitaciones más significativas entre los tipos de agentes de acuerdo con el canal E. En otras palabras, la SSA contiene y también justifica las relaciones más significativas entre esas entidades sintácticas que los agentes usan para describir los suyospercepciones del entorno. El registro (e) está completo ya que el registro (e) está completo, pero no es necesariamente sólido porque aunque el registro (e) es sólido, ˇf no es sujectivo en general (ver Apéndice B). Si log (e) también es sólido, entonces log (e) = log (a1 +a2) (ver Apéndice B). Eso significa que no existe una relación significativa entre los puntos de vista de los agentes del entorno según E. Es solo el hecho de que log (e) no es sólido lo que permite una relación significativa entre los puntos de vista de los agentes. Esta relación se expresa a nivel de tipo en términos de restricciones por Th (log (e)) y a nivel de token por NLOG (e).2.2 Enfercado de la lógica de SSA a través de la comunicación, hemos denominado log (e) la lógica de SSA. TH (log (e)) comprende las restricciones más significativas entre los tipos de agentes según E. El problema es que ninguno de los agentes puede hacer uso de esta teoría porque no conocen completamente E. En esta sección, presentamos un método mediante el cual los agentes obtienen aproximaciones a th (log (e)). También demostramos que estas aproximaciones se vuelven gradualmente más confiables a medida que se aplica el método. Los agentes pueden obtener aproximaciones al (log (e)) a través de la comunicación. A1 y A2 se comunican intercambiando información sobre sus percepciones de los estados del medio ambiente. Esta información se expresa en términos de sus propias relaciones de clasificación. Específicamente, si E está en un estado concreto E, suponemos que los agentes pueden transmitirse entre sí qué tipos están satisfechos por sus respectivas percepciones de E y cuáles no. Este intercambio genera un canal c = {fi: ai → 1280 el sexto intl. Conf.En agentes autónomos y sistemas de múltiples agentes (AAMAS 07) C} i∈ {1,2} y Th (log (c)) contiene las restricciones entre los tipos de agentes justificados por el hecho de que los agentes han observado e.Ahora, si E recurre a otro estado, E y los agentes proceden como antes, otro canal c = {fi: ai → c} i∈ {1,2} da cuenta de la nueva situación considerando también la información anterior. TH (log (c)) comprende las restricciones entre los tipos de agentes justificados por el hecho de que los agentes han observado E y E. El punto significativo es que C es un refinamiento de C (ver Apéndice A). El teorema 2.1 a continuación asegura que el canal refinado implique información más confiable. La comunicación supuestamente termina cuando los agentes han observado todos los estados del entorno. Nuevamente, esta situación puede ser modelada por un canal, llamarlo c ∗ = {f ∗ i: ai → c ∗} i∈ {1,2}. Teorema 2.2 establece que th (log (c ∗)) = th (log (e)). El Teorema 2.1 y el Teorema 2.2 aseguran que la aplicación del método los agentes pueden obtener aproximaciones a TH (log (e)) gradualmente más confiables. Teorema 2.1. Sea c = {fi: ai → c} i∈ {1,2} y c = {fi: ai → c} i∈ {1,2} sea dos canales. Si C es un refinamiento de C, entonces: 1. Th (log (c)) ⊆ th (log (c)) 2. Nlog (c) ⊇ nlog (c) prueba. Dado que C es un refinamiento de C, entonces existe un refinamiento Infomorfismo R de C a C;entonces fi = r ◦ fi. Deje a = def a1 + a2, f = def f1 + f2 y f = def f1 + f2.1. Sea γ y δ sean subconjuntos de típ (a) y suponga que γ log (c) δ, lo que significa ˆf [γ] c ˆf [δ]. Tenemos que probar γ log (c) δ, o de manera equivalente, ˆf [γ] c ˆf [δ]. Procedemos por Reducto Ad Absurdum. Supongamos que c ∈ Tok (c) no satisface el secuente ˆf [γ], ˆf [δ]. Entonces c | = c ˆf (γ) para todos γ ∈ γ y c | = c ˆf (Δ) para todos Δ ∈ δ. Elegamos un arbitrary γ ∈ γ. Tenemos que γ = i, α para algunos α ∈ Tip (ai) e i ∈ {1, 2}. Así ˆf (γ) = ˆf (i, α) = ˆfi (α) = ˆr ◦ ˆFi (α) = ˆr (ˆfi (α)). Por lo tanto: c | = c ˆf (γ) iff c | = c ˆr (ˆfi (α)) iff ˇr (c) | = c ˆfi (α) iff ˇr (c) | = c ˆf (i, α) Iff ˇr(c) | = c ˆf (γ) En consecuencia, ˇr (c) | = c ˆf (γ) para todos γ ∈ γ. Dado que ˆf [γ] c ˆf [δ], entonces existe δ ∗ ∈ δ tal que ˇr (c) | = c ˆf (δ ∗). Una secuencia de equivalencias similares a la anterior justifica c | = c ˆf (Δ ∗), contradiciendo que C es un contraejemplo a ˆF [γ], ˆf [δ]. Por lo tanto, γ log (c) δ como queríamos probar.2. Deje a1, a2 ∈ Tok (a) y asume A1, a2 ∈ Nlog (c). Por lo tanto, existe C Token en C de modo que A1, A2 = ˇf (C). Entonces tenemos ai = ˇfi (c) = ˇfi ◦ ˇr (c) = ˇfi (ˇr (c)), porque i ∈ {1, 2}. Por lo tanto, a1, a2 = ˇf (ˇr (c)) y a1, a2 ∈ Nlog (c). En consecuencia, nlog (c) ⊇ nlog (c) que concluye la prueba. Observación 2.1. El Teorema 2.1 afirma que el canal más refinado brinda información más confiable. Aunque su teoría tiene menos limitaciones, tiene fichas más normales a las que se aplican. En el resto de la sección, describimos explícitamente el proceso de comunicación y concluimos con la prueba del teorema 2.2. Supongamos que Typ (AI) es finito para i ∈ {1, 2} y S es infinito numerable, aunque el caso finito puede tratarse de forma similar. También elegimos un conjunto infinito numerable de símbolos {CN |n ∈ N} 1. Omitimos los superíndes de informes informes cuando no surge confusión. Los tipos generalmente se denotan por letras griegas y tokens por letras latinas, por lo que si F es un infomorfismo, F (α) ≡ ˆf (α) y F (a) ≡ ˇf (a). La comunicación de los agentes comienza a partir de la observación de E. Supongamos que E está en el estado e1 ∈ S = Tok (E). La percepción de A1 de E1 es F1 (E1) y la percepción A2S de E1 es F2 (E1). Damos por sentado que A1 puede comunicar A2 aquellos tipos que son y no están satisfechos con F1 (E1) de acuerdo con su clasificación A1. También puede hacer A2. Dado que tanto TYP (A1) como TYP (A2) son finitos, este proceso finalmente termina. Después de esta comunicación, surge un canal C1 = {F1 I: AI → C1} I = 1,2 (ver Figura 2). C1 A1 F1 1 == ||||||||A2 F1 2 AACCCCCCCC Figura 2: La primera etapa de comunicación, por un lado, C1 se define por: • tok (c1) = {c1} • typ (c1) = typ (a1 + a2) • c1 | = c1 i, αSi fi (e1) | = ai α (por cada i, α ∈ Tipo (a1 + a2)) por otro lado, f1 i, con i ∈ {1, 2}, se define por: • f1 i (α)= I, α (para cada α ∈ Tipo (AI)) • F1 I (C1) = FI (E1) log (C1) representa el razonamiento sobre la primera etapa de comunicación. Es fácil demostrar que (LG (C1)) = a (C1). El punto significativo es que ambos agentes conocen a C1 como el resultado de la comunicación. Por lo tanto, pueden calcular la teoría por separado TH (C1) = TYP (C1), C1 que contiene las restricciones entre los tipos de agentes justificados por el hecho de que los agentes han observado E1. Ahora, supongamos que E recurre a un nuevo estado E2. Los agentes pueden proceder como antes, intercambiando esta información sobre sus percepciones de E2. Otro canal C2 = {F2 I: AI → C2} i∈ {1,2} aparece. Definimos C2 para tener en cuenta también la información proporcionada por la etapa anterior de comunicación. Por un lado, C2 se define por: • Tok (C2) = {C1, C2} 1 Escribimos estos símbolos con superíntesis porque limitamos el uso de subíndices para las preocupaciones de los agentes. Tenga en cuenta que este conjunto se elige con la misma cardinalidad de S. el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (aamas 07) 1281 • typ (c2) = typ (a1 + a2) • ck | = c2 i, α if fi (ek) | = ai α (para cada k ∈ {1,2} e i, α ∈ Tipo (A1 + A2)) Por otro lado, F2 I, con i ∈ {1, 2}, se define por: • F2 I (α) = I, α (para cada α ∈typ (ai)) • f2 i (ck) = fi (ek) (para cada k ∈ {1, 2}) log (C2) representa el razonamiento sobre las etapas de comunicación anteriores y posteriores. TH (log (C2)) es igual a th (C2) = TYP (C2), C2, luego contiene las restricciones entre los tipos de agentes justificados por el hecho de que los agentes han observado E1 y E2. A1 y A2 conocen C2 para que puedan usar estas restricciones. El punto clave es que el canal C2 es un refinamiento de C1. Es fácil verificar que F1 se definiera como la función de identidad en los tipos y la función de inclusión en los tokens es un infomorfismo de refinamiento (ver en la parte inferior de la Figura 3). Por el Teorema 2.1, las restricciones C2 son más confiables que las restricciones C1. En la situación general, una vez que los estados E1, E2 ,..., se han observado EN - 1 (n ≥ 2) y aparece un nuevo estado en, canal Cn = {fn i: ai → cn} i∈ {1,2} informa sobre la comunicación de los agentes hasta ese momento. La definición de CN es similar a las anteriores y se pueden hacer comentarios análogos (ver en la parte superior de la Figura 3). Teoría th (log (cn)) = th (cn) = typ (cn), CN contiene las restricciones entre los tipos de agentes justificados por el hecho de que los agentes han observado E1, E2 ,..., es. Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, a2 fn 2 ggppppppppppppp fn - 1 2 wwnnnnnnnnnnnnnn f2 2 õõ f1 2 ØøCN - 1...C2 F1 C1 Figura 3: Comunicación de agentes Recuerde que hemos asumido que S es infinito numerable. Por lo tanto, es poco práctico dejar que la comunicación termine cuando A1 y A2 han observado todos los estados de entorno. En ese punto, la familia de canales {cn} n∈N informaría de todas las etapas de comunicación. Por lo tanto, depende de los agentes decidir cuándo dejar de comunicarse si se ha alcanzado una aproximación lo suficientemente buena para los fines de sus respectivas tareas. Pero el estudio de los posibles criterios de terminación está fuera del alcance de este documento y se fue para el trabajo futuro. Desde un punto de vista teórico, sin embargo, podemos considerar el canal c ∗ = {f ∗ i: ai → c ∗} i∈ {1,2} que informa el fin de la comunicación después de observar todos los estados de entorno. Por un lado, c ∗ se define por: • tok (c ∗) = {cn |n ∈ N} • typ (c ∗) = typ (a1 + a2) • cn | = c ∗ i, α if fi (en) | = ai α (para n ∈ N e i, α ∈ Tip (a1 + a2)) Por otro lado, f ∗ i, con i ∈ {1, 2}, se define por: • f ∗ i (α) = i, α (para α ∈ Tip (ai)) • f ∗ i (CN) = fi (en) (para n ∈ N) El teorema a continuación constituye la piedra angular del modelo expuesto en este documento. Asegura, junto con el Teorema 2.1, que en cada etapa de comunicación los agentes obtienen una teoría que se aproxima más a la teoría generada por la lógica de SSA. Teorema 2.2. Las siguientes afirmaciones tienen: 1. Para todos n ∈ N, C ∗ es un refinamiento de CN.2. Th (log (e)) = th (c ∗) = th (log (c ∗)). Prueba.1. Es fácil demostrar que para cada n ∈ N, Gn definido como la función de identidad en los tipos y la función de inclusión en los tokens es un infomorfismo de refinamiento de C ∗ a CN.2. La segunda igualdad es sencilla;El primero sigue directamente de: cn | = c ∗ i, α iff ˇfi (en) | = ai α (por definición de | = c ∗) iff en | = e ˆfi (α) (porque Fi es infomorfim) iff enf en| = E ˆf (i, α) (por definición de ˆf) e c ∗ gn a1 fn 1 99ooooooooooooo f ∗ 1 uuooooooooooooo f1 cc a2 f ∗2 ggooooooooooo fn 2 wwoooooooooooooo f2 ???????????????????? CN 1282 El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 3. Un ejemplo en la sección anterior que hemos descrito con gran detalle nuestro modelo formal para SSA. Sin embargo, todavía no hemos abordado el aspecto práctico del modelo. En esta sección, damos una pincelada de la visión pragmática de nuestro enfoque. Estudiamos un ejemplo muy simple y explicamos cómo los agentes pueden usar esas aproximaciones de la lógica de SSA que pueden obtener a través de la comunicación. Reflexionemos sobre un sistema que consiste en robots ubicados en una cuadrícula bidimensional que busca paquetes con el objetivo de moverlos a un determinado destino (Figura 4). Los robots pueden llevar solo un paquete a la vez y no pueden moverse a través de un paquete. Figura 4: Los robots del escenario tienen una vista parcial del dominio y existen dos tipos de robots según el campo visual que tienen. Algunos robots son capaces de observar los ocho cuadrados adyacentes, pero otros solo observan los tres cuadrados que tienen en el frente (ver Figura 5). Los llamamos URDL (forma abreviada de los robots de izquierda al alza) y LCR (abreviatura para los robots de izquierda-centro-derecha) respectivamente. Describir los estados del medio ambiente, así como las funciones de percepción de los robots, es bastante tedioso e incluso innecesario. Asumimos que el lector tiene todas esas descripciones en mente. Todos los robots en el sistema deben poder resolver los problemas de distribución de paquetes cooperativamente comunicando sus intenciones entre sí. Para comunicarse, los agentes envían mensajes utilizando alguna ontología. En nuestro escenario, coexisten dos ontologías, las ontologías UDRL y LCR. Ambos son muy simples y se limitan a describir lo que observan los robots. Figura 5: Robots Campo de visión Cuando un robot que lleva un paquete encuentra otro paquete que obstruye su camino, puede rodearlo o, si hay otro robot en su campo visual, pídale ayuda. Supongamos que dos robots URDL se encuentran en una situación como la que se muestra en la Figura 6. Robot1 (el que lleva un paquete) decide pedirle ayuda a Robot2 y envía una solicitud. Esta solicitud se escribe a continuación como un mensaje KQML y debe interpretarse intuitivamente como: Robot2, recoja el paquete ubicado en mi cuadrado Up, sabiendo que está ubicado en mi cuadrado de la derecha.`Solicitud: remitente robot1: receptor robot2: paquetes de idiomas distribución-lenguaje: ontología urdl-ontología: contenido (recoger u (paquete) porque UR (robot2) ´ Figura 6: Asistencia de robot Robot2 Comprende el contenido de la solicitud y puede usarUna regla representada por la siguiente restricción: 1, Ur (Robot2), 2, UL (Robot1), 1, U (paquete) 2, U (paquete) La restricción anterior debe interpretarse intuitivamente como: si Robot2 está situado en Robot1.-Right Square, Robot1 está situado en Robot2s Up-Left Square y un paquete se encuentra en Robot1s Up Square, luego un paquete se encuentra en Robot2s Up Square. Ahora, surgen problemas cuando un robot LCR y un robot URDL intentan interoperar. Ver Figura 7. Robot1 envía una solicitud del formulario: `Solicitud: remitente Robot1: receptor Robot2: PAQUETES DE IDIOMA DISTRIBUCIÓN Language: Ontology LCR-Antology: Content (Pick Up R (Robot2) porque C (Paquete) ´ Robot2 no comprende el contenido del contenido del contenido del contenido del contenidoSolicite pero deciden comenzar un proceso de alineación que corresponde con un canal C1. Una vez terminado, Robot2 busca en TH (C1) para restricciones similares a la esperada, es decir, las de la forma: 1, R (Robot2), 2, UL (Robot1), 1, C (Paquete) C1 2, λ(Paquete) donde λ ∈ {u, r, d, l, ur, dr, dl, ul}. A partir de estos, solo las siguientes limitaciones son plausibles según C1: el sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1283 Figura 7: Ontology Mismatch 1, R (Robot2), 2, UL (Robot1), 1, C (Paquete) C1 2, U (Paquete) 1, R (Robot2), 2, UL (Robot1), 1, C (paquete) C1 2, L (paquete) 1, R (Robot2), 2, UL (Robot1), 1, C (paquete) C1 2, DR (paquete) IFPosteriormente, ambos robots que adoptan los mismos roles participan en una situación como la que se muestra en la Figura 8, tiene lugar un nuevo proceso de alineación que corresponde con un canal C2. C2 también considera la información anterior y, por lo tanto, refina C1. La única restricción de las anteriores que permanece plausible según C2 es: 1, R (Robot2), 2, UL (Robot1), 1, C (Paquete) C2 2, U (paquete) Observe que esta restricción es un elemento deLa teoría de la lógica distribuida. Los agentes se comunican para cooperar con éxito y el éxito se garantiza utilizando restricciones de la lógica distribuida. Figura 8: Refinamiento 4. Conclusiones y trabajo adicional En este documento hemos expuesto un modelo formal de alineación semántica como una secuencia de refinamientos de canales de información que son relativos a los estados particulares del entorno en el que dos agentes se comunican y alinean sus respectivas conceptualizaciones de estos estados. Antes de nosotros, Kent [6] y Kalfoglou y Schorlemmer [4, 10] han aplicado la teoría de los canales para formalizar la alineación semántica utilizando también la visión de Barwise y Seligmans para centrarse en los tokens como facilitadores del flujo de información. Sin embargo, su enfoque de la alineación semántica, como la mayoría de los mecanismos de coincidencia de ontología desarrollados hasta la fecha (independientemente de si siguen un enfoque funcional, basado en el tiempo, o un enfoque basado en la interacción), aún define la alineación semántica en términosde decisiones de diseño a priori, como la taxonomía conceptual de las ontologías o las fuentes externas, provocadas en el proceso de alineación. En cambio, el modelo que hemos presentado en este documento hace explícito los estados particulares del entorno en el que se encuentran los agentes y están tratando de alinear gradualmente sus entidades ontológicas. En el futuro, nuestro esfuerzo se centrará en el lado práctico del problema de alineación semántica situada. Planeamos refinar aún más el modelo presentado aquí (por ejemplo, para incluir cuestiones pragmáticas como los criterios de terminación para el proceso de alineación) y idear protocolos de negociación de ontología concreta basados en este modelo que los agentes pueden promulgar. El modelo formal expuesto en este documento constituirá una base sólida de futuros resultados prácticos. Agradecimientos Este trabajo es apoyado bajo el Proyecto UPIC, patrocinado por el Ministerio de Educación y Ciencia de Spains bajo el número de subvención TIN2004-07461-C02-02 y también bajo el Proyecto de Investigación específica de Open Knowledge (Strep), patrocinado por la Comisión Europea bajo el número de contrato FP6-027253. Marco Schorlemmer cuenta con el apoyo de una beca de investigación de Ram´on Y Cajal del Ministerio de Educación y Ciencia de Spains, parcialmente financiado por el Fondo Social Europeo.5. Referencias [1] J. Barwise y J. Seligman. Flujo de información: la lógica de los sistemas distribuidos. Cambridge University Press, 1997. [2] C. Ghidini y F. Giunchiglia. Modelos locales semánticos o razonamiento contextual = localidad + compatibilidad. Inteligencia Artificial, 127 (2): 221-259, 2001. [3] F. Giunchiglia y P. Shvaiko. Matriota semántica. The Knowledge Engineering Review, 18 (3): 265-280, 2004. [4] Y. Kalfoglou y M. Schorlemmer. If-map: un método de mapeo de ontología basado en la teoría de flujo de información. En Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou y M. Schorlemmer. Mapeo de ontología: el estado del arte. The Knowledge Engineering Review, 18 (1): 1-31, 2003. [6] R. E. Kent. Integración semántica en el marco de flujo de información. En interoperabilidad e integración semántica, Dagstuhl Seminar Actisings 04391, 2005. [7] D. Lenat. CYC: Una inversión a gran escala en infraestructura de conocimiento. Comunicaciones de la ACM, 38 (11), 1995. [8] V. L´opez, M. Sabou y E. Motta. PowerMap: mapeo de la red semántica real sobre la marcha. Actas del ISWC06, 2006. [9] F. McNeill. Refinamiento de ontología dinámica. PhD 1284 El sexto intl. Conf.Sobre la tesis de agentes autónomos y sistemas de múltiples agentes (AAMAS 07), Escuela de Informática, Universidad de Edimburgo, 2006. [10] M. Schorlemmer e Y. Kalfoglou. Alineación de ontología progresiva para la coordinación del significado: una base teórica de información. En 4th int. Conf.en agentes autónomos y sistemas multiagentes, 2005. [11] P. Shvaiko y J. Euzenat. Una encuesta de enfoques de correspondencia basados en esquemas. En Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels. Los orígenes de las ontologías y las convenciones de comunicación en sistemas de múltiples agentes. En Journal of Autonomous Agents and Multi-Agent Systems, 1 (2), 169-194, 1998. [13] J. Van Diggelen et al. Anémona: un entorno de negociación de ontología mínimo efectivo en 5th Int. Conferencia conjunta.Sobre agentes autónomos y sistemas multiagentes, 2006 Apéndice A. Clasificación de términos de teoría del canal: es una tupla a = tok (a), typ (a), | = a where tok (a) es un conjunto de tokens, typ (a) es un conjunto de tipos y | = a es un binarioRelación entre TOK (A) y TYP (A). Si A | = A α, entonces se dice que es de tipo α.Infomorfismo: F: A → B De las clasificaciones A a B es un par infrariante de funciones F = ˆf, ˇf, donde ˆf: typ (a) → typ (b) y ˇf: tok (b) → tok (a), satisfactorioLa siguiente propiedad fundamental: ˇf (b) | = a α iff b | = b ˆf (α) para cada token b ∈ Tok (b) y cada tipo α ∈ Tip (a). Canal: consta de dos infomorfismos c = {fi: ai → c} i∈ {1,2} con un codomínico común c, llamado el núcleo de los tokens C. c se llaman conexiones y se dice que una conexión C conecte tokens ˇf1 (c) y ˇf2 (c) .2 suma: clasificaciones dadas A y B, la suma de A y B, denotada por A + B, es la clasificación con Tok (A + B) = Tok (A) × Tok (B)= {a, b |a ∈ Tok (a) y b ∈ Tok (b)}, typ (a + b) = typ (a) typ (b) = {i, γ |i = 1 y γ ∈ Tip (a) o i = 2 y γ ∈ Tipo (b)} y relación | = a+b definido por: a, b | = a+b 1, α si a | = a α a, b | = a + b 2, β si b | = b β dados infomorfismos F: A → C y G: B → C, la suma F + G: A + B → C se define en los tipos por ˆ (F +g) (1, α) = ˆf (α) y ˆ (f + g) (2, β) = ˆg (β), y en tokens por ˇ (f + g) (c) = ˇf (c), ˇg(C) . Teoría: Dado un conjunto σ, una secuencia de σ es un par γ, δ de subconjuntos de σ. Una relación binaria entre subconjuntos de σ se llama relación de consecuencia en σ. Una teoría es un par t = σ, donde es una relación de consecuencia en σ. Una secuencia γ, δ de σ para la cual γ δ se llama restricción de la teoría T. T es regular si satisface: 1. Identidad: α α 2. Debilitamiento: si γ δ, entonces γ, γ δ, δ 2 De hecho, esta es la definición de un canal binario. Un canal se puede definir con un conjunto de índice arbitrario.3. Corte global: si γ, π0 δ, π1 para cada partición π0, π1 de π (es decir, π0 ∪ π1 = π y π0 ∩ π1 = ∅), entonces γ δ para todos α ∈ σ y todos γ, γ, Δ,Δ, π ⊆ σ.3 Teoría generada por una clasificación: Sea A una clasificación. Un token a ∈ Tok (a) satisface un γ secuente γ, δ de tip (a) siempre que si a es de cada tipo en γ, entonces es de algún tipo en δ. La teoría generada por a, denotada por th (a), es la típica teoría (a), a donde γ a δ si cada token en A satisface γ, δ. Lógica local: es una tupla l = tok (l), typ (l), | = l, l, nl donde: 1. tok (l), typ (l), | = l es una clasificación denotada por CLA (l), 2. typ (l), l es una teoría regular denotada por th (l), 3. NL es un subconjunto de tok (l), llamado tokens normales de L, que satisfacen todas las restricciones de th (l). Una lógica local L es sólida si cada token en CLA (L) es normal, es decir, NL = Tok (L). L se completa si cada secuencia de típito (l) satisfecho por cada token normal es una restricción de th (l). La lógica local generada por una clasificación: dada una clasificación A, la lógica local generada por un registro escrito (a), es la lógica local en un (es decir, CLA (log (a)) = a), con th (log ((log (A)) = th (a) y tal que todas sus tokens sean normales, es decir, nlog (a) = tok (a). Imagen inversa: Dada un infomorfismo F: A → B y una lógica local L en B, la imagen inversa de L bajo F, denotada F - 1 [L], es la lógica local en un tipo que γ F - 1 [L]Δ si ˆf [γ] l ˆf [Δ] y nf - 1 [l] = ˇf [nl] = {a ∈ Tok (a) |a = ˇf (b) para algunos b ∈ Nl}. Lógica distribuida: Sea c = {fi: ai → c} i∈ {1,2} ser un canal y l una lógica local en su núcleo C, la lógica distribuida de C generada por L, escrito dlogc (l), es el es elImagen inversa de L bajo la suma F1 + F2. Refinamiento: Sea c = {fi: ai → c} i∈ {1,2} y c = {fi: ai → c} i∈ {1,2} sea dos canales con las mismas clasificaciones de componentes a1 y a2. Un infomorfismo de refinamiento de C a C es un infomorfismo R: C → C tal que para cada i ∈ {1, 2}, fi = R ◦Fi (es decir, ˆfi = ˆr ◦ ˆFi y ˇFi = ˇFi ◦ˇr). El canal C es un refinamiento de C si existe un infinamiento infomorfismo R de C a C. B. Teoría de la teoría del canal Teorema B.1. La lógica generada por una clasificación es sólida y completa. Además, dada una clasificación A y una lógica l en a, L es sólida y completa si y solo si l = log (a). Teorema B.2. Sea L una lógica en una clasificación B y F: A → B un infomorfismo.1. Si L está completo, entonces F - 1 [l] está completo.2. Si L es sólido y ˇf es Surjetivo, entonces F - 1 [L] es sólido.3 Todas las teorías consideradas en este documento son regulares. El sexto intl. Conf.en agentes autónomos y sistemas de múltiples agentes (AAMAS 07) 1285",
    "original_sentences": [
        "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
        "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
        "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
        "It hence makes the situation in which the alignment occurs explicit in the model.",
        "We resort to Channel Theory to carry out the formalisation.",
        "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
        "General Terms Theory 1.",
        "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
        "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
        "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
        "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
        "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
        "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
        "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
        "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
        "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
        "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
        "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
        "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
        "Imagine a situation in which two agents are facing each other in front of a checker board.",
        "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
        "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
        "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
        "The same agents situated differently may produce a different alignment.",
        "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
        "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
        "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
        "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
        "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
        "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
        "We shall therefore consider a scenario with two or more agents situated in an environment.",
        "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
        "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
        "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
        "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
        "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
        "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
        "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
        "These approximations gradually become more reliable as the method is applied.",
        "In Section 3 we report on an application of our method.",
        "Conclusions and further work are analyzed in Section 4.",
        "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
        "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
        "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
        "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
        "We further assume that each agent is able to observe the environment and has its own perception of it.",
        "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
        "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
        "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
        "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
        "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
        "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
        "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
        "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
        "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
        "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
        "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
        "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
        "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
        "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
        "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
        "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
        "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
        "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
        "Notice however that these constraints are obvious.",
        "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
        "As important as the notion of channel is the concept of distributed logic (see appendix A).",
        "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
        "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
        "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
        "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
        "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
        "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
        "Log(E) is the logic of SSA.",
        "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
        "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
        "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
        "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
        "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
        "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
        "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
        "We also prove these approximations gradually become more reliable as the method is applied.",
        "Agents can obtain approximations to Th(Log(E)) through communication.",
        "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
        "This information is expressed in terms of their own classification relations.",
        "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
        "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
        "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
        "The significant point is that C is a refinement of C (see appendix A).",
        "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
        "The communication supposedly ends when agents have observed all the environment states.",
        "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
        "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
        "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
        "Theorem 2.1.",
        "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
        "If C is a refinement of C then: 1.",
        "Th(Log(C )) ⊆ Th(Log(C)) 2.",
        "NLog(C ) ⊇ NLog(C) Proof.",
        "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
        "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
        "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
        "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
        "We proceed by reductio ad absurdum.",
        "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
        "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
        "Let us choose an arbitrary γ ∈ Γ.",
        "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
        "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
        "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
        "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
        "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
        "Hence Γ Log(C) Δ as we wanted to prove. 2.",
        "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
        "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
        "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
        "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
        "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
        "Remark 2.1.",
        "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
        "Even though its theory has less constraints, it has more normal tokens to which they apply.",
        "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
        "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
        "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
        "We omit informorphisms superscripts when no confusion arises.",
        "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
        "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
        "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
        "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
        "So can A2 do.",
        "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
        "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
        "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
        "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
        "The significant point is that both agents know C1 as the result of the communication.",
        "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
        "Now, let us assume that E turns to a new state e2 .",
        "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
        "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
        "We define C2 so as to take also into account the information provided by the previous stage of communication.",
        "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
        "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
        "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
        "A1 and A2 knows C2 so they can use these constraints.",
        "The key point is that channel C2 is a refinement of C1 .",
        "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
        "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
        "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
        "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
        "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
        "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
        "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
        "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
        "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
        "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
        "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
        "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
        "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
        "Theorem 2.2.",
        "The following statements hold: 1.",
        "For all n ∈ N, C∗ is a refinement of Cn . 2.",
        "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
        "Proof. 1.",
        "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
        "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
        "Cn 1282 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
        "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
        "However, we have not tackled the practical aspect of the model yet.",
        "In this section, we give a brushstroke of the pragmatic view of our approach.",
        "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
        "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
        "Robots can carry only one package at a time and they can not move through a package.",
        "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
        "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
        "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
        "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
        "We assume the reader has all those descriptions in mind.",
        "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
        "In order to communicate, agents send messages using some ontology.",
        "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
        "Both of them are very simple and are just confined to describe what robots observe.",
        "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
        "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
        "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
        "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
        "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
        "See Figure 7.",
        "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
        "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
        "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
        "C2 also considers the previous information and hence refines C1 .",
        "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
        "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
        "Figure 8: Refinement 4.",
        "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
        "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
        "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
        "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
        "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
        "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
        "The formal model exposed in this paper will constitute a solid base of future practical results.",
        "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
        "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
        "REFERENCES [1] J. Barwise and J. Seligman.",
        "Information Flow: The Logic of Distributed Systems.",
        "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
        "Local models semantics, or contextual reasoning = locality + compatibility.",
        "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
        "Semantic matching.",
        "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
        "IF-Map: An ontology-mapping method based on information-flow theory.",
        "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
        "Ontology mapping: The sate of the art.",
        "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
        "Semantic integration in the Information Flow Framework.",
        "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
        "CyC: A large-scale investment in knowledge infrastructure.",
        "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
        "PowerMap: Mapping the real Semantic Web on the fly.",
        "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
        "Dynamic Ontology Refinement.",
        "PhD 1284 The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
        "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
        "In 4th Int.",
        "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
        "A survey of schema-based matching approaches.",
        "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
        "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
        "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
        "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
        "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
        "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
        "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
        "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
        "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
        "A binary relation between subsets of Σ is called a consequence relation on Σ.",
        "A theory is a pair T = Σ, where is a consequence relation on Σ.",
        "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
        "Identity: α α 2.",
        "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
        "A channel can be defined with an arbitrary index set. 3.",
        "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
        "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
        "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
        "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
        "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
        "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
        "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
        "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
        "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
        "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
        "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
        "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
        "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
        "CHANNEL THEORY THEOREMS Theorem B.1.",
        "The logic generated by a classification is sound and complete.",
        "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
        "Theorem B.2.",
        "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
        "If L is complete then f−1 [L] is complete. 2.",
        "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
        "The Sixth Intl.",
        "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
    ],
    "error_count": 0,
    "keys": {
        "ontology": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT <br>ontology</br> matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most <br>ontology</br> matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An <br>ontology</br> is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of <br>ontology</br> matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most <br>ontology</br> matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require <br>ontology</br> matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those <br>ontology</br> fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an <br>ontology</br> and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered <br>ontology</br> (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the <br>ontology</br> of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some <br>ontology</br>.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :<br>ontology</br> URDL-<br>ontology</br> :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :<br>ontology</br> LCR-<br>ontology</br> :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: <br>ontology</br> mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most <br>ontology</br> matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete <br>ontology</br> negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An <br>ontology</br>-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "<br>ontology</br> mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic <br>ontology</br> Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive <br>ontology</br> alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal <br>ontology</br> Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un modelo formal para la alineación semántica situada Manuel Atencia Marco Schorlemmer IIIA, Instituto de Investigación de Inteligencia Artificial CSIC, Spanish National Research Council Bellaterra (Barcelona), Cataluña, España {Manu, Marco'@iiia.csic.es Abstract \"Ontology\" Matching es actualmente unTecnología clave para lograr la alineación semántica de las entidades ontológicas utilizadas por las aplicaciones basadas en el conocimiento y, por lo tanto, para permitir su interoperabilidad en entornos distribuidos como sistemas multiagentes.ontología",
                "Sin embargo, la mayoría de los mecanismos de coincidencia de \"ontología\" suponen que coinciden la integración previa y se basa en la semántica que se ha codificado a priori en jerarquías de conceptos o fuentes externas.ontología",
                "Introducción Una \"ontología\" se define comúnmente como una especificación de la conceptualización de un dominio particular.ontología",
                "Pero la proliferación de muchas ontologías diversas causadas por diferentes conceptualizaciones de incluso el mismo dominio, y su especificación posterior utilizando una terminología variable, ha resaltado la necesidad de técnicas de coincidencia de \"ontología\" que son capaces de calcular las relaciones semánticas entre las entidades de ontologías de ingeniería por separado.[5, 11] Hasta hace poco, la mayoría de los mecanismos de coincidencia de \"ontología\" desarrollados hasta ahora han adoptado un enfoque funcional clásico para el problema de heterogeneidad semántica, en el que la coincidencia de ontología se ve como un proceso que toma dos o más ontologías como entrada y produce una alineación semánticade entidades ontológicas como salida [3].ontología",
                "La comunicación de múltiples agentes, el intercambio de información entre pares y la composición del servicio web son una naturaleza descentralizada, dinámica y abierta, y requieren una coincidencia de \"ontología\" para realizarse localmente durante el tiempo de ejecución.ontología",
                "Ciertamente, existen esfuerzos para igualar eficientemente entidades ontológicas en tiempo de ejecución, tomando solo aquellos fragmentos de \"ontología\" que son necesarios para la tarea en cuestión [10, 13, 9, 8].ontología",
                "Este escenario recuerda a aquellos en los que un grupo de agentes distribuidos se adapta a formar una \"ontología\" y un léxico compartido de una manera emergente y ascendente, con solo interacciones locales y sin autoridad de control central [12].ontología",
                "En este artículo, sin embargo, abordamos el caso en el que los agentes ya están dotados de una \"ontología\" de ingeniería de arriba hacia abajo (incluso puede ser el mismo), que no se adaptan o refinan, sino para la cual quieren encontrarLas relaciones semánticas con ontologías separadas de otros agentes sobre la base de su comunicación dentro de una situación específica.ontología",
                "En segundo lugar, Typ (AI) contiene las entidades sintácticas por las cuales la IA describe sus percepciones, las que constituyen la \"ontología\" de la IA.ontología",
                "Para comunicarse, los agentes envían mensajes utilizando alguna \"ontología\".ontología"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "multi-agent system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as <br>multi-agent system</br>s, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como tal, las ontologías se han adoptado ampliamente como una tecnología clave que puede favorecer el intercambio de conocimientos en entornos distribuidos, como el \"sistema de múltiples agentes\", las bases de datos federadas o la web semántica.sistema de múltiples agentes"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "federated database": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, <br>federated database</br>s, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como tal, las ontologías se han adoptado ampliamente como una tecnología clave que puede favorecer el intercambio de conocimientos en entornos distribuidos, como los sistemas de agentes múltiples \", la base de datos federada\" o la web semántica.base de datos federada"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "semantic web": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the <br>semantic web</br>.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real <br>semantic web</br> on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como tal, las ontologías se han adoptado ampliamente como una tecnología clave que puede favorecer el intercambio de conocimientos en entornos distribuidos, como sistemas de agentes múltiples, bases de datos federadas o la \"web semántica\".web semántica",
                "PowerMap: mapeo de la verdadera \"web semántica\" sobre la marcha.web semántica"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "knowledge-based system": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating <br>knowledge-based system</br>s or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Además, la coincidencia a menudo se ha llevado a cabo en el tiempo de diseño, antes de integrar el \"sistema basado en el conocimiento\" o hacerlos interoperar.sistema basado en el conocimiento"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "disjoint union": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the <br>disjoint union</br> of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the <br>disjoint union</br> because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Afirmamos que el significado debe estar de acuerdo con E. La operación de suma (ver el Apéndice A) nos da una forma de unir las clasificaciones de los dos agentes del Canal E en una sola clasificación, a saber, A1 +A2, y también los dos infomorfismos juntosUn solo infomorfismo, F1 + F2: A1 + A2 → E. A1 + A2 ensambla clasificaciones de agentes de una manera muy gruesa.Tok (A1 + A2) es el producto cartesiano de Tok (A1) y Tok (A2), es decir, Tok (A1 + A2) = {P1, P2 |Pi ∈ Pi}, por lo que un token de A1 + A2 es un par de percepciones de agentes sin restricciones.Typ (A1 + A2) es la \"unión disjunta\" de TYP (A1) y TYP (A2), y P1, P2 es de tipo I, α si Pi es de tipo α.sindicato",
                "Ajuminamos la importancia para tomar la \"unión disjunta\" porque A1 y A2 podrían usar tipos idénticos con el propósito de describir sus respectivas percepciones de E. La clasificación A1 + A2 parece ser el lugar natural para buscar relaciones entre los tipos de agentes.sindicato"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "sum infomorphism": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the <br>sum infomorphism</br>, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Como dijimos anteriormente, el significado debe estar de acuerdo con el canal E. Las clasificaciones A1 + A2 y E están conectadas a través del \"Sum Infomorfismo\", F = F1 + F2, donde: • ˆf (i, α) = ˆFi (α) = ={e ∈ Tok (e) |Veri (e) | = ai α} para cada i, α ∈ Tip (a1 + a2) • ˇf (e) = ˇf1 (e), ˇf2 (e) = ver1 (e), ver2 (e) para cada e ∈Tok (e) Las restricciones significativas entre los tipos de agentes están de acuerdo con el canal E porque se calculan haciendo uso de F como exponemos a continuación.suma infomorfismo"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "constraint": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following <br>constraint</br>: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above <br>constraint</br> should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only <br>constraint</br> from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this <br>constraint</br> is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a <br>constraint</br> of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a <br>constraint</br> of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Esta solicitud se escribe a continuación como un mensaje KQML y debe interpretarse intuitivamente como: Robot2, recoja el paquete ubicado en mi cuadrado Up, sabiendo que está ubicado en mi cuadrado de la derecha.`Solicitud: remitente robot1: receptor robot2: paquetes de idiomas distribución-lenguaje: ontología urdl-ontología: contenido (recoger u (paquete) porque UR (robot2) ´ Figura 6: Asistencia de robot Robot2 Comprende el contenido de la solicitud y puede usarUna regla representada por la siguiente \"restricción\": 1, Ur (Robot2), 2, UL (Robot1), 1, U (paquete) 2, U (paquete) La \"restricción\" anterior debe interpretarse intuitivamente como: si Robot2 esSituado en Robot1, Robot1, Robot1 está situado en Robot2s Up-Left Square y un paquete se encuentra en Robot1s Up Square, luego un paquete se encuentra en Robot2S Up Square.",
                "La única \"restricción\" de las anteriores que permanece plausible según C2 es: 1, R (Robot2), 2, UL (Robot1), 1, C (paquete) C2 2, U (paquete) Observe que esta \"restricción\"es un elemento de la teoría de la lógica distribuida.restricción",
                "Una secuencia γ, Δ de σ para la cual γ δ se llama una \"restricción\" de la teoría T. T es regular si cumple: 1. Restricción",
                "L se completa si cada secuencia de típito (l) satisfecho por cada token normal es una \"restricción\" de th (l).restricción"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "information-channel refinement": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of <br>information-channel refinement</br>s in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of <br>information-channel refinement</br>s that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En particular, proporcionamos un modelo formal que formaliza la alineación semántica situada como una secuencia de \"refinamiento de canales de información\" en el sentido de la teoría de la teoría de la información de Barrawis y Seligmans [1].refinamiento de canal de información",
                "Conclusiones y trabajo adicional En este documento hemos expuesto un modelo formal de alineación semántica como una secuencia de \"refinamiento de canales de información\" que son relativos a los estados particulares del entorno en el que dos agentes se comunican y alinean sus respectivas conceptualizaciones de estos estados de estos estados.refinamiento de canal de información"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "distributed logic": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the <br>distributed logic</br> generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this <br>distributed logic</br>.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of <br>distributed logic</br> (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the <br>distributed logic</br>, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the <br>distributed logic</br>.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the <br>distributed logic</br>.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "<br>distributed logic</br>: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the <br>distributed logic</br> of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Primero, en la Sección 2.1 asociamos un canal al escenario en consideración y mostramos cómo la \"lógica distribuida\" generada por este canal proporciona las relaciones lógicas entre los puntos de vista de los agentes del entorno.lógica distribuida",
                "En segundo lugar, en la Sección 2.2 presentamos un método por el cual los agentes obtienen aproximaciones de esta \"lógica distribuida\".lógica distribuida",
                "Tan importante como la noción de canal es el concepto de \"lógica distribuida\" (ver Apéndice A).lógica distribuida",
                "Dado un canal C y una lógica L en su núcleo, Dlogc (l) representa el razonamiento sobre las relaciones entre los componentes de C justificados por L. Si L = log (c), la \"lógica distribuida\", denotamos por log (c), captura de manera lógica el flujo de información inherente al canal.lógica distribuida",
                "La única restricción de las anteriores que permanece plausible según C2 es: 1, R (Robot2), 2, UL (Robot1), 1, C (Paquete) C2 2, U (paquete) Observe que esta restricción es un elemento deLa teoría de la \"lógica distribuida\".lógica distribuida",
                "Los agentes se comunican para cooperar con éxito y el éxito se garantiza utilizando restricciones de la \"lógica distribuida\".lógica distribuida",
                "\"Lógica distribuida\": Sea c = {fi: ai → c} i∈ {1,2} un canal y l una lógica local en su núcleo c, la \"lógica distribuida\" de c generada por l, dlogc (l escrita (l), es la imagen inversa de L bajo la suma F1 + F2.lógica distribuida"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "semantic alignment": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated <br>semantic alignment</br> Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the <br>semantic alignment</br> of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a <br>semantic alignment</br> procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a <br>semantic alignment</br> of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that <br>semantic alignment</br> of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their <br>semantic alignment</br>, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated <br>semantic alignment</br> as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the <br>semantic alignment</br> that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated <br>semantic alignment</br> (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of <br>semantic alignment</br> as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise <br>semantic alignment</br> using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to <br>semantic alignment</br>, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines <br>semantic alignment</br> in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated <br>semantic alignment</br> problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "Un modelo formal para la \"alineación semántica\" situada Manuel Atencia Marco Schorlemmer IIIA, Instituto de Investigación de Inteligencia Artificial CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, España {Manu, Marcott@iiia.csic.es Resumen Ontology La coincidencia actualmente es actualmente unTecnología clave para lograr la \"alineación semántica\" de las entidades ontológicas utilizadas por las aplicaciones basadas en el conocimiento y, por lo tanto, para permitir su interoperabilidad en entornos distribuidos como sistemas multiagentes.alineación semántica",
                "En este artículo, presentamos un modelo formal para un procedimiento de \"alineación semántica\" que alinea incrementalmente conceptualizaciones de dos o más agentes en relación con su percepción respectiva del entorno o dominio en el que están actuando. Alineación semántica.",
                "Pero la proliferación de muchas ontologías diversas causadas por diferentes conceptualizaciones de incluso el mismo dominio, y su especificación posterior utilizando una terminología variable, ha resaltado la necesidad de técnicas de coincidencia ontológica que son capaces de calcular relaciones semánticas entre entidades de ontologías de ingeniería por separado.[5, 11] Hasta hace poco, la mayoría de los mecanismos de coincidencia de ontología desarrollados hasta ahora han adoptado un enfoque funcional clásico para el problema de heterogeneidad semántica, en el que la coincidencia de ontología se ve como un proceso que toma dos o más ontologías como entrada y produce una \"alineación semántica\"de entidades ontológicas como salida [3].alineación semántica",
                "Afirmamos que la \"alineación semántica\" de la terminología ontológica es, en última instancia, en relación con la situación particular en la que se lleva a cabo la alineación, y que esta situación debe ser explícita y llevada al mecanismo de alineación.alineación semántica",
                "Sin embargo, su \"alineación semántica\" solo será válida en el alcance de su interacción dentro de esta situación o entorno particular.alineación semántica",
                "En particular, proporcionamos un modelo formal que formaliza la \"alineación semántica\" situada como una secuencia de refinamientos de canales de información en el sentido de la teoría del flujo de información de Barwise y Seligmans [1].alineación semántica",
                "Análogamente, la \"alineación semántica\" que permitirá que la información fluya en última instancia será llevada a cabo por la situación particular en la que actúan los agentes.",
                "En la Sección 2 describimos nuestro modelo formal para la \"alineación semántica\" situada (SSA).alineación semántica",
                "Conclusiones y trabajo adicional En este documento hemos expuesto un modelo formal de \"alineación semántica\" como una secuencia de refinamientos de canales de información que son relativos a los estados particulares del entorno en el que dos agentes se comunican y alinean sus respectivas conceptualizaciones de estos estados.alineación semántica",
                "Antes de nosotros, Kent [6] y Kalfoglou y Schorlemmer [4, 10] han aplicado la teoría del canal para formalizar la \"alineación semántica\" utilizando también la visión de Barwise y Seligmans para centrarse en los tokens como facilitadores de flujo de información.alineación semántica"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "distribute logic": {
            "translated_key": "",
            "is_in_text": false,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-channel refinements in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-channel refinements that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [],
            "translated_text": "",
            "candidates": [],
            "error": []
        },
        "channel refinement": {
            "translated_key": "",
            "is_in_text": true,
            "original_annotated_sentences": [
                "A Formal Model for Situated Semantic Alignment Manuel Atencia Marco Schorlemmer IIIA, Artificial Intelligence Research Institute CSIC, Spanish National Research Council Bellaterra (Barcelona), Catalonia, Spain {manu, marco}@iiia.csic.es ABSTRACT Ontology matching is currently a key technology to achieve the semantic alignment of ontological entities used by knowledge-based applications, and therefore to enable their interoperability in distributed environments such as multiagent systems.",
                "Most ontology matching mechanisms, however, assume matching prior integration and rely on semantics that has been coded a priori in concept hierarchies or external sources.",
                "In this paper, we present a formal model for a semantic alignment procedure that incrementally aligns differing conceptualisations of two or more agents relative to their respective perception of the environment or domain they are acting in.",
                "It hence makes the situation in which the alignment occurs explicit in the model.",
                "We resort to Channel Theory to carry out the formalisation.",
                "Categories and Subject Descriptors I.2.11 [Artificial Intelligence]: Distributed Artificial Intelligence-coherence and coordination, multiagent systems; D.2.12 [Software Engineering]: Interoperability-data mapping; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-semantic networks, relation systems.",
                "General Terms Theory 1.",
                "INTRODUCTION An ontology is commonly defined as a specification of the conceptualisation of a particular domain.",
                "It fixes the vocabulary used by knowledge engineers to denote concepts and their relations, and it constrains the interpretation of this vocabulary to the meaning originally intended by knowledge engineers.",
                "As such, ontologies have been widely adopted as a key technology that may favour knowledge sharing in distributed environments, such as multi-agent systems, federated databases, or the Semantic Web.",
                "But the proliferation of many diverse ontologies caused by different conceptualisations of even the same domain -and their subsequent specification using varying terminology- has highlighted the need of ontology matching techniques that are capable of computing semantic relationships between entities of separately engineered ontologies. [5, 11] Until recently, most ontology matching mechanisms developed so far have taken a classical functional approach to the semantic heterogeneity problem, in which ontology matching is seen as a process taking two or more ontologies as input and producing a semantic alignment of ontological entities as output [3].",
                "Furthermore, matching often has been carried out at design-time, before integrating knowledge-based systems or making them interoperate.",
                "This might have been successful for clearly delimited and stable domains and for closed distributed systems, but it is untenable and even undesirable for the kind of applications that are currently deployed in open systems.",
                "Multi-agent communication, peer-to-peer information sharing, and webservice composition are all of a decentralised, dynamic, and open-ended nature, and they require ontology matching to be locally performed during run-time.",
                "In addition, in many situations peer ontologies are not even open for inspection (e.g., when they are based on commercially confidential information).",
                "Certainly, there exist efforts to efficiently match ontological entities at run-time, taking only those ontology fragment that are necessary for the task at hand [10, 13, 9, 8].",
                "Nevertheless, the techniques used by these systems to establish the semantic relationships between ontological entities -even though applied at run-time- still exploit a priori defined concept taxonomies as they are represented in the graph-based structures of the ontologies to be matched, use previously existing external sources such as thesauri (e.g., WordNet) and upper-level ontologies (e.g., CyC or SUMO), or resort to additional background knowledge repositories or shared instances.",
                "We claim that semantic alignment of ontological terminology is ultimately relative to the particular situation in which the alignment is carried out, and that this situation should be made explicit and brought into the alignment mechanism.",
                "Even two agents with identical conceptualisation capabilities, and using exactly the same vocabulary to specify their respective conceptualisations may fail to interoperate 1278 978-81-904262-7-5 (RPS) c 2007 IFAAMAS in a concrete situation because of their differing perception of the domain.",
                "Imagine a situation in which two agents are facing each other in front of a checker board.",
                "Agent A1 may conceptualise a figure on the board as situated on the left margin of the board, while agent A2 may conceptualise the same figure as situated on the right.",
                "Although the conceptualisation of left and right is done in exactly the same manner by both agents, and even if both use the terms left and right in their communication, they still will need to align their respective vocabularies if they want to successfully communicate to each other actions that change the position of figures on the checker board.",
                "Their semantic alignment, however, will only be valid in the scope of their interaction within this particular situation or environment.",
                "The same agents situated differently may produce a different alignment.",
                "This scenario is reminiscent to those in which a group of distributed agents adapt to form an ontology and a shared lexicon in an emergent, bottom-up manner, with only local interactions and no central control authority [12].",
                "This sort of self-organised emergence of shared meaning is namely ultimately grounded on the physical interaction of agents with the environment.",
                "In this paper, however, we address the case in which agents are already endowed with a top-down engineered ontology (it can even be the same one), which they do not adapt or refine, but for which they want to find the semantic relationships with separate ontologies of other agents on the grounds of their communication within a specific situation.",
                "In particular, we provide a formal model that formalises situated semantic alignment as a sequence of information-<br>channel refinement</br>s in the sense of Barwise and Seligmans theory of information flow [1].",
                "This theory is particularly useful for our endeavour because it models the flow of information occurring in distributed systems due to the particular situations -or tokens- that carry information.",
                "Analogously, the semantic alignment that will allow information to flow ultimately will be carried by the particular situation agents are acting in.",
                "We shall therefore consider a scenario with two or more agents situated in an environment.",
                "Each agent will have its own viewpoint of the environment so that, if the environment is in a concrete state, both agents may have different perceptions of this state.",
                "Because of these differences there may be a mismatch in the meaning of the syntactic entities by which agents describe their perceptions (and which constitute the agents respective ontologies).",
                "We state that these syntactic entities can be related according to the intrinsic semantics provided by the existing relationship between the agents viewpoint of the environment.",
                "The existence of this relationship is precisely justified by the fact that the agents are situated and observe the same environment.",
                "In Section 2 we describe our formal model for Situated Semantic Alignment (SSA).",
                "First, in Section 2.1 we associate a channel to the scenario under consideration and show how the distributed logic generated by this channel provides the logical relationships between the agents viewpoints of the environment.",
                "Second, in Section 2.2 we present a method by which agents obtain approximations of this distributed logic.",
                "These approximations gradually become more reliable as the method is applied.",
                "In Section 3 we report on an application of our method.",
                "Conclusions and further work are analyzed in Section 4.",
                "Finally, an appendix summarizes the terms and theorems of Channel theory used along the paper.",
                "We do not assume any knowledge of Channel Theory; we restate basic definitions and theorems in the appendix, but any detailed exposition of the theory is outside the scope of this paper. 2.",
                "A FORMAL MODEL FOR SSA 2.1 The Logic of SSA Consider a scenario with two agents A1 and A2 situated in an environment E (the generalization to any numerable set of agents is straightforward).",
                "We associate a numerable set S of states to E and, at any given instant, we suppose E to be in one of these states.",
                "We further assume that each agent is able to observe the environment and has its own perception of it.",
                "This ability is faithfully captured by a surjective function seei : S → Pi, where i ∈ {1, 2}, and typically see1 and see2 are different.",
                "According to Channel Theory, information is only viable where there is a systematic way of classifying some range of things as being this way or that, in other words, where there is a classification (see appendix A).",
                "So in order to be within the framework of Channel Theory, we must associate classifications to the components of our system.",
                "For each i ∈ {1, 2}, we consider a classification Ai that models Ais viewpoint of E. First, tok(Ai) is composed of Ais perceptions of E states, that is, tok(Ai) = Pi.",
                "Second, typ(Ai) contains the syntactic entities by which Ai describes its perceptions, the ones constituting the ontology of Ai.",
                "Finally, |=Ai synthesizes how Ai relates its perceptions with these syntactic entities.",
                "Now, with the aim of associating environment E with a classification E we choose the power classification of S as E, which is the classification whose set of types is equal to 2S , whose tokens are the elements of S, and for which a token e is of type ε if e ∈ ε.",
                "The reason for taking the power classification is because there are no syntactic entities that may play the role of types for E since, in general, there is no global conceptualisation of the environment.",
                "However, the set of types of the power classification includes all possible token configurations potentially described by types.",
                "Thus tok(E) = S, typ(E) = 2S and e |=E ε if and only if e ∈ ε.",
                "The notion of channel (see appendix A) is fundamental in Barwise and Seligmans theory.",
                "The information flow among the components of a distributed system is modelled in terms of a channel and the relationships among these components are expressed via infomorphisms (see appendix A) which provide a way of moving information between them.",
                "The information flow of the scenario under consideration is accurately described by channel E = {fi : Ai → E}i∈{1,2} defined as follows: • ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each α ∈ typ(Ai) • ˇfi(e) = seei(e) for each e ∈ tok(E) where i ∈ {1, 2}.",
                "Definition of ˇfi seems natural while ˆfi is defined in such a way that the fundamental property of the infomorphisms is fulfilled: ˇfi(e) |=Ai α iff seei(e) |=Ai α (by definition of ˇfi) iff e ∈ ˆfi(α) (by definition of ˆfi) iff e |=E ˆfi(α) (by definition of |=E) The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1279 Consequently, E is the core of channel E and a state e ∈ tok(E) connects agents perceptions ˇf1(e) and ˇf2(e) (see Figure 1). typ(E) typ(A1) ˆf1 99ttttttttt typ(A2) ˆf2 eeJJJJJJJJJ tok(E) |=E \u001f \u001f \u001f \u001f \u001f \u001f \u001f ˇf1yyttttttttt ˇf2 %%JJJJJJJJJ tok(A1) |=A1 \u001f \u001f \u001f \u001f \u001f \u001f \u001f tok(A2) |=A2 \u001f \u001f \u001f \u001f \u001f \u001f \u001f Figure 1: Channel E E explains the information flow of our scenario by virtue of agents A1 and A2 being situated and perceiving the same environment E. We want to obtain meaningful relations among agents syntactic entities, that is, agents types.",
                "We state that meaningfulness must be in accord with E. The sum operation (see appendix A) gives us a way of putting the two agents classifications of channel E together into a single classification, namely A1 +A2, and also the two infomorphisms together into a single infomorphism, f1 +f2 : A1 + A2 → E. A1 + A2 assembles agents classifications in a very coarse way. tok(A1 + A2) is the cartesian product of tok(A1) and tok(A2), that is, tok(A1 + A2) = { p1, p2 | pi ∈ Pi}, so a token of A1 + A2 is a pair of agents perceptions with no restrictions. typ(A1 + A2) is the disjoint union of typ(A1) and typ(A2), and p1, p2 is of type i, α if pi is of type α.",
                "We attach importance to take the disjoint union because A1 and A2 could use identical types with the purpose of describing their respective perceptions of E. Classification A1 + A2 seems to be the natural place in which to search for relations among agents types.",
                "Now, Channel Theory provides a way to make all these relations explicit in a logical fashion by means of theories and local logics (see appendix A).",
                "The theory generated by the sum classification, Th(A1 + A2), and hence its logic generated, Log(A1 + A2), involve all those constraints among agents types valid according to A1 +A2.",
                "Notice however that these constraints are obvious.",
                "As we stated above, meaningfulness must be in accord with channel E. Classifications A1 + A2 and E are connected via the sum infomorphism, f = f1 + f2, where: • ˆf( i, α ) = ˆfi(α) = {e ∈ tok(E) | seei(e) |=Ai α} for each i, α ∈ typ(A1 + A2) • ˇf(e) = ˇf1(e), ˇf2(e) = see1(e), see2(e) for each e ∈ tok(E) Meaningful constraints among agents types are in accord with channel E because they are computed making use of f as we expound below.",
                "As important as the notion of channel is the concept of distributed logic (see appendix A).",
                "Given a channel C and a logic L on its core, DLogC(L) represents the reasoning about relations among the components of C justified by L. If L = Log(C), the distributed logic, we denoted by Log(C), captures in a logical fashion the information flow inherent in the channel.",
                "In our case, Log(E) explains the relationship between the agents viewpoints of the environment in a logical fashion.",
                "On the one hand, constraints of Th(Log(E)) are defined by: Γ Log(E) Δ if ˆf[Γ] Log(E) ˆf[Δ] (1) where Γ, Δ ⊆ typ(A1 + A2).",
                "On the other hand, the set of normal tokens, NLog(E), is equal to the range of function ˇf: NLog(E) = ˇf[tok(E)] = { see1(e), see2(e) | e ∈ tok(E)} Therefore, a normal token is a pair of agents perceptions that are restricted by coming from the same environment state (unlike A1 + A2 tokens).",
                "All constraints of Th(Log(E)) are satisfied by all normal tokens (because of being a logic).",
                "In this particular case, this condition is also sufficient (the proof is straightforward); as alternative to (1) we have: Γ Log(E) Δ iff for all e ∈ tok(E), if (∀ i, γ ∈ Γ)[seei(e) |=Ai γ] then (∃ j, δ ∈ Δ)[seej(e) |=Aj δ] (2) where Γ, Δ ⊆ typ(A1 + A2).",
                "Log(E) is the logic of SSA.",
                "Th(Log(E)) comprises the most meaningful constraints among agents types in accord with channel E. In other words, the logic of SSA contains and also justifies the most meaningful relations among those syntactic entities that agents use in order to describe their own environment perceptions.",
                "Log(E) is complete since Log(E) is complete but it is not necessarily sound because although Log(E) is sound, ˇf is not surjective in general (see appendix B).",
                "If Log(E) is also sound then Log(E) = Log(A1 +A2) (see appendix B).",
                "That means there is no significant relation between agents points of view of the environment according to E. It is just the fact that Log(E) is unsound what allows a significant relation between the agents viewpoints.",
                "This relation is expressed at the type level in terms of constraints by Th(Log(E)) and at the token level by NLog(E). 2.2 Approaching the logic of SSA through communication We have dubbed Log(E) the logic of SSA.",
                "Th(Log(E)) comprehends the most meaningful constraints among agents types according to E. The problem is that neither agent can make use of this theory because they do not know E completely.",
                "In this section, we present a method by which agents obtain approximations to Th(Log(E)).",
                "We also prove these approximations gradually become more reliable as the method is applied.",
                "Agents can obtain approximations to Th(Log(E)) through communication.",
                "A1 and A2 communicate by exchanging information about their perceptions of environment states.",
                "This information is expressed in terms of their own classification relations.",
                "Specifically, if E is in a concrete state e, we assume that agents can convey to each other which types are satisfied by their respective perceptions of e and which are not.",
                "This exchange generates a channel C = {fi : Ai → 1280 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) C}i∈{1,2} and Th(Log(C)) contains the constraints among agents types justified by the fact that agents have observed e. Now, if E turns to another state e and agents proceed as before, another channel C = {fi : Ai → C }i∈{1,2} gives account of the new situation considering also the previous information.",
                "Th(Log(C )) comprises the constraints among agents types justified by the fact that agents have observed e and e .",
                "The significant point is that C is a refinement of C (see appendix A).",
                "Theorem 2.1 below ensures that the refined channel involves more reliable information.",
                "The communication supposedly ends when agents have observed all the environment states.",
                "Again this situation can be modeled by a channel, call it C∗ = {f∗ i : Ai → C∗ }i∈{1,2}.",
                "Theorem 2.2 states that Th(Log(C∗ )) = Th(Log(E)).",
                "Theorem 2.1 and Theorem 2.2 assure that applying the method agents can obtain approximations to Th(Log(E)) gradually more reliable.",
                "Theorem 2.1.",
                "Let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels.",
                "If C is a refinement of C then: 1.",
                "Th(Log(C )) ⊆ Th(Log(C)) 2.",
                "NLog(C ) ⊇ NLog(C) Proof.",
                "Since C is a refinement of C then there exists a refinement infomorphism r from C to C; so fi = r ◦ fi .",
                "Let A =def A1 + A2, f =def f1 + f2 and f =def f1 + f2. 1.",
                "Let Γ and Δ be subsets of typ(A) and assume that Γ Log(C ) Δ, which means ˆf [Γ] C ˆf [Δ].",
                "We have to prove Γ Log(C) Δ, or equivalently, ˆf[Γ] C ˆf[Δ].",
                "We proceed by reductio ad absurdum.",
                "Suppose c ∈ tok(C) does not satisfy the sequent ˆf[Γ], ˆf[Δ] .",
                "Then c |=C ˆf(γ) for all γ ∈ Γ and c |=C ˆf(δ) for all δ ∈ Δ.",
                "Let us choose an arbitrary γ ∈ Γ.",
                "We have that γ = i, α for some α ∈ typ(Ai) and i ∈ {1, 2}.",
                "Thus ˆf(γ) = ˆf( i, α ) = ˆfi(α) = ˆr ◦ ˆfi (α) = ˆr( ˆfi (α)).",
                "Therefore: c |=C ˆf(γ) iff c |=C ˆr( ˆfi (α)) iff ˇr(c) |=C ˆfi (α) iff ˇr(c) |=C ˆf ( i, α ) iff ˇr(c) |=C ˆf (γ) Consequently, ˇr(c) |=C ˆf (γ) for all γ ∈ Γ.",
                "Since ˆf [Γ] C ˆf [Δ] then there exists δ∗ ∈ Δ such that ˇr(c) |=C ˆf (δ∗ ).",
                "A sequence of equivalences similar to the above one justifies c |=C ˆf(δ∗ ), contradicting that c is a counterexample to ˆf[Γ], ˆf[Δ] .",
                "Hence Γ Log(C) Δ as we wanted to prove. 2.",
                "Let a1, a2 ∈ tok(A) and assume a1, a2 ∈ NLog(C).",
                "Therefore, there exists c token in C such that a1, a2 = ˇf(c).",
                "Then we have ai = ˇfi(c) = ˇfi ◦ ˇr(c) = ˇfi (ˇr(c)), for i ∈ {1, 2}.",
                "Hence a1, a2 = ˇf (ˇr(c)) and a1, a2 ∈ NLog(C ).",
                "Consequently, NLog(C ) ⊇ NLog(C) which concludes the proof.",
                "Remark 2.1.",
                "Theorem 2.1 asserts that the more refined channel gives more reliable information.",
                "Even though its theory has less constraints, it has more normal tokens to which they apply.",
                "In the remainder of the section, we explicitly describe the process of communication and we conclude with the proof of Theorem 2.2.",
                "Let us assume that typ(Ai) is finite for i ∈ {1, 2} and S is infinite numerable, though the finite case can be treated in a similar form.",
                "We also choose an infinite numerable set of symbols {cn | n ∈ N}1 .",
                "We omit informorphisms superscripts when no confusion arises.",
                "Types are usually denoted by greek letters and tokens by latin letters so if f is an infomorphism, f(α) ≡ ˆf(α) and f(a) ≡ ˇf(a).",
                "Agents communication starts from the observation of E. Let us suppose that E is in state e1 ∈ S = tok(E).",
                "A1s perception of e1 is f1(e1 ) and A2s perception of e1 is f2(e1 ).",
                "We take for granted that A1 can communicate A2 those types that are and are not satisfied by f1(e1 ) according to its classification A1.",
                "So can A2 do.",
                "Since both typ(A1) and typ(A2) are finite, this process eventually finishes.",
                "After this communication a channel C1 = {f1 i : Ai → C1 }i=1,2 arises (see Figure 2).",
                "C1 A1 f1 1 ==|||||||| A2 f1 2 aaCCCCCCCC Figure 2: The first communication stage On the one hand, C1 is defined by: • tok(C1 ) = {c1 } • typ(C1 ) = typ(A1 + A2) • c1 |=C1 i, α if fi(e1 ) |=Ai α (for every i, α ∈ typ(A1 + A2)) On the other hand, f1 i , with i ∈ {1, 2}, is defined by: • f1 i (α) = i, α (for every α ∈ typ(Ai)) • f1 i (c1 ) = fi(e1 ) Log(C1 ) represents the reasoning about the first stage of communication.",
                "It is easy to prove that Th(Log(C1 )) = Th(C1 ).",
                "The significant point is that both agents know C1 as the result of the communication.",
                "Hence they can compute separately theory Th(C1 ) = typ(C1 ), C1 which contains the constraints among agents types justified by the fact that agents have observed e1 .",
                "Now, let us assume that E turns to a new state e2 .",
                "Agents can proceed as before, exchanging this time information about their perceptions of e2 .",
                "Another channel C2 = {f2 i : Ai → C2 }i∈{1,2} comes up.",
                "We define C2 so as to take also into account the information provided by the previous stage of communication.",
                "On the one hand, C2 is defined by: • tok(C2 ) = {c1 , c2 } 1 We write these symbols with superindices because we limit the use of subindices for what concerns to agents.",
                "Note this set is chosen with the same cardinality of S. The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1281 • typ(C2 ) = typ(A1 + A2) • ck |=C2 i, α if fi(ek ) |=Ai α (for every k ∈ {1, 2} and i, α ∈ typ(A1 + A2)) On the other hand, f2 i , with i ∈ {1, 2}, is defined by: • f2 i (α) = i, α (for every α ∈ typ(Ai)) • f2 i (ck ) = fi(ek ) (for every k ∈ {1, 2}) Log(C2 ) represents the reasoning about the former and the later communication stages.",
                "Th(Log(C2 )) is equal to Th(C2 ) = typ(C2 ), C2 , then it contains the constraints among agents types justified by the fact that agents have observed e1 and e2 .",
                "A1 and A2 knows C2 so they can use these constraints.",
                "The key point is that channel C2 is a refinement of C1 .",
                "It is easy to check that f1 defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism (see at the bottom of Figure 3).",
                "By Theorem 2.1, C2 constraints are more reliable than C1 constraints.",
                "In the general situation, once the states e1 , e2 , . . . , en−1 (n ≥ 2) have been observed and a new state en appears, channel Cn = {fn i : Ai → Cn }i∈{1,2} informs about agents communication up to that moment.",
                "Cn definition is similar to the previous ones and analogous remarks can be made (see at the top of Figure 3).",
                "Theory Th(Log(Cn )) = Th(Cn ) = typ(Cn ), Cn contains the constraints among agents types justified by the fact that agents have observed e1 , e2 , . . . , en .",
                "Cn fn−1 \u0015\u0015 A1 fn−1 1 99PPPPPPPPPPPPP fn 1 UUnnnnnnnnnnnnn f2 1 %%44444444444444444444444444 f1 1 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, A2 fn 2 ggPPPPPPPPPPPPP fn−1 2 wwnnnnnnnnnnnnn f2 2 ÕÕ              f1 2 ØØ\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012\u0012 Cn−1 \u0015\u0015 . . . \u0015\u0015 C2 f1 \u0015\u0015 C1 Figure 3: Agents communication Remember we have assumed that S is infinite numerable.",
                "It is therefore unpractical to let communication finish when all environment states have been observed by A1 and A2.",
                "At that point, the family of channels {Cn }n∈N would inform of all the communication stages.",
                "It is therefore up to the agents to decide when to stop communicating should a good enough approximation have been reached for the purposes of their respective tasks.",
                "But the study of possible termination criteria is outside the scope of this paper and left for future work.",
                "From a theoretical point of view, however, we can consider the channel C∗ = {f∗ i : Ai → C∗ }i∈{1,2} which informs of the end of the communication after observing all environment states.",
                "On the one hand, C∗ is defined by: • tok(C∗ ) = {cn | n ∈ N} • typ(C∗ ) = typ(A1 + A2) • cn |=C∗ i, α if fi(en ) |=Ai α (for n ∈ N and i, α ∈ typ(A1 + A2)) On the other hand, f∗ i , with i ∈ {1, 2}, is defined by: • f∗ i (α) = i, α (for α ∈ typ(Ai)) • f∗ i (cn ) = fi(en ) (for n ∈ N) Theorem below constitutes the cornerstone of the model exposed in this paper.",
                "It ensures, together with Theorem 2.1, that at each communication stage agents obtain a theory that approximates more closely to the theory generated by the logic of SSA.",
                "Theorem 2.2.",
                "The following statements hold: 1.",
                "For all n ∈ N, C∗ is a refinement of Cn . 2.",
                "Th(Log(E)) = Th(C∗ ) = Th(Log(C∗ )).",
                "Proof. 1.",
                "It is easy to prove that for each n ∈ N, gn defined as the identity function on types and the inclusion function on tokens is a refinement infomorphism from C∗ to Cn . 2.",
                "The second equality is straightforward; the first one follows directly from: cn |=C∗ i, α iff ˇfi(en ) |=Ai α (by definition of |=C∗ ) iff en |=E ˆfi(α) (because fi is infomorphim) iff en |=E ˆf( i, α ) (by definition of ˆf) E C∗ gn \u0015\u0015 A1 fn 1 99OOOOOOOOOOOOO f∗ 1 UUooooooooooooo f1 cc A2 f∗ 2 ggOOOOOOOOOOOOO fn 2 wwooooooooooooo f2 ?????????????????",
                "Cn 1282 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 3.",
                "AN EXAMPLE In the previous section we have described in great detail our formal model for SSA.",
                "However, we have not tackled the practical aspect of the model yet.",
                "In this section, we give a brushstroke of the pragmatic view of our approach.",
                "We study a very simple example and explain how agents can use those approximations of the logic of SSA they can obtain through communication.",
                "Let us reflect on a system consisting of robots located in a two-dimensional grid looking for packages with the aim of moving them to a certain destination (Figure 4).",
                "Robots can carry only one package at a time and they can not move through a package.",
                "Figure 4: The scenario Robots have a partial view of the domain and there exist two kinds of robots according to the visual field they have.",
                "Some robots are capable of observing the eight adjoining squares but others just observe the three squares they have in front (see Figure 5).",
                "We call them URDL (shortened form of Up-Right-Down-Left) and LCR (abbreviation for Left-Center-Right) robots respectively.",
                "Describing the environment states as well as the robots perception functions is rather tedious and even unnecessary.",
                "We assume the reader has all those descriptions in mind.",
                "All robots in the system must be able to solve package distribution problems cooperatively by communicating their intentions to each other.",
                "In order to communicate, agents send messages using some ontology.",
                "In our scenario, there coexist two ontologies, the UDRL and LCR ontologies.",
                "Both of them are very simple and are just confined to describe what robots observe.",
                "Figure 5: Robots field of vision When a robot carrying a package finds another package obstructing its way, it can either go around it or, if there is another robot in its visual field, ask it for assistance.",
                "Let us suppose two URDL robots are in a situation like the one depicted in Figure 6.",
                "Robot1 (the one carrying a package) decides to ask Robot2 for assistance and sends a request.",
                "This request is written below as a KQML message and it should be interpreted intuitively as: Robot2, pick up the package located in my Up square, knowing that you are located in my Up-Right square. ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology URDL-ontology :content (pick up U(Package) because UR(Robot2) ´ Figure 6: Robot assistance Robot2 understands the content of the request and it can use a rule represented by the following constraint: 1, UR(Robot2) , 2, UL(Robot1) , 1, U(Package) 2, U(Package) The above constraint should be interpreted intuitively as: if Robot2 is situated in Robot1s Up-Right square, Robot1 is situated in Robot2s Up-Left square and a package is located in Robot1s Up square, then a package is located in Robot2s Up square.",
                "Now, problems arise when a LCR robot and a URDL robot try to interoperate.",
                "See Figure 7.",
                "Robot1 sends a request of the form: ` request :sender Robot1 :receiver Robot2 :language Packages distribution-language :ontology LCR-ontology :content (pick up R(Robot2) because C(Package) ´ Robot2 does not understand the content of the request but they decide to begin a process of alignment -corresponding with a channel C1 .",
                "Once finished, Robot2 searches in Th(C1 ) for constraints similar to the expected one, that is, those of the form: 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, λ(Package) where λ ∈ {U, R, D, L, UR, DR, DL, UL}.",
                "From these, only the following constraints are plausible according to C1 : The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1283 Figure 7: Ontology mismatch 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, U(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, L(Package) 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C1 2, DR(Package) If subsequently both robots adopting the same roles take part in a situation like the one depicted in Figure 8, a new process of alignment -corresponding with a channel C2 - takes place.",
                "C2 also considers the previous information and hence refines C1 .",
                "The only constraint from the above ones that remains plausible according to C2 is : 1, R(Robot2) , 2, UL(Robot1) , 1, C(Package) C2 2, U(Package) Notice that this constraint is an element of the theory of the distributed logic.",
                "Agents communicate in order to cooperate successfully and success is guaranteed using constrains of the distributed logic.",
                "Figure 8: Refinement 4.",
                "CONCLUSIONS AND FURTHER WORK In this paper we have exposed a formal model of semantic alignment as a sequence of information-<br>channel refinement</br>s that are relative to the particular states of the environment in which two agents communicate and align their respective conceptualisations of these states.",
                "Before us, Kent [6] and Kalfoglou and Schorlemmer [4, 10] have applied Channel Theory to formalise semantic alignment using also Barwise and Seligmans insight to focus on tokens as the enablers of information flow.",
                "Their approach to semantic alignment, however, like most ontology matching mechanisms developed to date (regardless of whether they follow a functional, design-time-based approach, or an interaction-based, runtime-based approach), still defines semantic alignment in terms of a priori design decisions such as the concept taxonomy of the ontologies or the external sources brought into the alignment process.",
                "Instead the model we have presented in this paper makes explicit the particular states of the environment in which agents are situated and are attempting to gradually align their ontological entities.",
                "In the future, our effort will focus on the practical side of the situated semantic alignment problem.",
                "We plan to further refine the model presented here (e.g., to include pragmatic issues such as termination criteria for the alignment process) and to devise concrete ontology negotiation protocols based on this model that agents may be able to enact.",
                "The formal model exposed in this paper will constitute a solid base of future practical results.",
                "Acknowledgements This work is supported under the UPIC project, sponsored by Spains Ministry of Education and Science under grant number TIN2004-07461-C02- 02 and also under the OpenKnowledge Specific Targeted Research Project (STREP), sponsored by the European Commission under contract number FP6-027253.",
                "Marco Schorlemmer is supported by a Ram´on y Cajal Research Fellowship from Spains Ministry of Education and Science, partially funded by the European Social Fund. 5.",
                "REFERENCES [1] J. Barwise and J. Seligman.",
                "Information Flow: The Logic of Distributed Systems.",
                "Cambridge University Press, 1997. [2] C. Ghidini and F. Giunchiglia.",
                "Local models semantics, or contextual reasoning = locality + compatibility.",
                "Artificial Intelligence, 127(2):221-259, 2001. [3] F. Giunchiglia and P. Shvaiko.",
                "Semantic matching.",
                "The Knowledge Engineering Review, 18(3):265-280, 2004. [4] Y. Kalfoglou and M. Schorlemmer.",
                "IF-Map: An ontology-mapping method based on information-flow theory.",
                "In Journal on Data Semantics I, LNCS 2800, 2003. [5] Y. Kalfoglou and M. Schorlemmer.",
                "Ontology mapping: The sate of the art.",
                "The Knowledge Engineering Review, 18(1):1-31, 2003. [6] R. E. Kent.",
                "Semantic integration in the Information Flow Framework.",
                "In Semantic Interoperability and Integration, Dagstuhl Seminar Proceedings 04391, 2005. [7] D. Lenat.",
                "CyC: A large-scale investment in knowledge infrastructure.",
                "Communications of the ACM, 38(11), 1995. [8] V. L´opez, M. Sabou, and E. Motta.",
                "PowerMap: Mapping the real Semantic Web on the fly.",
                "Proceedings of the ISWC06, 2006. [9] F. McNeill.",
                "Dynamic Ontology Refinement.",
                "PhD 1284 The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) thesis, School of Informatics, The University of Edinburgh, 2006. [10] M. Schorlemmer and Y. Kalfoglou.",
                "Progressive ontology alignment for meaning coordination: An information-theoretic foundation.",
                "In 4th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2005. [11] P. Shvaiko and J. Euzenat.",
                "A survey of schema-based matching approaches.",
                "In Journal on Data Semantics IV, LNCS 3730, 2005. [12] L. Steels.",
                "The Origins of Ontologies and Communication Conventions in Multi-Agent Systems.",
                "In Journal of Autonomous Agents and Multi-Agent Systems, 1(2), 169-194, 1998. [13] J. van Diggelen et al.",
                "ANEMONE: An Effective Minimal Ontology Negotiation Environment In 5th Int.",
                "Joint Conf. on Autonomous Agents and Multiagent Systems, 2006 APPENDIX A.",
                "CHANNEL THEORY TERMS Classification: is a tuple A = tok(A), typ(A), |=A where tok(A) is a set of tokens, typ(A) is a set of types and |=A is a binary relation between tok(A) and typ(A).",
                "If a |=A α then a is said to be of type α. Infomorphism: f : A → B from classifications A to B is a contravariant pair of functions f = ˆf, ˇf , where ˆf : typ(A) → typ(B) and ˇf : tok(B) → tok(A), satisfying the following fundamental property: ˇf(b) |=A α iff b |=B ˆf(α) for each token b ∈ tok(B) and each type α ∈ typ(A).",
                "Channel: consists of two infomorphisms C = {fi : Ai → C}i∈{1,2} with a common codomain C, called the core of C. C tokens are called connections and a connection c is said to connect tokens ˇf1(c) and ˇf2(c).2 Sum: given classifications A and B, the sum of A and B, denoted by A + B, is the classification with tok(A + B) = tok(A) × tok(B) = { a, b | a ∈ tok(A) and b ∈ tok(B)}, typ(A + B) = typ(A) typ(B) = { i, γ | i = 1 and γ ∈ typ(A) or i = 2 and γ ∈ typ(B)} and relation |=A+B defined by: a, b |=A+B 1, α if a |=A α a, b |=A+B 2, β if b |=B β Given infomorphisms f : A → C and g : B → C, the sum f + g : A + B → C is defined on types by ˆ(f + g)( 1, α ) = ˆf(α) and ˆ(f + g)( 2, β ) = ˆg(β), and on tokens by ˇ(f + g)(c) = ˇf(c), ˇg(c) .",
                "Theory: given a set Σ, a sequent of Σ is a pair Γ, Δ of subsets of Σ.",
                "A binary relation between subsets of Σ is called a consequence relation on Σ.",
                "A theory is a pair T = Σ, where is a consequence relation on Σ.",
                "A sequent Γ, Δ of Σ for which Γ Δ is called a constraint of the theory T. T is regular if it satisfies: 1.",
                "Identity: α α 2.",
                "Weakening: if Γ Δ, then Γ, Γ Δ, Δ 2 In fact, this is the definition of a binary channel.",
                "A channel can be defined with an arbitrary index set. 3.",
                "Global Cut: if Γ, Π0 Δ, Π1 for each partition Π0, Π1 of Π (i.e., Π0 ∪ Π1 = Π and Π0 ∩ Π1 = ∅), then Γ Δ for all α ∈ Σ and all Γ, Γ , Δ, Δ , Π ⊆ Σ.3 Theory generated by a classification: let A be a classification.",
                "A token a ∈ tok(A) satisfies a sequent Γ, Δ of typ(A) provided that if a is of every type in Γ then it is of some type in Δ.",
                "The theory generated by A, denoted by Th(A), is the theory typ(A), A where Γ A Δ if every token in A satisfies Γ, Δ .",
                "Local logic: is a tuple L = tok(L), typ(L), |=L , L , NL where: 1. tok(L), typ(L), |=L is a classification denoted by Cla(L), 2. typ(L), L is a regular theory denoted by Th(L), 3.",
                "NL is a subset of tok(L), called the normal tokens of L, which satisfy all constraints of Th(L).",
                "A local logic L is sound if every token in Cla(L) is normal, that is, NL = tok(L).",
                "L is complete if every sequent of typ(L) satisfied by every normal token is a constraint of Th(L).",
                "Local logic generated by a classification: given a classification A, the local logic generated by A, written Log(A), is the local logic on A (i.e., Cla(Log(A)) = A), with Th(Log(A)) = Th(A) and such that all its tokens are normal, i.e., NLog(A) = tok(A).",
                "Inverse image: given an infomorphism f : A → B and a local logic L on B, the inverse image of L under f, denoted f−1 [L], is the local logic on A such that Γ f−1[L] Δ if ˆf[Γ] L ˆf[Δ] and Nf−1[L] = ˇf[NL ] = {a ∈ tok(A) | a = ˇf(b) for some b ∈ NL }.",
                "Distributed logic: let C = {fi : Ai → C}i∈{1,2} be a channel and L a local logic on its core C, the distributed logic of C generated by L, written DLogC(L), is the inverse image of L under the sum f1 + f2.",
                "Refinement: let C = {fi : Ai → C}i∈{1,2} and C = {fi : Ai → C }i∈{1,2} be two channels with the same component classifications A1 and A2.",
                "A refinement infomorphism from C to C is an infomorphism r : C → C such that for each i ∈ {1, 2}, fi = r ◦fi (i.e., ˆfi = ˆr ◦ ˆfi and ˇfi = ˇfi ◦ˇr).",
                "Channel C is a refinement of C if there exists a refinement infomorphism r from C to C. B.",
                "CHANNEL THEORY THEOREMS Theorem B.1.",
                "The logic generated by a classification is sound and complete.",
                "Furthermore, given a classification A and a logic L on A, L is sound and complete if and only if L = Log(A).",
                "Theorem B.2.",
                "Let L be a logic on a classification B and f : A → B an infomorphism. 1.",
                "If L is complete then f−1 [L] is complete. 2.",
                "If L is sound and ˇf is surjective then f−1 [L] is sound. 3 All theories considered in this paper are regular.",
                "The Sixth Intl.",
                "Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1285"
            ],
            "original_annotated_samples": [],
            "translated_annotated_samples": [
                "En particular, proporcionamos un modelo formal que formaliza la alineación semántica situada como una secuencia de información: \"refinamiento de canales\" en el sentido de la teoría de la información del flujo de información de Barrawis y Seligmans [1].refinamiento de canal",
                "Conclusiones y trabajos adicionales en este documento Hemos expuesto un modelo formal de alineación semántica como una secuencia de información: \"refinamiento de canales\" que sean relativos a los estados particulares del entorno en el que dos agentes se comunican y alinean sus respectivas conceptualizaciones de estos estados.refinamiento de canal"
            ],
            "translated_text": "",
            "candidates": [],
            "error": []
        }
    }
}